# CUDA Backend - NVIDIA CUDA/PTX Code Generation
#
# Compiles MIR to CUDA PTX assembly code.
# PTX can be JIT-compiled by NVIDIA driver or assembled offline.

use compiler.mir_data.*
use compiler.backend.backend_api.*
use compiler.backend.codegen_types.{Codegen, CodegenOutput, CodegenOutputKind}
use compiler.backend.gpu_codegen_types.{GpuCodegen, GpuBarrierScope}
use compiler.backend.cuda_type_mapper.CudaTypeMapper
use compiler.backend.cuda.ptx_builder.PtxBuilder
use compiler.hir_types.MemorySpace

# ============================================================================
# CUDA Backend
# ============================================================================

class CudaBackend:
    """
    CUDA backend for GPU code generation.

    Compiles MIR modules to PTX assembly code for NVIDIA GPUs.
    PTX is a virtual instruction set that gets JIT-compiled to
    native GPU machine code (SASS) by the NVIDIA driver.

    Features:
    - Full MIR instruction support
    - Kernel function generation
    - Device function generation
    - Shared memory allocation
    - Atomic operations
    - Synchronization barriers

    Usage:
        val backend = CudaBackend__create((8, 6))  # SM 8.6
        val result = backend.compile(mir_module)
    """

    type_mapper: CudaTypeMapper
    compute_capability: (i64, i64)
    options: CompileOptions

    static fn create(compute_cap: (i64, i64)) -> CudaBackend:
        """Create CUDA backend for specific compute capability."""
        CudaBackend(
            type_mapper: CudaTypeMapper__create_sm(compute_cap.0, compute_cap.1),
            compute_capability: compute_cap,
            options: CompileOptions__default_options()
                .with_target(CodegenTarget.CudaPtx(compute_cap))
        )

    static fn create_with_options(compute_cap: (i64, i64), opts: CompileOptions) -> CudaBackend:
        """Create CUDA backend with custom options."""
        CudaBackend(
            type_mapper: CudaTypeMapper__create_sm(compute_cap.0, compute_cap.1),
            compute_capability: compute_cap,
            options: opts
        )

    static fn for_target(target: CodegenTarget) -> Result<CudaBackend, CompileError>:
        """Create CUDA backend from target."""
        match target.cuda_compute_capability():
            case Some(cap):
                Ok(CudaBackend__create(cap))
            case nil:
                Err(CompileError__backend_error(
                    BackendKind.Cuda,
                    "Target is not a CUDA target"
                ))

    fn compile(module: MirModule) -> Result<CudaCompiledModule, CompileError>:
        """Compile MIR module to PTX."""
        val builder = PtxBuilder__create(self.compute_capability)

        # Emit module header
        builder.emit_module_header(module.name)

        # Collect kernel functions
        var kernels: [CudaKernel] = []
        var device_functions: [CudaDeviceFunction] = []

        for (symbol, func) in module.functions:
            val result = self.compile_function(builder, func)
            match result:
                case Ok(kernel):
                    if kernel.is_kernel:
                        kernels = kernels.push(CudaKernel(
                            name: kernel.name,
                            ptx: kernel.ptx,
                            param_count: func.signature.params.len()
                        ))
                    else:
                        device_functions = device_functions.push(CudaDeviceFunction(
                            name: kernel.name,
                            ptx: kernel.ptx
                        ))
                case Err(e):
                    return Err(e)

        val ptx_code = builder.build()

        Ok(CudaCompiledModule(
            name: module.name,
            ptx: ptx_code,
            kernels: kernels,
            device_functions: device_functions,
            compute_capability: self.compute_capability
        ))

    fn compile_function(builder: PtxBuilder, func: MirFunction) -> Result<CompiledFunction, CompileError>:
        """Compile a single function."""
        val is_kernel = self.is_kernel_function(func)

        if is_kernel:
            builder.emit_kernel_entry(func.name, func.locals)
        else:
            builder.emit_device_function(
                func.name,
                func.locals,
                func.signature.return_type
            )

        # Emit register declarations for locals
        for local in func.locals:
            match local.kind:
                case Arg(_):
                    ()  # Arguments are parameters
                case Var | Temp | Return:
                    self.emit_local_decl(builder, local)

        # Compile each basic block
        for block in func.blocks:
            val label = "BB{block.id.id}"
            builder.emit_label(label)

            for inst in block.instructions:
                val result = self.compile_instruction(builder, inst)
                if result.is_err():
                    return Err(result.unwrap_err())

            self.compile_terminator(builder, block.terminator)

        if is_kernel:
            builder.emit_kernel_end()
        else:
            builder.emit_function_end()

        Ok(CompiledFunction(
            name: func.name,
            ptx: "",  # PTX is accumulated in builder
            is_kernel: is_kernel
        ))

    fn is_kernel_function(func: MirFunction) -> bool:
        """Check if function should be compiled as a kernel."""
        # For now, functions with GPU instructions are kernels
        for block in func.blocks:
            for inst in block.instructions:
                match inst.kind:
                    case GpuGlobalId(_, _) | GpuLocalId(_, _) | GpuBlockId(_, _) |
                         GpuBarrier(_) | GpuSharedAlloc(_, _, _) | GpuAtomicOp(_, _, _, _):
                        return true
                    case _:
                        ()
        false

    me emit_local_decl(builder: PtxBuilder, local: MirLocal):
        """Emit local variable declaration."""
        val reg = "%r{local.id.id}"
        match local.type_.kind:
            case I64: builder.emit_reg_decl(reg, PrimitiveType.I64)
            case I32: builder.emit_reg_decl(reg, PrimitiveType.I32)
            case I16: builder.emit_reg_decl(reg, PrimitiveType.I16)
            case I8: builder.emit_reg_decl(reg, PrimitiveType.I8)
            case U64: builder.emit_reg_decl(reg, PrimitiveType.U64)
            case U32: builder.emit_reg_decl(reg, PrimitiveType.U32)
            case U16: builder.emit_reg_decl(reg, PrimitiveType.U16)
            case U8: builder.emit_reg_decl(reg, PrimitiveType.U8)
            case F64: builder.emit_reg_decl(reg, PrimitiveType.F64)
            case F32: builder.emit_reg_decl(reg, PrimitiveType.F32)
            case Bool: builder.emit_pred_decl(reg)
            case _: ()

    fn compile_instruction(builder: PtxBuilder, inst: MirInst) -> Result<(), CompileError>:
        """Compile a single MIR instruction to PTX."""
        match inst.kind:
            case Const(dest, value, ty):
                self.compile_const(builder, dest, value, ty)

            case Copy(dest, src):
                self.compile_copy(builder, dest, src)

            case Move(dest, src):
                self.compile_copy(builder, dest, src)  # Same as copy in PTX

            case BinOp(dest, op, left, right):
                self.compile_binop(builder, dest, op, left, right)

            case UnaryOp(dest, op, operand):
                self.compile_unaryop(builder, dest, op, operand)

            case Load(dest, ptr):
                self.compile_load(builder, dest, ptr)

            case Store(ptr, value):
                self.compile_store(builder, ptr, value)

            # GPU-specific instructions
            case GpuGlobalId(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_compute_global_id(reg, dim)

            case GpuLocalId(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_thread_id(reg, dim)

            case GpuBlockId(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_block_id(reg, dim)

            case GpuBlockDim(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_block_dim(reg, dim)

            case GpuGridDim(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_grid_dim(reg, dim)

            case GpuBarrier(scope):
                match scope:
                    case Workgroup:
                        builder.emit_barrier()
                    case Device:
                        builder.emit_membar_gl()
                    case Subgroup:
                        # Warp-level sync (implicit in CUDA, explicit only needed for independent thread scheduling)
                        builder.emit_comment("warp sync")

            case GpuMemFence(scope):
                match scope:
                    case Workgroup:
                        builder.emit_membar_cta()
                    case Device:
                        builder.emit_membar_gl()
                    case All:
                        builder.emit_membar_sys()
                    case _:
                        builder.emit_membar_cta()

            case GpuSharedAlloc(dest, ty, size):
                val name = "shared_{dest.id}"
                val prim_ty = self.mir_type_to_primitive(ty)
                builder.emit_shared_memory(name, prim_ty, size)

            case GpuAtomicOp(dest, op, ptr, value):
                self.compile_atomic(builder, dest, op, ptr, value)

            # --- Function Calls ---

            case Call(dest, func, args):
                self.compile_call(builder, dest, func, args)

            case CallIndirect(dest, ptr, args, sig):
                # Indirect calls are not supported in PTX; emit comment
                builder.emit_comment("Indirect call not supported on GPU")

            # --- Aggregate Operations ---

            case Aggregate(dest, kind, operands):
                self.compile_aggregate(builder, dest, kind, operands)

            case GetField(dest, base, field):
                self.compile_get_field(builder, dest, base, field)

            case SetField(base, field, value):
                self.compile_set_field(builder, base, field, value)

            # --- Cast Operations ---

            case Cast(dest, operand, target):
                self.compile_cast(builder, dest, operand, target)

            case Bitcast(dest, operand, target):
                # Bitcast is a reinterpretation, use mov in PTX
                val dest_reg = "%r{dest.id}"
                val src_reg = self.operand_to_reg(operand)
                builder.emit_line("mov.b64 {dest_reg}, {src_reg};")

            # --- Memory Allocation ---

            case Alloc(dest, type_):
                self.compile_alloc(builder, dest, type_)

            case GetElementPtr(dest, base, indices):
                self.compile_gep(builder, dest, base, indices)

            # --- Checked Arithmetic ---

            case CheckedBinOp(dest, op, left, right):
                # Checked operations compile same as unchecked on GPU
                self.compile_binop(builder, dest, op, left, right)

            # --- Intrinsics ---

            case Intrinsic(dest, name, args):
                self.compile_intrinsic(builder, dest, name, args)

            # --- Debug ---

            case DebugValue(local, name):
                builder.emit_comment("debug: {name} = %r{local.id}")

            case Nop:
                ()

            case _:
                return Err(CompileError__backend_error(
                    BackendKind.Cuda,
                    "Unsupported instruction: {inst.kind}"
                ))

        Ok(())

    fn compile_const(builder: PtxBuilder, dest: LocalId, value: MirConstValue, ty: MirType):
        """Compile constant load."""
        val reg = "%r{dest.id}"
        match value:
            case Int(v):
                val prim_ty = self.mir_type_to_primitive(ty)
                builder.emit_const_int(reg, prim_ty, v)
            case Float(v):
                val prim_ty = self.mir_type_to_primitive(ty)
                builder.emit_const_float(reg, prim_ty, v)
            case Bool(v):
                val val_int = if v: 1 else: 0
                builder.emit_const_int(reg, PrimitiveType.Bool, val_int)
            case _:
                builder.emit_comment("Unsupported constant type")

    fn compile_copy(builder: PtxBuilder, dest: LocalId, src: LocalId):
        """Compile copy/move."""
        val dest_reg = "%r{dest.id}"
        val src_reg = "%r{src.id}"
        # Use mov instruction
        builder.emit_line("mov.u64 {dest_reg}, {src_reg};")

    fn compile_binop(builder: PtxBuilder, dest: LocalId, op: MirBinOp, left: MirOperand, right: MirOperand):
        """Compile binary operation."""
        val dest_reg = "%r{dest.id}"
        val left_reg = self.operand_to_reg(left)
        val right_reg = self.operand_to_reg(right)

        # Determine type from left operand
        val ty = PrimitiveType.I64  # Default, should be inferred

        match op:
            case Add:
                builder.emit_add(dest_reg, ty, left_reg, right_reg)
            case Sub:
                builder.emit_sub(dest_reg, ty, left_reg, right_reg)
            case Mul:
                builder.emit_mul(dest_reg, ty, left_reg, right_reg)
            case Div:
                builder.emit_div(dest_reg, ty, left_reg, right_reg)
            case Eq:
                builder.emit_compare(dest_reg, "eq", ty, left_reg, right_reg)
            case Ne:
                builder.emit_compare(dest_reg, "ne", ty, left_reg, right_reg)
            case Lt:
                builder.emit_compare(dest_reg, "lt", ty, left_reg, right_reg)
            case Le:
                builder.emit_compare(dest_reg, "le", ty, left_reg, right_reg)
            case Gt:
                builder.emit_compare(dest_reg, "gt", ty, left_reg, right_reg)
            case Ge:
                builder.emit_compare(dest_reg, "ge", ty, left_reg, right_reg)
            case _:
                builder.emit_comment("Unsupported binop: {op}")

    fn compile_unaryop(builder: PtxBuilder, dest: LocalId, op: MirUnaryOp, operand: MirOperand):
        """Compile unary operation."""
        val dest_reg = "%r{dest.id}"
        val src_reg = self.operand_to_reg(operand)
        val ty = PrimitiveType.I64

        match op:
            case Neg:
                builder.emit_neg(dest_reg, ty, src_reg)
            case Not:
                builder.emit_line("not.pred {dest_reg}, {src_reg};")
            case _:
                builder.emit_comment("Unsupported unaryop: {op}")

    fn compile_load(builder: PtxBuilder, dest: LocalId, ptr: MirOperand):
        """Compile load instruction."""
        val dest_reg = "%r{dest.id}"
        val addr_reg = self.operand_to_reg(ptr)
        builder.emit_load(dest_reg, PrimitiveType.I64, addr_reg, MemorySpace.Global)

    fn compile_store(builder: PtxBuilder, ptr: MirOperand, value: MirOperand):
        """Compile store instruction."""
        val addr_reg = self.operand_to_reg(ptr)
        val value_reg = self.operand_to_reg(value)
        builder.emit_store(PrimitiveType.I64, addr_reg, value_reg, MemorySpace.Global)

    fn compile_atomic(builder: PtxBuilder, dest: LocalId, op: GpuAtomicOpKind, ptr: MirOperand, value: MirOperand):
        """Compile atomic operation."""
        val dest_reg = "%r{dest.id}"
        val addr_reg = self.operand_to_reg(ptr)
        val value_reg = self.operand_to_reg(value)
        val ty = PrimitiveType.I64

        match op:
            case Add:
                builder.emit_atomic_add(dest_reg, ty, addr_reg, value_reg, MemorySpace.Global)
            case Min:
                builder.emit_atomic_min(dest_reg, ty, addr_reg, value_reg, MemorySpace.Global)
            case Max:
                builder.emit_atomic_max(dest_reg, ty, addr_reg, value_reg, MemorySpace.Global)
            case Exchange:
                builder.emit_line("atom.global.exch.u64 {dest_reg}, [{addr_reg}], {value_reg};")
            case _:
                builder.emit_comment("Unsupported atomic op: {op}")

    fn compile_call(builder: PtxBuilder, dest: LocalId?, func: MirOperand, args: [MirOperand]):
        """Compile device function call."""
        val arg_regs = args.map(\a: self.operand_to_reg(a))

        # Get function name from operand
        val func_name = self.operand_to_reg(func)

        if dest.?:
            val dest_reg = "%r{dest.unwrap().id}"
            builder.emit_call(dest_reg, PrimitiveType.I64, func_name, arg_regs)
        else:
            builder.emit_call_void(func_name, arg_regs)

    fn compile_aggregate(builder: PtxBuilder, dest: LocalId, kind: AggregateKind, operands: [MirOperand]):
        """Compile aggregate construction via local memory."""
        val dest_reg = "%r{dest.id}"
        # Store each field to local memory at offset
        builder.emit_comment("aggregate construct -> {dest_reg}")
        var offset = 0
        for i, operand in operands.enumerate():
            val src_reg = self.operand_to_reg(operand)
            # Store field value at base + offset in local memory
            builder.emit_line("st.local.u64 [{dest_reg}+{offset}], {src_reg};")
            offset = offset + 8

    fn compile_get_field(builder: PtxBuilder, dest: LocalId, base: MirOperand, field: i64):
        """Compile field access via offset load."""
        val dest_reg = "%r{dest.id}"
        val base_reg = self.operand_to_reg(base)
        val offset = field * 8
        builder.emit_line("ld.local.u64 {dest_reg}, [{base_reg}+{offset}];")

    fn compile_set_field(builder: PtxBuilder, base: MirOperand, field: i64, value: MirOperand):
        """Compile field write via offset store."""
        val base_reg = self.operand_to_reg(base)
        val value_reg = self.operand_to_reg(value)
        val offset = field * 8
        builder.emit_line("st.local.u64 [{base_reg}+{offset}], {value_reg};")

    fn compile_cast(builder: PtxBuilder, dest: LocalId, operand: MirOperand, target: MirType):
        """Compile type conversion using PTX cvt instruction."""
        val dest_reg = "%r{dest.id}"
        val src_reg = self.operand_to_reg(operand)
        val dest_ty = self.mir_type_to_primitive(target)
        # Infer source type - default to I64
        val src_ty = PrimitiveType.I64
        builder.emit_convert(dest_reg, dest_ty, src_reg, src_ty)

    fn compile_alloc(builder: PtxBuilder, dest: LocalId, type_: MirType):
        """Compile local memory allocation."""
        val dest_reg = "%r{dest.id}"
        val prim_ty = self.mir_type_to_primitive(type_)
        val size = type_.size_bytes()
        val alloc_name = "local_{dest.id}"
        builder.emit_local_alloc(alloc_name, prim_ty, 1)
        # Load address of local allocation into dest register
        builder.emit_line("mov.u64 {dest_reg}, {alloc_name};")

    fn compile_gep(builder: PtxBuilder, dest: LocalId, base: MirOperand, indices: [MirOperand]):
        """Compile GetElementPtr - pointer arithmetic with indices."""
        val dest_reg = "%r{dest.id}"
        val base_reg = self.operand_to_reg(base)
        # For simple single-index GEP, compute base + index * element_size
        if indices.length > 0:
            val idx_reg = self.operand_to_reg(indices.get(0))
            val tmp = builder.alloc_reg(PrimitiveType.U64)
            builder.emit_reg_decl(tmp, PrimitiveType.U64)
            # Assume 8-byte elements (i64/ptr)
            builder.emit_line("mul.lo.u64 {tmp}, {idx_reg}, 8;")
            builder.emit_line("add.u64 {dest_reg}, {base_reg}, {tmp};")
        else:
            builder.emit_line("mov.u64 {dest_reg}, {base_reg};")

    fn compile_intrinsic(builder: PtxBuilder, dest: LocalId?, name: text, args: [MirOperand]):
        """Compile math and GPU intrinsics."""
        val ty = PrimitiveType.F32  # Default to F32 for math intrinsics

        match name:
            case "sin":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_sin(d, ty, src)
            case "cos":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_cos(d, ty, src)
            case "sqrt":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_sqrt(d, ty, src)
            case "abs":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_abs(d, ty, src)
            case "min":
                val d = "%r{dest.unwrap().id}"
                val a = self.operand_to_reg(args.get(0))
                val b = self.operand_to_reg(args.get(1))
                builder.emit_intrinsic_min(d, ty, a, b)
            case "max":
                val d = "%r{dest.unwrap().id}"
                val a = self.operand_to_reg(args.get(0))
                val b = self.operand_to_reg(args.get(1))
                builder.emit_intrinsic_max(d, ty, a, b)
            case "fma":
                val d = "%r{dest.unwrap().id}"
                val a = self.operand_to_reg(args.get(0))
                val b = self.operand_to_reg(args.get(1))
                val c = self.operand_to_reg(args.get(2))
                builder.emit_intrinsic_fma(d, ty, a, b, c)
            case "exp2":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_ex2(d, ty, src)
            case "log2":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_lg2(d, ty, src)
            case "rcp":
                val d = "%r{dest.unwrap().id}"
                val src = self.operand_to_reg(args.get(0))
                builder.emit_intrinsic_rcp(d, ty, src)
            case _:
                builder.emit_comment("Unknown intrinsic: {name}")

    fn compile_terminator(builder: PtxBuilder, term: MirTerminator):
        """Compile block terminator."""
        match term:
            case Return(_):
                builder.emit_ret()
            case Goto(target):
                builder.emit_branch("BB{target.id}")
            case Branch(cond, true_target, false_target):
                val cond_reg = self.operand_to_reg(cond)
                builder.emit_branch_if(cond_reg, "BB{true_target.id}")
                builder.emit_branch("BB{false_target.id}")
            case Unreachable:
                builder.emit_line("trap;")
            case _:
                builder.emit_comment("Unsupported terminator")

    fn operand_to_reg(operand: MirOperand) -> text:
        """Convert operand to register name."""
        match operand.kind:
            case Copy(local): "%r{local.id}"
            case Move(local): "%r{local.id}"
            case Const(value, ty):
                # Constants should be loaded into temporaries
                "%const"

    fn mir_type_to_primitive(ty: MirType) -> PrimitiveType:
        """Convert MIR type to PrimitiveType."""
        match ty.kind:
            case I64: PrimitiveType.I64
            case I32: PrimitiveType.I32
            case I16: PrimitiveType.I16
            case I8: PrimitiveType.I8
            case U64: PrimitiveType.U64
            case U32: PrimitiveType.U32
            case U16: PrimitiveType.U16
            case U8: PrimitiveType.U8
            case F64: PrimitiveType.F64
            case F32: PrimitiveType.F32
            case Bool: PrimitiveType.Bool
            case _: PrimitiveType.I64

# ============================================================================
# CUDA Compilation Results
# ============================================================================

struct CudaCompiledModule:
    """Result of CUDA compilation."""
    name: text
    ptx: text
    kernels: [CudaKernel]
    device_functions: [CudaDeviceFunction]
    compute_capability: (i64, i64)

struct CudaKernel:
    """Compiled CUDA kernel."""
    name: text
    ptx: text
    param_count: i64

struct CudaDeviceFunction:
    """Compiled CUDA device function."""
    name: text
    ptx: text

struct CompiledFunction:
    """Internal compiled function result."""
    name: text
    ptx: text
    is_kernel: bool

# ============================================================================
# Codegen Trait Implementation
# ============================================================================

impl Codegen for CudaBackend:
    fn backend_kind() -> BackendKind: BackendKind.Cuda
    fn backend_name() -> text: "cuda"
    fn supports_target(target: CodegenTarget) -> bool: true
    fn output_kind() -> CodegenOutputKind: CodegenOutputKind.GpuCode

    fn compile_module(module: MirModule) -> Result<CodegenOutput, CompileError>:
        val result = self.compile(module)
        match result:
            case Ok(cuda_module):
                Ok(CodegenOutput.gpu(module.name, cuda_module.ptx))
            case Err(e):
                Err(e)

# ============================================================================
# GpuCodegen Trait Implementation
# ============================================================================

impl GpuCodegen for CudaBackend:
    fn compile_kernel(func_name: text, body: MirBody) -> Result<text, CompileError>:
        pass_todo

    fn compile_device_function(func_name: text, body: MirBody) -> Result<text, CompileError>:
        pass_todo

    fn compile_gpu_instruction(inst: MirInst) -> text:
        "// cuda instruction"

    fn compile_barrier(scope: GpuBarrierScope) -> text:
        match scope:
            case Workgroup: "bar.sync 0;"
            case Device: "membar.gl;"
            case Subgroup: "// warp sync"

    fn compile_atomic(op: GpuAtomicOpKind, dest: LocalId, ptr: MirOperand, value: MirOperand) -> text:
        "// atomic op"

    fn gpu_memory_model() -> text:
        "PTX unified memory"

    fn is_kernel_function(body: MirBody) -> bool:
        false

# ============================================================================
# Export
# ============================================================================

export CudaBackend
export CudaCompiledModule, CudaKernel, CudaDeviceFunction
