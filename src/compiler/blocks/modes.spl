# Block Lexer Modes and Syntax Features
#
# Defines how block content should be tokenized and parsed.

# ============================================================================
# Lexer Modes
# ============================================================================

"""Lexer mode determines tokenization behavior inside block."""
enum LexerMode:
    Normal          # Standard Simple tokenization
    Math            # ^ for power, ' for transpose, implicit mul
    Raw             # Capture as raw text (no tokenization)
    Custom(config: LexerConfig)  # Custom configuration

impl LexerMode:
    fn is_normal() -> bool:
        match self:
            case Normal: true
            case _: false

    fn is_math() -> bool:
        match self:
            case Math: true
            case _: false

    fn is_raw() -> bool:
        match self:
            case Raw: true
            case _: false

    fn to_string() -> text:
        match self:
            case Normal: "normal"
            case Math: "math"
            case Raw: "raw"
            case Custom(_): "custom"

# ============================================================================
# Lexer Configuration
# ============================================================================

struct LexerConfig:
    """Custom lexer configuration for blocks.

    Allows fine-grained control over tokenization behavior.
    """

    # Operator overrides
    caret_is_power: bool        # ^ as power operator (instead of error)
    quote_is_transpose: bool    # ' as postfix transpose
    implicit_mul: bool          # 2x -> 2*x, (a)(b) -> (a)*(b)

    # String handling
    interpolation: bool         # Allow ${...} interpolation
    raw_strings: bool           # Disable escape sequences

    # Delimiter handling
    preserve_braces: bool       # Include {} in payload
    preserve_newlines: bool     # Include newlines in payload

    # Block-specific
    allow_nested_blocks: bool   # Allow block{} inside this block

impl LexerConfig:
    static fn default() -> LexerConfig:
        """Default configuration: standard Simple tokenization."""
        LexerConfig(
            caret_is_power: false,
            quote_is_transpose: false,
            implicit_mul: false,
            interpolation: true,
            raw_strings: false,
            preserve_braces: false,
            preserve_newlines: true,
            allow_nested_blocks: true
        )

    static fn math() -> LexerConfig:
        """Math mode configuration."""
        LexerConfig(
            caret_is_power: true,
            quote_is_transpose: true,
            implicit_mul: true,
            interpolation: true,
            raw_strings: false,
            preserve_braces: false,
            preserve_newlines: true,
            allow_nested_blocks: true
        )

    static fn raw() -> LexerConfig:
        """Raw mode configuration: no tokenization."""
        LexerConfig(
            caret_is_power: false,
            quote_is_transpose: false,
            implicit_mul: false,
            interpolation: false,
            raw_strings: true,
            preserve_braces: false,
            preserve_newlines: true,
            allow_nested_blocks: false
        )

# ============================================================================
# Syntax Features
# ============================================================================

struct SyntaxFeatures:
    """Syntax features enabled for a block.

    Controls which special syntax elements are available.
    """

    # Math-like features
    power_caret: bool           # ^ is power (not XOR/error)
    transpose_quote: bool       # ' is transpose (postfix)
    implicit_multiplication: bool  # 2x means 2*x

    # Tensor features
    broadcast_ops: bool         # .+ .- .* ./ .^
    matrix_mul: bool            # @ operator

    # ML features
    auto_backward: bool         # Call .backward() on result
    disable_grad: bool          # Disable gradient tracking

    # Pipeline features
    pipe_forward: bool          # |> operator
    composition: bool           # >> << operators

    # DSL features
    custom_keywords: [text]     # Additional keywords to recognize

impl SyntaxFeatures:
    static fn default() -> SyntaxFeatures:
        """Default: no special features."""
        SyntaxFeatures(
            power_caret: false,
            transpose_quote: false,
            implicit_multiplication: false,
            broadcast_ops: false,
            matrix_mul: false,
            auto_backward: false,
            disable_grad: false,
            pipe_forward: false,
            composition: false,
            custom_keywords: []
        )

    static fn math() -> SyntaxFeatures:
        """Math block features: power, transpose, implicit mul, tensors."""
        SyntaxFeatures(
            power_caret: true,
            transpose_quote: true,
            implicit_multiplication: true,
            broadcast_ops: true,
            matrix_mul: true,
            auto_backward: false,
            disable_grad: false,
            pipe_forward: false,
            composition: false,
            custom_keywords: []
        )

    static fn loss() -> SyntaxFeatures:
        """Loss block: math features + auto-backward."""
        var f = SyntaxFeatures.math()
        f.auto_backward = true
        f

    static fn nograd() -> SyntaxFeatures:
        """Nograd block: math features + disabled gradients."""
        var f = SyntaxFeatures.math()
        f.disable_grad = true
        f

    static fn pipeline() -> SyntaxFeatures:
        """Pipeline features for functional composition."""
        SyntaxFeatures(
            power_caret: false,
            transpose_quote: false,
            implicit_multiplication: false,
            broadcast_ops: false,
            matrix_mul: false,
            auto_backward: false,
            disable_grad: false,
            pipe_forward: true,
            composition: true,
            custom_keywords: []
        )

    me with_power_caret() -> SyntaxFeatures:
        self.power_caret = true
        self

    me with_transpose() -> SyntaxFeatures:
        self.transpose_quote = true
        self

    me with_implicit_mul() -> SyntaxFeatures:
        self.implicit_multiplication = true
        self

    me with_broadcast() -> SyntaxFeatures:
        self.broadcast_ops = true
        self

    me with_matrix_mul() -> SyntaxFeatures:
        self.matrix_mul = true
        self

    me with_auto_backward() -> SyntaxFeatures:
        self.auto_backward = true
        self

    me with_disable_grad() -> SyntaxFeatures:
        self.disable_grad = true
        self

    me with_pipeline() -> SyntaxFeatures:
        self.pipe_forward = true
        self.composition = true
        self

    me with_keywords(keywords: [text]) -> SyntaxFeatures:
        self.custom_keywords = keywords
        self

# ============================================================================
# Span (Source Location)
# ============================================================================

struct Span:
    """Source location for error reporting.

    Defined here (in modes.spl) to avoid circular imports.
    The lexer module defines a compatible Span type.
    """
    start: i64
    end: i64
    line: i64
    col: i64

impl Span:
    static fn new(start: i64, end: i64, line: i64, col: i64) -> Span:
        Span(start: start, end: end, line: line, col: col)

    static fn empty() -> Span:
        Span(start: 0, end: 0, line: 0, col: 0)

    fn len() -> i64:
        self.end - self.start

    fn merge(other: Span) -> Span:
        val new_start = if self.start < other.start: self.start else: other.start
        val new_end = if self.end > other.end: self.end else: other.end
        val new_line = if self.line < other.line: self.line else: other.line
        val new_col = if self.line <= other.line: self.col else: other.col
        Span.new(new_start, new_end, new_line, new_col)

# ============================================================================
# Exports
# ============================================================================

# ============================================================================
# Pre-Lex Info (Tier 1+2 scanning data)
# ============================================================================

struct TextSpan:
    """Byte range within a payload."""
    start: i64
    end: i64

impl TextSpan:
    static fn new(start: i64, end: i64) -> TextSpan:
        TextSpan(start: start, end: end)

    fn len() -> i64:
        self.end - self.start

    fn contains(pos: i64) -> bool:
        pos >= self.start and pos < self.end

struct PreLexInfo:
    """Pre-lexing info collected during Tier 1/2 brace tracking.

    The main lexer collects this while scanning block payloads,
    so block handlers don't need to re-discover strings/comments/escapes.
    """
    string_spans: [TextSpan]       # "..." and '...' regions
    comment_spans: [TextSpan]      # Line/block comment regions
    escape_positions: [i64]        # Backslash positions
    brace_pairs: [(i64, i64)]      # Matched { } positions with depth

impl PreLexInfo:
    static fn empty() -> PreLexInfo:
        PreLexInfo(
            string_spans: [],
            comment_spans: [],
            escape_positions: [],
            brace_pairs: []
        )

    fn is_in_string(pos: i64) -> bool:
        """Check if a position is inside a string literal."""
        for span in self.string_spans:
            if span.contains(pos):
                return true
        false

    fn is_in_comment(pos: i64) -> bool:
        """Check if a position is inside a comment."""
        for span in self.comment_spans:
            if span.contains(pos):
                return true
        false

    fn is_escaped(pos: i64) -> bool:
        """Check if a position is escaped (preceded by backslash)."""
        if pos > 0:
            return (pos - 1) in self.escape_positions
        false

    fn is_protected(pos: i64) -> bool:
        """Check if a position is inside a string, comment, or escaped."""
        self.is_in_string(pos) or self.is_in_comment(pos) or self.is_escaped(pos)

# ============================================================================
# Block Skip Policy
# ============================================================================

enum BlockSkipPolicy:
    """Controls whether TreeSitter can skip a block in fast mode."""
    Skippable          # TreeSitter can skip in fast mode
    OutlineRequired    # Must run treesitter_outline even in fast mode
    AlwaysFull         # Always fully parse

impl BlockSkipPolicy:
    fn to_string() -> text:
        match self:
            case Skippable: "skippable"
            case OutlineRequired: "outline_required"
            case AlwaysFull: "always_full"

# ============================================================================
# Block Outline Info
# ============================================================================

struct BlockOutlineInfo:
    """Structural summary extracted by treesitter_outline.

    Provides enough info for IDE features without full parsing.
    """
    kind: text
    identifiers: [text]       # Variables/symbols referenced
    external_refs: [text]     # Table names, commands, file paths
    structure_kind: text?     # "query", "expression", "command"
    is_opaque: bool

impl BlockOutlineInfo:
    static fn opaque(kind: text) -> BlockOutlineInfo:
        """Create an opaque outline (no structural info)."""
        BlockOutlineInfo(
            kind: kind,
            identifiers: [],
            external_refs: [],
            structure_kind: nil,
            is_opaque: true
        )

# ============================================================================
# Block Tokens (sub-lexer output)
# ============================================================================

enum BlockTokenKind:
    """Token kinds produced by block sub-lexers."""
    Keyword
    Identifier
    Number
    StringLit
    Operator
    Punctuation
    Comment
    Whitespace
    Error

struct BlockToken:
    """Token produced by a block's sub-lexer."""
    kind: BlockTokenKind
    span: TextSpan
    value: text

impl BlockToken:
    static fn new(kind: BlockTokenKind, start: i64, end: i64, value: text) -> BlockToken:
        BlockToken(kind: kind, span: TextSpan(start: start, end: end), value: value)

# ============================================================================
# Exports
# ============================================================================

export LexerMode, LexerConfig, SyntaxFeatures, Span
export TextSpan, PreLexInfo
export BlockSkipPolicy, BlockOutlineInfo
export BlockToken, BlockTokenKind
