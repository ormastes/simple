# Core Simple — Lexer (Tokenizer)
#
# Shared core library: converts source text into a stream of tokens.
# Handles indentation tracking, literals, identifiers, keywords,
# operators, delimiters, and comments.
#
# Design: All state in module-level vars. Functions modify state.
# Works when compiled to C; NOT in the interpreter (closure bug).
#
# Split into modules:
#   lexer_scanners.spl — all scanning functions (number, string, ident,
#                        whitespace, indentation, main token scanner)

val _NL = "\n"
use compiler.core.tokens.{keyword_lookup}
use compiler.core.tokens.{TOK_INT_LIT, TOK_FLOAT_LIT, TOK_STRING_LIT}
use compiler.core.tokens.{TOK_BOOL_LIT, TOK_NIL_LIT, TOK_IDENT}
use compiler.core.tokens.{TOK_KW_FN, TOK_KW_VAL, TOK_KW_VAR}
use compiler.core.tokens.{TOK_KW_TRUE, TOK_KW_FALSE, TOK_KW_NIL}
use compiler.core.tokens.{TOK_PLUS, TOK_MINUS, TOK_STAR, TOK_SLASH, TOK_PERCENT}
use compiler.core.tokens.{TOK_EQ, TOK_NEQ, TOK_LT, TOK_GT, TOK_LEQ, TOK_GEQ}
use compiler.core.tokens.{TOK_SHL, TOK_SHR}
use compiler.core.tokens.{TOK_ASSIGN, TOK_PLUS_ASSIGN, TOK_MINUS_ASSIGN, TOK_WALRUS}
use compiler.core.tokens.{TOK_STAR_ASSIGN, TOK_SLASH_ASSIGN}
use compiler.core.tokens.{TOK_AND, TOK_OR, TOK_NOT}
use compiler.core.tokens.{TOK_LPAREN, TOK_RPAREN, TOK_LBRACKET, TOK_RBRACKET}
use compiler.core.tokens.{TOK_LBRACE, TOK_RBRACE}
use compiler.core.tokens.{TOK_COLON, TOK_COMMA, TOK_DOT, TOK_DOTDOT}
use compiler.core.tokens.{TOK_ARROW, TOK_FAT_ARROW, TOK_PIPE}
use compiler.core.tokens.{TOK_QUESTION, TOK_QUESTION_DOT, TOK_DOUBLE_QUESTION, TOK_DOT_QUESTION}
use compiler.core.tokens.{TOK_NEWLINE, TOK_INDENT, TOK_DEDENT, TOK_EOF, TOK_ERROR}
use compiler.core.tokens.{TOK_HASH_LBRACKET, TOK_DOTDOTDOT, TOK_DOTDOT_EQ}
use compiler.core.tokens.{TOK_SEMICOLON, TOK_AT, TOK_UNDERSCORE}
use compiler.core.tokens.{TOK_PIPE_FORWARD, TOK_DOUBLE_STAR}
use compiler.core.tokens.{TOK_SUFFIXED_INT, TOK_SUFFIXED_FLOAT}
use compiler.core.tokens.{TOK_LABEL, TOK_BACKTICK_IDENT}
use compiler.core.tokens.{TOK_AMPERSAND, TOK_AMP_COLON}
use compiler.core.lexer_chars.{is_digit, is_hex_digit, is_alpha, is_ident_char, is_space}
use compiler.core.lexer_types.{Token, Span, lex_span_new, lex_token_new, lex_token_eof}

# Import from split scanner module
use compiler.core.lexer_scanners.{lex_scan_number, lex_scan_string, lex_scan_ident}
use compiler.core.lexer_scanners.{lex_skip_spaces, lex_handle_indentation, lex_scan_token}

# Light wrapper struct to satisfy callers expecting a Lexer value.
struct Lexer:
    dummy: i64

# ===== Lexer State =====

var lex_source: text = ""
var lex_pos: i64 = 0
var lex_line: i64 = 1
var lex_col: i64 = 1
var lex_indent_stack: [i64] = [0]
var lex_pending_dedents: i64 = 0
var lex_at_line_start: bool = true
var lex_paren_depth: i64 = 0
var lex_round_paren_depth: i64 = 0

# Current token properties
var lex_cur_kind: i64 = 0
var lex_cur_text: text = ""
var lex_cur_line: i64 = 0
var lex_cur_col: i64 = 0
var lex_cur_suffix: text = ""

# ===== Public wrapper API =====

fn lexer_new(source: text) -> Lexer:
    # Initialize global lexer state for the given source and return a thin handle.
    lex_init(source)
    Lexer(dummy: 0)

fn token_eof(offset: i64, line: i64) -> Token:
    # Compatibility shim used by TreeSitter outline parser.
    lex_token_eof(line)

fn lexer_next_token(self: Lexer) -> Token:
    # Advance the global lexer and build a Token record.
    val kind: i64 = lex_next()
    val text = lex_token_text()
    val line = lex_token_line()
    val col = lex_token_col()
    val span = lex_span_new(0, text.len(), line, col)
    Token(kind: kind, span: span, text: text)

impl Lexer:
    static fn new(source: text) -> Lexer:
        lexer_new(source)

# ===== Lexer Init/State Access =====

fn lex_init(source: text):
    lex_source = source
    lex_pos = 0
    lex_line = 1
    lex_col = 1
    lex_indent_stack = []
    lex_indent_stack.push(0)
    lex_pending_dedents = 0
    lex_at_line_start = true
    lex_paren_depth = 0
    lex_round_paren_depth = 0
    lex_cur_kind = 0
    lex_cur_text = ""
    lex_cur_line = 0
    lex_cur_col = 0
    lex_cur_suffix = ""

fn lex_at_end() -> bool:
    lex_pos >= lex_source.len()

fn lex_peek() -> text:
    if lex_pos >= lex_source.len():
        return ""
    lex_source[lex_pos:lex_pos + 1]

fn lex_peek_next() -> text:
    val next_pos: i64 = lex_pos + 1
    if next_pos >= lex_source.len():
        return ""
    lex_source[next_pos:next_pos + 1]

fn lex_peek_at(offset: i64) -> text:
    val target: i64 = lex_pos + offset
    if target >= lex_source.len():
        return ""
    lex_source[target:target + 1]

fn lex_advance() -> text:
    if lex_pos >= lex_source.len():
        return ""
    val c: text = lex_source[lex_pos:lex_pos + 1]
    lex_pos = lex_pos + 1
    if c == _NL:
        lex_line = lex_line + 1
        lex_col = 1
        lex_at_line_start = true
    else:
        lex_col = lex_col + 1
    c

fn lex_match_char(expected: text) -> bool:
    if lex_pos >= lex_source.len():
        return false
    if lex_source[lex_pos:lex_pos + 1] != expected:
        return false
    lex_pos = lex_pos + 1
    lex_col = lex_col + 1
    true

# ===== Token Construction =====

fn lex_make_token(kind: i64, token_text: text, start_line: i64, start_col: i64):
    lex_cur_kind = kind
    lex_cur_text = token_text
    lex_cur_line = start_line
    lex_cur_col = start_col

fn lex_make_simple(kind: i64, token_text: text):
    lex_make_token(kind, token_text, lex_line, lex_col)

# ===== Public API =====

fn lex_next() -> i64:
    # Check for pending dedents
    if lex_pending_dedents > 0:
        lex_pending_dedents = lex_pending_dedents - 1
        lex_cur_kind = TOK_DEDENT
        lex_cur_text = ""
        return TOK_DEDENT

    lex_scan_token()
    return lex_cur_kind

fn lex_token_kind() -> i64:
    lex_cur_kind

fn lex_token_text() -> text:
    lex_cur_text

fn lex_token_line() -> i64:
    lex_cur_line

fn lex_token_col() -> i64:
    lex_cur_col

# ===== Convenience: Tokenize All =====
# Tokenizes entire source and returns arrays of token data.
# Useful for testing and debugging.

var lex_all_kinds: [i64] = []
var lex_all_texts: [text] = []
var lex_all_lines: [i64] = []
var lex_all_cols: [i64] = []

fn lex_tokenize_all(source: text) -> i64:
    lex_init(source)
    lex_all_kinds = []
    lex_all_texts = []
    lex_all_lines = []
    lex_all_cols = []

    for i in 0..1000000:
        val kind: i64 = lex_next()
        lex_all_kinds.push(kind)
        lex_all_texts.push(lex_cur_text)
        lex_all_lines.push(lex_cur_line)
        lex_all_cols.push(lex_cur_col)
        if kind == TOK_EOF:
            break

    lex_all_kinds.len()

export Lexer, lexer_new, lexer_next_token, token_eof
