# CUDA Backend - NVIDIA CUDA/PTX Code Generation
#
# Compiles MIR to CUDA PTX assembly code.
# PTX can be JIT-compiled by NVIDIA driver or assembled offline.

use compiler.mir_data.*
use compiler.backend.backend_api.*
use compiler.backend.cuda_type_mapper.CudaTypeMapper
use compiler.backend.cuda.ptx_builder.PtxBuilder
use compiler.hir_types.MemorySpace

# ============================================================================
# CUDA Backend
# ============================================================================

class CudaBackend:
    """
    CUDA backend for GPU code generation.

    Compiles MIR modules to PTX assembly code for NVIDIA GPUs.
    PTX is a virtual instruction set that gets JIT-compiled to
    native GPU machine code (SASS) by the NVIDIA driver.

    Features:
    - Full MIR instruction support
    - Kernel function generation
    - Device function generation
    - Shared memory allocation
    - Atomic operations
    - Synchronization barriers

    Usage:
        val backend = CudaBackend__create((8, 6))  # SM 8.6
        val result = backend.compile(mir_module)
    """

    type_mapper: CudaTypeMapper
    compute_capability: (i64, i64)
    options: CompileOptions

    static fn create(compute_cap: (i64, i64)) -> CudaBackend:
        """Create CUDA backend for specific compute capability."""
        CudaBackend(
            type_mapper: CudaTypeMapper__create_sm(compute_cap.0, compute_cap.1),
            compute_capability: compute_cap,
            options: CompileOptions__default_options()
                .with_target(CodegenTarget.CudaPtx(compute_cap))
        )

    static fn create_with_options(compute_cap: (i64, i64), opts: CompileOptions) -> CudaBackend:
        """Create CUDA backend with custom options."""
        CudaBackend(
            type_mapper: CudaTypeMapper__create_sm(compute_cap.0, compute_cap.1),
            compute_capability: compute_cap,
            options: opts
        )

    static fn for_target(target: CodegenTarget) -> Result<CudaBackend, CompileError>:
        """Create CUDA backend from target."""
        match target.cuda_compute_capability():
            case Some(cap):
                Ok(CudaBackend__create(cap))
            case nil:
                Err(CompileError__backend_error(
                    BackendKind.Cuda,
                    "Target is not a CUDA target"
                ))

    fn compile(module: MirModule) -> Result<CudaCompiledModule, CompileError>:
        """Compile MIR module to PTX."""
        val builder = PtxBuilder__create(self.compute_capability)

        # Emit module header
        builder.emit_module_header(module.name)

        # Collect kernel functions
        var kernels: [CudaKernel] = []
        var device_functions: [CudaDeviceFunction] = []

        for (symbol, func) in module.functions:
            val result = self.compile_function(builder, func)
            match result:
                case Ok(kernel):
                    if kernel.is_kernel:
                        kernels = kernels.push(CudaKernel(
                            name: kernel.name,
                            ptx: kernel.ptx,
                            param_count: func.signature.params.len()
                        ))
                    else:
                        device_functions = device_functions.push(CudaDeviceFunction(
                            name: kernel.name,
                            ptx: kernel.ptx
                        ))
                case Err(e):
                    return Err(e)

        val ptx_code = builder.build()

        Ok(CudaCompiledModule(
            name: module.name,
            ptx: ptx_code,
            kernels: kernels,
            device_functions: device_functions,
            compute_capability: self.compute_capability
        ))

    fn compile_function(builder: PtxBuilder, func: MirFunction) -> Result<CompiledFunction, CompileError>:
        """Compile a single function."""
        val is_kernel = self.is_kernel_function(func)

        if is_kernel:
            builder.emit_kernel_entry(func.name, func.locals)
        else:
            builder.emit_device_function(
                func.name,
                func.locals,
                func.signature.return_type
            )

        # Emit register declarations for locals
        for local in func.locals:
            match local.kind:
                case Arg(_):
                    ()  # Arguments are parameters
                case Var | Temp | Return:
                    self.emit_local_decl(builder, local)

        # Compile each basic block
        for block in func.blocks:
            val label = "BB{block.id.id}"
            builder.emit_label(label)

            for inst in block.instructions:
                val result = self.compile_instruction(builder, inst)
                if result.is_err():
                    return Err(result.unwrap_err())

            self.compile_terminator(builder, block.terminator)

        if is_kernel:
            builder.emit_kernel_end()
        else:
            builder.emit_function_end()

        Ok(CompiledFunction(
            name: func.name,
            ptx: "",  # PTX is accumulated in builder
            is_kernel: is_kernel
        ))

    fn is_kernel_function(func: MirFunction) -> bool:
        """Check if function should be compiled as a kernel."""
        # For now, functions with GPU instructions are kernels
        for block in func.blocks:
            for inst in block.instructions:
                match inst.kind:
                    case GpuGlobalId(_, _) | GpuLocalId(_, _) | GpuBlockId(_, _) |
                         GpuBarrier(_) | GpuSharedAlloc(_, _, _) | GpuAtomicOp(_, _, _, _):
                        return true
                    case _:
                        ()
        false

    me emit_local_decl(builder: PtxBuilder, local: MirLocal):
        """Emit local variable declaration."""
        val reg = "%r{local.id.id}"
        match local.type_.kind:
            case I64: builder.emit_reg_decl(reg, PrimitiveType.I64)
            case I32: builder.emit_reg_decl(reg, PrimitiveType.I32)
            case I16: builder.emit_reg_decl(reg, PrimitiveType.I16)
            case I8: builder.emit_reg_decl(reg, PrimitiveType.I8)
            case U64: builder.emit_reg_decl(reg, PrimitiveType.U64)
            case U32: builder.emit_reg_decl(reg, PrimitiveType.U32)
            case U16: builder.emit_reg_decl(reg, PrimitiveType.U16)
            case U8: builder.emit_reg_decl(reg, PrimitiveType.U8)
            case F64: builder.emit_reg_decl(reg, PrimitiveType.F64)
            case F32: builder.emit_reg_decl(reg, PrimitiveType.F32)
            case Bool: builder.emit_pred_decl(reg)
            case _: ()

    fn compile_instruction(builder: PtxBuilder, inst: MirInst) -> Result<(), CompileError>:
        """Compile a single MIR instruction to PTX."""
        match inst.kind:
            case Const(dest, value, ty):
                self.compile_const(builder, dest, value, ty)

            case Copy(dest, src):
                self.compile_copy(builder, dest, src)

            case Move(dest, src):
                self.compile_copy(builder, dest, src)  # Same as copy in PTX

            case BinOp(dest, op, left, right):
                self.compile_binop(builder, dest, op, left, right)

            case UnaryOp(dest, op, operand):
                self.compile_unaryop(builder, dest, op, operand)

            case Load(dest, ptr):
                self.compile_load(builder, dest, ptr)

            case Store(ptr, value):
                self.compile_store(builder, ptr, value)

            # GPU-specific instructions
            case GpuGlobalId(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_compute_global_id(reg, dim)

            case GpuLocalId(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_thread_id(reg, dim)

            case GpuBlockId(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_block_id(reg, dim)

            case GpuBlockDim(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_block_dim(reg, dim)

            case GpuGridDim(dest, dim):
                val reg = "%r{dest.id}"
                builder.emit_get_grid_dim(reg, dim)

            case GpuBarrier(scope):
                match scope:
                    case Workgroup:
                        builder.emit_barrier()
                    case Device:
                        builder.emit_membar_gl()
                    case Subgroup:
                        # Warp-level sync (implicit in CUDA, explicit only needed for independent thread scheduling)
                        builder.emit_comment("warp sync")

            case GpuMemFence(scope):
                match scope:
                    case Workgroup:
                        builder.emit_membar_cta()
                    case Device:
                        builder.emit_membar_gl()
                    case All:
                        builder.emit_membar_sys()
                    case _:
                        builder.emit_membar_cta()

            case GpuSharedAlloc(dest, ty, size):
                val name = "shared_{dest.id}"
                val prim_ty = self.mir_type_to_primitive(ty)
                builder.emit_shared_memory(name, prim_ty, size)

            case GpuAtomicOp(dest, op, ptr, value):
                self.compile_atomic(builder, dest, op, ptr, value)

            case Nop:
                ()

            case _:
                return Err(CompileError__backend_error(
                    BackendKind.Cuda,
                    "Unsupported instruction: {inst.kind}"
                ))

        Ok(())

    fn compile_const(builder: PtxBuilder, dest: LocalId, value: MirConstValue, ty: MirType):
        """Compile constant load."""
        val reg = "%r{dest.id}"
        match value:
            case Int(v):
                val prim_ty = self.mir_type_to_primitive(ty)
                builder.emit_const_int(reg, prim_ty, v)
            case Float(v):
                val prim_ty = self.mir_type_to_primitive(ty)
                builder.emit_const_float(reg, prim_ty, v)
            case Bool(v):
                val val_int = if v: 1 else: 0
                builder.emit_const_int(reg, PrimitiveType.Bool, val_int)
            case _:
                builder.emit_comment("Unsupported constant type")

    fn compile_copy(builder: PtxBuilder, dest: LocalId, src: LocalId):
        """Compile copy/move."""
        val dest_reg = "%r{dest.id}"
        val src_reg = "%r{src.id}"
        # Use mov instruction
        builder.emit_line("mov.u64 {dest_reg}, {src_reg};")

    fn compile_binop(builder: PtxBuilder, dest: LocalId, op: MirBinOp, left: MirOperand, right: MirOperand):
        """Compile binary operation."""
        val dest_reg = "%r{dest.id}"
        val left_reg = self.operand_to_reg(left)
        val right_reg = self.operand_to_reg(right)

        # Determine type from left operand
        val ty = PrimitiveType.I64  # Default, should be inferred

        match op:
            case Add:
                builder.emit_add(dest_reg, ty, left_reg, right_reg)
            case Sub:
                builder.emit_sub(dest_reg, ty, left_reg, right_reg)
            case Mul:
                builder.emit_mul(dest_reg, ty, left_reg, right_reg)
            case Div:
                builder.emit_div(dest_reg, ty, left_reg, right_reg)
            case Eq:
                builder.emit_compare(dest_reg, "eq", ty, left_reg, right_reg)
            case Ne:
                builder.emit_compare(dest_reg, "ne", ty, left_reg, right_reg)
            case Lt:
                builder.emit_compare(dest_reg, "lt", ty, left_reg, right_reg)
            case Le:
                builder.emit_compare(dest_reg, "le", ty, left_reg, right_reg)
            case Gt:
                builder.emit_compare(dest_reg, "gt", ty, left_reg, right_reg)
            case Ge:
                builder.emit_compare(dest_reg, "ge", ty, left_reg, right_reg)
            case _:
                builder.emit_comment("Unsupported binop: {op}")

    fn compile_unaryop(builder: PtxBuilder, dest: LocalId, op: MirUnaryOp, operand: MirOperand):
        """Compile unary operation."""
        val dest_reg = "%r{dest.id}"
        val src_reg = self.operand_to_reg(operand)
        val ty = PrimitiveType.I64

        match op:
            case Neg:
                builder.emit_neg(dest_reg, ty, src_reg)
            case Not:
                builder.emit_line("not.pred {dest_reg}, {src_reg};")
            case _:
                builder.emit_comment("Unsupported unaryop: {op}")

    fn compile_load(builder: PtxBuilder, dest: LocalId, ptr: MirOperand):
        """Compile load instruction."""
        val dest_reg = "%r{dest.id}"
        val addr_reg = self.operand_to_reg(ptr)
        builder.emit_load(dest_reg, PrimitiveType.I64, addr_reg, MemorySpace.Global)

    fn compile_store(builder: PtxBuilder, ptr: MirOperand, value: MirOperand):
        """Compile store instruction."""
        val addr_reg = self.operand_to_reg(ptr)
        val value_reg = self.operand_to_reg(value)
        builder.emit_store(PrimitiveType.I64, addr_reg, value_reg, MemorySpace.Global)

    fn compile_atomic(builder: PtxBuilder, dest: LocalId, op: GpuAtomicOpKind, ptr: MirOperand, value: MirOperand):
        """Compile atomic operation."""
        val dest_reg = "%r{dest.id}"
        val addr_reg = self.operand_to_reg(ptr)
        val value_reg = self.operand_to_reg(value)
        val ty = PrimitiveType.I64

        match op:
            case Add:
                builder.emit_atomic_add(dest_reg, ty, addr_reg, value_reg, MemorySpace.Global)
            case Min:
                builder.emit_atomic_min(dest_reg, ty, addr_reg, value_reg, MemorySpace.Global)
            case Max:
                builder.emit_atomic_max(dest_reg, ty, addr_reg, value_reg, MemorySpace.Global)
            case Exchange:
                builder.emit_line("atom.global.exch.u64 {dest_reg}, [{addr_reg}], {value_reg};")
            case _:
                builder.emit_comment("Unsupported atomic op: {op}")

    fn compile_terminator(builder: PtxBuilder, term: MirTerminator):
        """Compile block terminator."""
        match term:
            case Return(_):
                builder.emit_ret()
            case Goto(target):
                builder.emit_branch("BB{target.id}")
            case Branch(cond, true_target, false_target):
                val cond_reg = self.operand_to_reg(cond)
                builder.emit_branch_if(cond_reg, "BB{true_target.id}")
                builder.emit_branch("BB{false_target.id}")
            case Unreachable:
                builder.emit_line("trap;")
            case _:
                builder.emit_comment("Unsupported terminator")

    fn operand_to_reg(operand: MirOperand) -> text:
        """Convert operand to register name."""
        match operand.kind:
            case Copy(local): "%r{local.id}"
            case Move(local): "%r{local.id}"
            case Const(value, ty):
                # Constants should be loaded into temporaries
                "%const"

    fn mir_type_to_primitive(ty: MirType) -> PrimitiveType:
        """Convert MIR type to PrimitiveType."""
        match ty.kind:
            case I64: PrimitiveType.I64
            case I32: PrimitiveType.I32
            case I16: PrimitiveType.I16
            case I8: PrimitiveType.I8
            case U64: PrimitiveType.U64
            case U32: PrimitiveType.U32
            case U16: PrimitiveType.U16
            case U8: PrimitiveType.U8
            case F64: PrimitiveType.F64
            case F32: PrimitiveType.F32
            case Bool: PrimitiveType.Bool
            case _: PrimitiveType.I64

# ============================================================================
# CUDA Compilation Results
# ============================================================================

struct CudaCompiledModule:
    """Result of CUDA compilation."""
    name: text
    ptx: text
    kernels: [CudaKernel]
    device_functions: [CudaDeviceFunction]
    compute_capability: (i64, i64)

struct CudaKernel:
    """Compiled CUDA kernel."""
    name: text
    ptx: text
    param_count: i64

struct CudaDeviceFunction:
    """Compiled CUDA device function."""
    name: text
    ptx: text

struct CompiledFunction:
    """Internal compiled function result."""
    name: text
    ptx: text
    is_kernel: bool

# ============================================================================
# Export
# ============================================================================

export CudaBackend
export CudaCompiledModule, CudaKernel, CudaDeviceFunction
