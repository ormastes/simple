# x86_64 Instruction Encoder
#
# Encodes MachInst to raw bytes for x86_64 architecture.
# x86_64 is variable-length (1-15 bytes per instruction).
#
# Encoding format:
#   [REX prefix] [Opcode 1-3 bytes] [ModRM] [SIB] [Displacement] [Immediate]
#
# REX prefix: 0x40 | W(3) | R(2) | X(1) | B(0)
#   W = 1 for 64-bit operand size
#   R = extends ModRM reg field
#   X = extends SIB index field
#   B = extends ModRM r/m or SIB base field

use compiler.backend.native.mach_inst.{MachReg, MachRegKind, physical_reg, reg_id, Operand, OperandKind, MachInst, MachBlock, MachFunction, MachModule, EncodedReloc, EncodedFunction, new_encoded_function}
use compiler.backend.native.mach_inst.{X86_RAX, X86_RCX, X86_RDX, X86_RBX, X86_RSP, X86_RBP, X86_RSI, X86_RDI, X86_R8, X86_R9, X86_R10, X86_R11, X86_R12, X86_R13, X86_R14, X86_R15}
use compiler.backend.native.mach_inst.{X86_OP_MOV_REG_REG, X86_OP_MOV_REG_IMM, X86_OP_MOV_REG_MEM, X86_OP_MOV_MEM_REG, X86_OP_ADD, X86_OP_SUB, X86_OP_IMUL, X86_OP_IDIV, X86_OP_AND, X86_OP_OR, X86_OP_XOR, X86_OP_SHL, X86_OP_SAR, X86_OP_SHR, X86_OP_NEG, X86_OP_NOT, X86_OP_CMP, X86_OP_TEST, X86_OP_SETE, X86_OP_SETNE, X86_OP_SETL, X86_OP_SETLE, X86_OP_SETG, X86_OP_SETGE, X86_OP_SETB, X86_OP_SETBE, X86_OP_SETA, X86_OP_SETAE, X86_OP_JMP, X86_OP_JE, X86_OP_JNE, X86_OP_JNZ, X86_OP_JZ, X86_OP_CALL, X86_OP_RET, X86_OP_PUSH, X86_OP_POP, X86_OP_LEA, X86_OP_MOVZX, X86_OP_CQO, X86_OP_NOP, X86_OP_INT3, X86_OP_SYSCALL, X86_OP_ADD_IMM, X86_OP_SUB_IMM, X86_OP_CMP_IMM, X86_OP_MOV_REG_IMM32, X86_OP_CALL_INDIRECT}
use compiler.backend.native.elf_writer.{ByteBuffer, new_byte_buffer, buf_len, buf_write_u8, buf_write_u32_le, buf_write_u64_le, buf_write_bytes}

# ============================================================================
# Encoding Helpers
# ============================================================================

fn reg_low3(reg_id: i64) -> i64:
    reg_id % 8

fn reg_needs_rex_b(reg_id: i64) -> bool:
    reg_id >= 8

fn rex_w() -> i64:
    0x48

fn rex_wr(reg: i64) -> i64:
    var rx = 0x48
    if reg >= 8:
        rx = rx + 4  # REX.R
    rx

fn rex_wrb(reg: i64, rm: i64) -> i64:
    var rx = 0x48  # REX.W
    if reg >= 8:
        rx = rx + 4  # REX.R
    if rm >= 8:
        rx = rx + 1  # REX.B
    rx

fn rex_wb(rm: i64) -> i64:
    var rx = 0x48  # REX.W
    if rm >= 8:
        rx = rx + 1  # REX.B
    rx

fn modrm(md: i64, reg: i64, rm: i64) -> i64:
    ((md % 4) * 64) + ((reg_low3(reg)) * 8) + reg_low3(rm)

fn modrm_direct(reg: i64, rm: i64) -> i64:
    modrm(3, reg, rm)

fn modrm_indirect(reg: i64, rm: i64) -> i64:
    modrm(0, reg, rm)

fn modrm_disp8(reg: i64, rm: i64) -> i64:
    modrm(1, reg, rm)

fn modrm_disp32(reg: i64, rm: i64) -> i64:
    modrm(2, reg, rm)

fn sib_byte(scale: i64, index: i64, base: i64) -> i64:
    ((scale % 4) * 64) + ((reg_low3(index)) * 8) + reg_low3(base)

# Convert signed i64 to bytes
fn i32_byte0(v: i64) -> i64:
    var masked = v
    if masked < 0:
        masked = masked + 4294967296
    masked % 256

fn i32_byte1(v: i64) -> i64:
    var masked = v
    if masked < 0:
        masked = masked + 4294967296
    (masked / 256) % 256

fn i32_byte2(v: i64) -> i64:
    var masked = v
    if masked < 0:
        masked = masked + 4294967296
    (masked / 65536) % 256

fn i32_byte3(v: i64) -> i64:
    var masked = v
    if masked < 0:
        masked = masked + 4294967296
    (masked / 16777216) % 256

fn emit_i32(buf: [i64], value: i64) -> [i64]:
    var result = buf
    result = result + [i32_byte0(value)]
    result = result + [i32_byte1(value)]
    result = result + [i32_byte2(value)]
    result = result + [i32_byte3(value)]
    result

fn emit_i64_bytes(buf: [i64], value: i64) -> [i64]:
    var result = emit_i32(buf, value % 4294967296)
    result = emit_i32(result, value / 4294967296)
    result

fn fits_in_i8(value: i64) -> bool:
    value >= -128 and value <= 127

fn fits_in_i32(value: i64) -> bool:
    value >= -2147483648 and value <= 2147483647

# ============================================================================
# Get register IDs from operands
# ============================================================================

fn get_phys_reg_id(op: Operand) -> i64:
    match op.kind:
        case Reg(reg):
            match reg.kind:
                case Physical(id): id
                case Virtual(id): id
        case _: 0

fn get_mem_base_id(op: Operand) -> i64:
    match op.kind:
        case Mem(base, _):
            match base.kind:
                case Physical(id): id
                case Virtual(id): id
        case _: 0

fn get_mem_offset(op: Operand) -> i64:
    match op.kind:
        case Mem(_, offset): offset
        case _: 0

fn get_imm_value(op: Operand) -> i64:
    match op.kind:
        case Imm(v): v
        case _: 0

fn get_label_id(op: Operand) -> i64:
    match op.kind:
        case Label(id): id
        case _: 0

fn get_sym_name(op: Operand) -> text:
    match op.kind:
        case Sym(name): name
        case _: ""

# ============================================================================
# Instruction Encoding
# ============================================================================

struct EncodeContext:
    code: [i64]
    relocations: [EncodedReloc]
    block_offsets: Dict<i64, i64>
    pending_jumps: [PendingJump]

struct PendingJump:
    code_offset: i64
    target_block: i64
    is_conditional: bool

fn new_encode_context() -> EncodeContext:
    EncodeContext(code: [], relocations: [], block_offsets: {}, pending_jumps: [])

fn encode_function(func: MachFunction) -> EncodedFunction:
    var ectx = new_encode_context()

    # First pass: encode all instructions, record block offsets and pending jumps
    for block in func.blocks:
        # Record block offset
        var offsets = ectx.block_offsets
        offsets[block.block_id] = ectx.code_len(code)
        ectx = EncodeContext(code: ectx.code, relocations: ectx.relocations, block_offsets: offsets, pending_jumps: ectx.pending_jumps)

        for inst in block.insts:
            ectx = encode_inst(ectx, inst)

    # Second pass: patch jump targets
    for pending in ectx.pending_jumps:
        val target_block = pending.target_block
        if ectx.block_offsets_contains(block_offsets, target_block):
            val target_offset = ectx.block_offsets[target_block]
            val jump_end = pending.code_offset + 4
            val rel32 = target_offset - jump_end
            # Patch the 4-byte displacement at code_offset
            var patched_code = ectx.code
            patched_code[pending.code_offset] = i32_byte0(rel32)
            patched_code[pending.code_offset + 1] = i32_byte1(rel32)
            patched_code[pending.code_offset + 2] = i32_byte2(rel32)
            patched_code[pending.code_offset + 3] = i32_byte3(rel32)
            ectx = EncodeContext(code: patched_code, relocations: ectx.relocations, block_offsets: ectx.block_offsets, pending_jumps: ectx.pending_jumps)

    EncodedFunction(name: func.name, code: ectx.code, relocations: ectx.relocations)

fn encode_inst(ectx: EncodeContext, inst: MachInst) -> EncodeContext:
    var code = ectx.code
    var relocs = ectx.relocations
    var pending = ectx.pending_jumps

    if inst.opcode == X86_OP_NOP:
        code = code + [0x90]

    elif inst.opcode == X86_OP_INT3:
        code = code + [0xcc]

    elif inst.opcode == X86_OP_RET:
        code = code + [0xc3]

    elif inst.opcode == X86_OP_CQO:
        # REX.W + 99
        code = code + [0x48, 0x99]

    elif inst.opcode == X86_OP_SYSCALL:
        code = code + [0x0f, 0x05]

    elif inst.opcode == X86_OP_MOV_REG_REG:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, dst)]
        code = code + [0x89]
        code = code + [modrm_direct(src, dst)]

    elif inst.opcode == X86_OP_MOV_REG_IMM:
        val dst = get_phys_reg_id(inst.operands[0])
        val imm = get_imm_value(inst.operands[1])
        # REX.W + B8+rd + imm64
        code = code + [rex_wb(dst)]
        code = code + [0xb8 + reg_low3(dst)]
        code = emit_i64_bytes(code, imm)

    elif inst.opcode == X86_OP_MOV_REG_IMM32:
        val dst = get_phys_reg_id(inst.operands[0])
        val imm = get_imm_value(inst.operands[1])
        if dst >= 8:
            code = code + [0x41]
        code = code + [0xb8 + reg_low3(dst)]
        code = emit_i32(code, imm)

    elif inst.opcode == X86_OP_MOV_REG_MEM:
        val dst = get_phys_reg_id(inst.operands[0])
        val base = get_mem_base_id(inst.operands[1])
        val offset = get_mem_offset(inst.operands[1])
        code = code + [rex_wrb(dst, base)]
        code = code + [0x8b]
        code = encode_mem_operand(code, dst, base, offset)

    elif inst.opcode == X86_OP_MOV_MEM_REG:
        val base = get_mem_base_id(inst.operands[0])
        val offset = get_mem_offset(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, base)]
        code = code + [0x89]
        code = encode_mem_operand(code, src, base, offset)

    elif inst.opcode == X86_OP_ADD:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, dst)]
        code = code + [0x01]
        code = code + [modrm_direct(src, dst)]

    elif inst.opcode == X86_OP_ADD_IMM:
        val dst = get_phys_reg_id(inst.operands[0])
        val imm = get_imm_value(inst.operands[1])
        code = code + [rex_wb(dst)]
        code = code + [0x81]
        code = code + [modrm_direct(0, dst)]
        code = emit_i32(code, imm)

    elif inst.opcode == X86_OP_SUB:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, dst)]
        code = code + [0x29]
        code = code + [modrm_direct(src, dst)]

    elif inst.opcode == X86_OP_SUB_IMM:
        val dst = get_phys_reg_id(inst.operands[0])
        val imm = get_imm_value(inst.operands[1])
        if imm == 0:
            # No-op for sub rsp, 0
            code = code + [0x90]
        else:
            code = code + [rex_wb(dst)]
            code = code + [0x81]
            code = code + [modrm_direct(5, dst)]  # /5 = sub
            code = emit_i32(code, imm)

    elif inst.opcode == X86_OP_IMUL:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(dst, src)]
        code = code + [0x0f, 0xaf]
        code = code + [modrm_direct(dst, src)]

    elif inst.opcode == X86_OP_IDIV:
        val src = get_phys_reg_id(inst.operands[0])
        code = code + [rex_wb(src)]
        code = code + [0xf7]
        code = code + [modrm_direct(7, src)]  # /7 = idiv

    elif inst.opcode == X86_OP_AND:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, dst)]
        code = code + [0x21]
        code = code + [modrm_direct(src, dst)]

    elif inst.opcode == X86_OP_OR:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, dst)]
        code = code + [0x09]
        code = code + [modrm_direct(src, dst)]

    elif inst.opcode == X86_OP_XOR:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(src, dst)]
        code = code + [0x31]
        code = code + [modrm_direct(src, dst)]

    elif inst.opcode == X86_OP_SHL:
        val dst = get_phys_reg_id(inst.operands[0])
        # SHL r/m64, cl (D3 /4)
        code = code + [rex_wb(dst)]
        code = code + [0xd3]
        code = code + [modrm_direct(4, dst)]

    elif inst.opcode == X86_OP_SAR:
        val dst = get_phys_reg_id(inst.operands[0])
        code = code + [rex_wb(dst)]
        code = code + [0xd3]
        code = code + [modrm_direct(7, dst)]

    elif inst.opcode == X86_OP_SHR:
        val dst = get_phys_reg_id(inst.operands[0])
        code = code + [rex_wb(dst)]
        code = code + [0xd3]
        code = code + [modrm_direct(5, dst)]

    elif inst.opcode == X86_OP_NEG:
        val dst = get_phys_reg_id(inst.operands[0])
        code = code + [rex_wb(dst)]
        code = code + [0xf7]
        code = code + [modrm_direct(3, dst)]

    elif inst.opcode == X86_OP_NOT:
        val dst = get_phys_reg_id(inst.operands[0])
        code = code + [rex_wb(dst)]
        code = code + [0xf7]
        code = code + [modrm_direct(2, dst)]

    elif inst.opcode == X86_OP_CMP:
        val left = get_phys_reg_id(inst.operands[0])
        val right = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(right, left)]
        code = code + [0x39]
        code = code + [modrm_direct(right, left)]

    elif inst.opcode == X86_OP_CMP_IMM:
        val dst = get_phys_reg_id(inst.operands[0])
        val imm = get_imm_value(inst.operands[1])
        code = code + [rex_wb(dst)]
        code = code + [0x81]
        code = code + [modrm_direct(7, dst)]  # /7 = cmp
        code = emit_i32(code, imm)

    elif inst.opcode == X86_OP_TEST:
        val left = get_phys_reg_id(inst.operands[0])
        val right = get_phys_reg_id(inst.operands[1])
        code = code + [rex_wrb(right, left)]
        code = code + [0x85]
        code = code + [modrm_direct(right, left)]

    elif inst.opcode == X86_OP_SETE:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x94, dst)

    elif inst.opcode == X86_OP_SETNE:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x95, dst)

    elif inst.opcode == X86_OP_SETL:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x9c, dst)

    elif inst.opcode == X86_OP_SETLE:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x9e, dst)

    elif inst.opcode == X86_OP_SETG:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x9f, dst)

    elif inst.opcode == X86_OP_SETGE:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x9d, dst)

    elif inst.opcode == X86_OP_SETB:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x92, dst)

    elif inst.opcode == X86_OP_SETBE:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x96, dst)

    elif inst.opcode == X86_OP_SETA:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x97, dst)

    elif inst.opcode == X86_OP_SETAE:
        val dst = get_phys_reg_id(inst.operands[0])
        code = encode_setcc(code, 0x93, dst)

    elif inst.opcode == X86_OP_MOVZX:
        val dst = get_phys_reg_id(inst.operands[0])
        val src = get_phys_reg_id(inst.operands[1])
        # movzx r64, r8 -> REX.W + 0F B6 /r
        code = code + [rex_wrb(dst, src)]
        code = code + [0x0f, 0xb6]
        code = code + [modrm_direct(dst, src)]

    elif inst.opcode == X86_OP_LEA:
        val dst = get_phys_reg_id(inst.operands[0])
        # Check if second operand is a symbol or memory
        match inst.operands[1].kind:
            case Sym(name):
                # LEA with RIP-relative addressing for symbol
                code = code + [rex_wr(dst)]
                code = code + [0x8d]
                code = code + [modrm(0, dst, 5)]  # [rip+disp32]
                # Record relocation
                val reloc = EncodedReloc(
                    offset: code_len(code),
                    symbol_name: name,
                    reloc_type: 2,  # R_X86_64_PC32
                    addend: -4
                )
                relocs = relocs + [reloc]
                code = emit_i32(code, 0)  # Placeholder
            case Mem(base, offset):
                val base_id = reg_id(base)
                code = code + [rex_wrb(dst, base_id)]
                code = code + [0x8d]
                code = encode_mem_operand(code, dst, base_id, offset)
            case _:
                code = code + [0x90]  # NOP fallback

    elif inst.opcode == X86_OP_PUSH:
        val src = get_phys_reg_id(inst.operands[0])
        if src >= 8:
            code = code + [0x41]  # REX.B
        code = code + [0x50 + reg_low3(src)]

    elif inst.opcode == X86_OP_POP:
        val dst = get_phys_reg_id(inst.operands[0])
        if dst >= 8:
            code = code + [0x41]  # REX.B
        code = code + [0x58 + reg_low3(dst)]

    elif inst.opcode == X86_OP_JMP:
        match inst.operands[0].kind:
            case Label(block_id):
                code = code + [0xe9]  # JMP rel32
                val jump_offset = code_len(code)
                code = emit_i32(code, 0)  # Placeholder
                pending = pending + [PendingJump(code_offset: jump_offset, target_block: block_id, is_conditional: false)]
            case _:
                code = code + [0x90]

    elif inst.opcode == X86_OP_JE:
        val block_id = get_label_id(inst.operands[0])
        code = code + [0x0f, 0x84]  # JE rel32
        val jump_offset = code_len(code)
        code = emit_i32(code, 0)
        pending = pending + [PendingJump(code_offset: jump_offset, target_block: block_id, is_conditional: true)]

    elif inst.opcode == X86_OP_JNE:
        val block_id = get_label_id(inst.operands[0])
        code = code + [0x0f, 0x85]  # JNE rel32
        val jump_offset = code_len(code)
        code = emit_i32(code, 0)
        pending = pending + [PendingJump(code_offset: jump_offset, target_block: block_id, is_conditional: true)]

    elif inst.opcode == X86_OP_JNZ:
        val block_id = get_label_id(inst.operands[0])
        code = code + [0x0f, 0x85]  # JNZ = JNE rel32
        val jump_offset = code_len(code)
        code = emit_i32(code, 0)
        pending = pending + [PendingJump(code_offset: jump_offset, target_block: block_id, is_conditional: true)]

    elif inst.opcode == X86_OP_JZ:
        val block_id = get_label_id(inst.operands[0])
        code = code + [0x0f, 0x84]  # JZ = JE rel32
        val jump_offset = code_len(code)
        code = emit_i32(code, 0)
        pending = pending + [PendingJump(code_offset: jump_offset, target_block: block_id, is_conditional: true)]

    elif inst.opcode == X86_OP_CALL:
        match inst.operands[0].kind:
            case Sym(name):
                code = code + [0xe8]  # CALL rel32
                val reloc = EncodedReloc(
                    offset: code_len(code),
                    symbol_name: name,
                    reloc_type: 4,  # R_X86_64_PLT32
                    addend: -4
                )
                relocs = relocs + [reloc]
                code = emit_i32(code, 0)  # Placeholder
            case Label(block_id):
                code = code + [0xe8]
                val jump_offset = code_len(code)
                code = emit_i32(code, 0)
                pending = pending + [PendingJump(code_offset: jump_offset, target_block: block_id, is_conditional: false)]
            case _:
                code = code + [0x90]

    elif inst.opcode == X86_OP_CALL_INDIRECT:
        val src = get_phys_reg_id(inst.operands[0])
        # call *reg: FF /2
        code = code + [rex_wb(src)]
        code = code + [0xff]
        code = code + [modrm_direct(2, src)]

    else:
        # Unknown opcode - emit NOP
        code = code + [0x90]

    EncodeContext(code: code, relocations: relocs, block_offsets: ectx.block_offsets, pending_jumps: pending)

# ============================================================================
# Memory Operand Encoding Helper
# ============================================================================

fn encode_mem_operand(code: [i64], reg: i64, base: i64, offset: i64) -> [i64]:
    var result = code
    val base_low = reg_low3(base)

    if base_low == 4:
        # RSP/R12 needs SIB byte
        if offset == 0:
            result = result + [modrm_indirect(reg, base)]
            result = result + [sib_byte(0, 4, base)]  # SIB: no index, RSP base
        elif fits_in_i8(offset):
            result = result + [modrm_disp8(reg, base)]
            result = result + [sib_byte(0, 4, base)]
            result = result + [i32_byte0(offset)]
        else:
            result = result + [modrm_disp32(reg, base)]
            result = result + [sib_byte(0, 4, base)]
            result = emit_i32(result, offset)
    elif base_low == 5:
        if offset == 0:
        # RBP/R13 with disp=0 needs explicit disp8(0)
        result = result + [modrm_disp8(reg, base)]
        result = result + [0]
    else:
        if offset == 0:
            result = result + [modrm_indirect(reg, base)]
        elif fits_in_i8(offset):
            result = result + [modrm_disp8(reg, base)]
            result = result + [i32_byte0(offset)]
        else:
            result = result + [modrm_disp32(reg, base)]
            result = emit_i32(result, offset)

    result

# ============================================================================
# SetCC Encoding Helper
# ============================================================================

fn encode_setcc(code: [i64], cc_opcode: i64, dst: i64) -> [i64]:
    var result = code
    # setCC uses 8-bit register, need REX if dst >= 4 (spl, bpl, sil, dil)
    if dst >= 8:
        result = result + [0x41]  # REX.B
    elif dst >= 4:
        result = result + [0x40]  # REX (for spl etc.)
    result = result + [0x0f, cc_opcode]
    result = result + [modrm_direct(0, dst)]
    result

# ============================================================================
# Module Encoding
# ============================================================================

fn encode_module(module: MachModule) -> [EncodedFunction]:
    var results: [EncodedFunction] = []
    for func in module.functions:
        val encoded = encode_function(func)
        results = results + [encoded]
    results

# ============================================================================
# Exports
# ============================================================================

export EncodeContext, new_encode_context, PendingJump
export encode_function, encode_inst, encode_module
export EncodedReloc, EncodedFunction
