# Test Runner Main Orchestration
#
# Main test runner entry point and orchestration logic.
# Supports DB writes, parallel execution, feature DB, and doc generation.

use app.cli_util (get_cli_args)

use test_runner_types.*
use test_runner_args.parse_test_args
use test_runner_files.{discover_test_files, print_discovery_summary, strip_ansi}
use test_runner_execute.{run_test_file_interpreter, run_test_file_smf, run_test_file_native}
use test_runner_output.{print_result, print_summary}
use test_db_core.{TestDatabase, micros_to_rfc3339}
use test_db_types.*
use feature_db.FeatureDatabase
use doc_generator.generate_all_docs
use rust_test_runner.{run_rust_tests, RustTestResult}

# =========================================================================
# Environment Variable Propagation
# =========================================================================

fn propagate_env_vars(options: TestOptions):
    # Set SIMPLE_TEST_MODE for child processes
    val mode_str = mode_to_str(options.mode)
    rt_env_set("SIMPLE_TEST_MODE", mode_str)

    if options.only_slow:
        rt_env_set("SIMPLE_TEST_FILTER", "slow")
    elif options.only_skipped:
        rt_env_set("SIMPLE_TEST_FILTER", "skipped")

    if options.show_tags:
        rt_env_set("SIMPLE_TEST_SHOW_TAGS", "1")

# =========================================================================
# Runner
# =========================================================================

fn run_tests(options: TestOptions) -> TestRunResult:
    val files = discover_test_files(options.path, options)

    if options.list:
        print_discovery_summary(files, options)
        return TestRunResult(files: [], total_passed: 0, total_failed: 0, total_skipped: 0, total_timed_out: 0, total_duration_ms: 0)

    if files.len() == 0:
        print "No test files found in {options.path}"
        return TestRunResult(files: [], total_passed: 0, total_failed: 0, total_skipped: 0, total_timed_out: 0, total_duration_ms: 0)

    # Propagate env vars
    propagate_env_vars(options)

    val mode_name = mode_to_str(options.mode)
    print "Running {files.len()} test file(s) [mode: {mode_name}]..."
    if options.has_seed:
        print "Seed: {options.seed}"
    print ""

    var results: List<TestFileResult> = []
    var total_passed = 0
    var total_failed = 0
    var total_skipped = 0
    var total_timed_out = 0
    var total_duration_ms = 0

    for file_path in files:
        val result = run_single_test(file_path, options)
        print_result(result, options.format)

        total_passed = total_passed + result.passed
        total_failed = total_failed + result.failed
        total_skipped = total_skipped + result.skipped
        if result.timed_out:
            total_timed_out = total_timed_out + 1
        total_duration_ms = total_duration_ms + result.duration_ms
        results.push(result)

        if options.fail_fast and (result.failed > 0 or result.timed_out):
            print ""
            print "Stopping early (--fail-fast)"
            break

    TestRunResult(
        files: results,
        total_passed: total_passed,
        total_failed: total_failed,
        total_skipped: total_skipped,
        total_timed_out: total_timed_out,
        total_duration_ms: total_duration_ms
    )

fn mode_to_str(mode: TestExecutionMode) -> text:
    match mode:
        case TestExecutionMode.Interpreter:
            "interpreter"
        case TestExecutionMode.Smf:
            "smf"
        case TestExecutionMode.Native:
            "native"

fn run_single_test(file_path: text, options: TestOptions) -> TestFileResult:
    match options.mode:
        case TestExecutionMode.Interpreter:
            run_test_file_interpreter(file_path, options)
        case TestExecutionMode.Smf:
            run_test_file_smf(file_path, options)
        case TestExecutionMode.Native:
            run_test_file_native(file_path, options)

# =========================================================================
# C4: Individual test result parsing from output
# =========================================================================

fn parse_individual_results(output: text, file_path: text) -> List<IndividualTestResult>:
    var results: List<IndividualTestResult> = []
    val lines = output.split("\n")
    var current_group = ""

    for line in lines:
        val clean = strip_ansi(line.trim())
        if clean == "":
            continue

        # Detect group headers (indented describe/context blocks)
        # Format: "GroupName" followed by indented tests
        if clean.ends_with(":") and not clean.starts_with(" ") and not clean.contains(" "):
            current_group = clean[:-1]
            continue

        # Parse individual test lines: checkmark/cross + name
        if clean.len() > 2:
            val prefix = clean[0:2]
            if prefix == "  ":
                # Indented line might be a test result
                val inner = clean.trim()
                if inner.len() > 2:
                    val marker = inner[0:3]
                    if marker.contains("\u2713") or marker.contains("\u2714") or inner.starts_with("PASS"):
                        val name = extract_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Passed,
                            error_message: ""
                        ))
                    elif marker.contains("\u2717") or marker.contains("\u2718") or inner.starts_with("FAIL"):
                        val name = extract_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Failed,
                            error_message: ""
                        ))
                    elif marker.contains("\u25CB") or marker.contains("\u25EF") or inner.starts_with("SKIP") or inner.starts_with("PEND"):
                        val name = extract_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Skipped,
                            error_message: ""
                        ))

    results

fn extract_test_name(line: text) -> text:
    # Strip leading marker characters and whitespace
    var i = 0
    while i < line.len():
        val ch = line[i:i + 1]
        if ch != " " and ch != "\u2713" and ch != "\u2714" and ch != "\u2717" and ch != "\u2718" and ch != "\u25CB" and ch != "\u25EF":
            break
        i = i + 1
    # Also skip "PASS ", "FAIL ", "SKIP ", "PEND " prefixes
    val rest = line[i:].trim()
    if rest.starts_with("PASS ") or rest.starts_with("FAIL ") or rest.starts_with("SKIP ") or rest.starts_with("PEND "):
        return rest[5:].trim()
    rest

# =========================================================================
# M1: Derive test category from file path
# =========================================================================

fn categorize_test_file(file_path: text) -> text:
    if file_path.contains("test/std/") or file_path.contains("test/lib/std/"):
        return "Unit"
    if file_path.contains("test/system/"):
        return "Integration"
    if file_path.contains("test/app/"):
        return "System"
    if file_path.contains("src/std/test/"):
        return "Unit"
    if file_path.contains("src/app/"):
        return "System"
    "Unknown"

# =========================================================================
# Main Entry Point
# =========================================================================

fn main() -> i64:
    # Set recursion guard so child processes use Rust runner
    rt_env_set("SIMPLE_TEST_RUNNER_RUST", "1")

    val all_args = get_cli_args()

    # Filter out "test" command if present (from CLI dispatch)
    var args: List<str> = []
    var skip_first = all_args.len() > 0 and all_args[0] == "test"
    var i = 0
    if skip_first:
        i = 1
    while i < all_args.len():
        args.push(all_args[i])
        i = i + 1

    val options = parse_test_args(args)

    # Handle run management commands
    if has_run_management_flag(args):
        return handle_run_management(args)

    print "Simple Test Runner v0.4.0"
    print ""

    val run_start = rt_time_now_unix_micros()
    val result = run_tests(options)
    val run_end = rt_time_now_unix_micros()

    print_summary(result, options.format)

    # Write test DB (now with proper file locking and atomic writes)
    if not options.list and result.files.len() > 0:
        update_test_database(result, run_start, run_end)

    if result.is_ok():
        return 0
    return 1

fn has_run_management_flag(args: List<text>) -> bool:
    for arg in args:
        if arg == "--list-runs" or arg == "--cleanup-runs" or arg == "--prune-runs" or arg.starts_with("--prune-runs="):
            return true
    false

fn handle_run_management(args: List<text>) -> i64:
    val db_result = TestDatabase.load()
    if db_result.err.?:
        print "Error: Could not load test database: {db_result.unwrap_err()}"
        return 1

    val db = db_result.unwrap()

    for arg in args:
        if arg == "--list-runs":
            val status_filter = get_flag_value(args, "--runs-status") ?? "all"
            val runs = db.list_runs(status_filter)
            print "Test Runs ({runs.len()}):"
            print ""
            print "| Run ID | Status | Tests | Passed | Failed | Start Time |"
            print "|--------|--------|-------|--------|--------|------------|"
            for r in runs:
                print "| {r.run_id} | {r.status} | {r.test_count} | {r.passed} | {r.failed} | {r.start_time} |"
            return 0

        if arg == "--cleanup-runs":
            db.cleanup_stale_runs(2)
            val save_result = db.save()
            if save_result.err.?:
                print "Error saving: {save_result.unwrap_err()}"
                return 1
            print "Stale runs cleaned up."
            return 0

        if arg.starts_with("--prune-runs="):
            val count_str = arg[13:]
            val keep = count_str.to_int_or(50)
            db.prune_runs(keep)
            val save_result = db.save()
            if save_result.err.?:
                print "Error saving: {save_result.unwrap_err()}"
                return 1
            print "Runs pruned (keeping {keep})."
            return 0

    0

fn get_flag_value(args: List<text>, flag: text) -> text?:
    var i = 0
    while i < args.len():
        if args[i] == flag and i + 1 < args.len():
            return Some(args[i + 1])
        if args[i].starts_with(flag + "="):
            return Some(args[i][flag.len() + 1:])
        i = i + 1
    None

# =========================================================================
# Test Database Update (C5: per-test granularity, M1: categorization)
# =========================================================================

fn update_test_database(result: TestRunResult, run_start: i64, run_end: i64):
    val db_result = TestDatabase.load()
    if db_result.err.?:
        print "Warning: Could not load test database: {db_result.unwrap_err()}"
        return

    val db = db_result.unwrap()

    # Start run tracking
    val run_id = db.start_run()

    # Update individual test results
    for file_result in result.files:
        # M1: Derive category from file path
        val category = categorize_test_file(file_result.path)

        # C4/C5: Try to parse individual test results from output
        val individual = parse_individual_results(file_result.error, file_result.path)

        if individual.len() > 0:
            # Per-test granularity: record each test separately
            for test in individual:
                val full_name = "{file_result.path}::{test.group}::{test.name}"
                val suite_name = if test.group != "": test.group else: "default"
                val duration = file_result.duration_ms.to_float() / individual.len().to_float()

                db.update_test_result(
                    full_name,
                    file_result.path,
                    suite_name,
                    category,
                    test.status,
                    duration
                )
        else:
            # Fallback: file-level result
            val status = if file_result.failed > 0:
                TestStatus.Failed
            elif file_result.timed_out:
                TestStatus.Failed
            else:
                TestStatus.Passed

            val suite_name = "default"
            val duration = file_result.duration_ms.to_float()

            db.update_test_result(
                file_result.path,
                file_result.path,
                suite_name,
                category,
                status,
                duration
            )

    # Complete run
    val total = result.total_passed + result.total_failed
    db.complete_run(run_id, total, result.total_passed, result.total_failed, result.total_timed_out)

    # Save database
    val save_result = db.save()
    if save_result.err.?:
        print "Warning: Could not save test database: {save_result.unwrap_err()}"
        return

    # Generate documentation (M3: includes feature DB auto-update)
    val fdb = FeatureDatabase.load()
    # M3: Auto-update feature status from test results
    update_features_from_tests(fdb, result)
    fdb.save()
    generate_all_docs(db, fdb)

# =========================================================================
# M3/H6: Update feature status from test results
# =========================================================================

fn update_features_from_tests(fdb: FeatureDatabase, result: TestRunResult):
    # Build set of failed file paths
    var failed_files: Dict<text, bool> = {}
    for file_result in result.files:
        if file_result.failed > 0 or file_result.timed_out:
            failed_files[file_result.path] = true

    # Update features that reference spec files
    var i = 0
    while i < fdb.features.len():
        val spec = fdb.features[i].spec
        if spec != "" and spec.ends_with(".spl"):
            if failed_files.contains_key(spec):
                fdb.features[i].status = "failed"
            else:
                # If the spec was in test results and passed, mark complete
                for file_result in result.files:
                    if file_result.path == spec and file_result.failed == 0 and not file_result.timed_out:
                        if fdb.features[i].status != "complete":
                            fdb.features[i].status = "complete"
        i = i + 1

# =========================================================================
# Exports
# =========================================================================

export propagate_env_vars, run_tests, mode_to_str, run_single_test
export parse_individual_results, categorize_test_file
export update_test_database, update_features_from_tests
export handle_run_management, has_run_management_flag
export main
