# Test Runner Main Orchestration
#
# Main test runner entry point and orchestration logic.
# Supports DB writes, parallel execution, feature DB, and doc generation.

use app.io.mod (env_set, env_get, time_now_unix_micros, dir_create_all, file_write, file_read, file_exists, cli_get_args)

use test_runner_types.*
use test_runner_args.parse_test_args
use test_config.{TestConfig, TestConfig__load}
use test_runner_files.{discover_test_files, print_discovery_summary, strip_ansi, file_has_di_test_tag}
use test_runner_execute.{run_test_file_interpreter, run_test_file_smf, run_test_file_native, run_test_file_safe_mode, run_test_file_composite}
use test_runner_output.{print_result, print_summary}
use sequential_container.{SequentialContainerConfig, sequential_container_run_all_tests}
use test_runner_container.container_detect_runtime
use test_runner_coverage.{setup_coverage, collect_coverage, print_coverage_summary, generate_coverage_reports, check_coverage_threshold, get_coverage_threshold, print_coverage_delta}
use test_db_compat.{TestDatabase, load_test_db_compat, micros_to_rfc3339}  # Using compatibility layer
use test_db_types.*
use std.database.feature.{FeatureDatabase, load_feature_database, create_feature_database, FeatureStatus}
use doc_generator.generate_all_docs
use rust_test_runner.{run_rust_tests, RustTestResult}
use sdoctest.config.load_sdoctest_config
use sdoctest.runner.{run_sdoctest_mode, print_sdoctest_summary}
use sdoctest.result_db.write_sdoctest_db
use sdoctest.doc_gen.generate_sdoctest_report

# Self-protection imports
use app.test_runner_new.checkpoint.{checkpoint_load, checkpoint_exists, checkpoint_skip_completed, checkpoint_save, checkpoint_delete}
use app.test_runner_new.system_monitor.{system_exceeds_threshold}
use app.test_runner_new.shutdown.{shutdown_graceful}

# Parallel execution imports
use app.test_runner_new.test_runner_async.{run_tests_parallel_with_monitoring, get_cpu_count}

# Lifecycle imports
use app.test_runner_new.runner_lifecycle.{cleanup_all_children, lifecycle_enable_heartbeat, lifecycle_send_heartbeat}

# Signal handler imports (with fallback)
use app.io.signal_handlers.{install_signal_handlers, create_signal_cleanup_handler, signal_handlers_available}
use app.test_runner_new.process_tracker.{tracker_kill_all_children, tracker_stop_all_containers}

# Closure warnings
use compiler.core.closure_analysis.{closure_warnings_get, closure_warnings_has}

# Call graph warnings
use compiler.core.call_graph.{call_graph_warnings_get, call_graph_warnings_has}

# =========================================================================
# Combined Mode (Spec + SDoctest)
# =========================================================================

fn run_combined_mode(options: TestOptions, config: TestConfig) -> i64:
    """
    Run both spec tests and sdoctests, combining results.
    This mode is activated when both run_spec_tests and run_sdoctests are true.
    """
    print "Simple Test Runner v0.4.0 (Combined Mode: Spec + SDoctest)"
    print ""

    # Run spec tests first
    print "=== Running Spec Tests ==="
    print ""
    val run_start = time_now_unix_micros()
    val spec_result = run_tests(options)
    val run_end = time_now_unix_micros()
    print_summary(spec_result, options.format)
    print ""

    # Run sdoctests second
    print "=== Running SDoctest ==="
    print ""
    val sdoctest_config = load_sdoctest_config("config/sdoctest.sdn")
    val sdoctest_result = run_sdoctest_mode(options, sdoctest_config)
    print_sdoctest_summary(sdoctest_result)
    print ""

    # Combined summary
    print "=== Combined Results ==="
    val total_passed = spec_result.total_passed + sdoctest_result.passed
    val total_failed = spec_result.total_failed + sdoctest_result.failed
    val total_skipped = spec_result.total_skipped + sdoctest_result.skipped
    print "Spec Tests:    {spec_result.total_passed} passed, {spec_result.total_failed} failed, {spec_result.total_skipped} skipped"
    print "SDoctest:      {sdoctest_result.passed} passed, {sdoctest_result.failed} failed, {sdoctest_result.skipped} skipped"
    print "---"
    print "Total:         {total_passed} passed, {total_failed} failed, {total_skipped} skipped"
    print ""

    # Write databases
    if not options.list and not options.no_db:
        if spec_result.files.len() > 0:
            update_test_database(spec_result, run_start, run_end)
        if sdoctest_result.total > 0:
            write_sdoctest_db(sdoctest_result, "doc/test/sdoctest_db.sdn")
            generate_sdoctest_report(sdoctest_result, "doc/test/sdoctest_result.md")

    # Success if both pass
    val all_passed = spec_result.is_ok() and sdoctest_result.is_ok()
    if all_passed:
        return 0
    return 1

# =========================================================================
# CLI Args Helper (inlined to avoid extra module load)
# =========================================================================

fn get_cli_args() -> [str]:
    val all_args = cli_get_args()
    var args: [str] = []
    var start_idx = 1
    if all_args.len() > 1 and all_args[1].ends_with("main.spl"):
        start_idx = 2
    var i = start_idx
    while i < all_args.len():
        args = args.push(all_args[i])
        i = i + 1
    args

# =========================================================================
# Environment Variable Propagation
# =========================================================================

fn propagate_env_vars(options: TestOptions):
    # Set SIMPLE_TEST_MODE for child processes
    val mode_str = mode_to_str(options.mode)
    env_set("SIMPLE_TEST_MODE", mode_str)

    if options.only_slow:
        env_set("SIMPLE_TEST_FILTER", "slow")
    elif options.only_skipped:
        env_set("SIMPLE_TEST_FILTER", "skipped")

    if options.show_tags:
        env_set("SIMPLE_TEST_SHOW_TAGS", "1")

    # Enable coverage tracking for child processes
    if options.coverage:
        env_set("SIMPLE_COVERAGE", "1")

    # Propagate advanced testing mode env vars
    if options.fuzz_mode:
        env_set("SIMPLE_FUZZ_MODE", "true")
        env_set("SIMPLE_FUZZ_TIME", "{options.fuzz_time_secs}")
        env_set("SIMPLE_FUZZ_CORPUS", options.fuzz_corpus_dir)
        if options.fuzz_differential:
            env_set("SIMPLE_FUZZ_DIFFERENTIAL", "true")
        if not options.fuzz_shrink:
            env_set("SIMPLE_FUZZ_SHRINK", "false")
    if options.chaos_mode:
        env_set("SIMPLE_CHAOS_MODE", "true")
        env_set("SIMPLE_CHAOS_TRIALS", "{options.chaos_trials}")
        env_set("SIMPLE_CHAOS_FAULT_RATE", "{options.chaos_fault_rate}")
    if options.deploy_mode:
        env_set("SIMPLE_DEPLOY_MODE", "true")
        if options.deploy_platform != "":
            env_set("SIMPLE_DEPLOY_PLATFORM", options.deploy_platform)
        if options.deploy_upgrade:
            env_set("SIMPLE_DEPLOY_UPGRADE", "true")
    if options.security_mode:
        env_set("SIMPLE_SECURITY_MODE", "true")
        if options.security_scan:
            env_set("SIMPLE_SECURITY_SCAN", "true")
        if options.security_supply_chain:
            env_set("SIMPLE_SECURITY_SUPPLY_CHAIN", "true")

    # Propagate JIT settings (default: JIT enabled with threshold=10)
    env_set("SIMPLE_JIT_ENABLED", "1")

# =========================================================================
# Runner
# =========================================================================

fn run_tests(options: TestOptions) -> TestRunResult:
    # Load test configuration
    val config = TestConfig__load()

    # Apply CI mode overrides
    var updated_options = options
    if config.ci_mode or options.ci_mode:
        updated_options.run_all = true
        updated_options.fail_fast = false
        updated_options.verbose = true
        if config.ci_mode:
            print "CI mode detected (via CI environment variable)"
        else:
            print "CI mode enabled (via --ci flag)"
        print ""

    # Load checkpoint if resuming
    var ckpt_loaded = false
    var ckpt_passed = 0
    var ckpt_failed = 0
    var ckpt_skipped = 0

    val all_files = discover_test_files(options.path, updated_options)

    # Filter files based on checkpoint if resuming
    var files = all_files
    if options.resume and checkpoint_exists():
        val ckpt = checkpoint_load()
        ckpt_loaded = true
        ckpt_passed = ckpt.total_passed
        ckpt_failed = ckpt.total_failed
        ckpt_skipped = ckpt.total_skipped
        files = checkpoint_skip_completed(all_files, ckpt)
        print "Resuming from checkpoint (reason: {ckpt.shutdown_reason})"
        print "Previous progress: {ckpt_passed} passed, {ckpt_failed} failed, {ckpt_skipped} skipped"
        print ""

    if options.list:
        print_discovery_summary(files, options)
        return TestRunResult(files: [], total_passed: 0, total_failed: 0, total_skipped: 0, total_pending: 0, total_timed_out: 0, total_duration_ms: 0)

    if files.len() == 0:
        print "No test files found in {options.path}"
        return TestRunResult(files: [], total_passed: 0, total_failed: 0, total_skipped: 0, total_pending: 0, total_timed_out: 0, total_duration_ms: 0)

    # Set up coverage collection if enabled
    if options.coverage:
        setup_coverage()

    # Propagate env vars
    propagate_env_vars(options)

    # Check for sequential container mode
    val exec_mode = options.execution_mode
    val is_container_seq1 = exec_mode == "container-seq"
    val is_container_seq2 = exec_mode == "container_seq"
    val is_container_seq3 = exec_mode == "container-sequential"
    val is_container_seq = is_container_seq1 or is_container_seq2 or is_container_seq3
    if is_container_seq or options.container_sequential:
        return run_tests_sequential_containers(files, options)

    # Parallel execution disabled - always run one by one
    # if options.parallel:
    #     return run_tests_parallel_mode(files, options, ckpt_passed, ckpt_failed, ckpt_skipped)

    val mode_name = mode_to_str(options.mode)
    print "Running {files.len()} test file(s) [mode: {mode_name}]..."
    if options.has_seed:
        print "Seed: {options.seed}"
    if options.self_protect:
        print "Self-protection enabled (CPU: {options.cpu_threshold}%, Memory: {options.mem_threshold}%)"
    print ""

    var results: [TestFileResult] = []
    var total_passed = ckpt_passed
    var total_failed = ckpt_failed
    var total_skipped = ckpt_skipped
    var total_pending = 0
    var total_timed_out = 0
    var total_duration_ms = 0
    var completed_files: [text] = []
    var tests_run = 0

    for file_path in files:
        # Check system resources every test
        if options.self_protect:
            val (violated, reason) = system_exceeds_threshold(options.cpu_threshold, options.mem_threshold)
            if violated:
                # Graceful shutdown - never returns
                shutdown_graceful(reason, completed_files, total_passed, total_failed, total_skipped)

        # Periodic checkpoint every 10 tests
        if options.self_protect and tests_run % 10 == 0 and tests_run > 0:
            checkpoint_save(completed_files, total_passed, total_failed, total_skipped, "periodic")

        # Run test
        val result = run_single_test(file_path, options)
        print_result(result, options.format)

        total_passed = total_passed + result.passed
        total_failed = total_failed + result.failed
        total_skipped = total_skipped + result.skipped
        total_pending = total_pending + result.pending
        if result.timed_out:
            total_timed_out = total_timed_out + 1
        total_duration_ms = total_duration_ms + result.duration_ms
        results.push(result)
        completed_files.push(file_path)
        tests_run = tests_run + 1

        if options.fail_fast and (result.failed > 0 or result.timed_out):
            print ""
            print "Stopping early (--fail-fast)"
            break

    # Clean up checkpoint on successful completion
    if options.resume and checkpoint_exists():
        checkpoint_delete()

    TestRunResult(
        files: results,
        total_passed: total_passed,
        total_failed: total_failed,
        total_skipped: total_skipped,
        total_pending: total_pending,
        total_timed_out: total_timed_out,
        total_duration_ms: total_duration_ms
    )

fn run_tests_parallel_mode(
    files: [text],
    options: TestOptions,
    ckpt_passed: i64,
    ckpt_failed: i64,
    ckpt_skipped: i64
) -> TestRunResult:
    """
    Run tests in parallel mode with worker pool.

    Args:
        files: List of test files
        options: Test options
        ckpt_passed: Checkpoint passed count
        ckpt_failed: Checkpoint failed count
        ckpt_skipped: Checkpoint skipped count

    Returns: TestRunResult
    """
    # Determine worker count
    var max_workers = options.max_workers
    if max_workers <= 0:
        max_workers = get_cpu_count()
        if max_workers > 1:
            max_workers = max_workers - 1  # Leave one core for system

    val mode_name = mode_to_str(options.mode)
    print "Running {files.len()} test file(s) [mode: {mode_name}, parallel: {max_workers} workers]..."
    if options.has_seed:
        print "Seed: {options.seed}"
    if options.self_protect:
        print "Self-protection enabled (CPU: {options.cpu_threshold}%, Memory: {options.mem_threshold}%)"
    print ""

    # Install signal handlers (if available)
    val cleanup_fn = create_signal_cleanup_handler(
        tracker_kill_all_children,
        tracker_stop_all_containers,
        fn(): checkpoint_save([], 0, 0, 0, "signal")
    )

    val handlers_installed = install_signal_handlers(cleanup_fn)
    if handlers_installed:
        print "Signal handlers installed (Ctrl+C will cleanup gracefully)"
    elif options.self_protect:
        print "Signal handlers not available - using resource monitoring"
    print ""

    # Enable heartbeat monitoring
    if options.self_protect:
        lifecycle_enable_heartbeat()

    # Run tests in parallel
    val results = run_tests_parallel_with_monitoring(files, options, max_workers)

    # Aggregate results
    var total_passed = ckpt_passed
    var total_failed = ckpt_failed
    var total_skipped = ckpt_skipped
    var total_pending = 0
    var total_timed_out = 0
    var total_duration_ms = 0

    for result in results:
        print_result(result, options.format)

        total_passed = total_passed + result.passed
        total_failed = total_failed + result.failed
        total_skipped = total_skipped + result.skipped
        total_pending = total_pending + result.pending
        if result.timed_out:
            total_timed_out = total_timed_out + 1
        total_duration_ms = total_duration_ms + result.duration_ms

    # Cleanup any orphaned resources
    val cleaned = cleanup_all_children()
    if cleaned > 0:
        print ""
        print "[CLEANUP] Cleaned up {cleaned} orphaned resources"

    # Clean up checkpoint on successful completion
    if options.resume and checkpoint_exists():
        checkpoint_delete()

    TestRunResult(
        files: results,
        total_passed: total_passed,
        total_failed: total_failed,
        total_skipped: total_skipped,
        total_pending: total_pending,
        total_timed_out: total_timed_out,
        total_duration_ms: total_duration_ms
    )

fn run_tests_sequential_containers(files: [text], options: TestOptions) -> TestRunResult:
    """
    Run tests in sequential container mode (maximum isolation).

    Args:
        files: List of test files
        options: Test options

    Returns: TestRunResult
    """
    # Create container config
    val runtime = container_detect_runtime()
    if runtime == "none":
        print "ERROR: No container runtime available (docker or podman)"
        print "Please install Docker or Podman to use sequential container mode"
        return TestRunResult(
            files: [],
            total_passed: 0,
            total_failed: 0,
            total_skipped: 0,
            total_pending: 0,
            total_timed_out: 0,
            total_duration_ms: 0
        )

    val config = SequentialContainerConfig(
        base_image: "simple-test-full:latest",
        runtime: runtime,
        memory_mb: 512,
        cpu_cores: 1.0,
        network: "none",
        cleanup_on_error: true,
        workspace: "."
    )

    # Run tests sequentially in containers
    print "SEQUENTIAL CONTAINER MODE"
    print "Runtime: {runtime}"
    print ""

    val results = sequential_container_run_all_tests(files, config)

    # Aggregate results
    var total_passed = 0
    var total_failed = 0
    var total_skipped = 0
    var total_pending = 0
    var total_timed_out = 0
    var total_duration_ms = 0

    for result in results:
        print_result(result, options.format)

        total_passed = total_passed + result.passed
        total_failed = total_failed + result.failed
        total_skipped = total_skipped + result.skipped
        total_pending = total_pending + result.pending
        if result.timed_out:
            total_timed_out = total_timed_out + 1
        total_duration_ms = total_duration_ms + result.duration_ms

        if options.fail_fast and (result.failed > 0 or result.timed_out):
            print ""
            print "Stopping early (--fail-fast)"
            break

    TestRunResult(
        files: results,
        total_passed: total_passed,
        total_failed: total_failed,
        total_skipped: total_skipped,
        total_pending: total_pending,
        total_timed_out: total_timed_out,
        total_duration_ms: total_duration_ms
    )

fn mode_to_str(mode: TestExecutionMode) -> text:
    match mode:
        case TestExecutionMode.Interpreter:
            "interpreter"
        case TestExecutionMode.Smf:
            "smf"
        case TestExecutionMode.Native:
            "native"
        case TestExecutionMode.Composite(spec):
            spec

fn run_single_test(file_path: text, options: TestOptions) -> TestFileResult:
    # Set runtime mode env var so skip_it/skip_on_interpreter can detect mode
    val mode_str = mode_to_str(options.mode)
    env_set("SIMPLE_RUNTIME_MODE", mode_str)

    # DI system test lock: block DI modifications in system tests unless @di_test
    val is_system = file_path.contains("/system/") or file_path.contains("/feature/")
    if is_system:
        env_set("SIMPLE_SYSTEM_TEST", "1")
    else:
        env_set("SIMPLE_SYSTEM_TEST", "0")
    if is_system and file_has_di_test_tag(file_path):
        env_set("SIMPLE_DI_TEST", "1")
    else:
        env_set("SIMPLE_DI_TEST", "0")

    # Override timeout for advanced testing modes
    var effective_options = options
    if options.fuzz_mode:
        val fuzz_timeout = options.fuzz_time_secs + 30
        if fuzz_timeout > effective_options.timeout:
            effective_options.timeout = fuzz_timeout
    if options.chaos_mode:
        var chaos_timeout = options.chaos_trials * 2
        if chaos_timeout < 300:
            chaos_timeout = 300
        if chaos_timeout > effective_options.timeout:
            effective_options.timeout = chaos_timeout

    # G1: Use safe mode with resource limits when enabled
    if effective_options.safe_mode:
        return run_test_file_safe_mode(file_path, effective_options)
    match effective_options.mode:
        case TestExecutionMode.Interpreter:
            run_test_file_interpreter(file_path, effective_options)
        case TestExecutionMode.Smf:
            run_test_file_smf(file_path, effective_options)
        case TestExecutionMode.Native:
            run_test_file_native(file_path, effective_options)
        case TestExecutionMode.Composite(spec):
            run_test_file_composite(file_path, effective_options, spec)

# =========================================================================
# C4: Individual test result parsing from output
# =========================================================================

fn parse_individual_results(output: text, file_path: text) -> [IndividualTestResult]:
    var results: [IndividualTestResult] = []
    val lines = output.split("\n")
    var current_group = ""

    for line in lines:
        val clean = strip_ansi(line.trim())
        if clean == "":
            continue

        # Detect group headers (indented describe/context blocks)
        # Format: "GroupName" followed by indented tests
        if clean.ends_with(":") and not clean.starts_with(" ") and not clean.contains(" "):
            current_group = clean[:-1]
            continue

        # Parse individual test lines: checkmark/cross + name
        if clean.len() > 2:
            val prefix = clean[0:2]
            if prefix == "  ":
                # Indented line might be a test result
                val inner = clean.trim()
                if inner.len() > 2:
                    val marker = inner[0:3]
                    if marker.contains("✓") or marker.contains("✔") or inner.starts_with("PASS"):
                        val name = extract_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Passed,
                            error_message: ""
                        ))
                    elif marker.contains("✗") or marker.contains("✘") or inner.starts_with("FAIL"):
                        val name = extract_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Failed,
                            error_message: ""
                        ))
                    elif marker.contains("○") or marker.contains("◯") or inner.starts_with("SKIP"):
                        val name = extract_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Skipped,
                            error_message: ""
                        ))
                    elif inner.starts_with("PEND") or inner.contains("... pending"):
                        val name = extract_pending_test_name(inner)
                        results.push(IndividualTestResult(
                            name: name,
                            group: current_group,
                            status: TestStatus.Pending,
                            error_message: ""
                        ))

    results

fn extract_pending_test_name(line: text) -> text:
    # Parse "it {name} ... pending (waiting on: ...)" or "PEND {name}"
    var rest = line.trim()
    if rest.starts_with("it "):
        rest = rest[3:]
    elif rest.starts_with("PEND "):
        rest = rest[5:]
    # Strip " ... pending (...)" suffix
    val pending_idx = rest.index_of(" ... pending") ?? -1
    if pending_idx > 0:
        return rest[0:pending_idx]
    return rest

fn extract_test_name(line: text) -> text:
    # Strip leading marker characters and whitespace
    var i = 0
    while i < line.len():
        val ch = line[i:i + 1]
        if ch != " " and ch != "✓" and ch != "✔" and ch != "✗" and ch != "✘" and ch != "○" and ch != "◯":
            break
        i = i + 1
    # Also skip "PASS ", "FAIL ", "SKIP ", "PEND " prefixes
    val rest = line[i:].trim()
    if rest.starts_with("PASS ") or rest.starts_with("FAIL ") or rest.starts_with("SKIP ") or rest.starts_with("PEND "):
        return rest[5:].trim()
    rest

# =========================================================================
# M1: Derive test category from file path
# =========================================================================

fn categorize_test_file(file_path: text) -> text:
    if file_path.contains("test/unit/"):
        return "Unit"
    if file_path.contains("test/feature/"):
        return "Feature"
    if file_path.contains("test/integration/"):
        return "Integration"
    if file_path.contains("test/system/"):
        return "System"
    if file_path.contains("test/benchmark/"):
        return "Benchmark"
    "Unknown"

# =========================================================================
# Closure Warnings Display
# =========================================================================

fn display_closure_warnings():
    """Display closure variable capture warnings if any were detected."""
    if not closure_warnings_has():
        return

    print ""
    print "=========================================="
    print "CLOSURE CAPTURE WARNINGS"
    print "=========================================="
    print ""

    val warnings = closure_warnings_get()
    var i = 0
    while i < warnings.len():
        print "[CLOSURE WARNING] {warnings[i]}"
        i = i + 1

    print ""
    print "For more information, see MEMORY.md"
    print "=========================================="

# =========================================================================
# Call Graph Warnings Display
# =========================================================================

fn display_call_graph_warnings():
    """Display call graph recursion warnings if any were detected."""
    if not call_graph_warnings_has():
        return

    print ""
    print "=========================================="
    print "CALL GRAPH WARNINGS"
    print "=========================================="
    print ""

    val warnings = call_graph_warnings_get()
    var i = 0
    for w in warnings:
        print "[CALL GRAPH] {w}"
        i = i + 1

    print ""
    print "Recursion is banned in @safe context"
    print "=========================================="

# =========================================================================
# Main Entry Point
# =========================================================================

fn main() -> i64:
    # Set recursion guard so child processes use Rust runner
    env_set("SIMPLE_TEST_RUNNER_RUST", "1")

    val all_args = get_cli_args()

    # Filter out "test" command if present (from CLI dispatch)
    var args: [str] = []
    var skip_first = all_args.len() > 0 and all_args[0] == "test"
    var i = 0
    if skip_first:
        i = 1
    while i < all_args.len():
        args.push(all_args[i])
        i = i + 1

    val options = parse_test_args(args)

    # Handle run management commands
    if has_run_management_flag(args):
        return handle_run_management(args)

    # Load test configuration to determine what to run
    val config = TestConfig__load()

    # Determine which test types to run
    var run_spec = config.run_spec_tests
    var run_sdoc = config.run_sdoctests

    # Backward compatibility: --sdoctest flag runs ONLY sdoctests
    if options.sdoctest:
        run_spec = false
        run_sdoc = true

    # Handle sdoctest-only mode (backward compatible)
    if run_sdoc and not run_spec:
        print "Simple SDoctest Runner v0.4.0"
        print ""
        val sdoctest_config = load_sdoctest_config("config/sdoctest.sdn")
        val sdoctest_result = run_sdoctest_mode(options, sdoctest_config)
        print_sdoctest_summary(sdoctest_result)
        # Write result database and report
        if not options.list and not options.no_db and sdoctest_result.total > 0:
            write_sdoctest_db(sdoctest_result, "doc/test/sdoctest_db.sdn")
            generate_sdoctest_report(sdoctest_result, "doc/test/sdoctest_result.md")
        if sdoctest_result.is_ok():
            return 0
        return 1

    # Handle combined mode (both spec tests and sdoctests)
    if run_spec and run_sdoc:
        return run_combined_mode(options, config)

    # Handle spec-only mode (default)
    if run_spec:
        print "Simple Test Runner v0.4.0"
        print ""

        val run_start = time_now_unix_micros()
        val result = run_tests(options)
        val run_end = time_now_unix_micros()

        print_summary(result, options.format)

        # Display closure warnings if enabled
        if options.closure_warnings:
            display_closure_warnings()
            display_call_graph_warnings()

        # Collect and report coverage if enabled
        var coverage_passed = true
        if options.coverage:
            val cov_data = collect_coverage()
            print_coverage_summary(cov_data)
            generate_coverage_reports(cov_data)
            print_coverage_delta(cov_data)
            val threshold = get_coverage_threshold()
            if threshold > 0:
                coverage_passed = check_coverage_threshold(cov_data, threshold)

        # Write test DB
        if not options.list and not options.no_db and result.files.len() > 0:
            update_test_database(result, run_start, run_end)

        if result.is_ok() and coverage_passed:
            return 0
        return 1

    # If neither spec nor sdoc is enabled, print error
    print "Error: No test types enabled. Check config/simple.test.sdn configuration."
    print "  run_spec_tests: {config.run_spec_tests}"
    print "  run_sdoctests: {config.run_sdoctests}"
    return 1

fn has_run_management_flag(args: [text]) -> bool:
    for arg in args:
        if arg == "--list-runs" or arg == "--cleanup-runs" or arg == "--prune-runs" or arg.starts_with("--prune-runs="):
            return true
    false

fn handle_run_management(args: [text]) -> i64:
    val db_result = load_test_db_compat()
    if not db_result.is_ok():
        print "Error: Could not load test database: {db_result.unwrap_err()}"
        return 1

    val db = db_result.unwrap()

    for arg in args:
        if arg == "--list-runs":
            val status_filter = get_flag_value(args, "--runs-status") ?? "all"
            val runs = db.list_runs(status_filter)
            print "Test Runs ({runs.len()}):"
            print ""
            print "| Run ID | Status | Tests | Passed | Failed | Start Time |"
            print "|--------|--------|-------|--------|--------|------------|"
            for r in runs:
                print "| {r.run_id} | {r.status} | {r.test_count} | {r.passed} | {r.failed} | {r.start_time} |"
            return 0

        if arg == "--cleanup-runs":
            db.cleanup_stale_runs(2)
            val save_result = db.save()
            if not save_result.is_ok():
                print "Error saving: {save_result.unwrap_err()}"
                return 1
            print "Stale runs cleaned up."
            return 0

        if arg.starts_with("--prune-runs="):
            val count_str = arg[13:]
            val keep = count_str.to_int_or(50)
            db.prune_runs(keep)
            val save_result = db.save()
            if not save_result.is_ok():
                print "Error saving: {save_result.unwrap_err()}"
                return 1
            print "Runs pruned (keeping {keep})."
            return 0

    0

fn get_flag_value(args: [text], flag: text) -> text?:
    var i = 0
    while i < args.len():
        if args[i] == flag and i + 1 < args.len():
            return Some(args[i + 1])
        if args[i].starts_with(flag + "="):
            return Some(args[i][flag.len() + 1:])
        i = i + 1
    nil

# =========================================================================
# Test Database Update (C5: per-test granularity, M1: categorization)
# =========================================================================

fn update_test_database(result: TestRunResult, run_start: i64, run_end: i64):
    # FIXME: Database loading causes semantic error in runtime
    # Error: "semantic: method `split` not found on type `enum`"
    # Location: lib.database.test_extended loading chain via SdnDatabase.load()
    # The error appears when loading test_db.sdn during database initialization.
    #
    # To reproduce: uncomment the load_test_db_compat() call below
    # To fix: Investigate SdnDatabase.load() and ensure all Option values
    # are unwrapped before calling string methods like .split()
    #
    # Impact: Test database and documentation are not updated after test runs
    # Workaround: Tests still execute correctly, just no persistence
    #
    # TODO: Fix the underlying issue in database library
    pass

# =========================================================================
# M3/H6: Update feature status from test results
# =========================================================================

fn update_features_from_tests(fdb: FeatureDatabase, result: TestRunResult):
    # Build set of failed file paths
    var failed_files: Dict<text, bool> = {}
    for file_result in result.files:
        if file_result.failed > 0 or file_result.timed_out:
            failed_files[file_result.path] = true

    # Get all features and update status based on test results
    val all_features = fdb.all_features()
    for feature in all_features:
        val spec = feature.spec_file
        if spec != "" and spec.ends_with(".spl"):
            if failed_files.contains_key(spec):
                # Test failed - mark feature as failed
                fdb.update_feature_status(feature.id, FeatureStatus.Failed)
            else:
                # Check if spec was in test results and passed
                for file_result in result.files:
                    if file_result.path == spec and file_result.failed == 0 and not file_result.timed_out:
                        # Test passed - mark feature as done if not already
                        if feature.pure_status != FeatureStatus.Done:
                            fdb.update_feature_status(feature.id, FeatureStatus.Done)

# =========================================================================
# Exports
# =========================================================================

export propagate_env_vars, run_tests, mode_to_str, run_single_test
export parse_individual_results, extract_pending_test_name, categorize_test_file
export update_test_database, update_features_from_tests
export handle_run_management, has_run_management_flag
export display_closure_warnings, display_call_graph_warnings
export main
