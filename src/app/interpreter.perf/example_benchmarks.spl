# Example Benchmarks
#
# Demonstrates benchmarking the performance optimizations.
# Run these to measure the impact of each optimization.

from benchmark import {
    Benchmark,
    BenchmarkSuite,
    BenchmarkConfig,
    run_benchmark,
    compare_benchmarks
}
from ../collections import {PersistentDict, PersistentVec}
from ../lazy import {LazySeq, Lazy, naturals, fibonacci}
from ../core/symbol import {intern, resolve}

export run_all_benchmarks, run_collection_benchmarks, run_lazy_benchmarks
export run_symbol_benchmarks, run_comparison_benchmarks

# ============================================================================
# Symbol Interning Benchmarks
# ============================================================================

fn run_symbol_benchmarks() -> [BenchmarkResult]:
    """Benchmark symbol interning vs string operations."""
    var suite = BenchmarkSuite.new("Symbol Interning")
        .with_config(BenchmarkConfig.quick())

    # Intern strings
    suite.add_fn("intern_1000_strings", \:
        for i in 0..1000:
            intern("variable_{i}")
    )

    # Resolve symbols
    suite.add_fn("resolve_1000_symbols", \:
        for i in 0..1000:
            val sym = intern("var_{i}")
            resolve(sym)
    )

    # Compare: symbol lookup vs string comparison
    suite.add_fn("symbol_equality_1000", \:
        val sym1 = intern("test_symbol")
        val sym2 = intern("test_symbol")
        for _ in 0..1000:
            val _ = sym1.id == sym2.id
    )

    suite.add_fn("string_equality_1000", \:
        val s1 = "test_symbol_with_longer_name"
        val s2 = "test_symbol_with_longer_name"
        for _ in 0..1000:
            val _ = s1 == s2
    )

    suite.run()

# ============================================================================
# Persistent Collection Benchmarks
# ============================================================================

fn run_collection_benchmarks() -> [BenchmarkResult]:
    """Benchmark persistent collections."""
    var suite = BenchmarkSuite.new("Persistent Collections")
        .with_config(BenchmarkConfig.quick())

    # PersistentDict operations
    suite.add_fn("dict_insert_100", \:
        var dict = PersistentDict<i64, i64>.new()
        for i in 0..100:
            dict = dict.set(i, i * 2)
    )

    suite.add_fn("dict_lookup_100", \:
        var dict = PersistentDict<i64, i64>.new()
        for i in 0..100:
            dict = dict.set(i, i * 2)

        for i in 0..100:
            val _ = dict.get(i)
    )

    suite.add_fn("dict_update_100", \:
        var dict = PersistentDict<i64, i64>.new()
        for i in 0..100:
            dict = dict.set(i, i)

        # Update all values
        for i in 0..100:
            dict = dict.set(i, i * 3)
    )

    # PersistentVec operations
    suite.add_fn("vec_push_100", \:
        var vec = PersistentVec<i64>.new()
        for i in 0..100:
            vec = vec.push(i)
    )

    suite.add_fn("vec_random_access_100", \:
        var vec = PersistentVec<i64>.new()
        for i in 0..100:
            vec = vec.push(i)

        for i in 0..100:
            val _ = vec.get(i)
    )

    suite.add_fn("vec_map_100", \:
        var vec = PersistentVec<i64>.new()
        for i in 0..100:
            vec = vec.push(i)

        val _ = vec.map(\x: x * 2)
    )

    # Structural sharing test
    suite.add_fn("dict_branching_versions", \:
        var base = PersistentDict<i64, i64>.new()
        for i in 0..50:
            base = base.set(i, i)

        # Create 10 branches from the same base
        var versions: [PersistentDict<i64, i64>] = []
        for i in 0..10:
            versions = versions.push(base.set(1000 + i, i))
    )

    suite.run()

# ============================================================================
# Lazy Evaluation Benchmarks
# ============================================================================

fn run_lazy_benchmarks() -> [BenchmarkResult]:
    """Benchmark lazy evaluation."""
    var suite = BenchmarkSuite.new("Lazy Evaluation")
        .with_config(BenchmarkConfig.quick())

    # Lazy value creation (should be fast)
    suite.add_fn("lazy_create_100", \:
        for i in 0..100:
            val _ = Lazy.new(\: i * i)
    )

    # Lazy value forcing
    suite.add_fn("lazy_force_100", \:
        var lazies: [Lazy<i64>] = []
        for i in 0..100:
            lazies = lazies.push(Lazy.new(\: i * 2))

        for l in lazies:
            var lazy = l
            val _ = lazy.force()
    )

    # Lazy sequence - take from infinite
    suite.add_fn("lazy_seq_take_100", \:
        val first100 = naturals().take(100).to_array()
    )

    # Lazy sequence - filter and take
    suite.add_fn("lazy_seq_filter_take_50", \:
        val evens = naturals()
            .filter(\x: x % 2 == 0)
            .take(50)
            .to_array()
    )

    # Lazy sequence - map and take
    suite.add_fn("lazy_seq_map_take_100", \:
        val squares = naturals()
            .map(\x: x * x)
            .take(100)
            .to_array()
    )

    # Fibonacci sequence
    suite.add_fn("fibonacci_first_30", \:
        val fibs = fibonacci().take(30).to_array()
    )

    # Lazy chaining
    suite.add_fn("lazy_chain_operations", \:
        val result = naturals()
            .map(\x: x * 2)
            .filter(\x: x % 4 == 0)
            .map(\x: x + 1)
            .take(25)
            .to_array()
    )

    suite.run()

# ============================================================================
# Comparison Benchmarks
# ============================================================================

fn run_comparison_benchmarks():
    """Run benchmarks comparing optimized vs naive implementations."""

    # Compare eager vs lazy evaluation
    print("=== Eager vs Lazy Comparison ===")

    val eager_result = run_benchmark("eager_filter_map", 100, \:
        var arr: [i64] = []
        for i in 0..1000:
            arr = arr.push(i)

        # Eager: compute all, then filter, then map
        var filtered: [i64] = []
        for x in arr:
            if x % 2 == 0:
                filtered = filtered.push(x)

        var mapped: [i64] = []
        for x in filtered:
            mapped = mapped.push(x * 2)

        # Take only first 10
        val result = mapped[0:10]
    )

    val lazy_result = run_benchmark("lazy_filter_map", 100, \:
        # Lazy: only compute what's needed
        val result = naturals()
            .filter(\x: x % 2 == 0)
            .map(\x: x * 2)
            .take(10)
            .to_array()
    )

    val comparison = compare_benchmarks(eager_result, lazy_result)
    print(comparison.summary())
    print("")

    # Compare mutable dict vs persistent dict
    print("=== Mutable vs Persistent Dict ===")

    val mutable_result = run_benchmark("mutable_dict_ops", 100, \:
        var dict: Dict<i64, i64> = {}
        for i in 0..100:
            dict[i] = i * 2

        for i in 0..100:
            val _ = dict.get(i)
    )

    val persistent_result = run_benchmark("persistent_dict_ops", 100, \:
        var dict = PersistentDict<i64, i64>.new()
        for i in 0..100:
            dict = dict.set(i, i * 2)

        for i in 0..100:
            val _ = dict.get(i)
    )

    val dict_comparison = compare_benchmarks(mutable_result, persistent_result)
    print(dict_comparison.summary())

# ============================================================================
# Run All Benchmarks
# ============================================================================

fn run_all_benchmarks():
    """Run all benchmark suites and print results."""
    print("========================================")
    print("  Performance Optimization Benchmarks")
    print("========================================")
    print("")

    # Symbol interning
    print("--- Symbol Interning ---")
    val symbol_results = run_symbol_benchmarks()
    for result in symbol_results:
        print("  {result}")
    print("")

    # Collections
    print("--- Persistent Collections ---")
    val collection_results = run_collection_benchmarks()
    for result in collection_results:
        print("  {result}")
    print("")

    # Lazy evaluation
    print("--- Lazy Evaluation ---")
    val lazy_results = run_lazy_benchmarks()
    for result in lazy_results:
        print("  {result}")
    print("")

    # Comparisons
    run_comparison_benchmarks()

    print("")
    print("========================================")
    print("  Benchmarks Complete")
    print("========================================")

# ============================================================================
# Quick Smoke Test
# ============================================================================

fn quick_smoke_test():
    """Quick sanity check that all benchmarks run."""
    print("Running quick smoke test...")

    # Just run one of each type
    val _ = run_benchmark("smoke_symbol", 5, \:
        intern("test")
    )

    val _ = run_benchmark("smoke_dict", 5, \:
        var d = PersistentDict<i64, i64>.new()
        d = d.set(1, 2)
    )

    val _ = run_benchmark("smoke_lazy", 5, \:
        naturals().take(10).to_array()
    )

    print("Smoke test passed!")
