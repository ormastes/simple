# Core duplication detection engine using Karp-Rabin rolling hash

use app.duplicate_check.config.{DuplicationConfig}
use app.duplicate_check.tokenizer.{SimpleToken, tokenize, token_to_string}
use app.duplicate_check.features.{FeatureVec, extract_token_frequencies, build_feature_vector, cosine_similarity}
use app.duplicate_check.debug.{debug_log, debug_log_progress}
use app.io.mod.{file_read, shell}

struct DuplicateBlock:
    file: text
    line_start: i64
    line_end: i64
    token_count: i64
    hash_value: i64
    code: text

struct DuplicateGroup:
    blocks: [DuplicateBlock]
    occurrences: i64
    total_lines: i64
    impact_score: i64

fn compute_token_hash(token_str: text) -> i64:
    """Compute hash of a single token string."""
    val bytes = token_str.bytes()
    var char_code = 0
    var j = 0
    while j < bytes.len():
        char_code = char_code + bytes[j]
        j = j + 1
    char_code

fn rolling_hash_cached(tokens: [SimpleToken], start: i64, window: i64, token_str_cache: {text: text}) -> i64:
    """Compute rolling hash for token window using pre-computed token strings."""
    var hash = 0
    val base = 31
    val mod_val = 1000000007

    var i = 0
    while i < window and start + i < tokens.len():
        val token_str = token_str_cache["{start + i}"]
        val char_code = compute_token_hash(token_str)

        hash = (hash * base + char_code) % mod_val
        i = i + 1

    hash

fn rolling_hash(tokens: [SimpleToken], start: i64, window: i64) -> i64:
    """Compute rolling hash for token window. Legacy version without cache."""
    var hash = 0
    val base = 31
    val mod_val = 1000000007

    var i = 0
    while i < window and start + i < tokens.len():
        val token = tokens[start + i]
        val token_str = token_to_string(token)
        val char_code = compute_token_hash(token_str)

        hash = (hash * base + char_code) % mod_val
        i = i + 1

    hash

fn extract_code_snippet(tokens: [SimpleToken], start: i64, end: i64) -> text:
    var code = ""
    var i = start
    while i < end and i < tokens.len():
        code = code + tokens[i].value
        i = i + 1
    code

fn find_duplicates_in_file(file_path: text, config: DuplicationConfig) -> [DuplicateBlock]:
    val content = file_read(file_path)
    val tokens = tokenize(content, config)

    var token_str_cache = {}
    var j = 0
    while j < tokens.len():
        token_str_cache["{j}"] = token_to_string(tokens[j])
        j = j + 1

    var batches = []
    var current_batch = []
    val batch_size = 100
    var i = 0

    while i + config.min_tokens <= tokens.len():
        val hash_val = rolling_hash_cached(tokens, i, config.min_tokens, token_str_cache)

        val start_line = tokens[i].line
        var end_idx = i + config.min_tokens
        val end_line = if end_idx < tokens.len(): tokens[end_idx].line else: tokens[tokens.len() - 1].line

        val line_count = end_line - start_line + 1

        if line_count >= config.min_lines:
            val code = extract_code_snippet(tokens, i, end_idx)

            val new_block = DuplicateBlock(
                file: file_path,
                line_start: start_line,
                line_end: end_line,
                token_count: config.min_tokens,
                hash_value: hash_val,
                code: code
            )

            current_batch = current_batch + [new_block]

            if current_batch.len() >= batch_size:
                batches = batches + [current_batch]
                current_batch = []

        i = i + 1

    if current_batch.len() > 0:
        batches = batches + [current_batch]

    var blocks = []
    for batch in batches:
        blocks = blocks + batch

    blocks

fn group_by_hash(all_blocks: [DuplicateBlock], config: DuplicationConfig) -> [DuplicateGroup]:
    var hash_map = {}

    for block in all_blocks:
        val hash_key = "{block.hash_value}"

        if hash_map.contains_key(hash_key):
            val existing = hash_map[hash_key]
            hash_map[hash_key] = existing + [block]
        else:
            hash_map[hash_key] = [block]

    var groups = []

    for hash_key in hash_map.keys():
        val blocks = hash_map[hash_key]

        if blocks.len() >= 2:
            var total_lines = 0
            for block in blocks:
                total_lines = total_lines + (block.line_end - block.line_start + 1)

            val impact = blocks.len() * total_lines

            if impact >= config.min_impact:
                groups = groups + [DuplicateGroup(
                    blocks: blocks,
                    occurrences: blocks.len(),
                    total_lines: total_lines,
                    impact_score: impact
                )]

    groups

fn sort_by_impact(groups: [DuplicateGroup]) -> [DuplicateGroup]:
    var sorted = groups
    var n = sorted.len()

    var i = 0
    while i < n - 1:
        var j = 0
        while j < n - i - 1:
            if sorted[j].impact_score < sorted[j + 1].impact_score:
                val temp = sorted[j]
                sorted[j] = sorted[j + 1]
                sorted[j + 1] = temp
            j = j + 1
        i = i + 1

    sorted

fn collect_files(path: text, config: DuplicationConfig) -> [text]:
    var files = []

    if path.ends_with(".spl"):
        files = [path]
    else:
        val result = shell("find {path} -name '*.spl' 2>/dev/null || true")
        val output = result.stdout
        val lines = output.split("\n")

        for line in lines:
            val trimmed = line.trim()
            if trimmed.len() > 0:
                var excluded = false
                for pattern in config.exclude_patterns:
                    var clean_pattern = ""
                    var i = 0
                    while i < pattern.len():
                        val ch = pattern[i:i+1]
                        if ch != "*" and ch != "/":
                            clean_pattern = clean_pattern + ch
                        i = i + 1

                    if trimmed.contains(clean_pattern):
                        excluded = true
                        break

                if not excluded:
                    files = files + [trimmed]

    files

fn refine_groups_with_similarity(exact_groups: [DuplicateGroup], all_blocks: [DuplicateBlock], config: DuplicationConfig) -> [DuplicateGroup]:
    """Refine hash-based groups using cosine similarity to catch fuzzy duplicates."""
    print "Extracting features from {all_blocks.len()} blocks..."
    var file_cache = {}
    var feature_vector_map = {}

    var block_idx = 0
    var total_blocks = all_blocks.len()
    for block in all_blocks:
        if block_idx % 20 == 0 or block_idx == total_blocks - 1:
            print "  Extracted features: {block_idx}/{total_blocks}"

        var tokens = []
        if file_cache.contains_key(block.file):
            tokens = file_cache[block.file]
        else:
            val content = file_read(block.file)
            tokens = tokenize(content, config)
            file_cache[block.file] = tokens

        var token_start = 0
        var found_start = false
        var i = 0
        while i < tokens.len():
            if tokens[i].line == block.line_start:
                token_start = i
                found_start = true
                break
            i = i + 1

        if found_start:
            var token_end = token_start + block.token_count
            if token_end > tokens.len():
                token_end = tokens.len()

            val freq_map = extract_token_frequencies(tokens, token_start, token_end)
            val feature_vec = build_feature_vector(block_idx, freq_map)
            feature_vector_map["{block_idx}"] = feature_vec

        block_idx = block_idx + 1

    print "Extracted {feature_vector_map.keys().len()} feature vectors"

    var refined_groups = []

    for group in exact_groups:
        refined_groups = refined_groups + [group]

    print "Starting pairwise comparison of {all_blocks.len()} blocks..."
    var comparison_count = 0
    var match_count = 0
    var i = 0
    while i < all_blocks.len():
        if i % 10 == 0:
            print "  Comparing block {i}/{all_blocks.len()} ({comparison_count} comparisons, {match_count} matches)"
        var j = i + 1
        while j < all_blocks.len():
            comparison_count = comparison_count + 1
            val block1 = all_blocks[i]
            val block2 = all_blocks[j]

            val same_hash = block1.hash_value == block2.hash_value
            if same_hash:
                j = j + 1
                continue

            val key_i = "{i}"
            val key_j = "{j}"
            val has_vec_i = feature_vector_map.contains_key(key_i)
            val has_vec_j = feature_vector_map.contains_key(key_j)
            val both_have_vecs = has_vec_i and has_vec_j

            if both_have_vecs:
                val feature_vec1 = feature_vector_map[key_i]
                val feature_vec2 = feature_vector_map[key_j]

                val similarity = cosine_similarity(feature_vec1, feature_vec2)

                val above_threshold = similarity >= config.similarity_threshold
                if above_threshold:
                    match_count = match_count + 1
                    val new_group = DuplicateGroup(
                        blocks: [block1, block2],
                        occurrences: 2,
                        total_lines: (block1.line_end - block1.line_start + 1) + (block2.line_end - block2.line_start + 1),
                        impact_score: 2 * ((block1.line_end - block1.line_start + 1) + (block2.line_end - block2.line_start + 1))
                    )

                    val meets_impact = new_group.impact_score >= config.min_impact
                    if meets_impact:
                        refined_groups = refined_groups + [new_group]

            j = j + 1
        i = i + 1

    print "Completed {comparison_count} comparisons, found {match_count} similar pairs"
    refined_groups

fn find_duplicates(files: [text], config: DuplicationConfig) -> [DuplicateGroup]:
    var all_blocks = []

    print "Scanning {files.len()} files..."

    for file in files:
        val blocks = find_duplicates_in_file(file, config)
        all_blocks = all_blocks + blocks

    print "Found {all_blocks.len()} potential duplicate blocks"

    val exact_groups = group_by_hash(all_blocks, config)

    val refined_groups = if config.use_cosine_similarity:
        print "Refining with cosine similarity..."
        val result = refine_groups_with_similarity(exact_groups, all_blocks, config)
        result
    else:
        exact_groups

    val sorted = sort_by_impact(refined_groups)

    print "Grouped into {sorted.len()} duplicate groups"

    sorted

export DuplicateBlock, DuplicateGroup, find_duplicates, collect_files
