# Core duplication detection engine using Karp-Rabin rolling hash

use app.duplicate_check.config.{DuplicationConfig}
use app.duplicate_check.tokenizer.{SimpleToken, tokenize, token_to_string}
use app.duplicate_check.features.{FeatureVec, extract_token_frequencies, build_feature_vector, cosine_similarity}
use app.duplicate_check.debug.{debug_log, debug_log_progress}
use app.duplicate_check.cache.{TokenCacheManager, get_tokens_cached}
use app.io.mod.{file_read, shell}

# StringInterner for efficient token code computation
struct TokenInterner:
    strings: {text: i64}
    reverse: {i64: text}
    next_id: i64

fn new_token_interner() -> TokenInterner:
    TokenInterner(strings: {}, reverse: {}, next_id: 0)

fn intern_token(interner: TokenInterner, token_str: text) -> i64:
    if interner.strings.contains_key(token_str):
        interner.strings[token_str]
    else:
        val id = interner.next_id
        interner.strings[token_str] = id
        interner.reverse[id] = token_str
        interner.next_id = interner.next_id + 1
        id

struct DuplicateBlock:
    file: text
    line_start: i64
    line_end: i64
    token_count: i64
    hash_value: i64
    code: text

struct DuplicateGroup:
    blocks: [DuplicateBlock]
    occurrences: i64
    total_lines: i64
    impact_score: i64

fn rolling_hash(tokens: [SimpleToken], start: i64, window: i64) -> i64:
    """Compute rolling hash for token window with inlined character hashing."""
    var hash = 0
    val base = 31
    val mod_val = 1000000007

    var i = 0
    while i < window and start + i < tokens.len():
        val token = tokens[start + i]
        val token_str = token_to_string(token)

        val bytes = token_str.bytes()
        var char_code = 0
        var j = 0
        while j < bytes.len():
            char_code = char_code + bytes[j]
            j = j + 1

        hash = (hash * base + char_code) % mod_val
        i = i + 1

    hash

fn extract_code_snippet(tokens: [SimpleToken], start: i64, end: i64) -> text:
    var code = ""
    var i = start
    while i < end and i < tokens.len():
        code = code + tokens[i].value
        i = i + 1
    code

fn compute_token_code(token: SimpleToken) -> i64:
    """Compute hash code for a single token."""
    val token_str = token_to_string(token)
    val bytes = token_str.bytes()
    var char_code = 0
    var j = 0
    while j < bytes.len():
        char_code = char_code + bytes[j]
        j = j + 1
    char_code

fn power_mod(base: i64, exp: i64, mod_val: i64) -> i64:
    """Compute base^exp % mod_val efficiently."""
    var result = 1
    var b = base % mod_val
    var e = exp

    while e > 0:
        if e % 2 == 1:
            result = (result * b) % mod_val
        b = (b * b) % mod_val
        e = e / 2

    result

fn find_duplicates_in_file(file_path: text, config: DuplicationConfig, cache_manager: TokenCacheManager) -> [DuplicateBlock]:
    val tokens = get_tokens_cached(cache_manager, file_path, fn(path: text) -> [SimpleToken]:
        val content = file_read(path)
        tokenize(content, config)
    )

    if tokens.len() < config.min_tokens:
        return []

    val base = 31
    val mod_val = 1000000007

    var token_codes = []
    var idx = 0
    while idx < tokens.len():
        token_codes = token_codes + [compute_token_code(tokens[idx])]
        idx = idx + 1

    val window_size = config.min_tokens
    val base_power = power_mod(base, window_size - 1, mod_val)

    var hash = 0
    var j = 0
    while j < window_size:
        hash = (hash * base + token_codes[j]) % mod_val
        j = j + 1

    var batches = []
    var current_batch = []
    val batch_size = 100
    var i = 0

    while i + config.min_tokens <= tokens.len():
        if i > 0:
            val old_code = token_codes[i - 1]
            val new_code = token_codes[i + window_size - 1]

            hash = (hash - (old_code * base_power) % mod_val + mod_val) % mod_val
            hash = (hash * base + new_code) % mod_val

        val start_line = tokens[i].line
        var end_idx = i + config.min_tokens
        val end_line = if end_idx < tokens.len(): tokens[end_idx].line else: tokens[tokens.len() - 1].line

        val line_count = end_line - start_line + 1

        if line_count >= config.min_lines:
            val code = extract_code_snippet(tokens, i, end_idx)

            val new_block = DuplicateBlock(
                file: file_path,
                line_start: start_line,
                line_end: end_line,
                token_count: config.min_tokens,
                hash_value: hash,
                code: code
            )

            current_batch = current_batch + [new_block]

            if current_batch.len() >= batch_size:
                batches = batches + [current_batch]
                current_batch = []

        i = i + 1

    if current_batch.len() > 0:
        batches = batches + [current_batch]

    var blocks = []
    for batch in batches:
        blocks = blocks + batch

    blocks

fn group_by_hash(all_blocks: [DuplicateBlock], config: DuplicationConfig) -> [DuplicateGroup]:
    var hash_map = {}

    for block in all_blocks:
        val hash_key = "{block.hash_value}"

        if hash_map.contains_key(hash_key):
            val existing = hash_map[hash_key]
            hash_map[hash_key] = existing + [block]
        else:
            hash_map[hash_key] = [block]

    var groups = []

    for hash_key in hash_map.keys():
        val blocks = hash_map[hash_key]

        if blocks.len() >= 2:
            var total_lines = 0
            for block in blocks:
                total_lines = total_lines + (block.line_end - block.line_start + 1)

            val impact = blocks.len() * total_lines

            if impact >= config.min_impact:
                groups = groups + [DuplicateGroup(
                    blocks: blocks,
                    occurrences: blocks.len(),
                    total_lines: total_lines,
                    impact_score: impact
                )]

    groups

fn sort_by_impact(groups: [DuplicateGroup]) -> [DuplicateGroup]:
    var sorted = groups
    var n = sorted.len()

    var i = 0
    while i < n - 1:
        var j = 0
        while j < n - i - 1:
            if sorted[j].impact_score < sorted[j + 1].impact_score:
                val temp = sorted[j]
                sorted[j] = sorted[j + 1]
                sorted[j + 1] = temp
            j = j + 1
        i = i + 1

    sorted

fn collect_files(path: text, config: DuplicationConfig) -> [text]:
    var files = []

    if path.ends_with(".spl"):
        files = [path]
    else:
        val result = shell("find {path} -name '*.spl' 2>/dev/null || true")
        val output = result.stdout
        val lines = output.split("\n")

        for line in lines:
            val trimmed = line.trim()
            if trimmed.len() > 0:
                var excluded = false
                for pattern in config.exclude_patterns:
                    var clean_pattern = ""
                    var i = 0
                    while i < pattern.len():
                        val ch = pattern[i:i+1]
                        if ch != "*" and ch != "/":
                            clean_pattern = clean_pattern + ch
                        i = i + 1

                    if trimmed.contains(clean_pattern):
                        excluded = true
                        break

                if not excluded:
                    files = files + [trimmed]

    files

fn refine_groups_with_similarity(exact_groups: [DuplicateGroup], all_blocks: [DuplicateBlock], config: DuplicationConfig, cache_manager: TokenCacheManager) -> [DuplicateGroup]:
    """Refine hash-based groups using cosine similarity to catch fuzzy duplicates."""
    val max_blocks_to_analyze = 500
    var blocks_to_analyze = all_blocks
    if all_blocks.len() > max_blocks_to_analyze:
        print "WARNING: Too many blocks ({all_blocks.len()}), limiting to first {max_blocks_to_analyze}"
        var limited = []
        var idx = 0
        while idx < max_blocks_to_analyze:
            limited = limited + [all_blocks[idx]]
            idx = idx + 1
        blocks_to_analyze = limited

    print "Extracting features from {blocks_to_analyze.len()} blocks..."
    var file_cache = {}
    var feature_vector_map = {}

    var block_idx = 0
    var total_blocks = blocks_to_analyze.len()
    for block in blocks_to_analyze:
        if block_idx % 20 == 0 or block_idx == total_blocks - 1:
            print "  Extracted features: {block_idx}/{total_blocks}"

        var tokens = []
        if file_cache.contains_key(block.file):
            tokens = file_cache[block.file]
        else:
            tokens = get_tokens_cached(cache_manager, block.file, fn(path: text) -> [SimpleToken]:
                val content = file_read(path)
                tokenize(content, config)
            )
            file_cache[block.file] = tokens

        var token_start = 0
        var found_start = false
        var i = 0
        while i < tokens.len():
            if tokens[i].line == block.line_start:
                token_start = i
                found_start = true
                break
            i = i + 1

        if found_start:
            var token_end = token_start + block.token_count
            if token_end > tokens.len():
                token_end = tokens.len()

            val freq_map = extract_token_frequencies(tokens, token_start, token_end)
            val feature_vec = build_feature_vector(block_idx, freq_map)
            feature_vector_map["{block_idx}"] = feature_vec

        block_idx = block_idx + 1

    print "Extracted {feature_vector_map.keys().len()} feature vectors"

    var refined_groups = []

    for group in exact_groups:
        refined_groups = refined_groups + [group]

    print "Starting pairwise comparison of {blocks_to_analyze.len()} blocks..."
    val total_possible = (blocks_to_analyze.len() * (blocks_to_analyze.len() - 1)) / 2
    print "  Max comparisons: {total_possible} (optimizations will reduce this)"

    var comparison_count = 0
    var skipped_same_file = 0
    var skipped_same_hash = 0
    var match_count = 0
    val max_comparisons_per_block = 50
    val max_total_comparisons = 10000

    var i = 0
    while i < blocks_to_analyze.len() and comparison_count < max_total_comparisons:
        if i % 10 == 0:
            print "  Block {i}/{blocks_to_analyze.len()}: {comparison_count} comparisons, {match_count} matches, skipped {skipped_same_file} same-file + {skipped_same_hash} same-hash"

        if comparison_count >= max_total_comparisons:
            print "  Reached comparison limit ({max_total_comparisons}), stopping early"
            break

        val block1 = blocks_to_analyze[i]
        val key_i = "{i}"
        val has_vec_i = feature_vector_map.contains_key(key_i)

        if not has_vec_i:
            i = i + 1
            continue

        val feature_vec1 = feature_vector_map[key_i]
        var comparisons_this_block = 0
        var j = i + 1

        while j < blocks_to_analyze.len() and comparisons_this_block < max_comparisons_per_block:
            val block2 = blocks_to_analyze[j]

            val same_file = block1.file == block2.file
            if same_file:
                skipped_same_file = skipped_same_file + 1
                j = j + 1
                continue

            val same_hash = block1.hash_value == block2.hash_value
            if same_hash:
                skipped_same_hash = skipped_same_hash + 1
                j = j + 1
                continue

            val key_j = "{j}"
            val has_vec_j = feature_vector_map.contains_key(key_j)

            if has_vec_j:
                comparison_count = comparison_count + 1
                comparisons_this_block = comparisons_this_block + 1

                val feature_vec2 = feature_vector_map[key_j]
                val similarity = cosine_similarity(feature_vec1, feature_vec2)

                val above_threshold = similarity >= config.similarity_threshold
                if above_threshold:
                    match_count = match_count + 1
                    val new_group = DuplicateGroup(
                        blocks: [block1, block2],
                        occurrences: 2,
                        total_lines: (block1.line_end - block1.line_start + 1) + (block2.line_end - block2.line_start + 1),
                        impact_score: 2 * ((block1.line_end - block1.line_start + 1) + (block2.line_end - block2.line_start + 1))
                    )

                    val meets_impact = new_group.impact_score >= config.min_impact
                    if meets_impact:
                        refined_groups = refined_groups + [new_group]

            j = j + 1
        i = i + 1

    print "Completed {comparison_count} comparisons, found {match_count} similar pairs"
    refined_groups

fn find_duplicates(files: [text], config: DuplicationConfig, cache_manager: TokenCacheManager) -> [DuplicateGroup]:
    var all_blocks = []

    print "Scanning {files.len()} files..."

    for file in files:
        val blocks = find_duplicates_in_file(file, config, cache_manager)
        all_blocks = all_blocks + blocks

    print "Found {all_blocks.len()} potential duplicate blocks"

    val exact_groups = group_by_hash(all_blocks, config)

    val refined_groups = if config.use_cosine_similarity:
        print "Refining with cosine similarity..."
        val result = refine_groups_with_similarity(exact_groups, all_blocks, config, cache_manager)
        result
    else:
        exact_groups

    val sorted = sort_by_impact(refined_groups)

    print "Grouped into {sorted.len()} duplicate groups"

    sorted

export DuplicateBlock, DuplicateGroup, find_duplicates, collect_files
