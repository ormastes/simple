# Core duplication detection engine using Karp-Rabin rolling hash

use app.duplicate_check.config.{DuplicationConfig}
use app.duplicate_check.tokenizer.{SimpleToken, tokenize, token_to_string}
use app.duplicate_check.features.{FeatureVec, extract_token_frequencies, build_feature_vector, cosine_similarity}
use app.duplicate_check.debug.{debug_log, debug_log_progress}
use app.duplicate_check.cache.{TokenCacheManager, get_cached_tokens, set_cached_tokens}
use app.io.mod.{file_read, shell}
use std.text.{NL}

# StringInterner for efficient token code computation
struct TokenInterner:
    strings: {text: i64}
    reverse: {i64: text}
    next_id: i64

fn new_token_interner() -> TokenInterner:
    TokenInterner(strings: {}, reverse: {}, next_id: 0)

fn intern_token(interner: TokenInterner, token_str: text) -> i64:
    if interner.strings.contains_key(token_str):
        interner.strings[token_str]
    else:
        val id = interner.next_id
        interner.strings[token_str] = id
        interner.reverse[id] = token_str
        interner.next_id = interner.next_id + 1
        id

fn log_progress(config: DuplicationConfig, message: text):
    """Print progress message if not quiet."""
    if not config.quiet:
        print message

fn log_verbose(config: DuplicationConfig, message: text):
    """Print verbose message if verbose mode enabled."""
    if config.verbose and not config.quiet:
        print message

struct DuplicateBlock:
    file: text
    line_start: i64
    line_end: i64
    token_count: i64
    hash_value: i64
    code: text

struct DuplicateGroup:
    blocks: [DuplicateBlock]
    occurrences: i64
    total_lines: i64
    impact_score: i64

fn rolling_hash(tokens: [SimpleToken], start: i64, window: i64) -> i64:
    """Compute rolling hash for token window with inlined character hashing."""
    var hash = 0
    val base = 31
    val mod_val = 1000000007

    var i = 0
    while i < window and start + i < tokens.len():
        val token = tokens[start + i]
        val token_str = token_to_string(token)

        val bytes = token_str.bytes()
        var char_code = 0
        var j = 0
        while j < bytes.len():
            char_code = char_code + bytes[j]
            j = j + 1

        hash = (hash * base + char_code) % mod_val
        i = i + 1

    hash

fn extract_code_snippet(tokens: [SimpleToken], start: i64, end_pos: i64) -> text:
    var parts: [text] = []
    var i = start
    while i < end_pos and i < tokens.len():
        parts = parts.push(tokens[i].value)
        i = i + 1
    var code = ""
    for p in parts:
        code = code + p
    code

fn compute_token_code(token: SimpleToken) -> i64:
    """Compute hash code for a single token."""
    val token_str = token_to_string(token)
    val bytes = token_str.bytes()
    var char_code = 0
    var j = 0
    while j < bytes.len():
        char_code = char_code + bytes[j]
        j = j + 1
    char_code

fn power_mod(base: i64, exp: i64, mod_val: i64) -> i64:
    """Compute base^exp % mod_val efficiently."""
    var result = 1
    var b = base % mod_val
    var e = exp

    while e > 0:
        if e % 2 == 1:
            result = (result * b) % mod_val
        b = (b * b) % mod_val
        e = e / 2

    result

fn find_duplicates_in_file(file_path: text, config: DuplicationConfig, cache_manager: TokenCacheManager, interner: TokenInterner) -> [DuplicateBlock]:
    val cached = get_cached_tokens(cache_manager, file_path)
    var tokens = []
    if cached != nil:
        tokens = cached
    else:
        val content = file_read(file_path)
        tokens = tokenize(content, config)
        set_cached_tokens(cache_manager, file_path, tokens)

    if tokens.len() < config.min_tokens:
        return []

    val base = 31
    val mod_val = 1000000007

    var token_codes = []
    var idx = 0
    while idx < tokens.len():
        val token_str = token_to_string(tokens[idx])
        val token_id = intern_token(interner, token_str)
        token_codes = token_codes + [token_id]
        idx = idx + 1

    val window_size = config.min_tokens
    val base_power = power_mod(base, window_size - 1, mod_val)

    var hash = 0
    var j = 0
    while j < window_size:
        hash = (hash * base + token_codes[j]) % mod_val
        j = j + 1

    var batches = []
    var current_batch = []
    val batch_size = 100
    var i = 0

    while i + config.min_tokens <= tokens.len():
        if i > 0:
            val old_code = token_codes[i - 1]
            val new_code = token_codes[i + window_size - 1]

            hash = (hash - (old_code * base_power) % mod_val + mod_val) % mod_val
            hash = (hash * base + new_code) % mod_val

        val start_line = tokens[i].line
        var end_idx = i + config.min_tokens
        val end_line = if end_idx < tokens.len(): tokens[end_idx].line else: tokens[tokens.len() - 1].line

        val line_count = end_line - start_line + 1

        if line_count >= config.min_lines:
            val code = extract_code_snippet(tokens, i, end_idx)

            val new_block = DuplicateBlock(
                file: file_path,
                line_start: start_line,
                line_end: end_line,
                token_count: config.min_tokens,
                hash_value: hash,
                code: code
            )

            current_batch = current_batch + [new_block]

            if current_batch.len() >= batch_size:
                batches = batches + [current_batch]
                current_batch = []

        i = i + 1

    if current_batch.len() > 0:
        batches = batches + [current_batch]

    var blocks = []
    for batch in batches:
        blocks = blocks + batch

    blocks

fn group_by_hash(all_blocks: [DuplicateBlock], config: DuplicationConfig) -> [DuplicateGroup]:
    var hash_map = {}

    for block in all_blocks:
        val hash_key = "{block.hash_value}"

        if hash_map.contains_key(hash_key):
            val existing = hash_map[hash_key]
            hash_map[hash_key] = existing + [block]
        else:
            hash_map[hash_key] = [block]

    var groups = []

    for hash_key in hash_map.keys():
        val blocks = hash_map[hash_key]

        if blocks.len() >= 2:
            var total_lines = 0
            for block in blocks:
                total_lines = total_lines + (block.line_end - block.line_start + 1)

            val impact = blocks.len() * total_lines

            if impact >= config.min_impact:
                groups = groups + [DuplicateGroup(
                    blocks: blocks,
                    occurrences: blocks.len(),
                    total_lines: total_lines,
                    impact_score: impact
                )]

    groups

fn sort_by_impact(groups: [DuplicateGroup]) -> [DuplicateGroup]:
    var sorted = groups
    var n = sorted.len()

    var i = 0
    while i < n - 1:
        var j = 0
        while j < n - i - 1:
            if sorted[j].impact_score < sorted[j + 1].impact_score:
                val temp = sorted[j]
                sorted[j] = sorted[j + 1]
                sorted[j + 1] = temp
            j = j + 1
        i = i + 1

    sorted

fn collect_files(path: text, config: DuplicationConfig) -> [text]:
    var files = []

    if path.ends_with(".spl"):
        files = [path]
    else:
        val result = shell("find {path} -name '*.spl' 2>/dev/null || true")
        val output = result.stdout
        val lines = output.split(NL)

        for line in lines:
            val trimmed = line.trim()
            if trimmed.len() > 0:
                var excluded = false
                for pattern in config.exclude_patterns:
                    var clean_pattern = ""
                    var i = 0
                    while i < pattern.len():
                        val ch = pattern[i:i+1]
                        if ch != "*" and ch != "/":
                            clean_pattern = clean_pattern + ch
                        i = i + 1

                    if trimmed.contains(clean_pattern):
                        excluded = true
                        break

                if not excluded:
                    files = files + [trimmed]

    files

fn refine_groups_with_similarity(exact_groups: [DuplicateGroup], all_blocks: [DuplicateBlock], config: DuplicationConfig, cache_manager: TokenCacheManager) -> [DuplicateGroup]:
    """Refine hash-based groups using cosine similarity to catch fuzzy duplicates."""
    val max_blocks_to_analyze = 500
    var blocks_to_analyze = all_blocks
    if all_blocks.len() > max_blocks_to_analyze:
        log_progress(config, "WARNING: Too many blocks ({all_blocks.len()}), sampling {max_blocks_to_analyze} across files")
        # Sample blocks distributed across files instead of taking first N
        # Group blocks by file, then round-robin pick from each file
        var file_blocks = {}
        for block in all_blocks:
            if file_blocks.contains_key(block.file):
                file_blocks[block.file] = file_blocks[block.file] + [block]
            else:
                file_blocks[block.file] = [block]

        val file_keys = file_blocks.keys()
        var limited = []
        var round = 0
        var done = false
        while not done:
            var added_any = false
            for fk in file_keys:
                val fblocks = file_blocks[fk]
                if round < fblocks.len():
                    limited = limited + [fblocks[round]]
                    added_any = true
                    if limited.len() >= max_blocks_to_analyze:
                        done = true
                        break
            if not added_any:
                done = true
            round = round + 1
        blocks_to_analyze = limited
        log_verbose(config, "  Sampled from {file_keys.len()} files")

    log_progress(config, "Pre-computing features for {blocks_to_analyze.len()} blocks...")
    # Pre-compute all feature vectors upfront (avoids closure capture bug with inner fns)
    var file_token_cache = {}
    var feature_vectors = []
    var block_idx = 0
    while block_idx < blocks_to_analyze.len():
        val block = blocks_to_analyze[block_idx]

        var tokens = []
        if file_token_cache.contains_key(block.file):
            tokens = file_token_cache[block.file]
        else:
            val cached_tok = get_cached_tokens(cache_manager, block.file)
            if cached_tok != nil:
                tokens = cached_tok
            else:
                val content = file_read(block.file)
                tokens = tokenize(content, config)
                set_cached_tokens(cache_manager, block.file, tokens)
            file_token_cache[block.file] = tokens

        var token_start = 0
        var found_start = false
        var ti = 0
        while ti < tokens.len():
            if tokens[ti].line == block.line_start:
                token_start = ti
                found_start = true
                break
            ti = ti + 1

        var feature_vec = FeatureVec(block_id: block_idx, features: {}, magnitude: 0.0)
        if found_start:
            var token_end = token_start + block.token_count
            if token_end > tokens.len():
                token_end = tokens.len()
            val freq_map = extract_token_frequencies(tokens, token_start, token_end)
            feature_vec = build_feature_vector(block_idx, freq_map)

        feature_vectors = feature_vectors + [feature_vec]
        block_idx = block_idx + 1

    log_progress(config, "Extracted {feature_vectors.len()} feature vectors from {file_token_cache.keys().len()} files")

    var refined_groups = []

    for group in exact_groups:
        refined_groups = refined_groups + [group]

    log_progress(config, "Starting pairwise comparison of {blocks_to_analyze.len()} blocks...")
    val total_possible = (blocks_to_analyze.len() * (blocks_to_analyze.len() - 1)) / 2
    log_verbose(config, "  Max comparisons: {total_possible} (optimizations will reduce this)")

    var comparison_count = 0
    var skipped_same_file = 0
    var match_count = 0
    val max_comparisons_per_block = 50
    val max_total_comparisons = 10000

    var i = 0
    while i < blocks_to_analyze.len() and comparison_count < max_total_comparisons:
        if i % 50 == 0:
            log_verbose(config, "  Block {i}/{blocks_to_analyze.len()}: {comparison_count} comparisons, {match_count} matches")

        if comparison_count >= max_total_comparisons:
            log_progress(config, "  Reached comparison limit ({max_total_comparisons}), stopping early")
            break

        val block1 = blocks_to_analyze[i]
        var comparisons_this_block = 0
        var j = i + 1

        while j < blocks_to_analyze.len() and comparisons_this_block < max_comparisons_per_block:
            val block2 = blocks_to_analyze[j]

            val same_file = block1.file == block2.file
            if same_file:
                skipped_same_file = skipped_same_file + 1
                j = j + 1
                continue

            comparison_count = comparison_count + 1
            comparisons_this_block = comparisons_this_block + 1

            val feature_vec1 = feature_vectors[i]
            val feature_vec2 = feature_vectors[j]
            val similarity = cosine_similarity(feature_vec1, feature_vec2)

            val above_threshold = similarity >= config.similarity_threshold
            if above_threshold:
                match_count = match_count + 1
                val new_group = DuplicateGroup(
                    blocks: [block1, block2],
                    occurrences: 2,
                    total_lines: (block1.line_end - block1.line_start + 1) + (block2.line_end - block2.line_start + 1),
                    impact_score: 2 * ((block1.line_end - block1.line_start + 1) + (block2.line_end - block2.line_start + 1))
                )

                val meets_impact = new_group.impact_score >= config.min_impact
                if meets_impact:
                    refined_groups = refined_groups + [new_group]

            j = j + 1
        i = i + 1

    log_progress(config, "Completed {comparison_count} comparisons, found {match_count} similar pairs")
    refined_groups

fn find_duplicates(files: [text], config: DuplicationConfig, cache_manager: TokenCacheManager) -> [DuplicateGroup]:
    var all_blocks = []

    val interner = new_token_interner()

    log_progress(config, "Scanning {files.len()} files...")

    for file in files:
        val blocks = find_duplicates_in_file(file, config, cache_manager, interner)
        all_blocks = all_blocks + blocks
        log_verbose(config, "  {file}: {blocks.len()} blocks")

    log_progress(config, "Found {all_blocks.len()} potential duplicate blocks")

    val exact_groups = group_by_hash(all_blocks, config)

    val refined_groups = if config.use_cosine_similarity:
        log_progress(config, "Refining with cosine similarity...")
        val result = refine_groups_with_similarity(exact_groups, all_blocks, config, cache_manager)
        result
    else:
        exact_groups

    val sorted = sort_by_impact(refined_groups)

    log_progress(config, "Grouped into {sorted.len()} duplicate groups")

    sorted

fn find_duplicates_with_options(
    files: [text],
    config: DuplicationConfig,
    cache_manager: TokenCacheManager,
    use_parallel: bool,
    use_incremental: bool,
    incremental_cache_path: text
) -> [DuplicateGroup]:
    # Parallel and incremental modes require cross-module imports (parallel.spl, incremental.spl).
    # Falling back to sequential processing until module imports are fully supported.
    if use_parallel:
        eprint("[WARNING] Parallel duplicate detection not available; falling back to sequential processing")
    if use_incremental:
        eprint("[WARNING] Incremental duplicate detection not available; falling back to full scan")
    find_duplicates(files, config, cache_manager)

export DuplicateBlock, DuplicateGroup, find_duplicates, find_duplicates_with_options, collect_files
