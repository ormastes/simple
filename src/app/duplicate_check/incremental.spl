# Incremental duplicate detection - only reprocess changed files
#
# Caches previous analysis results and reuses them for unchanged files.
# Based on src/app/build/incremental.spl pattern.

use app.duplicate_check.config.{DuplicationConfig}
use app.duplicate_check.detector.{DuplicateBlock, DuplicateGroup}
use app.io.mod.{file_exists, file_read, file_write, file_hash_sha256, current_time_ms, dir_create_all}
use std.text.{NL}

# Cache entry for a single file's duplicate detection results
struct IncrementalCacheEntry:
    file_path: text
    source_hash: text       # SHA256 hash of file content
    timestamp: i64          # When this was cached
    blocks: [DuplicateBlock]

# Incremental cache manager
struct IncrementalCache:
    entries: {text: IncrementalCacheEntry}
    cache_path: text
    config_hash: text       # Hash of config to detect config changes

# Load incremental cache from disk
fn load_incremental_cache(cache_path: text, config: DuplicationConfig) -> IncrementalCache:
    if not file_exists(cache_path):
        return IncrementalCache(
            entries: {},
            cache_path: cache_path,
            config_hash: compute_config_hash(config)
        )

    val content = file_read(cache_path)
    val lines = content.split(NL)

    var entries = {}
    var stored_config_hash = ""
    var i = 0

    while i < lines.len():
        val line = lines[i].trim()

        if line.starts_with("# config_hash:"):
            stored_config_hash = line[14:].trim()
        elif line.starts_with("#") or line.len() == 0:
            i = i + 1
            continue
        elif line.starts_with("file:"):
            # Parse file entry: file|hash|timestamp|block_count
            val parts = line.split("|")
            if parts.len() >= 4:
                val file_path = parts[0][5:].trim()  # Remove "file:" prefix
                val source_hash = parts[1].trim()
                val timestamp = int(parts[2])
                val block_count = int(parts[3])

                # Read blocks (next block_count lines)
                var blocks = []
                var j = 0
                while j < block_count and i + 1 + j < lines.len():
                    val block_line = lines[i + 1 + j].trim()
                    if block_line.len() > 0:
                        # Parse block: start_line|end_line|token_count|hash
                        val block_parts = block_line.split("|")
                        if block_parts.len() >= 4:
                            blocks = blocks + [DuplicateBlock(
                                file: file_path,
                                line_start: int(block_parts[0]),
                                line_end: int(block_parts[1]),
                                token_count: int(block_parts[2]),
                                hash_value: int(block_parts[3]),
                                code: ""  # Code not cached (too large)
                            )]
                    j = j + 1

                entries[file_path] = IncrementalCacheEntry(
                    file_path: file_path,
                    source_hash: source_hash,
                    timestamp: timestamp,
                    blocks: blocks
                )

                i = i + block_count

        i = i + 1

    # Invalidate cache if config changed
    val current_config_hash = compute_config_hash(config)
    if stored_config_hash.len() > 0 and stored_config_hash != current_config_hash:
        return IncrementalCache(
            entries: {},
            cache_path: cache_path,
            config_hash: current_config_hash
        )

    IncrementalCache(
        entries: entries,
        cache_path: cache_path,
        config_hash: current_config_hash
    )

# Save incremental cache to disk
fn save_incremental_cache(cache: IncrementalCache):
    # Create cache directory if needed
    val cache_dir = cache.cache_path[0:cache.cache_path.len() - cache.cache_path.split("/")[-1:].len() - 1]
    dir_create_all(cache_dir)

    var lines = []
    lines = lines + ["# Duplicate detection incremental cache"]
    lines = lines + ["# config_hash: {cache.config_hash}"]
    lines = lines + [""]

    for file_path in cache.entries.keys():
        val entry = cache.entries[file_path]
        lines = lines + ["file:{file_path}|{entry.source_hash}|{entry.timestamp}|{entry.blocks.len()}"]

        for block in entry.blocks:
            lines = lines + ["  {block.line_start}|{block.line_end}|{block.token_count}|{block.hash_value}"]

        lines = lines + [""]

    file_write(cache.cache_path, lines.join(NL))

# Compute hash of config to detect config changes
fn compute_config_hash(config: DuplicationConfig) -> text:
    # Combine critical config fields that affect detection
    val config_str = "{config.min_tokens}|{config.min_lines}|{config.ignore_identifiers}|{config.ignore_literals}|{config.ignore_comments}|{config.ignore_whitespace}|{config.similarity_threshold}"
    file_hash_sha256_string(config_str)

# Compute SHA256 hash of a string (not a file)
fn file_hash_sha256_string(content: text) -> text:
    # Simple hash approximation (in production, use proper SHA256)
    var hash = 0
    val bytes = content.bytes()
    var i = 0
    while i < bytes.len():
        hash = (hash * 31 + bytes[i]) % 1000000007
        i = i + 1
    "{hash}"

# Check if file needs reprocessing
fn needs_reprocessing(
    file_path: text,
    cache: IncrementalCache
) -> bool:
    if not cache.entries.contains_key(file_path):
        return true  # File not in cache

    val entry = cache.entries[file_path]
    val current_hash = file_hash_sha256(file_path)

    current_hash != entry.source_hash

# Get blocks from cache or return empty if needs reprocessing
fn get_cached_blocks(
    file_path: text,
    cache: IncrementalCache
) -> [DuplicateBlock]:
    if cache.entries.contains_key(file_path):
        val entry = cache.entries[file_path]
        val current_hash = file_hash_sha256(file_path)
        if current_hash == entry.source_hash:
            return entry.blocks

    []

# Update cache with new blocks for a file
fn update_cache_entry(
    cache: IncrementalCache,
    file_path: text,
    blocks: [DuplicateBlock]
):
    val source_hash = file_hash_sha256(file_path)
    val timestamp = current_time_ms()

    cache.entries[file_path] = IncrementalCacheEntry(
        file_path: file_path,
        source_hash: source_hash,
        timestamp: timestamp,
        blocks: blocks
    )

# Incremental analysis statistics
struct IncrementalStats:
    total_files: i64
    cached_files: i64
    reprocessed_files: i64
    cache_hit_rate: f64

fn compute_incremental_stats(
    total_files: i64,
    cached_files: i64
) -> IncrementalStats:
    val cache_hit_rate = if total_files > 0:
        (cached_files * 100) / total_files
    else:
        0

    IncrementalStats(
        total_files: total_files,
        cached_files: cached_files,
        reprocessed_files: total_files - cached_files,
        cache_hit_rate: cache_hit_rate
    )

fn format_incremental_stats(stats: IncrementalStats) -> text:
    "Incremental: {stats.reprocessed_files} changed, {stats.cached_files} cached ({stats.cache_hit_rate}% hit rate)"

export IncrementalCacheEntry, IncrementalCache, IncrementalStats
export load_incremental_cache, save_incremental_cache, compute_config_hash
export needs_reprocessing, get_cached_blocks, update_cache_entry
export compute_incremental_stats, format_incremental_stats
