# Benchmarking Utilities
#
# Micro-benchmarking framework for measuring performance of
# individual operations and comparing implementations.
#
# Features:
# - Warm-up runs to stabilize JIT/caches
# - Statistical analysis (mean, stddev, percentiles)
# - Comparison between implementations
# - Memory allocation tracking
#
# Example:
#     value result = run_benchmark("dict_insert", 1000, \:
#         var dict = PersistentDict.new()
#         for i in 0..100:
#             dict = dict.set(i, i * 2)
#     )
#     print(result.summary())

export Benchmark, BenchmarkResult, BenchmarkSuite, BenchmarkConfig
export run_benchmark, run_suite, compare_benchmarks

# ============================================================================
# Configuration
# ============================================================================

struct BenchmarkConfig:
    """Configuration for benchmark runs."""
    warmup_iterations: i64    # Warm-up runs before measuring
    iterations: i64           # Measured iterations
    min_time_ms: i64          # Minimum total time (for short ops)
    max_time_ms: i64          # Maximum total time (timeout)
    gc_between_runs: bool     # Force GC between iterations
    collect_memory_stats: bool # Track memory usage

impl BenchmarkConfig:
    static fn default() -> BenchmarkConfig:
        BenchmarkConfig(
            warmup_iterations: 10,
            iterations: 100,
            min_time_ms: 1000,
            max_time_ms: 60000,
            gc_between_runs: true,
            collect_memory_stats: true
        )

    static fn quick() -> BenchmarkConfig:
        """Quick benchmark for development."""
        BenchmarkConfig(
            warmup_iterations: 3,
            iterations: 20,
            min_time_ms: 100,
            max_time_ms: 5000,
            gc_between_runs: false,
            collect_memory_stats: false
        )

    static fn thorough() -> BenchmarkConfig:
        """Thorough benchmark for accurate measurements."""
        BenchmarkConfig(
            warmup_iterations: 50,
            iterations: 1000,
            min_time_ms: 5000,
            max_time_ms: 300000,
            gc_between_runs: true,
            collect_memory_stats: true
        )

# Manually desugared static method for bootstrap runtime compatibility
fn BenchmarkConfig__default() -> BenchmarkConfig:
    BenchmarkConfig(
        warmup_iterations: 10,
        iterations: 100,
        min_time_ms: 1000,
        max_time_ms: 60000,
        gc_between_runs: true,
        collect_memory_stats: true
    )

# ============================================================================
# Benchmark Result
# ============================================================================

struct BenchmarkResult:
    """Result of a benchmark run."""
    name: text
    iterations: i64

    # Timing (nanoseconds)
    total_time_ns: i64
    min_time_ns: i64
    max_time_ns: i64
    mean_time_ns: i64
    median_time_ns: i64
    stddev_ns: i64
    percentile_95_ns: i64
    percentile_99_ns: i64

    # Memory
    bytes_allocated: i64
    peak_memory: i64
    gc_count: i64

    # Raw data
    times_ns: [i64]

impl BenchmarkResult:
    static fn new(name: text) -> BenchmarkResult:
        BenchmarkResult(
            name: name,
            iterations: 0,
            total_time_ns: 0,
            min_time_ns: 0,
            max_time_ns: 0,
            mean_time_ns: 0,
            median_time_ns: 0,
            stddev_ns: 0,
            percentile_95_ns: 0,
            percentile_99_ns: 0,
            bytes_allocated: 0,
            peak_memory: 0,
            gc_count: 0,
            times_ns: []
        )

    fn mean_time_us() -> f64:
        """Mean time in microseconds."""
        (self.mean_time_ns as f64) / 1000.0

    fn mean_time_ms() -> f64:
        """Mean time in milliseconds."""
        (self.mean_time_ns as f64) / 1000000.0

    fn ops_per_second() -> f64:
        """Operations per second."""
        if self.mean_time_ns == 0:
            0.0
        else:
            1000000000.0 / (self.mean_time_ns as f64)

    fn coefficient_of_variation() -> f64:
        """Coefficient of variation (stddev / mean)."""
        if self.mean_time_ns == 0:
            0.0
        else:
            (self.stddev_ns as f64) / (self.mean_time_ns as f64)

    fn summary() -> text:
        """Generate summary text."""
        var lines: [text] = []
        lines = lines.push("Benchmark: {self.name}")
        lines = lines.push("  Iterations: {self.iterations}")
        lines = lines.push("  Mean: {format_duration(self.mean_time_ns)}")
        lines = lines.push("  Min:  {format_duration(self.min_time_ns)}")
        lines = lines.push("  Max:  {format_duration(self.max_time_ns)}")
        lines = lines.push("  Stddev: {format_duration(self.stddev_ns)}")
        lines = lines.push("  P95: {format_duration(self.percentile_95_ns)}")
        lines = lines.push("  P99: {format_duration(self.percentile_99_ns)}")
        lines = lines.push("  Ops/sec: {self.ops_per_second():.2}")

        if self.bytes_allocated > 0:
            lines = lines.push("  Memory: {format_bytes(self.bytes_allocated)}")

        lines.join("\n")

    fn compare_to(other: BenchmarkResult) -> BenchmarkComparison:
        """Compare this result to another."""
        BenchmarkComparison.new(self, other)

impl Display for BenchmarkResult:
    fn fmt() -> text:
        "{self.name}: {format_duration(self.mean_time_ns)} ({self.ops_per_second():.0} ops/s)"

# ============================================================================
# Benchmark Comparison
# ============================================================================

struct BenchmarkComparison:
    """Comparison between two benchmark results."""
    baseline_name: text
    candidate_name: text
    speedup: f64           # > 1 means candidate is faster
    time_diff_ns: i64      # Absolute difference
    percent_change: f64    # Percentage change

impl BenchmarkComparison:
    static fn new(baseline: BenchmarkResult, candidate: BenchmarkResult) -> BenchmarkComparison:
        val speedup = if candidate.mean_time_ns > 0:
            (baseline.mean_time_ns as f64) / (candidate.mean_time_ns as f64)
        else:
            0.0

        val diff = baseline.mean_time_ns - candidate.mean_time_ns

        val percent = if baseline.mean_time_ns > 0:
            ((diff as f64) / (baseline.mean_time_ns as f64)) * 100.0
        else:
            0.0

        BenchmarkComparison(
            baseline_name: baseline.name,
            candidate_name: candidate.name,
            speedup: speedup,
            time_diff_ns: diff,
            percent_change: percent
        )

    fn is_faster() -> bool:
        self.speedup > 1.0

    fn is_slower() -> bool:
        self.speedup < 1.0

    fn summary() -> text:
        val direction = if self.is_faster(): "faster" else: "slower"
        val factor = if self.is_faster(): self.speedup else: 1.0 / self.speedup

        "{self.candidate_name} vs {self.baseline_name}: {factor:.2}x {direction} ({self.percent_change:.1}%)"

# ============================================================================
# Benchmark
# ============================================================================

struct Benchmark:
    """A single benchmark definition."""
    name: text
    description: text
    setup: fn()            # Setup function (not measured)
    teardown: fn()         # Teardown function (not measured)
    benchmark_fn: fn()     # The function to benchmark
    config: BenchmarkConfig

impl Benchmark:
    static fn new(name: text, f: fn()) -> Benchmark:
        Benchmark(
            name: name,
            description: "",
            setup: \: (),
            teardown: \: (),
            benchmark_fn: f,
            config: BenchmarkConfig__default()
        )

    static fn with_config(name: text, f: fn(), config: BenchmarkConfig) -> Benchmark:
        Benchmark(
            name: name,
            description: "",
            setup: \: (),
            teardown: \: (),
            benchmark_fn: f,
            config: config
        )

    fn with_setup(setup_fn: fn()) -> Benchmark:
        Benchmark(
            name: self.name,
            description: self.description,
            setup: setup_fn,
            teardown: self.teardown,
            benchmark_fn: self.benchmark_fn,
            config: self.config
        )

    fn with_teardown(teardown_fn: fn()) -> Benchmark:
        Benchmark(
            name: self.name,
            description: self.description,
            setup: self.setup,
            teardown: teardown_fn,
            benchmark_fn: self.benchmark_fn,
            config: self.config
        )

    fn with_description(desc: text) -> Benchmark:
        Benchmark(
            name: self.name,
            description: desc,
            setup: self.setup,
            teardown: self.teardown,
            benchmark_fn: self.benchmark_fn,
            config: self.config
        )

    fn run() -> BenchmarkResult:
        """Run the benchmark."""
        run_benchmark_internal(self)

# ============================================================================
# Benchmark Suite
# ============================================================================

struct BenchmarkSuite:
    """A collection of related benchmarks."""
    name: text
    description: text
    benchmarks: [Benchmark]
    config: BenchmarkConfig

impl BenchmarkSuite:
    static fn new(name: text) -> BenchmarkSuite:
        BenchmarkSuite(
            name: name,
            description: "",
            benchmarks: [],
            config: BenchmarkConfig__default()
        )

    me add(benchmark: Benchmark):
        """Add a benchmark to the suite."""
        self.benchmarks = self.benchmarks.push(benchmark)

    me add_fn(name: text, f: fn()):
        """Add a simple function benchmark."""
        self.benchmarks = self.benchmarks.push(
            Benchmark.with_config(name, f, self.config)
        )

    fn with_config(config: BenchmarkConfig) -> BenchmarkSuite:
        BenchmarkSuite(
            name: self.name,
            description: self.description,
            benchmarks: self.benchmarks,
            config: config
        )

    fn run() -> [BenchmarkResult]:
        """Run all benchmarks in the suite."""
        var results: [BenchmarkResult] = []
        for benchmark in self.benchmarks:
            val result = benchmark.run()
            results = results.push(result)
        results

    fn run_and_compare() -> SuiteComparison:
        """Run benchmarks and generate comparison."""
        val results = self.run()
        SuiteComparison.new(self.name, results)

struct SuiteComparison:
    """Comparison of all benchmarks in a suite."""
    suite_name: text
    results: [BenchmarkResult]
    fastest_name: text
    slowest_name: text

impl SuiteComparison:
    static fn new(name: text, results: [BenchmarkResult]) -> SuiteComparison:
        var fastest = results[0]
        var slowest = results[0]

        for result in results:
            if result.mean_time_ns < fastest.mean_time_ns:
                fastest = result
            if result.mean_time_ns > slowest.mean_time_ns:
                slowest = result

        SuiteComparison(
            suite_name: name,
            results: results,
            fastest_name: fastest.name,
            slowest_name: slowest.name
        )

    fn summary() -> text:
        var lines: [text] = []
        lines = lines.push("=== Suite: {self.suite_name} ===")
        lines = lines.push("")

        for result in self.results:
            val marker = if result.name == self.fastest_name: " (fastest)"
                        else if result.name == self.slowest_name: " (slowest)"
                        else: ""
            lines = lines.push("  {result}{marker}")

        lines = lines.push("")
        lines = lines.push("Fastest: {self.fastest_name}")
        lines = lines.push("Slowest: {self.slowest_name}")

        lines.join("\n")

# ============================================================================
# Benchmark Execution
# ============================================================================

fn run_benchmark(name: text, iterations: i64, f: fn()) -> BenchmarkResult:
    """Run a simple benchmark."""
    val config = BenchmarkConfig(
        warmup_iterations: 10,
        iterations: iterations,
        min_time_ms: 100,
        max_time_ms: 60000,
        gc_between_runs: false,
        collect_memory_stats: false
    )
    val benchmark = Benchmark.with_config(name, f, config)
    run_benchmark_internal(benchmark)

fn run_suite(suite: BenchmarkSuite) -> [BenchmarkResult]:
    """Run a benchmark suite."""
    suite.run()

fn run_benchmark_internal(benchmark: Benchmark) -> BenchmarkResult:
    """Internal benchmark execution."""
    var result = BenchmarkResult.new(benchmark.name)
    var times: [i64] = []

    # Setup
    benchmark.setup()

    # Warmup
    for _ in 0..benchmark.config.warmup_iterations:
        benchmark.benchmark_fn()

    # Measured runs
    for _ in 0..benchmark.config.iterations:
        val start = current_time_ns()
        benchmark.benchmark_fn()
        val end = current_time_ns()
        val elapsed = end - start
        times = times.push(elapsed)

    # Teardown
    benchmark.teardown()

    # Calculate statistics
    result.times_ns = times
    result.iterations = times.len()
    result = calculate_statistics(result)

    result

fn calculate_statistics(result: BenchmarkResult) -> BenchmarkResult:
    """Calculate statistical metrics from raw times."""
    val times = result.times_ns
    if times.len() == 0:
        return result

    # Sort for percentiles
    val sorted = times.sort()

    # Total and mean
    var total: i64 = 0
    for t in times:
        total = total + t
    val mean = total / times.len()

    # Min/Max
    val min_val = sorted[0]
    val max_val = sorted[sorted.len() - 1]

    # Median
    val mid = sorted.len() / 2
    val median = if sorted.len() % 2 == 0:
        (sorted[mid - 1] + sorted[mid]) / 2
    else:
        sorted[mid]

    # Standard deviation
    var variance: i64 = 0
    for t in times:
        val diff = t - mean
        variance = variance + diff * diff
    variance = variance / times.len()
    val stddev = sqrt(variance)

    # Percentiles
    val p95_idx = (sorted.len() * 95) / 100
    val p99_idx = (sorted.len() * 99) / 100

    BenchmarkResult(
        name: result.name,
        iterations: result.iterations,
        total_time_ns: total,
        min_time_ns: min_val,
        max_time_ns: max_val,
        mean_time_ns: mean,
        median_time_ns: median,
        stddev_ns: stddev,
        percentile_95_ns: sorted[p95_idx],
        percentile_99_ns: sorted[p99_idx],
        bytes_allocated: result.bytes_allocated,
        peak_memory: result.peak_memory,
        gc_count: result.gc_count,
        times_ns: result.times_ns
    )

fn compare_benchmarks(baseline: BenchmarkResult, candidate: BenchmarkResult) -> BenchmarkComparison:
    """Compare two benchmark results."""
    BenchmarkComparison.new(baseline, candidate)

# ============================================================================
# Formatting Utilities
# ============================================================================

fn format_duration(ns: i64) -> text:
    """Format nanoseconds as human-readable duration."""
    if ns < 1000:
        "{ns}ns"
    else if ns < 1000000:
        val us = (ns as f64) / 1000.0
        "{us:.2}Âµs"
    else if ns < 1000000000:
        val ms = (ns as f64) / 1000000.0
        "{ms:.2}ms"
    else:
        val s = (ns as f64) / 1000000000.0
        "{s:.3}s"

fn format_bytes(bytes: i64) -> text:
    """Format bytes as human-readable size."""
    if bytes < 1024:
        "{bytes} B"
    else if bytes < 1024 * 1024:
        val kb = (bytes as f64) / 1024.0
        "{kb:.1} KB"
    else if bytes < 1024 * 1024 * 1024:
        val mb = (bytes as f64) / (1024.0 * 1024.0)
        "{mb:.1} MB"
    else:
        val gb = (bytes as f64) / (1024.0 * 1024.0 * 1024.0)
        "{gb:.2} GB"

# ============================================================================
# Helper Functions
# ============================================================================

fn current_time_ns() -> i64:
    """Get current time in nanoseconds (placeholder)."""
    current_time_ms() * 1000000

fn current_time_ms() -> i64:
    """Get current time in milliseconds (placeholder)."""
    0

fn sqrt(n: i64) -> i64:
    """Integer square root (placeholder)."""
    if n <= 0:
        0
    else:
        var x = n
        var y = (x + 1) / 2
        while y < x:
            x = y
            y = (x + n / x) / 2
        x
