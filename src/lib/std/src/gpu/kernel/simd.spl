# GPU SIMD - GPU-specific SIMD operations
#
# Provides SIMD vector operations optimized for GPU execution.

# GPU vector types (typically 128-512 bits wide)
struct GpuVec4f:
    x: f32
    y: f32
    z: f32
    w: f32

    static fn new(x: f32, y: f32, z: f32, w: f32) -> GpuVec4f:
        return GpuVec4f(x: x, y: y, z: z, w: w)

    static fn splat(v: f32) -> GpuVec4f:
        return GpuVec4f(x: v, y: v, z: v, w: v)

    static fn zero() -> GpuVec4f:
        return GpuVec4f.splat(0.0)

    static fn one() -> GpuVec4f:
        return GpuVec4f.splat(1.0)

struct GpuVec4i:
    x: i32
    y: i32
    z: i32
    w: i32

    static fn new(x: i32, y: i32, z: i32, w: i32) -> GpuVec4i:
        return GpuVec4i(x: x, y: y, z: z, w: w)

    static fn splat(v: i32) -> GpuVec4i:
        return GpuVec4i(x: v, y: v, z: v, w: v)

    static fn zero() -> GpuVec4i:
        return GpuVec4i.splat(0)

# Vector arithmetic operations
@gpu_intrinsic("simd_add_f32x4")
fn add_vec4f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(x: a.x + b.x, y: a.y + b.y, z: a.z + b.z, w: a.w + b.w)

@gpu_intrinsic("simd_sub_f32x4")
fn sub_vec4f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(x: a.x - b.x, y: a.y - b.y, z: a.z - b.z, w: a.w - b.w)

@gpu_intrinsic("simd_mul_f32x4")
fn mul_vec4f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(x: a.x * b.x, y: a.y * b.y, z: a.z * b.z, w: a.w * b.w)

@gpu_intrinsic("simd_div_f32x4")
fn div_vec4f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(x: a.x / b.x, y: a.y / b.y, z: a.z / b.z, w: a.w / b.w)

# Integer vector operations
@gpu_intrinsic("simd_add_i32x4")
fn add_vec4i(a: GpuVec4i, b: GpuVec4i) -> GpuVec4i:
    return GpuVec4i(x: a.x + b.x, y: a.y + b.y, z: a.z + b.z, w: a.w + b.w)

@gpu_intrinsic("simd_sub_i32x4")
fn sub_vec4i(a: GpuVec4i, b: GpuVec4i) -> GpuVec4i:
    return GpuVec4i(x: a.x - b.x, y: a.y - b.y, z: a.z - b.z, w: a.w - b.w)

@gpu_intrinsic("simd_mul_i32x4")
fn mul_vec4i(a: GpuVec4i, b: GpuVec4i) -> GpuVec4i:
    return GpuVec4i(x: a.x * b.x, y: a.y * b.y, z: a.z * b.z, w: a.w * b.w)

# Fused multiply-add (a * b + c)
@gpu_intrinsic("simd_fma_f32x4")
fn fma_vec4f(a: GpuVec4f, b: GpuVec4f, c: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(
        x: a.x * b.x + c.x,
        y: a.y * b.y + c.y,
        z: a.z * b.z + c.z,
        w: a.w * b.w + c.w
    )

# Dot product
@gpu_intrinsic("simd_dot_f32x4")
fn dot_vec4f(a: GpuVec4f, b: GpuVec4f) -> f32:
    return a.x * b.x + a.y * b.y + a.z * b.z + a.w * b.w

# Cross product (3D, w component is 0)
@gpu_intrinsic("simd_cross_f32x3")
fn cross_vec3f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(
        x: a.y * b.z - a.z * b.y,
        y: a.z * b.x - a.x * b.z,
        z: a.x * b.y - a.y * b.x,
        w: 0.0
    )

# Length/magnitude
@gpu_intrinsic("simd_length_f32x4")
fn length_vec4f(v: GpuVec4f) -> f32:
    return sqrt(dot_vec4f(v, v))

# Normalize
@gpu_intrinsic("simd_normalize_f32x4")
fn normalize_vec4f(v: GpuVec4f) -> GpuVec4f:
    val len = length_vec4f(v)
    if len > 0.0:
        return div_vec4f(v, GpuVec4f.splat(len))
    return GpuVec4f.zero()

# Min/max per component
@gpu_intrinsic("simd_min_f32x4")
fn min_vec4f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(
        x: if a.x < b.x: a.x else: b.x,
        y: if a.y < b.y: a.y else: b.y,
        z: if a.z < b.z: a.z else: b.z,
        w: if a.w < b.w: a.w else: b.w
    )

@gpu_intrinsic("simd_max_f32x4")
fn max_vec4f(a: GpuVec4f, b: GpuVec4f) -> GpuVec4f:
    return GpuVec4f(
        x: if a.x > b.x: a.x else: b.x,
        y: if a.y > b.y: a.y else: b.y,
        z: if a.z > b.z: a.z else: b.z,
        w: if a.w > b.w: a.w else: b.w
    )

# Clamp per component
@gpu_intrinsic("simd_clamp_f32x4")
fn clamp_vec4f(v: GpuVec4f, min_v: GpuVec4f, max_v: GpuVec4f) -> GpuVec4f:
    return min_vec4f(max_vec4f(v, min_v), max_v)

# Linear interpolation
@gpu_intrinsic("simd_lerp_f32x4")
fn lerp_vec4f(a: GpuVec4f, b: GpuVec4f, t: f32) -> GpuVec4f:
    val one_minus_t = 1.0 - t
    return add_vec4f(
        mul_vec4f(a, GpuVec4f.splat(one_minus_t)),
        mul_vec4f(b, GpuVec4f.splat(t))
    )

# FFI for GPU intrinsics
extern fn sqrt(x: f32) -> f32

export GpuVec4f, GpuVec4i
export add_vec4f, sub_vec4f, mul_vec4f, div_vec4f
export add_vec4i, sub_vec4i, mul_vec4i
export fma_vec4f, dot_vec4f, cross_vec3f
export length_vec4f, normalize_vec4f
export min_vec4f, max_vec4f, clamp_vec4f, lerp_vec4f
