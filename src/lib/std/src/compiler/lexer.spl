"""
Compiler Lexer API

Provides access to the Simple lexer for token-level inspection and testing.
Uses FFI to call the Rust lexer.
"""

use core.option.*
use core.list.*

# FFI function for tokenization
extern fn lexer_tokenize(source: text) -> List<Dict<text, text>>

# Token information from lexer
struct Token:
    kind: text       # Token kind (e.g., "Identifier", "Skip", "Integer")
    text: text       # Original source text
    name: text?      # For Identifier tokens
    pattern: text?   # For Identifier tokens ("Immutable", "Mutable", etc.)
    value: i64?      # For Integer tokens

impl Token:
    fn to_string() -> text:
        match self.kind:
            "Identifier":
                val n = self.name ?? "?"
                val p = self.pattern ?? "?"
                return "Identifier({n}, {p})"
            "Integer":
                val v = self.value ?? 0
                return "Integer({v})"
            _:
                return self.kind

    fn is_identifier() -> bool:
        self.kind == "Identifier"

    fn is_keyword() -> bool:
        self.kind != "Identifier" and self.kind != "Integer" and self.kind != "Float" and self.kind != "Eof"

# Tokenize source code using Rust lexer via FFI
pub fn tokenize(source: text) -> List<Token>:
    # Call FFI function
    val raw_tokens = lexer_tokenize(source)

    # Convert raw dictionaries to Token structs
    var tokens = []
    for raw in raw_tokens:
        val token = Token(
            kind: raw.get("kind") ?? "Unknown",
            text: raw.get("text") ?? "",
            name: raw.get("name"),
            pattern: raw.get("pattern"),
            value: raw.get("value")
        )
        tokens.append(token)

    return tokens

# Get just the token kind names
pub fn token_kinds(source: text) -> List<text>:
    return tokenize(source).map(\t: t.kind)

# Get just the token texts
pub fn token_texts(source: text) -> List<text>:
    return tokenize(source).map(\t: t.text)

# Get identifier names from source
pub fn identifier_names(source: text) -> List<text>:
    var names = []
    for token in tokenize(source):
        if token.is_identifier():
            if token.name.?:
                names.append(token.name ?? "")
    return names
