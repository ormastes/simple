# Training Handlers and Callbacks
#
# Provides training callbacks for monitoring, checkpointing, and early stopping.
#
# ## Classes
# - `EarlyStopping`: Stop training when metric stops improving
# - `ModelCheckpoint`: Save model at best/periodic checkpoints
# - `GradientClipper`: Clip gradients to prevent explosion
# - `ProgressBar`: Display training progress
# - `LRFinder`: Find optimal learning rate
# - `ModelSummary`: Print model architecture summary
#
# ## Example
# ```simple
# import ml.engine.handlers as h
#
# # Early stopping
# val early_stop = h.EarlyStopping(patience=10, min_delta=0.001)
#
# for epoch in range(100):
#     val loss = train_epoch()
#     if early_stop.should_stop(loss):
#         print("Early stopping!")
#         break
#
# # Gradient clipping
# val clipper = h.GradientClipper(max_norm=1.0)
# clipper.clip(model.parameters())
# ```

pub use EarlyStopping, ModelCheckpoint, GradientClipper, ProgressBar, LRFinder, ModelSummary


# ============================================================================
# EarlyStopping
# ============================================================================

class EarlyStopping:
    """Stop training when monitored metric stops improving.

    Tracks a metric and determines when to stop training if no improvement
    is seen for 'patience' number of epochs.

    Attributes:
        patience: Number of epochs with no improvement before stopping
        min_delta: Minimum change to qualify as improvement
        mode: 'min' or 'max' (minimize or maximize metric)
        best: Best metric value seen
        counter: Number of epochs without improvement

    Example:
        ```simple
        val early_stop = EarlyStopping(patience=10, min_delta=0.001)

        for epoch in range(100):
            val val_loss = validate()
            if early_stop.should_stop(val_loss):
                print("Stopping early at epoch {epoch}")
                break
        ```
    """
    patience: i32
    min_delta: f64
    mode: str
    best: f64
    counter: i32
    stopped_epoch: i32

    fn __init__(
        patience: i32 = 10,
        min_delta: f64 = 0.0,
        mode: str = "min"
    ):
        """Initialize early stopping.

        Args:
            patience: Epochs without improvement before stopping (default: 10)
            min_delta: Minimum improvement threshold (default: 0.0)
            mode: 'min' to minimize, 'max' to maximize (default: 'min')
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.stopped_epoch = -1

        # Initialize best based on mode
        if mode == "min":
            self.best = 1e10  # Large value
        else:
            self.best = -1e10  # Small value

    fn should_stop(metric: f64) -> bool:
        """Check if training should stop.

        Args:
            metric: Current metric value

        Returns:
            True if training should stop
        """
        val is_improvement = if self.mode == "min":
            metric < self.best - self.min_delta
        else:
            metric > self.best + self.min_delta

        if is_improvement:
            self.best = metric
            self.counter = 0
            return false
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return true
            return false

    fn reset():
        """Reset early stopping state."""
        self.counter = 0
        if self.mode == "min":
            self.best = 1e10
        else:
            self.best = -1e10


# ============================================================================
# ModelCheckpoint
# ============================================================================

class ModelCheckpoint:
    """Save model checkpoints during training.

    Saves model weights at best performance or at regular intervals.

    Attributes:
        filepath: Path template for saving (use {epoch} and {metric} placeholders)
        save_best_only: Only save when metric improves
        mode: 'min' or 'max' for metric
        best: Best metric value seen

    Example:
        ```simple
        val checkpoint = ModelCheckpoint(
            filepath="checkpoints/model_{epoch}.pt",
            save_best_only=true,
            mode="min"
        )

        for epoch in range(100):
            val loss = train_epoch()
            checkpoint.step(epoch, loss, model)
        ```
    """
    filepath: str
    save_best_only: bool
    mode: str
    best: f64
    save_freq: i32

    fn __init__(
        filepath: str = "checkpoint_{epoch}.pt",
        save_best_only: bool = true,
        mode: str = "min",
        save_freq: i32 = 1
    ):
        """Initialize checkpoint saver.

        Args:
            filepath: Save path template (default: "checkpoint_{epoch}.pt")
            save_best_only: Only save on improvement (default: true)
            mode: 'min' or 'max' (default: 'min')
            save_freq: Save every N epochs if not save_best_only (default: 1)
        """
        self.filepath = filepath
        self.save_best_only = save_best_only
        self.mode = mode
        self.save_freq = save_freq

        if mode == "min":
            self.best = 1e10
        else:
            self.best = -1e10

    fn step(epoch: i32, metric: f64, model_handle: u64):
        """Check and potentially save checkpoint.

        Args:
            epoch: Current epoch number
            metric: Current metric value
            model_handle: Handle to model to save
        """
        if self.save_best_only:
            val is_better = if self.mode == "min":
                metric < self.best
            else:
                metric > self.best

            if is_better:
                self.best = metric
                self._save(epoch, metric, model_handle)
        else:
            if epoch % self.save_freq == 0:
                self._save(epoch, metric, model_handle)

    fn _save(epoch: i32, metric: f64, model_handle: u64):
        """Save model checkpoint."""
        # Format filepath with epoch and metric
        val path = self.filepath
            .replace("{epoch}", epoch.to_string())
            .replace("{metric}", metric.to_string())

        # Save via FFI
        @rt_torch_save_checkpoint(model_handle, path.as_ptr(), path.len() as i32)


# ============================================================================
# GradientClipper
# ============================================================================

class GradientClipper:
    """Clip gradients to prevent gradient explosion.

    Clips gradients by value or by norm to ensure stable training.

    Attributes:
        max_norm: Maximum gradient norm (for norm clipping)
        max_value: Maximum gradient value (for value clipping)
        norm_type: Norm type for norm clipping (1, 2, or inf)

    Example:
        ```simple
        val clipper = GradientClipper(max_norm=1.0)

        # In training loop, after backward pass:
        loss.backward()
        clipper.clip(model.parameters())
        optimizer.step()
        ```
    """
    max_norm: f64
    max_value: f64
    norm_type: f64

    fn __init__(
        max_norm: f64 = 0.0,
        max_value: f64 = 0.0,
        norm_type: f64 = 2.0
    ):
        """Initialize gradient clipper.

        Args:
            max_norm: Max gradient norm (0 to disable, default: 0.0)
            max_value: Max gradient value (0 to disable, default: 0.0)
            norm_type: Norm type (default: 2.0 for L2 norm)
        """
        self.max_norm = max_norm
        self.max_value = max_value
        self.norm_type = norm_type

    fn clip(params: [u64]) -> f64:
        """Clip gradients of parameters.

        Args:
            params: List of parameter handles

        Returns:
            Total gradient norm before clipping
        """
        if self.max_norm > 0.0:
            return @rt_torch_clip_grad_norm(
                params.data_ptr(),
                params.len() as i32,
                self.max_norm,
                self.norm_type
            )
        elif self.max_value > 0.0:
            @rt_torch_clip_grad_value(
                params.data_ptr(),
                params.len() as i32,
                self.max_value
            )
            return 0.0
        else:
            return 0.0


# ============================================================================
# ProgressBar
# ============================================================================

class ProgressBar:
    """Display training progress bar.

    Shows progress during training with metrics.

    Attributes:
        total: Total number of steps
        current: Current step
        width: Bar width in characters

    Example:
        ```simple
        val pbar = ProgressBar(total=100)

        for epoch in range(100):
            val loss = train_epoch()
            pbar.update(1, {"loss": loss})

        pbar.close()
        ```
    """
    total: i32
    current: i32
    width: i32
    desc: str

    fn __init__(total: i32, desc: str = "", width: i32 = 40):
        """Initialize progress bar.

        Args:
            total: Total number of steps
            desc: Description prefix (default: "")
            width: Bar width in chars (default: 40)
        """
        self.total = total
        self.current = 0
        self.width = width
        self.desc = desc

    fn update(n: i32 = 1, metrics: {str: f64} = {}):
        """Update progress bar.

        Args:
            n: Number of steps to advance (default: 1)
            metrics: Dictionary of metrics to display
        """
        self.current += n

        val pct = (self.current as f64 / self.total as f64) * 100.0
        val filled = ((self.current as f64 / self.total as f64) * self.width as f64) as i32

        # Build progress bar string
        var bar = "["
        for i in range(self.width):
            if i < filled:
                bar = bar + "="
            else:
                bar = bar + " "
        bar = bar + "]"

        # Print progress
        var line = "\r"
        if self.desc != "":
            line = line + self.desc + ": "
        line = line + "{self.current}/{self.total} {bar} {pct:.1f}%"

        # Add metrics
        for (key, value) in metrics:
            line = line + " - {key}: {value:.4f}"

        print(line, end="")

    fn close():
        """Close progress bar and print newline."""
        print("")


# ============================================================================
# LRFinder
# ============================================================================

class LRFinder:
    """Learning rate range finder.

    Runs training with exponentially increasing learning rate to find
    optimal learning rate range.

    Attributes:
        min_lr: Starting learning rate
        max_lr: Ending learning rate
        num_steps: Number of steps to run

    Example:
        ```simple
        val finder = LRFinder(min_lr=1e-7, max_lr=10.0)
        val (lrs, losses) = finder.run(model, train_loader)

        # Plot lrs vs losses to find optimal LR
        # Good LR is where loss is still decreasing but not minimum
        ```
    """
    min_lr: f64
    max_lr: f64
    num_steps: i32
    lrs: [f64]
    losses: [f64]

    fn __init__(
        min_lr: f64 = 1e-7,
        max_lr: f64 = 10.0,
        num_steps: i32 = 100
    ):
        """Initialize LR finder.

        Args:
            min_lr: Starting learning rate (default: 1e-7)
            max_lr: Ending learning rate (default: 10.0)
            num_steps: Number of steps (default: 100)
        """
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.num_steps = num_steps
        self.lrs = []
        self.losses = []

    fn get_lr_at_step(step: i32) -> f64:
        """Get learning rate for given step.

        Uses exponential schedule from min_lr to max_lr.

        Args:
            step: Current step number

        Returns:
            Learning rate for this step
        """
        val mult = (self.max_lr / self.min_lr) ** (1.0 / self.num_steps as f64)
        return self.min_lr * (mult ** step as f64)

    fn suggest_lr() -> f64:
        """Suggest optimal learning rate.

        Finds steepest loss decrease point.

        Returns:
            Suggested learning rate
        """
        if self.lrs.len() < 2:
            return self.min_lr

        # Find point with steepest negative slope
        var best_lr = self.lrs[0]
        var best_slope = 0.0

        for i in range(1, self.losses.len()):
            val slope = self.losses[i] - self.losses[i - 1]
            if slope < best_slope:
                best_slope = slope
                best_lr = self.lrs[i]

        return best_lr


# ============================================================================
# ModelSummary
# ============================================================================

class ModelSummary:
    """Print model architecture summary.

    Displays layer information, parameter counts, and shapes.

    Example:
        ```simple
        val summary = ModelSummary()
        summary.print(model, input_size=[1, 3, 224, 224])
        ```
    """
    include_params: bool
    max_depth: i32

    fn __init__(include_params: bool = true, max_depth: i32 = 3):
        """Initialize model summary.

        Args:
            include_params: Include parameter count (default: true)
            max_depth: Maximum depth to traverse (default: 3)
        """
        self.include_params = include_params
        self.max_depth = max_depth

    fn print_summary(model_handle: u64, input_shape: [i64]):
        """Print model summary.

        Args:
            model_handle: Handle to model
            input_shape: Example input shape
        """
        print("=" * 60)
        print("Model Summary")
        print("=" * 60)
        print("Input shape: {input_shape}")
        print("-" * 60)

        # Get summary info via FFI
        val total_params = @rt_torch_model_param_count(model_handle)
        val trainable_params = @rt_torch_model_trainable_param_count(model_handle)

        print("-" * 60)
        print("Total parameters: {total_params}")
        print("Trainable parameters: {trainable_params}")
        print("Non-trainable parameters: {total_params - trainable_params}")
        print("=" * 60)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_save_checkpoint(model: u64, path: *u8, path_len: i32) -> i32
extern fn rt_torch_clip_grad_norm(params: *u64, num_params: i32, max_norm: f64, norm_type: f64) -> f64
extern fn rt_torch_clip_grad_value(params: *u64, num_params: i32, max_value: f64) -> i32
extern fn rt_torch_model_param_count(model: u64) -> i64
extern fn rt_torch_model_trainable_param_count(model: u64) -> i64
