# Optimization Algorithms
#
# Provides optimization algorithms and learning rate schedulers for training neural networks.
#
# ## Optimizers
# - `Optimizer`: Base class for all optimizers
# - `SGD`: Stochastic Gradient Descent with momentum
# - `Adam`: Adaptive Moment Estimation
# - `AdamW`: Adam with decoupled weight decay
# - `RMSprop`: Root Mean Square Propagation
#
# ## Learning Rate Schedulers
# - `LRScheduler`: Base class for learning rate schedulers
# - `StepLR`: Decays learning rate by gamma every step_size epochs
# - `ExponentialLR`: Decays learning rate by gamma every epoch
# - `CosineAnnealingLR`: Cosine annealing schedule
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.nn as nn
# import ml.torch.optim as optim
#
# # Create model and optimizer
# val model = MyModel()
# val optimizer = optim.Adam(model.parameters(), lr=0.001)
# val scheduler = optim.StepLR(optimizer, step_size=30, gamma=0.1)
#
# # Training loop
# for epoch in range(100):
#     val output = model(inputs)
#     val loss = compute_loss(output, labels)
#
#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()
#     scheduler.step()
# ```

pub use Optimizer, SGD, Adam, AdamW, RMSprop
pub use LRScheduler, StepLR, ExponentialLR, CosineAnnealingLR
pub use ReduceLROnPlateau, OneCycleLR, WarmupCosineAnnealingLR, LinearWarmupLR

use .. as torch


# ============================================================================
# Optimizer Base Class
# ============================================================================

class Optimizer:
    """Base class for all optimizers.

    All optimizer classes should inherit from this.
    """
    optimizer_handle: u64
    lr: f64

    fn __init__(lr: f64):
        """Initialize optimizer.

        Args:
            lr: Learning rate
        """
        self.lr = lr
        self.optimizer_handle = 0

    fn __del__():
        """Free optimizer resources."""
        if self.optimizer_handle != 0:
            @rt_torch_optimizer_free(self.optimizer_handle)

    fn zero_grad():
        """Zero all parameter gradients.

        Call this before backward pass to clear old gradients.
        """
        if self.optimizer_handle != 0:
            @rt_torch_optimizer_zero_grad(self.optimizer_handle)

    fn step():
        """Update parameters using computed gradients.

        Call this after backward pass to apply gradients.
        """
        if self.optimizer_handle != 0:
            @rt_torch_optimizer_step(self.optimizer_handle)


# ============================================================================
# SGD Optimizer
# ============================================================================

class SGD(Optimizer):
    """Stochastic Gradient Descent optimizer.

    Implements SGD with optional momentum and weight decay.

    Update rule:
        v = momentum * v + (1 - dampening) * grad
        param = param - lr * (v + weight_decay * param)

    Example:
        ```simple
        val optimizer = optim.SGD(
            model.parameters(),
            lr=0.1,
            momentum=0.9,
            weight_decay=0.0001
        )
        ```
    """
    momentum: f64
    weight_decay: f64

    fn __init__(params: [u64],
        lr: f64,
        momentum: f64 = 0.0,
        weight_decay: f64 = 0.0
    ):
        """Initialize SGD optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate
            momentum: Momentum factor (default: 0.0)
            weight_decay: Weight decay (L2 penalty) (default: 0.0)
        """
        super().__init__(lr)
        self.momentum = momentum
        self.weight_decay = weight_decay

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_sgd_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            momentum,
            weight_decay
        )
        if self.optimizer_handle == 0:
            panic("Failed to create SGD optimizer")


# ============================================================================
# Adam Optimizer
# ============================================================================

class Adam(Optimizer):
    """Adam optimizer (Adaptive Moment Estimation).

    Maintains per-parameter adaptive learning rates using first and second
    moment estimates of gradients.

    Example:
        ```simple
        val optimizer = optim.Adam(
            model.parameters(),
            lr=0.001,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        ```
    """
    beta1: f64
    beta2: f64
    eps: f64
    weight_decay: f64

    fn __init__(params: [u64],
        lr: f64 = 0.001,
        betas: (f64, f64) = (0.9, 0.999),
        eps: f64 = 1e-8,
        weight_decay: f64 = 0.0
    ):
        """Initialize Adam optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate (default: 0.001)
            betas: Coefficients for first and second moment (default: (0.9, 0.999))
            eps: Term for numerical stability (default: 1e-8)
            weight_decay: Weight decay (L2 penalty) (default: 0.0)
        """
        super().__init__(lr)
        self.beta1 = betas.0
        self.beta2 = betas.1
        self.eps = eps
        self.weight_decay = weight_decay

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_adam_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            self.beta1,
            self.beta2,
            eps,
            weight_decay
        )
        if self.optimizer_handle == 0:
            panic("Failed to create Adam optimizer")


# ============================================================================
# AdamW Optimizer
# ============================================================================

class AdamW(Optimizer):
    """AdamW optimizer (Adam with decoupled weight decay).

    Decouples weight decay from gradient-based updates, often leading to
    better generalization than standard Adam with L2 regularization.

    Example:
        ```simple
        val optimizer = optim.AdamW(
            model.parameters(),
            lr=0.001,
            weight_decay=0.01
        )
        ```
    """
    beta1: f64
    beta2: f64
    eps: f64
    weight_decay: f64

    fn __init__(params: [u64],
        lr: f64 = 0.001,
        betas: (f64, f64) = (0.9, 0.999),
        eps: f64 = 1e-8,
        weight_decay: f64 = 0.01
    ):
        """Initialize AdamW optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate (default: 0.001)
            betas: Coefficients for first and second moment (default: (0.9, 0.999))
            eps: Term for numerical stability (default: 1e-8)
            weight_decay: Weight decay (default: 0.01)
        """
        super().__init__(lr)
        self.beta1 = betas.0
        self.beta2 = betas.1
        self.eps = eps
        self.weight_decay = weight_decay

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_adamw_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            self.beta1,
            self.beta2,
            eps,
            weight_decay
        )
        if self.optimizer_handle == 0:
            panic("Failed to create AdamW optimizer")


# ============================================================================
# RMSprop Optimizer
# ============================================================================

class RMSprop(Optimizer):
    """RMSprop optimizer (Root Mean Square Propagation).

    Maintains a moving average of squared gradients to normalize the gradient.
    Particularly effective for recurrent neural networks.

    Update rule:
        E[g²]_t = α * E[g²]_(t-1) + (1 - α) * g²_t
        param = param - lr * g / (sqrt(E[g²]) + ε)

    Example:
        ```simple
        val optimizer = optim.RMSprop(
            model.parameters(),
            lr=0.01,
            alpha=0.99,
            eps=1e-8
        )
        ```
    """
    alpha: f64
    eps: f64
    weight_decay: f64
    momentum: f64

    fn __init__(params: [u64],
        lr: f64 = 0.01,
        alpha: f64 = 0.99,
        eps: f64 = 1e-8,
        weight_decay: f64 = 0.0,
        momentum: f64 = 0.0
    ):
        """Initialize RMSprop optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate (default: 0.01)
            alpha: Smoothing constant for moving average (default: 0.99)
            eps: Term for numerical stability (default: 1e-8)
            weight_decay: Weight decay (L2 penalty) (default: 0.0)
            momentum: Momentum factor (default: 0.0)
        """
        super().__init__(lr)
        self.alpha = alpha
        self.eps = eps
        self.weight_decay = weight_decay
        self.momentum = momentum

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_rmsprop_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            alpha,
            eps,
            weight_decay,
            momentum
        )
        if self.optimizer_handle == 0:
            panic("Failed to create RMSprop optimizer")


# ============================================================================
# Learning Rate Schedulers
# ============================================================================

class LRScheduler:
    """Base class for learning rate schedulers.

    Adjusts the learning rate during training.
    """
    optimizer: Optimizer
    last_epoch: i32

    fn __init__(optimizer: Optimizer, last_epoch: i32 = -1):
        """Initialize scheduler.

        Args:
            optimizer: Wrapped optimizer
            last_epoch: The index of last epoch (default: -1)
        """
        self.optimizer = optimizer
        self.last_epoch = last_epoch

    fn step():
        """Update learning rate."""
        self.last_epoch += 1
        val new_lr = self.get_lr()
        self.optimizer.lr = new_lr
        # Update optimizer handle's LR via FFI
        if self.optimizer.optimizer_handle != 0:
            @rt_torch_optimizer_set_lr(self.optimizer.optimizer_handle, new_lr)

    fn get_lr() -> f64:
        """Calculate current learning rate.

        Override in subclass.
        """
        return self.optimizer.lr


class StepLR(LRScheduler):
    """Decays learning rate by gamma every step_size epochs.

    Example:
        ```simple
        val optimizer = optim.Adam(model.parameters(), lr=0.1)
        val scheduler = optim.StepLR(optimizer, step_size=30, gamma=0.1)

        for epoch in range(100):
            train(...)
            scheduler.step()
        ```
    """
    step_size: i32
    gamma: f64
    base_lr: f64

    fn __init__(optimizer: Optimizer, step_size: i32, gamma: f64 = 0.1):
        """Initialize StepLR scheduler.

        Args:
            optimizer: Wrapped optimizer
            step_size: Period of learning rate decay
            gamma: Multiplicative factor of learning rate decay (default: 0.1)
        """
        super().__init__(optimizer)
        self.step_size = step_size
        self.gamma = gamma
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate."""
        val decay_count = (self.last_epoch + 1) / self.step_size
        return self.base_lr * (self.gamma ** decay_count)


class ExponentialLR(LRScheduler):
    """Decays learning rate by gamma every epoch.

    Example:
        ```simple
        val optimizer = optim.Adam(model.parameters(), lr=0.1)
        val scheduler = optim.ExponentialLR(optimizer, gamma=0.95)

        for epoch in range(100):
            train(...)
            scheduler.step()
        ```
    """
    gamma: f64
    base_lr: f64

    fn __init__(optimizer: Optimizer, gamma: f64):
        """Initialize ExponentialLR scheduler.

        Args:
            optimizer: Wrapped optimizer
            gamma: Multiplicative factor of learning rate decay
        """
        super().__init__(optimizer)
        self.gamma = gamma
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate."""
        return self.base_lr * (self.gamma ** (self.last_epoch + 1))


class CosineAnnealingLR(LRScheduler):
    """Set learning rate using cosine annealing schedule.

    Learning rate is set to:
        η_t = η_min + (η_max - η_min) * (1 + cos(π * T_cur / T_max)) / 2

    where η_max is base_lr, T_cur is current epoch, T_max is max epochs.

    Example:
        ```simple
        val optimizer = optim.Adam(model.parameters(), lr=0.1)
        val scheduler = optim.CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001)

        for epoch in range(100):
            train(...)
            scheduler.step()
        ```
    """
    T_max: i32
    eta_min: f64
    base_lr: f64

    fn __init__(optimizer: Optimizer, T_max: i32, eta_min: f64 = 0.0):
        """Initialize CosineAnnealingLR scheduler.

        Args:
            optimizer: Wrapped optimizer
            T_max: Maximum number of iterations
            eta_min: Minimum learning rate (default: 0.0)
        """
        super().__init__(optimizer)
        self.T_max = T_max
        self.eta_min = eta_min
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate using cosine annealing."""
        if self.last_epoch == 0:
            return self.base_lr

        # Use cosine annealing formula
        # η_t = η_min + (η_max - η_min) * (1 + cos(π * T_cur / T_max)) / 2
        val progress = (self.last_epoch % self.T_max) as f64 / self.T_max as f64
        val pi = 3.141592653589793
        val cosine_value = @rt_torch_cos(pi * progress)
        val cosine_factor = (1.0 + cosine_value) / 2.0
        return self.eta_min + (self.base_lr - self.eta_min) * cosine_factor


class ReduceLROnPlateau:
    """Reduce learning rate when a metric has stopped improving.

    Models often benefit from reducing the learning rate by a factor
    of 2-10 once learning stagnates. This scheduler reads a metric
    quantity and if no improvement is seen for a 'patience' number
    of epochs, the learning rate is reduced.

    Attributes:
        optimizer: Wrapped optimizer
        mode: 'min' or 'max' - minimize or maximize metric
        factor: Factor by which learning rate is reduced
        patience: Number of epochs with no improvement
        threshold: Threshold for measuring improvement
        cooldown: Epochs to wait after LR reduction before resuming

    Example:
        ```simple
        val scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=10)

        for epoch in range(100):
            train(...)
            val loss = validate(...)
            scheduler.step(loss)  # Pass metric value
        ```
    """
    optimizer: Optimizer
    mode: str
    factor: f64
    patience: i32
    threshold: f64
    cooldown: i32
    min_lr: f64
    _best: f64
    _num_bad_epochs: i32
    _cooldown_counter: i32

    fn __init__(optimizer: Optimizer,
        mode: str = "min",
        factor: f64 = 0.1,
        patience: i32 = 10,
        threshold: f64 = 1e-4,
        cooldown: i32 = 0,
        min_lr: f64 = 0.0
    ):
        """Initialize ReduceLROnPlateau scheduler.

        Args:
            optimizer: Wrapped optimizer
            mode: 'min' or 'max' (default: 'min')
            factor: LR reduction factor (default: 0.1)
            patience: Epochs to wait (default: 10)
            threshold: Improvement threshold (default: 1e-4)
            cooldown: Cooldown epochs (default: 0)
            min_lr: Minimum LR (default: 0.0)
        """
        self.optimizer = optimizer
        self.mode = mode
        self.factor = factor
        self.patience = patience
        self.threshold = threshold
        self.cooldown = cooldown
        self.min_lr = min_lr
        self._num_bad_epochs = 0
        self._cooldown_counter = 0

        # Initialize best value based on mode
        if mode == "min":
            self._best = 1e30  # Large value
        else:
            self._best = -1e30  # Small value

    fn step(metric: f64):
        """Update scheduler with current metric value.

        Args:
            metric: Current metric value to monitor
        """
        if self._cooldown_counter > 0:
            self._cooldown_counter -= 1
            return

        val improved = self._is_better(metric)

        if improved:
            self._best = metric
            self._num_bad_epochs = 0
        else:
            self._num_bad_epochs += 1

        if self._num_bad_epochs > self.patience:
            self._reduce_lr()
            self._cooldown_counter = self.cooldown
            self._num_bad_epochs = 0

    fn _is_better(metric: f64) -> bool:
        """Check if metric improved."""
        if self.mode == "min":
            return metric < self._best - self.threshold
        else:
            return metric > self._best + self.threshold

    fn _reduce_lr():
        """Reduce learning rate."""
        val old_lr = self.optimizer.lr
        val new_lr = old_lr * self.factor
        if new_lr < self.min_lr:
            new_lr = self.min_lr
        self.optimizer.lr = new_lr
        if self.optimizer.optimizer_handle != 0:
            @rt_torch_optimizer_set_lr(self.optimizer.optimizer_handle, new_lr)


class LinearWarmupLR(LRScheduler):
    """Linear warmup scheduler.

    Linearly increases learning rate from 0 to base_lr over warmup_steps.

    Example:
        ```simple
        val scheduler = LinearWarmupLR(optimizer, warmup_steps=1000)
        ```
    """
    warmup_steps: i32
    base_lr: f64

    fn __init__(optimizer: Optimizer, warmup_steps: i32):
        """Initialize linear warmup scheduler.

        Args:
            optimizer: Wrapped optimizer
            warmup_steps: Number of warmup steps
        """
        super().__init__(optimizer)
        self.warmup_steps = warmup_steps
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate."""
        if self.last_epoch < self.warmup_steps:
            return self.base_lr * (self.last_epoch + 1) as f64 / self.warmup_steps as f64
        return self.base_lr


class WarmupCosineAnnealingLR(LRScheduler):
    """Cosine annealing with linear warmup.

    Combines linear warmup with cosine decay. Common in transformer training.

    Attributes:
        warmup_steps: Steps for linear warmup
        T_max: Total steps for cosine decay (after warmup)
        eta_min: Minimum learning rate

    Example:
        ```simple
        val scheduler = WarmupCosineAnnealingLR(
            optimizer,
            warmup_steps=1000,
            T_max=100000,
            eta_min=1e-6
        )
        ```
    """
    warmup_steps: i32
    T_max: i32
    eta_min: f64
    base_lr: f64

    fn __init__(optimizer: Optimizer, warmup_steps: i32, T_max: i32, eta_min: f64 = 0.0):
        """Initialize warmup cosine scheduler.

        Args:
            optimizer: Wrapped optimizer
            warmup_steps: Number of warmup steps
            T_max: Total training steps (including warmup)
            eta_min: Minimum learning rate (default: 0.0)
        """
        super().__init__(optimizer)
        self.warmup_steps = warmup_steps
        self.T_max = T_max
        self.eta_min = eta_min
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate with warmup."""
        if self.last_epoch < self.warmup_steps:
            # Linear warmup phase
            return self.base_lr * (self.last_epoch + 1) as f64 / self.warmup_steps as f64

        # Cosine decay phase
        val decay_steps = self.T_max - self.warmup_steps
        val current_step = self.last_epoch - self.warmup_steps
        val progress = current_step as f64 / decay_steps as f64
        val pi = 3.141592653589793
        val cosine_value = @rt_torch_cos(pi * progress)
        val cosine_factor = (1.0 + cosine_value) / 2.0
        return self.eta_min + (self.base_lr - self.eta_min) * cosine_factor


class OneCycleLR(LRScheduler):
    """1Cycle learning rate policy.

    The 1Cycle policy anneals the learning rate from an initial learning
    rate to some maximum learning rate and then from that maximum learning
    rate to some minimum learning rate much lower than the initial learning rate.

    This policy was initially described in the paper "Super-Convergence".

    Attributes:
        max_lr: Maximum learning rate in the cycle
        total_steps: Total training steps
        pct_start: Percentage of cycle spent increasing LR
        div_factor: Initial LR = max_lr / div_factor
        final_div_factor: Final LR = max_lr / final_div_factor

    Example:
        ```simple
        val scheduler = OneCycleLR(
            optimizer,
            max_lr=0.1,
            total_steps=10000,
            pct_start=0.3
        )
        ```
    """
    max_lr: f64
    total_steps: i32
    pct_start: f64
    div_factor: f64
    final_div_factor: f64
    initial_lr: f64
    final_lr: f64
    up_steps: i32

    fn __init__(optimizer: Optimizer,
        max_lr: f64,
        total_steps: i32,
        pct_start: f64 = 0.3,
        div_factor: f64 = 25.0,
        final_div_factor: f64 = 10000.0
    ):
        """Initialize OneCycleLR scheduler.

        Args:
            optimizer: Wrapped optimizer
            max_lr: Maximum learning rate
            total_steps: Total training steps
            pct_start: Fraction for warmup (default: 0.3)
            div_factor: Initial LR divisor (default: 25.0)
            final_div_factor: Final LR divisor (default: 10000.0)
        """
        super().__init__(optimizer)
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.pct_start = pct_start
        self.div_factor = div_factor
        self.final_div_factor = final_div_factor

        # Calculate LR bounds
        self.initial_lr = max_lr / div_factor
        self.final_lr = max_lr / final_div_factor
        self.up_steps = (total_steps as f64 * pct_start) as i32

        # Set initial LR
        optimizer.lr = self.initial_lr

    fn get_lr() -> f64:
        """Calculate current learning rate."""
        if self.last_epoch < self.up_steps:
            # Warmup phase: linear from initial_lr to max_lr
            val progress = self.last_epoch as f64 / self.up_steps as f64
            return self.initial_lr + (self.max_lr - self.initial_lr) * progress
        else:
            # Annealing phase: cosine from max_lr to final_lr
            val down_steps = self.total_steps - self.up_steps
            val current = self.last_epoch - self.up_steps
            val progress = current as f64 / down_steps as f64
            val pi = 3.141592653589793
            val cosine_value = @rt_torch_cos(pi * progress)
            val cosine_factor = (1.0 + cosine_value) / 2.0
            return self.final_lr + (self.max_lr - self.final_lr) * cosine_factor


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_optimizer_free(optimizer: u64) -> i32
extern fn rt_torch_optimizer_zero_grad(optimizer: u64) -> i32
extern fn rt_torch_optimizer_step(optimizer: u64) -> i32
extern fn rt_torch_optimizer_set_lr(optimizer: u64, lr: f64) -> i32

extern fn rt_torch_sgd_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    momentum: f64,
    weight_decay: f64
) -> u64

extern fn rt_torch_adam_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    beta1: f64,
    beta2: f64,
    eps: f64,
    weight_decay: f64
) -> u64

extern fn rt_torch_adamw_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    beta1: f64,
    beta2: f64,
    eps: f64,
    weight_decay: f64
) -> u64

extern fn rt_torch_rmsprop_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    alpha: f64,
    eps: f64,
    weight_decay: f64,
    momentum: f64
) -> u64

extern fn rt_torch_cos(x: f64) -> f64
