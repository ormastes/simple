# PyTorch - GPU-Accelerated Tensor Operations
#
# Wrapper around PyTorch's C++ backend for tensor operations and neural networks.
# Supports both CPU and CUDA devices for high-performance computation.
#
# ## Modules
# - `nn`: Neural network layers and modules
# - `optim`: Optimization algorithms (SGD, Adam, AdamW)
# - `autograd`: Automatic differentiation
# - `data`: Dataset and DataLoader utilities
# - `transforms`: Data augmentation and preprocessing
# - `utils`: Training utilities (TensorBoard, checkpointing, AMP)
# - `vision`: Computer vision models and datasets (ResNet, VGG, ImageNet)
# - `distributed`: Multi-GPU training (DDP, NCCL, process groups)
# - `onnx`: Model export (ONNX, TorchScript)
#
# ## Core Components (from submodules)
# - `device`: Device management (CPU/CUDA enumeration)
# - `dtype`: Data type enumeration (Float32, Float64, Int32, Int64)
# - `tensor`: Multi-dimensional array class with GPU support
# - `typed_tensor`: Tensor with compile-time dimension tracking and inference
# - `factory`: Tensor factory functions (zeros, ones, randn, arange, stack)
# - `checkpoint`: Model save/load utilities
# - `training`: Training utilities (ParameterTracker, EarlyStopping)
#
# ## Classes
# - `Tensor`: Multi-dimensional array with GPU support
# - `TypedTensor`: Tensor with compile-time dimension inference and runtime verification
# - `TensorType`: Type-level tensor shape and dtype specification
# - `DimSpec`: Dimension specification (exact, named, ranged, dynamic)
# - `MemoryReport`: Memory estimation report for typed tensors
# - `Device`: Device specification (CPU/CUDA)
# - `DType`: Data type enumeration
# - `ParameterTracker`: Track parameter and gradient statistics during training
# - `ParameterStats`: Statistics for a single parameter
# - `EarlyStopping`: Early stopping callback to prevent overfitting
#
# ## Functions
# - `cuda_available()`: Check if CUDA is available
# - `cuda_device_count()`: Get number of CUDA devices
# - `zeros()`, `ones()`, `randn()`, `arange()`, `stack()`: Tensor factory functions
# - `save()`, `load()`: Model checkpoint save/load
#
# ## Example
# ```simple
# import ml.torch as torch
#
# # Check CUDA availability
# if torch.cuda_available():
#     print("Found {torch.cuda_device_count()} CUDA devices")
#     val device = torch.Device.CUDA(0)
# else:
#     print("CUDA not available, using CPU")
#     val device = torch.Device.CPU
#
# # Create tensors
# val x = torch.zeros([3, 3], device=device)
# val y = torch.ones([3, 3], device=device)
# val z = x + y
# ```

# Import and re-export from submodules
use device.{Device, device_code}
# Note: cuda_available and cuda_device_count temporarily disabled due to parser issue
use dtype.{DType, dtype_code}
use tensor_class.{Tensor}
use typed_tensor.{TypedTensor, TensorType, DimSpec, MemoryReport}
use factory.{from_data, zeros, ones, randn, arange, stack, select, zeros_like}
use checkpoint.{save, load}
use training.{ParameterStats, ParameterTracker, EarlyStopping}

# Import new math/science modules
use fft
use linalg
use simple_math

# Import FFI helper utilities
use ffi_helpers

# Export core types and functions
export Device, device_code, DType, dtype_code
export Tensor
export TypedTensor, TensorType, DimSpec, MemoryReport
export ParameterTracker, ParameterStats, EarlyStopping
export from_data, zeros, ones, randn, arange, stack, select, zeros_like
# export cuda_available, cuda_device_count  # Temporarily disabled
export save, load

# Import submodules (these are exported as modules)
# Temporarily disabled to isolate parse error
# import nn
# import optim
# import autograd
# import data
# import transforms
# import utils
# import vision
# import distributed
# import onnx

# New ML infrastructure modules
# Cache and validation modules use explicit function callbacks instead of lambdas
use cache
use validation
