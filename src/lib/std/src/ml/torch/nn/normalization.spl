# Neural Network - Normalization Layers
#
# Batch normalization and layer normalization for neural networks.

pub use BatchNorm1d, BatchNorm2d, LayerNorm

use ml.torch.tensor_class.{Tensor}
use base.{FFIModule, Module}

# ============================================================================
# Batch Normalization Layers
# ============================================================================

class BatchNorm1d(FFIModule):
    """1D Batch Normalization layer.

    Normalizes mini-batches of 1D inputs (features or sequences).
    Helps stabilize training and allows higher learning rates.

    Example:
        ```simple
        # Input: [batch, features]
        val bn = nn.BatchNorm1d(num_features=128)
        val normalized = bn(x)
        ```
    """
    num_features: i32
    eps: f64
    momentum: f64
    affine: bool
    track_running_stats: bool

    fn __init__(num_features: i32,
        eps: f64 = 1e-5,
        momentum: f64 = 0.1,
        affine: bool = true,
        track_running_stats: bool = true
    ):
        """Initialize 1D batch normalization layer.

        Args:
            num_features: Number of features (C from [N, C] or [N, C, L])
            eps: Epsilon for numerical stability (default: 1e-5)
            momentum: Momentum for running mean/variance (default: 0.1)
            affine: If True, learns scale and shift parameters (default: True)
            track_running_stats: Track running statistics for inference (default: True)
        """
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        # Create module via FFI
        self.module_handle = @rt_torch_batchnorm1d_new(
            num_features,
            eps,
            momentum,
            affine as i32,
            track_running_stats as i32
        )
        self.validate_handle("BatchNorm1d")

    fn forward(x: Tensor) -> Tensor:
        """Apply batch normalization.

        Args:
            x: Input tensor [N, C] or [N, C, L]

        Returns:
            Normalized tensor (same shape as input)
        """
        val handle = @rt_torch_batchnorm1d_forward(
            self.module_handle,
            x.handle,
            self.training as i32
        )
        return self.wrap_output(handle, "BatchNorm1d forward")


class BatchNorm2d(FFIModule):
    """2D Batch Normalization layer.

    Normalizes mini-batches of 2D inputs (images/feature maps).
    Applied to convolutional networks.

    Example:
        ```simple
        # Input: [batch, channels, height, width]
        val bn = nn.BatchNorm2d(num_features=64)
        val normalized = bn(feature_maps)
        ```
    """
    num_features: i32
    eps: f64
    momentum: f64
    affine: bool
    track_running_stats: bool

    fn __init__(num_features: i32,
        eps: f64 = 1e-5,
        momentum: f64 = 0.1,
        affine: bool = true,
        track_running_stats: bool = true
    ):
        """Initialize 2D batch normalization layer.

        Args:
            num_features: Number of channels (C from [N, C, H, W])
            eps: Epsilon for numerical stability (default: 1e-5)
            momentum: Momentum for running mean/variance (default: 0.1)
            affine: If True, learns scale and shift parameters (default: True)
            track_running_stats: Track running statistics for inference (default: True)
        """
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.affine = affine
        self.track_running_stats = track_running_stats

        # Create module via FFI
        self.module_handle = @rt_torch_batchnorm2d_new(
            num_features,
            eps,
            momentum,
            affine as i32,
            track_running_stats as i32
        )
        self.validate_handle("BatchNorm2d")

    fn forward(x: Tensor) -> Tensor:
        """Apply 2D batch normalization.

        Args:
            x: Input tensor [N, C, H, W]

        Returns:
            Normalized tensor (same shape as input)
        """
        val handle = @rt_torch_batchnorm2d_forward(
            self.module_handle,
            x.handle,
            self.training as i32
        )
        return self.wrap_output(handle, "BatchNorm2d forward")


# ============================================================================
# Layer Normalization
# ============================================================================

class LayerNorm(FFIModule):
    """Layer Normalization.

    Normalizes across features instead of batch dimension.
    More stable than BatchNorm for small batches and RNNs/Transformers.

    Example:
        ```simple
        # Input: [batch, seq_len, features]
        val ln = nn.LayerNorm(normalized_shape=[512])
        val normalized = ln(x)
        ```
    """
    normalized_shape: [i64]
    eps: f64
    elementwise_affine: bool

    fn __init__(normalized_shape: [i64],
        eps: f64 = 1e-5,
        elementwise_affine: bool = true
    ):
        """Initialize layer normalization.

        Args:
            normalized_shape: Shape of the input to normalize over
            eps: Epsilon for numerical stability (default: 1e-5)
            elementwise_affine: If True, learns scale and shift (default: True)
        """
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        self.elementwise_affine = elementwise_affine

        # Create module via FFI
        self.module_handle = @rt_torch_layernorm_new(
            normalized_shape.data_ptr(),
            normalized_shape.len() as i32,
            eps,
            elementwise_affine as i32
        )
        self.validate_handle("LayerNorm")

    fn forward(x: Tensor) -> Tensor:
        """Apply layer normalization.

        Args:
            x: Input tensor

        Returns:
            Normalized tensor (same shape as input)
        """
        val handle = @rt_torch_layernorm_forward(self.module_handle, x.handle)
        return self.wrap_output(handle, "LayerNorm forward")
