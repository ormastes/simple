# Neural Network - Dropout Layer
#
# Dropout regularization for neural networks.

pub use Dropout

use ml.torch.tensor_class.{Tensor}
use base.{FFIModule}

class Dropout(FFIModule):
    """Dropout regularization layer.

    Randomly zeroes elements with probability p during training.

    Example:
        ```simple
        val dropout = nn.Dropout(p=0.5)
        dropout.train()  # Enable dropout
        val train_out = dropout(x)  # Some elements zeroed

        dropout.eval()  # Disable dropout
        val eval_out = dropout(x)  # No elements zeroed
        ```
    """
    p: f64

    fn __init__(p: f64 = 0.5):
        """Initialize dropout layer.

        Args:
            p: Probability of zeroing each element (default: 0.5)
        """
        super().__init__()
        self.p = p

        # Create module via FFI
        self.module_handle = @rt_torch_dropout_new(p, 0)  # inplace=0
        self.validate_handle("Dropout")

    fn forward(x: Tensor) -> Tensor:
        """Apply dropout.

        Args:
            x: Input tensor

        Returns:
            Output tensor (same shape as input)
        """
        val handle = @rt_torch_dropout_forward(
            self.module_handle,
            x.handle,
            self.training as i32
        )
        return self.wrap_output(handle, "Dropout forward")
