# Neural Network - Embedding Layer
#
# Embedding layer for mapping indices to dense vectors.

export Embedding

use ml.torch.tensor_class.{Tensor}
use base.{FFIModule}

class Embedding(FFIModule):
    """Embedding layer for mapping indices to dense vectors.

    Commonly used for word embeddings in NLP and item embeddings in recommendation systems.

    Example:
        ```simple
        # Vocabulary size 1000, embedding dimension 128
        val embedding = nn.Embedding(num_embeddings=1000, embedding_dim=128)

        # Input: [batch_size, sequence_length] of integer indices
        val indices = torch.tensor([[1, 2, 3], [4, 5, 6]])
        val embedded = embedding(indices)  # [batch, seq_len, embedding_dim]
        ```
    """
    num_embeddings: i32
    embedding_dim: i32

    fn __init__(num_embeddings: i32, embedding_dim: i32, padding_idx: i32 = -1):
        """Initialize embedding layer.

        Args:
            num_embeddings: Size of vocabulary (number of unique indices)
            embedding_dim: Dimension of embedding vectors
            padding_idx: If specified, entries at this index are zero vectors (default: -1 = none)
        """
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim

        # Create module via FFI
        self.module_handle = @rt_torch_embedding_new(
            num_embeddings,
            embedding_dim,
            padding_idx
        )
        self.validate_handle("Embedding")

    fn forward(indices: Tensor) -> Tensor:
        """Look up embeddings for input indices.

        Args:
            indices: Integer tensor of indices [batch, seq_len] or [batch]

        Returns:
            Embedded tensor [batch, seq_len, embedding_dim] or [batch, embedding_dim]
        """
        val handle = @rt_torch_embedding_forward(self.module_handle, indices.handle)
        return self.wrap_output(handle, "Embedding forward")
