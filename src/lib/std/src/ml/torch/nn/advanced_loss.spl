# Advanced Loss Functions
#
# Additional loss functions for specialized training scenarios.
#
# ## Classes
# - `FocalLoss`: Focal loss for imbalanced classification
# - `LabelSmoothingLoss`: Cross entropy with label smoothing
# - `HuberLoss`: Smooth L1 / Huber loss for regression
# - `BCEWithLogitsLoss`: BCE with built-in sigmoid
#
# ## Example
# ```simple
# import ml.torch.nn.advanced_loss as loss
# import ml.torch as torch
#
# # Focal loss for imbalanced data
# val focal = loss.FocalLoss(alpha=0.25, gamma=2.0)
# val loss_val = focal.forward(logits, targets)
#
# # Label smoothing for better generalization
# val smooth = loss.LabelSmoothingLoss(smoothing=0.1)
# val loss_val = smooth.forward(logits, targets)
# ```

export FocalLoss, LabelSmoothingLoss, HuberLoss, BCEWithLogitsLoss

use ml.torch.tensor_class.{Tensor}


# ============================================================================
# Focal Loss
# ============================================================================

class FocalLoss:
    """Focal loss for imbalanced classification.

    Focuses learning on hard examples by down-weighting easy examples.

    Loss = -alpha * (1 - p_t)^gamma * log(p_t)

    where p_t is the probability of correct class.

    Attributes:
        alpha: Weighting factor for positive class
        gamma: Focusing parameter (higher = more focus on hard examples)
        reduction: Reduction mode

    Example:
        ```simple
        val criterion = FocalLoss(alpha=0.25, gamma=2.0)
        val loss = criterion.forward(logits, targets)
        ```
    """
    alpha: f64
    gamma: f64
    reduction: str

    fn __init__(alpha: f64 = 0.25, gamma: f64 = 2.0, reduction: str = "mean"):
        """Initialize focal loss.

        Args:
            alpha: Balance factor (default: 0.25)
            gamma: Focusing parameter (default: 2.0)
            reduction: Reduction mode (default: "mean")
        """
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    fn forward(predictions: Tensor, targets: Tensor) -> Tensor:
        """Compute focal loss.

        Args:
            predictions: Raw logits [batch_size, num_classes]
            targets: Target class indices [batch_size]

        Returns:
            Loss tensor
        """
        val handle = @rt_torch_focal_loss(
            predictions.handle,
            targets.handle,
            self.alpha,
            self.gamma,
            self.reduction.as_ptr(),
            self.reduction.len() as i32
        )
        if handle == 0:
            panic("Focal loss computation failed")
        return Tensor(handle)


# ============================================================================
# Label Smoothing Loss
# ============================================================================

class LabelSmoothingLoss:
    """Cross entropy with label smoothing.

    Smooths hard labels to prevent overconfidence:
        target_smooth = (1 - smoothing) * one_hot + smoothing / num_classes

    Attributes:
        smoothing: Label smoothing factor (0.0 to 1.0)
        reduction: Reduction mode

    Example:
        ```simple
        val criterion = LabelSmoothingLoss(smoothing=0.1)
        val loss = criterion.forward(logits, targets)
        ```
    """
    smoothing: f64
    reduction: str

    fn __init__(smoothing: f64 = 0.1, reduction: str = "mean"):
        """Initialize label smoothing loss.

        Args:
            smoothing: Smoothing factor (default: 0.1)
            reduction: Reduction mode (default: "mean")
        """
        if smoothing < 0.0 or smoothing > 1.0:
            panic("smoothing must be between 0 and 1")
        self.smoothing = smoothing
        self.reduction = reduction

    fn forward(predictions: Tensor, targets: Tensor) -> Tensor:
        """Compute label smoothing loss.

        Args:
            predictions: Raw logits [batch_size, num_classes]
            targets: Target class indices [batch_size]

        Returns:
            Loss tensor
        """
        val handle = @rt_torch_label_smoothing_loss(
            predictions.handle,
            targets.handle,
            self.smoothing,
            self.reduction.as_ptr(),
            self.reduction.len() as i32
        )
        if handle == 0:
            panic("Label smoothing loss computation failed")
        return Tensor(handle)


# ============================================================================
# Huber Loss
# ============================================================================

class HuberLoss:
    """Huber loss (Smooth L1 loss).

    Combines L1 and L2 loss, being quadratic for small errors
    and linear for large errors. Less sensitive to outliers than MSE.

    Loss = 0.5 * (y - f(x))Â² if |y - f(x)| < delta
           delta * (|y - f(x)| - 0.5 * delta) otherwise

    Attributes:
        delta: Threshold between L1 and L2 loss
        reduction: Reduction mode

    Example:
        ```simple
        val criterion = HuberLoss(delta=1.0)
        val loss = criterion.forward(predictions, targets)
        ```
    """
    delta: f64
    reduction: str

    fn __init__(delta: f64 = 1.0, reduction: str = "mean"):
        """Initialize Huber loss.

        Args:
            delta: Threshold (default: 1.0)
            reduction: Reduction mode (default: "mean")
        """
        self.delta = delta
        self.reduction = reduction

    fn forward(predictions: Tensor, targets: Tensor) -> Tensor:
        """Compute Huber loss.

        Args:
            predictions: Predicted values
            targets: Target values

        Returns:
            Loss tensor
        """
        val handle = @rt_torch_huber_loss(
            predictions.handle,
            targets.handle,
            self.delta,
            self.reduction.as_ptr(),
            self.reduction.len() as i32
        )
        if handle == 0:
            panic("Huber loss computation failed")
        return Tensor(handle)


# ============================================================================
# BCE With Logits Loss
# ============================================================================

class BCEWithLogitsLoss:
    """Binary cross entropy with built-in sigmoid.

    More numerically stable than applying sigmoid then BCE.
    Combines Sigmoid layer and BCELoss in one class.

    Attributes:
        pos_weight: Weight for positive samples (for imbalanced data)
        reduction: Reduction mode

    Example:
        ```simple
        val criterion = BCEWithLogitsLoss()
        val loss = criterion.forward(raw_logits, targets)  # No sigmoid needed
        ```
    """
    pos_weight: f64
    reduction: str

    fn __init__(pos_weight: f64 = 1.0, reduction: str = "mean"):
        """Initialize BCE with logits loss.

        Args:
            pos_weight: Weight for positive class (default: 1.0)
            reduction: Reduction mode (default: "mean")
        """
        self.pos_weight = pos_weight
        self.reduction = reduction

    fn forward(predictions: Tensor, targets: Tensor) -> Tensor:
        """Compute BCE with logits loss.

        Args:
            predictions: Raw logits (before sigmoid)
            targets: Target values (0 or 1)

        Returns:
            Loss tensor
        """
        val handle = @rt_torch_bce_with_logits_loss(
            predictions.handle,
            targets.handle,
            self.pos_weight,
            self.reduction.as_ptr(),
            self.reduction.len() as i32
        )
        if handle == 0:
            panic("BCE with logits loss computation failed")
        return Tensor(handle)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_focal_loss(
    predictions: u64,
    targets: u64,
    alpha: f64,
    gamma: f64,
    reduction: *u8,
    reduction_len: i32
) -> u64

extern fn rt_torch_label_smoothing_loss(
    predictions: u64,
    targets: u64,
    smoothing: f64,
    reduction: *u8,
    reduction_len: i32
) -> u64

extern fn rt_torch_huber_loss(
    predictions: u64,
    targets: u64,
    delta: f64,
    reduction: *u8,
    reduction_len: i32
) -> u64

extern fn rt_torch_bce_with_logits_loss(
    predictions: u64,
    targets: u64,
    pos_weight: f64,
    reduction: *u8,
    reduction_len: i32
) -> u64
