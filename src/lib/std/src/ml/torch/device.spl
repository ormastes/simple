# Device Management for PyTorch
#
# Device enumeration and CUDA availability functions.

export Device, device_code, device_from_code
# Temporary: Commented out cuda functions while debugging
# export cuda_available, cuda_device_count


# ============================================================================
# Device Enum
# ============================================================================

enum Device:
    CPU
    CUDA(i32)

impl Device:
    # =========================================================================
    # Helper Methods
    # =========================================================================

    pub fn is_cpu(self) -> bool:
        """Check if device is CPU.

        Returns:
            true for CPU device

        Example:
            Device.CPU.is_cpu()  # → true
            Device.CUDA(0).is_cpu()  # → false
        """
        match self:
            Device.CPU: true
            _: false

    pub fn is_cuda(self) -> bool:
        """Check if device is CUDA GPU.

        Returns:
            true for any CUDA device

        Example:
            Device.CUDA(0).is_cuda()  # → true
            Device.CPU.is_cuda()  # → false
        """
        match self:
            Device.CUDA(_): true
            _: false

    pub fn cuda_id(self) -> Option<i32>:
        """Get CUDA device ID if this is a CUDA device.

        Returns:
            Some(id) for CUDA devices, nil for CPU

        Example:
            Device.CUDA(2).cuda_id()  # → Some(2)
            Device.CPU.cuda_id()  # → nil
        """
        match self:
            Device.CUDA(id): Some(id)
            Device.CPU: nil

    pub fn is_accelerated(self) -> bool:
        """Check if device provides hardware acceleration.

        Returns:
            true for CUDA devices (hardware accelerated)

        Example:
            Device.CUDA(0).is_accelerated()  # → true
            Device.CPU.is_accelerated()  # → false
        """
        return self.is_cuda()

    pub fn supports_fp16(self) -> bool:
        """Check if device supports half-precision (FP16).

        Returns:
            true for CUDA devices (most support FP16)

        Example:
            Device.CUDA(0).supports_fp16()  # → true
            Device.CPU.supports_fp16()  # → false
        """
        return self.is_cuda()

    pub fn supports_tensor_cores(self) -> bool:
        """Check if device might have tensor cores.

        Returns:
            true for CUDA devices (Volta+ GPUs have tensor cores)

        Note:
            This is approximate - actual support depends on GPU architecture

        Example:
            Device.CUDA(0).supports_tensor_cores()  # → true (if Volta+)
        """
        return self.is_cuda()

    pub fn name(self) -> text:
        """Get human-readable device name.

        Returns:
            text name of the device

        Example:
            Device.CPU.name()  # → "CPU"
            Device.CUDA(2).name()  # → "CUDA:2"
        """
        match self:
            Device.CPU: "CPU"
            Device.CUDA(id): "CUDA:{id}"

    pub fn short_name(self) -> text:
        """Get short device name (without ID).

        Returns:
            "cpu" or "cuda"

        Example:
            Device.CUDA(0).short_name()  # → "cuda"
            Device.CPU.short_name()  # → "cpu"
        """
        if self.is_cpu():
            return "cpu"
        else:
            return "cuda"

    pub fn summary(self) -> text:
        """Get summary of device with details.

        Returns:
            Human-readable summary

        Example:
            Device.CUDA(0).summary()
            # → "Device: CUDA:0 (accelerated, FP16 support)"
        """
        val name = self.name()
        if self.is_cpu():
            return "Device: {name} (no acceleration)"
        else:
            match self:
                Device.CUDA(id):
                    return "Device: CUDA:{id} (accelerated, FP16 support)"
                _:
                    return "Device: {name}"

    pub fn requires_synchronization(self) -> bool:
        """Check if operations on this device may be asynchronous.

        Returns:
            true for CUDA (async execution), false for CPU

        Example:
            Device.CUDA(0).requires_synchronization()  # → true
            Device.CPU.requires_synchronization()  # → false
        """
        return self.is_cuda()

    pub fn is_default(self) -> bool:
        """Check if this is the default device.

        Returns:
            true for CPU (always available) or CUDA:0

        Example:
            Device.CPU.is_default()  # → true
            Device.CUDA(0).is_default()  # → true
            Device.CUDA(1).is_default()  # → false
        """
        match self:
            Device.CPU: true
            Device.CUDA(0): true
            _: false

# Device helper functions
fn device_code(device: Device) -> i32:
    """Convert device to FFI device code.

    Args:
        device: Device enum instance

    Returns:
        Device code (0=CPU, 1=CUDA:0, 2=CUDA:1, ...)
    """
    match device:
        Device.CPU -> 0
        Device.CUDA(id) -> id + 1


fn device_from_code(code: i32) -> Device:
    """Convert FFI device code to Device enum.

    Args:
        code: Device code from FFI (0=CPU, 1=CUDA:0, 2=CUDA:1, ...)

    Returns:
        Device enum instance
    """
    if code == 0:
        return Device.CPU
    return Device.CUDA(code - 1)


# ============================================================================
# CUDA Functions
# ============================================================================

# fn cuda_available() -> bool:
#     """Check if CUDA is available.
#
#     Returns:
#         True if CUDA is available, False otherwise
#     """
#     val result = @rt_torch_cuda_available()
#     return result != 0


# fn cuda_device_count() -> i32:
#     """Get number of CUDA devices.
#
#     Returns:
#         Number of CUDA devices (0 if CUDA not available)
#     """
#     return @rt_torch_cuda_device_count()


# ============================================================================
# External FFI Functions
# ============================================================================

# Temporary: Commented out while debugging
# extern fn rt_torch_cuda_available() -> i32
# extern fn rt_torch_cuda_device_count() -> i32
