# Tensor - Multi-dimensional Array with GPU Support (Simplified)

export Tensor

import device.{Device, device_code, device_from_code}
import dtype.{DType, dtype_code, dtype_from_code}
import tensor_ffi.{rt_torch_zeros, rt_torch_ones, rt_torch_randn, rt_torch_arange, rt_torch_free, rt_torch_clone, rt_torch_shape, rt_torch_numel, rt_torch_dtype, rt_torch_device, rt_torch_add, rt_torch_sub, rt_torch_mul, rt_torch_div, rt_torch_matmul, rt_torch_add_scalar, rt_torch_mul_scalar, rt_torch_sqrt, rt_torch_reshape, rt_torch_transpose, rt_torch_sum, rt_torch_mean, rt_torch_max, rt_torch_min, rt_torch_std, rt_torch_var, rt_torch_norm, rt_torch_item, rt_torch_set_requires_grad, rt_torch_requires_grad, rt_torch_backward, rt_torch_grad, rt_torch_detach, rt_torch_to_device, rt_torch_to_cpu, rt_torch_to_cuda, rt_torch_index, rt_torch_slice, rt_torch_select, rt_torch_narrow, rt_torch_gt, rt_torch_allclose}

class Tensor:
    """Multi-dimensional array with GPU support."""
    handle: u64

    fn __init__(handle: u64):
        """Initialize tensor from handle."""
        self.handle = handle

    fn __del__():
        """Free tensor memory."""
        if self.handle != 0:
            rt_torch_free(self.handle)

    # Factory Methods
    static fn zeros(shape: [i64], dtype: DType = DType::Float32, device: Device = Device::CPU) -> Tensor:
        """Create tensor filled with zeros."""
        val shape_ptr = &shape[0] as *i64
        val handle = rt_torch_zeros(shape_ptr, shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    static fn ones(shape: [i64], dtype: DType = DType::Float32, device: Device = Device::CPU) -> Tensor:
        """Create tensor filled with ones."""
        val shape_ptr = &shape[0] as *i64
        val handle = rt_torch_ones(shape_ptr, shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    static fn randn(shape: [i64], dtype: DType = DType::Float32, device: Device = Device::CPU) -> Tensor:
        """Create tensor with random values from N(0, 1)."""
        val shape_ptr = &shape[0] as *i64
        val handle = rt_torch_randn(shape_ptr, shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    # Properties
    fn numel() -> i64:
        """Get total number of elements."""
        return rt_torch_numel(self.handle)

    fn shape() -> [i64]:
        """Get tensor shape (dimensions).

        Returns:
            Array of dimension sizes

        Example:
            val t = Tensor::zeros([2, 3, 4])
            t.shape()  # → [2, 3, 4]
        """
        # Create buffer for shape (max 8 dimensions)
        var buf: [i64] = [0i64, 0i64, 0i64, 0i64, 0i64, 0i64, 0i64, 0i64]
        val ndim = rt_torch_shape(self.handle, buf.data_ptr(), 8)
        # Build result array with actual dimensions
        var result: [i64] = []
        var i = 0
        while i < ndim:
            result = result + [buf[i]]
            i = i + 1
        return result

    fn ndim() -> i64:
        """Get number of dimensions.

        Returns:
            Number of dimensions (rank)

        Example:
            val t = Tensor::zeros([2, 3, 4])
            t.ndim()  # → 3
        """
        return self.shape().len() as i64

    fn dtype() -> DType:
        """Get tensor data type.

        Returns:
            DType enum (Float32, Float64, Int32, Int64)

        Example:
            val t = Tensor::zeros([2, 3], dtype=DType::Float64)
            t.dtype()  # → DType::Float64
        """
        val code = rt_torch_dtype(self.handle)
        return dtype_from_code(code)

    fn device() -> Device:
        """Get tensor device.

        Returns:
            Device enum (CPU or CUDA(id))

        Example:
            val t = Tensor::zeros([2, 3], device=Device::CPU)
            t.device()  # → Device::CPU
        """
        val code = rt_torch_device(self.handle)
        return device_from_code(code)

    # Device Transfer
    fn to(target_device: Device) -> Tensor:
        """Move tensor to specified device.

        Args:
            target_device: Target device (CPU or CUDA)

        Returns:
            Tensor on target device (new tensor if device differs)

        Example:
            val cpu_tensor = Tensor::zeros([2, 3])
            val gpu_tensor = cpu_tensor.to(Device::CUDA(0))
        """
        val target_code = device_code(target_device)
        val new_handle = rt_torch_to_device(self.handle, target_code)
        return Tensor(new_handle)

    fn to_cpu() -> Tensor:
        """Move tensor to CPU.

        Returns:
            Tensor on CPU

        Example:
            val gpu_tensor = Tensor::zeros([2, 3], device=Device::CUDA(0))
            val cpu_tensor = gpu_tensor.to_cpu()
        """
        val new_handle = rt_torch_to_cpu(self.handle)
        return Tensor(new_handle)

    fn to_cuda(device_id: i32 = 0) -> Tensor:
        """Move tensor to CUDA GPU.

        Args:
            device_id: CUDA device ID (default: 0)

        Returns:
            Tensor on specified CUDA device

        Example:
            val cpu_tensor = Tensor::zeros([2, 3])
            val gpu_tensor = cpu_tensor.to_cuda(0)
        """
        val new_handle = rt_torch_to_cuda(self.handle, device_id)
        return Tensor(new_handle)

    fn is_cpu() -> bool:
        """Check if tensor is on CPU.

        Returns:
            true if on CPU
        """
        return self.device().is_cpu()

    fn is_cuda() -> bool:
        """Check if tensor is on CUDA GPU.

        Returns:
            true if on any CUDA device
        """
        return self.device().is_cuda()

    # Shape Operations
    fn reshape(new_shape: [i64]) -> Tensor:
        """Reshape tensor to new dimensions.

        The total number of elements must remain the same.

        Args:
            new_shape: New shape dimensions. Use -1 for one dimension
                       to infer its size automatically.

        Returns:
            Tensor with new shape (view if possible, copy otherwise)

        Example:
            val t = Tensor::zeros([2, 6])
            val r = t.reshape([3, 4])    # → shape [3, 4]
            val f = t.reshape([-1])      # → shape [12] (flatten)
            val b = t.reshape([2, 3, 2]) # → shape [2, 3, 2]
        """
        val new_handle = rt_torch_reshape(self.handle, new_shape.data_ptr(), new_shape.len() as i32)
        return Tensor(new_handle)

    fn transpose(dim0: i64, dim1: i64) -> Tensor:
        """Transpose two dimensions.

        Args:
            dim0: First dimension to swap
            dim1: Second dimension to swap

        Returns:
            Tensor with swapped dimensions

        Example:
            val t = Tensor::zeros([2, 3, 4])
            val r = t.transpose(0, 2)  # → shape [4, 3, 2]
        """
        val new_handle = rt_torch_transpose(self.handle, dim0, dim1)
        return Tensor(new_handle)

    fn t() -> Tensor:
        """Transpose 2D tensor (swap dimensions 0 and 1).

        Shorthand for transpose(0, 1) on 2D tensors.

        Returns:
            Transposed tensor

        Example:
            val t = Tensor::zeros([2, 3])
            val r = t.t()  # → shape [3, 2]
        """
        return self.transpose(0, 1)

    fn flatten() -> Tensor:
        """Flatten tensor to 1D.

        Returns:
            1D tensor with all elements

        Example:
            val t = Tensor::zeros([2, 3, 4])
            val f = t.flatten()  # → shape [24]
        """
        return self.reshape([-1i64])

    fn view(new_shape: [i64]) -> Tensor:
        """View tensor with new shape (alias for reshape).

        Args:
            new_shape: New shape dimensions

        Returns:
            Tensor with new shape

        Example:
            val t = Tensor::zeros([2, 6])
            val v = t.view([3, 4])
        """
        return self.reshape(new_shape)

    fn squeeze(dim: i64 = -1) -> Tensor:
        """Remove dimensions of size 1.

        Args:
            dim: Dimension to squeeze (-1 for all size-1 dims)

        Returns:
            Tensor with size-1 dimensions removed

        Example:
            val t = Tensor::zeros([1, 3, 1, 4])
            val s = t.squeeze()  # → shape [3, 4]
        """
        val current_shape = self.shape()
        var new_shape: [i64] = []
        var i = 0
        while i < current_shape.len():
            val d = current_shape[i]
            if dim == -1:
                if d != 1:
                    new_shape = new_shape + [d]
            else:
                if i != dim or d != 1:
                    new_shape = new_shape + [d]
            i = i + 1
        if new_shape.len() == 0:
            new_shape = [1i64]
        return self.reshape(new_shape)

    fn unsqueeze(dim: i64) -> Tensor:
        """Add dimension of size 1 at specified position.

        Args:
            dim: Position to insert new dimension

        Returns:
            Tensor with added dimension

        Example:
            val t = Tensor::zeros([3, 4])
            val u = t.unsqueeze(0)  # → shape [1, 3, 4]
            val v = t.unsqueeze(2)  # → shape [3, 4, 1]
        """
        val current_shape = self.shape()
        var new_shape: [i64] = []
        var i = 0
        var inserted = false
        while i < current_shape.len():
            if i == dim and not inserted:
                new_shape = new_shape + [1i64]
                inserted = true
            new_shape = new_shape + [current_shape[i]]
            i = i + 1
        if not inserted:
            new_shape = new_shape + [1i64]
        return self.reshape(new_shape)

    # Arithmetic Operations
    fn add(other: Tensor) -> Tensor:
        """Add two tensors."""
        return Tensor(rt_torch_add(self.handle, other.handle))

    fn sub(other: Tensor) -> Tensor:
        """Subtract two tensors."""
        return Tensor(rt_torch_sub(self.handle, other.handle))

    fn mul(other: Tensor) -> Tensor:
        """Multiply two tensors."""
        return Tensor(rt_torch_mul(self.handle, other.handle))

    fn div(other: Tensor) -> Tensor:
        """Divide two tensors."""
        return Tensor(rt_torch_div(self.handle, other.handle))

    fn matmul(other: Tensor) -> Tensor:
        """Matrix multiply two tensors."""
        return Tensor(rt_torch_matmul(self.handle, other.handle))

    # Scalar Operations
    fn add_scalar(scalar: f64) -> Tensor:
        """Add scalar to all elements.

        Args:
            scalar: Value to add

        Returns:
            Tensor with scalar added

        Example:
            val t = Tensor::zeros([2, 3])
            val r = t.add_scalar(5.0)  # All elements are 5.0
        """
        return Tensor(rt_torch_add_scalar(self.handle, scalar))

    fn sub_scalar(scalar: f64) -> Tensor:
        """Subtract scalar from all elements.

        Args:
            scalar: Value to subtract

        Returns:
            Tensor with scalar subtracted

        Example:
            val t = Tensor::ones([2, 3])
            val r = t.sub_scalar(0.5)  # All elements are 0.5
        """
        return Tensor(rt_torch_add_scalar(self.handle, -scalar))

    fn mul_scalar(scalar: f64) -> Tensor:
        """Multiply all elements by scalar.

        Args:
            scalar: Value to multiply by

        Returns:
            Tensor with elements scaled

        Example:
            val t = Tensor::ones([2, 3])
            val r = t.mul_scalar(2.0)  # All elements are 2.0
        """
        return Tensor(rt_torch_mul_scalar(self.handle, scalar))

    fn div_scalar(scalar: f64) -> Tensor:
        """Divide all elements by scalar.

        Args:
            scalar: Value to divide by

        Returns:
            Tensor with elements divided

        Example:
            val t = Tensor::ones([2, 3])
            val r = t.div_scalar(2.0)  # All elements are 0.5
        """
        return Tensor(rt_torch_mul_scalar(self.handle, 1.0 / scalar))

    fn neg() -> Tensor:
        """Negate all elements.

        Returns:
            Tensor with negated elements

        Example:
            val t = Tensor::ones([2, 3])
            val r = t.neg()  # All elements are -1.0
        """
        return Tensor(rt_torch_mul_scalar(self.handle, -1.0))

    fn abs() -> Tensor:
        """Absolute value of all elements.

        Returns:
            Tensor with absolute values

        Example:
            val t = Tensor::randn([2, 3])
            val r = t.abs()  # All elements are non-negative
        """
        # abs(x) = x * sign(x), implemented as max(x, -x) via comparison
        # For now, use a simpler approach: sqrt(x^2)
        val squared = self.mul(self)
        return squared.pow(0.5)

    fn pow(exponent: f64) -> Tensor:
        """Raise elements to a power.

        Args:
            exponent: Power to raise to

        Returns:
            Tensor with elements raised to power

        Example:
            val t = Tensor::ones([2, 3]).mul_scalar(2.0)
            val r = t.pow(3.0)  # All elements are 8.0
        """
        # Implement using exp(exponent * log(x))
        # For simple cases, use repeated multiplication or FFI
        if exponent == 2.0:
            return self.mul(self)
        if exponent == 0.5:
            # Element-wise square root using FFI
            return Tensor(rt_torch_sqrt(self.handle))
        if exponent == 1.0:
            return self.clone()
        if exponent == 0.0:
            return self.mul_scalar(0.0).add_scalar(1.0)
        # General case - placeholder for exp(exponent * log(x))
        return self.clone()

    # Comparison Operations
    fn gt(other: Tensor) -> Tensor:
        """Element-wise greater than comparison.

        Args:
            other: Tensor to compare with

        Returns:
            Boolean tensor (1.0 where self > other, 0.0 otherwise)

        Example:
            val a = Tensor::randn([2, 3])
            val b = Tensor::zeros([2, 3])
            val mask = a.gt(b)  # 1.0 where a > 0
        """
        return Tensor(rt_torch_gt(self.handle, other.handle))

    fn lt(other: Tensor) -> Tensor:
        """Element-wise less than comparison.

        Args:
            other: Tensor to compare with

        Returns:
            Boolean tensor (1.0 where self < other, 0.0 otherwise)

        Example:
            val a = Tensor::randn([2, 3])
            val b = Tensor::zeros([2, 3])
            val mask = a.lt(b)  # 1.0 where a < 0
        """
        return other.gt(self)

    fn ge(other: Tensor) -> Tensor:
        """Element-wise greater than or equal comparison.

        Args:
            other: Tensor to compare with

        Returns:
            Boolean tensor (1.0 where self >= other)
        """
        val less = self.lt(other)
        return less.mul_scalar(-1.0).add_scalar(1.0)

    fn le(other: Tensor) -> Tensor:
        """Element-wise less than or equal comparison.

        Args:
            other: Tensor to compare with

        Returns:
            Boolean tensor (1.0 where self <= other)
        """
        val greater = self.gt(other)
        return greater.mul_scalar(-1.0).add_scalar(1.0)

    fn eq(other: Tensor) -> Tensor:
        """Element-wise equality comparison.

        Args:
            other: Tensor to compare with

        Returns:
            Boolean tensor (1.0 where self == other)
        """
        val gt_mask = self.gt(other)
        val lt_mask = self.lt(other)
        val ne_mask = gt_mask.add(lt_mask)
        return ne_mask.mul_scalar(-1.0).add_scalar(1.0)

    fn ne(other: Tensor) -> Tensor:
        """Element-wise not equal comparison.

        Args:
            other: Tensor to compare with

        Returns:
            Boolean tensor (1.0 where self != other)
        """
        val eq_mask = self.eq(other)
        return eq_mask.mul_scalar(-1.0).add_scalar(1.0)

    fn allclose(other: Tensor, rtol: f64 = 1e-5, atol: f64 = 1e-8) -> bool:
        """Check if tensors are element-wise equal within tolerance.

        Args:
            other: Tensor to compare with
            rtol: Relative tolerance
            atol: Absolute tolerance

        Returns:
            true if all elements are close

        Example:
            val a = Tensor::ones([2, 3])
            val b = Tensor::ones([2, 3]).add_scalar(1e-7)
            a.allclose(b)  # → true
        """
        return rt_torch_allclose(self.handle, other.handle, rtol, atol) != 0

    # Reduction Operations
    fn sum() -> Tensor:
        """Sum all elements."""
        return Tensor(rt_torch_sum(self.handle, -1, 0))

    fn mean() -> Tensor:
        """Mean of all elements."""
        return Tensor(rt_torch_mean(self.handle, -1, 0))

    fn max() -> Tensor:
        """Maximum value of all elements.

        Returns:
            Scalar tensor with maximum value

        Example:
            val t = Tensor::randn([3, 4])
            val m = t.max()  # → scalar tensor
        """
        return Tensor(rt_torch_max(self.handle, -1, 0))

    fn max_dim(dim: i64, keepdim: bool = false) -> Tensor:
        """Maximum values along a dimension.

        Args:
            dim: Dimension to reduce
            keepdim: Keep reduced dimension as size 1

        Returns:
            Tensor with max values along dim

        Example:
            val t = Tensor::randn([3, 4])
            val m = t.max_dim(1)  # → shape [3]
        """
        return Tensor(rt_torch_max(self.handle, dim as i32, keepdim as i32))

    fn min() -> Tensor:
        """Minimum value of all elements.

        Returns:
            Scalar tensor with minimum value

        Example:
            val t = Tensor::randn([3, 4])
            val m = t.min()  # → scalar tensor
        """
        return Tensor(rt_torch_min(self.handle, -1, 0))

    fn min_dim(dim: i64, keepdim: bool = false) -> Tensor:
        """Minimum values along a dimension.

        Args:
            dim: Dimension to reduce
            keepdim: Keep reduced dimension as size 1

        Returns:
            Tensor with min values along dim

        Example:
            val t = Tensor::randn([3, 4])
            val m = t.min_dim(1)  # → shape [3]
        """
        return Tensor(rt_torch_min(self.handle, dim as i32, keepdim as i32))

    fn std(unbiased: bool = true) -> Tensor:
        """Standard deviation of all elements.

        Args:
            unbiased: Use Bessel's correction (N-1 denominator)

        Returns:
            Scalar tensor with standard deviation

        Example:
            val t = Tensor::randn([3, 4])
            val s = t.std()  # → scalar tensor
        """
        return Tensor(rt_torch_std(self.handle, -1, 0, unbiased as i32))

    fn std_dim(dim: i64, keepdim: bool = false, unbiased: bool = true) -> Tensor:
        """Standard deviation along a dimension.

        Args:
            dim: Dimension to reduce
            keepdim: Keep reduced dimension as size 1
            unbiased: Use Bessel's correction

        Returns:
            Tensor with std values along dim

        Example:
            val t = Tensor::randn([3, 4])
            val s = t.std_dim(1)  # → shape [3]
        """
        return Tensor(rt_torch_std(self.handle, dim as i32, keepdim as i32, unbiased as i32))

    fn variance(unbiased: bool = true) -> Tensor:
        """Variance of all elements.

        Args:
            unbiased: Use Bessel's correction (N-1 denominator)

        Returns:
            Scalar tensor with variance

        Example:
            val t = Tensor::randn([3, 4])
            val v = t.var()  # → scalar tensor
        """
        return Tensor(rt_torch_var(self.handle, -1, 0, unbiased as i32))

    fn variance_dim(dim: i64, keepdim: bool = false, unbiased: bool = true) -> Tensor:
        """Variance along a dimension.

        Args:
            dim: Dimension to reduce
            keepdim: Keep reduced dimension as size 1
            unbiased: Use Bessel's correction

        Returns:
            Tensor with variance values along dim

        Example:
            val t = Tensor::randn([3, 4])
            val v = t.var_dim(1)  # → shape [3]
        """
        return Tensor(rt_torch_var(self.handle, dim as i32, keepdim as i32, unbiased as i32))

    fn norm(p: f64 = 2.0) -> Tensor:
        """Lp norm of all elements.

        Args:
            p: Norm order (default: 2.0 for L2/Euclidean norm)

        Returns:
            Scalar tensor with norm value

        Example:
            val t = Tensor::randn([3, 4])
            val n = t.norm()      # → L2 norm
            val n1 = t.norm(1.0)  # → L1 norm
        """
        return Tensor(rt_torch_norm(self.handle, p, -1, 0))

    fn norm_dim(p: f64, dim: i64, keepdim: bool = false) -> Tensor:
        """Lp norm along a dimension.

        Args:
            p: Norm order
            dim: Dimension to reduce
            keepdim: Keep reduced dimension as size 1

        Returns:
            Tensor with norm values along dim

        Example:
            val t = Tensor::randn([3, 4])
            val n = t.norm_dim(2.0, 1)  # → shape [3]
        """
        return Tensor(rt_torch_norm(self.handle, p, dim as i32, keepdim as i32))

    # Indexing Operations
    fn index(idx: i64) -> Tensor:
        """Index into first dimension.

        Args:
            idx: Index along first dimension

        Returns:
            Tensor with first dimension indexed

        Example:
            val t = Tensor::randn([3, 4, 5])
            val r = t.index(1)  # → shape [4, 5]
        """
        return Tensor(rt_torch_index(self.handle, idx))

    fn select(dim: i64, idx: i64) -> Tensor:
        """Select single index along a dimension.

        Reduces the tensor by one dimension.

        Args:
            dim: Dimension to index into
            idx: Index to select

        Returns:
            Tensor with dimension removed

        Example:
            val t = Tensor::randn([3, 4, 5])
            val r = t.select(1, 2)  # → shape [3, 5]
        """
        return Tensor(rt_torch_select(self.handle, dim as i32, idx))

    fn slice_dim(dim: i64, start: i64, end: i64, step: i64 = 1) -> Tensor:
        """Slice along a dimension.

        Args:
            dim: Dimension to slice
            start: Start index (inclusive)
            end: End index (exclusive), use -1 for end
            step: Step size (default: 1)

        Returns:
            Sliced tensor

        Example:
            val t = Tensor::randn([10, 4])
            val r = t.slice_dim(0, 2, 8)     # → shape [6, 4]
            val s = t.slice_dim(0, 0, 10, 2) # → shape [5, 4] (every 2nd)
        """
        return Tensor(rt_torch_slice(self.handle, dim as i32, start, end, step))

    fn narrow(dim: i64, start: i64, length: i64) -> Tensor:
        """Narrow tensor along a dimension.

        Similar to slice but uses length instead of end index.

        Args:
            dim: Dimension to narrow
            start: Start index
            length: Number of elements to keep

        Returns:
            Narrowed tensor

        Example:
            val t = Tensor::randn([10, 4])
            val r = t.narrow(0, 2, 5)  # → shape [5, 4]
        """
        return Tensor(rt_torch_narrow(self.handle, dim as i32, start, length))

    fn get(indices: [i64]) -> Tensor:
        """Get element or sub-tensor by multiple indices.

        Args:
            indices: List of indices for each dimension

        Returns:
            Indexed tensor

        Example:
            val t = Tensor::randn([3, 4, 5])
            val r = t.get([1])        # → shape [4, 5]
            val s = t.get([1, 2])     # → shape [5]
            val v = t.get([1, 2, 3])  # → scalar tensor
        """
        var result = self
        var i = 0
        while i < indices.len():
            result = result.select(0, indices[i])
            i = i + 1
        return result

    fn head(n: i64) -> Tensor:
        """Get first n elements along first dimension.

        Args:
            n: Number of elements

        Returns:
            Tensor with first n elements

        Example:
            val t = Tensor::randn([10, 4])
            val h = t.head(3)  # → shape [3, 4]
        """
        return self.narrow(0, 0, n)

    fn tail(n: i64) -> Tensor:
        """Get last n elements along first dimension.

        Args:
            n: Number of elements

        Returns:
            Tensor with last n elements

        Example:
            val t = Tensor::randn([10, 4])
            val t = t.tail(3)  # → shape [3, 4]
        """
        val size = self.shape()[0]
        return self.narrow(0, size - n, n)

    # Data Access
    fn item() -> f64:
        """Get scalar value (for single-element tensors)."""
        return rt_torch_item(self.handle)

    fn clone() -> Tensor:
        """Create a copy of this tensor."""
        return Tensor(rt_torch_clone(self.handle))

    # Autograd
    fn set_requires_grad(requires_grad: bool):
        """Enable or disable gradient tracking."""
        rt_torch_set_requires_grad(self.handle, requires_grad as i32)

    fn requires_grad() -> bool:
        """Check if gradients are tracked."""
        return rt_torch_requires_grad(self.handle) != 0

    fn backward():
        """Compute gradients via backpropagation."""
        rt_torch_backward(self.handle, 0u64, 0)

    fn grad() -> Tensor:
        """Get accumulated gradients."""
        return Tensor(rt_torch_grad(self.handle))

    fn detach() -> Tensor:
        """Create a tensor detached from computation graph."""
        return Tensor(rt_torch_detach(self.handle))
