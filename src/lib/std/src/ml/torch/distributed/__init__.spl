# PyTorch Distributed Training - Multi-GPU Support
#
# Distributed data parallel training across multiple GPUs and nodes.
# Supports NCCL backend for efficient GPU communication.
#
# ## Classes
# - `ProcessGroup`: Communication group for distributed processes
# - `DistributedDataParallel`: Wrapper for multi-GPU training
#
# ## Functions
# - `init_process_group()`: Initialize distributed backend
# - `destroy_process_group()`: Cleanup distributed resources
# - `get_rank()`: Get current process rank
# - `get_world_size()`: Get total number of processes
# - `barrier()`: Synchronize all processes
# - `all_reduce()`: Reduce tensor across all processes
# - `all_gather()`: Gather tensors from all processes
# - `broadcast()`: Broadcast tensor from root to all processes
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.distributed as dist
#
# dist.init_process_group(backend="nccl", rank=0, world_size=4)
# val device = torch.Device::CUDA(dist.get_rank())
# val ddp_model = dist.DistributedDataParallel(model, device_ids=[dist.get_rank()])
# ```

# Import from submodules
from backend import {Backend, ReduceOp}
from process_group import {
    ProcessGroup,
    init_process_group, destroy_process_group,
    is_initialized, is_available, get_rank, get_world_size, barrier
}
from collective import {all_reduce, all_gather, broadcast, reduce_scatter}
from ddp import {DistributedDataParallel}

# Export all public APIs
export Backend, ReduceOp
export ProcessGroup
export init_process_group, destroy_process_group
export is_initialized, is_available, get_rank, get_world_size, barrier
export all_reduce, all_gather, broadcast, reduce_scatter
export DistributedDataParallel
