# Collective Communication Operations
#
# All-reduce, all-gather, broadcast, and reduce-scatter operations.

export all_reduce, all_gather, broadcast, reduce_scatter

import ml.torch.tensor_class.{Tensor}
import backend.ReduceOp
import process_group.{ProcessGroup, _global_process_group}
import ffi.{
    rt_torch_dist_all_reduce, rt_torch_dist_all_gather,
    rt_torch_dist_broadcast, rt_torch_dist_reduce_scatter
}

# ============================================================================
# Collective Communication Operations
# ============================================================================

fn all_reduce(
    tensor: Tensor,
    op: ReduceOp = ReduceOp.SUM,
    group: ProcessGroup = None,
    async: bool = false
) -> Tensor:
    """Reduce tensor across all processes.

    Performs reduction operation and stores result in all processes.
    For example, with ReduceOp.SUM, each process will receive the sum
    of all tensors across all processes.

    Args:
        tensor: Tensor to reduce (must be same shape on all processes)
        op: Reduction operation (default: SUM)
        group: Process group (default: global group)
        async: Return before operation completes (default: false)

    Returns:
        Reduced tensor (in-place modification)
    """
    val pg = group if group is not None else _global_process_group

    if pg is None:
        panic("Process group not initialized")

    var result_handle = 0u64

    @rt_torch_dist_all_reduce(
        tensor.handle,
        op.code(),
        pg.handle,
        async as i32,
        &result_handle
    )

    if result_handle == 0:
        panic("all_reduce failed")

    return Tensor(result_handle)


fn all_gather(
    tensor: Tensor,
    group: ProcessGroup = None,
    async: bool = false
) -> <Tensor>:
    """Gather tensors from all processes.

    Each process contributes a tensor, and all processes receive
    a list of tensors from all processes.

    Args:
        tensor: Tensor to contribute
        group: Process group (default: global group)
        async: Return before operation completes (default: false)

    Returns:
        List of tensors from all processes (length = world_size)
    """
    val pg = group if group is not None else _global_process_group

    if pg is None:
        panic("Process group not initialized")

    var handles = [0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64, 0u64]  # Max 16 processes

    @rt_torch_dist_all_gather(
        tensor.handle,
        pg.handle,
        async as i32,
        handles.data_ptr(),
        handles.len() as i32
    )

    var result = []
    for i in range(pg.world_size):
        if handles[i] != 0:
            result.append(Tensor(handles[i]))

    return result


fn broadcast(
    tensor: Tensor,
    src: i64 = 0,
    group: ProcessGroup = None,
    async: bool = false
) -> Tensor:
    """Broadcast tensor from source process to all processes.

    The source process sends its tensor to all other processes.

    Args:
        tensor: Tensor to broadcast (only used on src rank)
        src: Source rank (default: 0)
        group: Process group (default: global group)
        async: Return before operation completes (default: false)

    Returns:
        Broadcasted tensor (same on all processes)
    """
    val pg = group if group is not None else _global_process_group

    if pg is None:
        panic("Process group not initialized")

    var result_handle = 0u64

    @rt_torch_dist_broadcast(
        tensor.handle,
        src,
        pg.handle,
        async as i32,
        &result_handle
    )

    if result_handle == 0:
        panic("broadcast failed")

    return Tensor(result_handle)


fn reduce_scatter(
    input_list: <Tensor>,
    op: ReduceOp = ReduceOp.SUM,
    group: ProcessGroup = None,
    async: bool = false
) -> Tensor:
    """Reduce and scatter tensor list.

    Reduces tensors across all processes and scatters the result.
    Each process receives a portion of the reduced result.

    Args:
        input_list: List of tensors to reduce (length = world_size)
        op: Reduction operation (default: SUM)
        group: Process group (default: global group)
        async: Return before operation completes (default: false)

    Returns:
        Scattered portion of reduced result
    """
    val pg = group if group is not None else _global_process_group

    if pg is None:
        panic("Process group not initialized")

    var handles = []
    for tensor in input_list:
        handles.append(tensor.handle)

    var result_handle = 0u64

    @rt_torch_dist_reduce_scatter(
        handles.data_ptr(),
        handles.len() as i32,
        op.code(),
        pg.handle,
        async as i32,
        &result_handle
    )

    if result_handle == 0:
        panic("reduce_scatter failed")

    return Tensor(result_handle)
