# Advanced Tensor Operations - Reshape, Broadcast, and Memory Profiling
#
# Demonstrates advanced tensor dimension inference operations including:
# - Reshape with element count verification
# - Broadcasting rules and shape inference
# - Memory profiling and optimization
# - Runtime shape verification

print("============================================================")
print("  ADVANCED TENSOR OPERATIONS")
print("============================================================")
print("")

# ============================================================================
# Core Types
# ============================================================================

enum Dim:
    Literal(value: i32)
    Named(name: text, lo: i32, hi: i32)
    Var(id: i32)
    Unknown
    Broadcast

enum ShapeError:
    LiteralMismatch(expected: i32, actual: i32)
    RankMismatch(left_rank: i32, right_rank: i32)
    MatmulIncompatible(k1: i32, k2: i32)
    ReshapeMismatch(input_elems: i32, output_elems: i32)
    InferenceError(msg: text)

class TensorShape:
    dims: List<Dim>

    fn __init__(dims: List<Dim>):
        self.dims = dims

    fn ndim(self) -> i32:
        self.dims.len()

enum ShapeResult:
    Ok(shape: TensorShape)
    Err(error: ShapeError)

# ============================================================================
# Utilities
# ============================================================================

fn dim_to_string(d: Dim) -> text:
    match d:
        case Dim.Literal(v):
            "{v}"
        case Dim.Named(n, lo, hi):
            if lo == hi:
                "{n}={lo}"
            else:
                "{n}:{lo}..{hi}"
        case Dim.Var(id):
            "α{id}"
        case Dim.Unknown:
            "*"
        case Dim.Broadcast:
            "?"

fn shape_to_string(shape: TensorShape) -> text:
    if shape.dims.len() == 0:
        return "[]"

    var result = "["
    var first = true
    for d in shape.dims:
        if not first:
            result = result + ", "
        result = result + dim_to_string(d)
        first = false
    result + "]"

fn count_elements(shape: TensorShape) -> (i32, i32):
    """Calculate min and max element counts"""
    var min_elems = 1
    var max_elems = 1

    for d in shape.dims:
        match d:
            case Dim.Literal(v):
                min_elems = min_elems * v
                max_elems = max_elems * v
            case Dim.Named(_, lo, hi):
                min_elems = min_elems * lo
                max_elems = max_elems * hi
            case _:
                min_elems = min_elems * 1
                max_elems = max_elems * 1000

    (min_elems, max_elems)

# ============================================================================
# Reshape Operation
# ============================================================================

fn verify_reshape(input: TensorShape, output: TensorShape) -> ShapeResult:
    """Verify that reshape preserves element count"""
    val (in_min, in_max) = count_elements(input)
    val (out_min, out_max) = count_elements(output)

    # Check if ranges overlap
    if in_max < out_min or out_max < in_min:
        return ShapeResult.Err(ShapeError.ReshapeMismatch(
            input_elems: in_min,
            output_elems: out_min
        ))

    ShapeResult.Ok(shape: output)

print("Test 1: Reshape Operations")
print("------------------------------------------------------------")

# Example 1: Simple reshape (matrix to vector)
val matrix = TensorShape(dims: [Dim.Literal(value: 4), Dim.Literal(value: 6)])
val vector = TensorShape(dims: [Dim.Literal(value: 24)])

print("Reshape: {shape_to_string(matrix)} -> {shape_to_string(vector)}")
match verify_reshape(matrix, vector):
    case ShapeResult.Ok(_):
        print("OK: Valid reshape (4x6 = 24 elements)")
    case ShapeResult.Err(e):
        print("FAIL: Invalid reshape")

print("About to start Example 2...")

# Example 2: Reshape with batch dimension
val batched_input = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 784)
])
val reshaped = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 28),
    Dim.Literal(value: 28)
])

print("Reshape: {shape_to_string(batched_input)} -> {shape_to_string(reshaped)}")
match verify_reshape(batched_input, reshaped):
    case ShapeResult.Ok(_):
        print("OK: Valid reshape (784 = 28x28)")
    case ShapeResult.Err(e):
        print("FAIL: Invalid reshape")

# Example 3: Invalid reshape (element count mismatch)
val input_bad = TensorShape(dims: [Dim.Literal(value: 10), Dim.Literal(value: 5)])
val output_bad = TensorShape(dims: [Dim.Literal(value: 12), Dim.Literal(value: 4)])

print("Reshape: {shape_to_string(input_bad)} -> {shape_to_string(output_bad)}")
match verify_reshape(input_bad, output_bad):
    case ShapeResult.Ok(_):
        print("FAIL: Should have failed!")
    case ShapeResult.Err(e):
        print("OK: Correctly rejected (50 != 48 elements)")

print("")

# ============================================================================
# Broadcasting Operations
# ============================================================================

fn can_broadcast(d1: Dim, d2: Dim) -> bool:
    """Check if two dimensions can broadcast"""
    match d1:
        case Dim.Literal(1):
            true
        case Dim.Literal(v1):
            match d2:
                case Dim.Literal(1):
                    true
                case Dim.Literal(v2):
                    v1 == v2
                case _:
                    true
        case _:
            true

fn infer_broadcast_dim(d1: Dim, d2: Dim) -> Dim:
    """Infer result dimension from broadcasting"""
    match d1:
        case Dim.Literal(1):
            d2
        case Dim.Literal(v1):
            match d2:
                case Dim.Literal(1):
                    d1
                case Dim.Literal(v2):
                    if v1 == v2:
                        d1
                    else:
                        Dim.Unknown
                case _:
                    d1
        case Dim.Named(n, lo, hi):
            d1
        case _:
            Dim.Unknown

fn broadcast_shapes(s1: TensorShape, s2: TensorShape) -> ShapeResult:
    """Compute broadcast result shape"""
    val n1 = s1.ndim()
    val n2 = s2.ndim()
    val max_ndim = if n1 > n2: n1 else: n2

    var result_dims: List<Dim> = []

    var i = 0
    while i < max_ndim:
        val idx1 = n1 - 1 - i
        val idx2 = n2 - 1 - i

        val d1: Dim
        val d2: Dim

        if idx1 >= 0:
            d1 = s1.dims[idx1]
        else:
            d1 = Dim.Literal(value: 1)

        if idx2 >= 0:
            d2 = s2.dims[idx2]
        else:
            d2 = Dim.Literal(value: 1)

        if can_broadcast(d1, d2):
            result_dims.push(infer_broadcast_dim(d1, d2))
        else:
            return ShapeResult.Err(ShapeError.InferenceError(msg: "Cannot broadcast dimensions"))

        i = i + 1

    # Reverse to get correct order
    var final_dims: List<Dim> = []
    var j = result_dims.len() - 1
    while j >= 0:
        final_dims.push(result_dims[j])
        j = j - 1

    ShapeResult.Ok(shape: TensorShape(dims: final_dims))

print("Test 2: Broadcasting Operations")
print("------------------------------------------------------------")

# Example 1: Scalar broadcast
val scalar = TensorShape(dims: [])
val matrix2 = TensorShape(dims: [Dim.Literal(value: 3), Dim.Literal(value: 4)])

print("Broadcast: scalar + {shape_to_string(matrix2)}")
print("OK: Scalar broadcasts to any shape -> {shape_to_string(matrix2)}")

# Example 2: Vector broadcast to matrix
val vector_shape = TensorShape(dims: [Dim.Literal(value: 1), Dim.Literal(value: 4)])
val matrix_shape = TensorShape(dims: [Dim.Literal(value: 3), Dim.Literal(value: 4)])

print("Broadcast: {shape_to_string(vector_shape)} + {shape_to_string(matrix_shape)}")
match broadcast_shapes(vector_shape, matrix_shape):
    case ShapeResult.Ok(result):
        print("OK: Result: {shape_to_string(result)}")
    case ShapeResult.Err(e):
        print("FAIL: Broadcast failed")

# Example 3: Incompatible broadcast
val a = TensorShape(dims: [Dim.Literal(value: 3), Dim.Literal(value: 5)])
val b = TensorShape(dims: [Dim.Literal(value: 3), Dim.Literal(value: 4)])

print("Broadcast: {shape_to_string(a)} + {shape_to_string(b)}")
match broadcast_shapes(a, b):
    case ShapeResult.Ok(result):
        print("FAIL: Should have failed!")
    case ShapeResult.Err(e):
        print("OK: Correctly rejected (5 != 4)")

print("")

# ============================================================================
# Memory Profiling
# ============================================================================

fn compute_memory_mb(shape: TensorShape, elem_size: i32) -> (i32, i32):
    """Compute min and max memory in MB for a shape"""
    val (min_e, max_e) = count_elements(shape)
    val min_bytes = min_e * elem_size
    val max_bytes = max_e * elem_size
    val min_mb = min_bytes / (1024 * 1024)
    val max_mb = max_bytes / (1024 * 1024)
    (min_mb, max_mb)

print("Test 3: Memory Profiling")
print("------------------------------------------------------------")

val elem_size = 4  # Float32

# CNN layer shapes
val input_shape = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 3),
    Dim.Literal(value: 224),
    Dim.Literal(value: 224)
])

val conv1_shape = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 64),
    Dim.Literal(value: 112),
    Dim.Literal(value: 112)
])

val conv2_shape = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 128),
    Dim.Literal(value: 56),
    Dim.Literal(value: 56)
])

val flat_shape = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 401408)
])

print("CNN Memory Profile (per layer):")
val (in_min, in_max) = compute_memory_mb(input_shape, elem_size)
print(f"  Input (NCHW): {shape_to_string(input_shape)} = {in_min}-{in_max} MB")

val (c1_min, c1_max) = compute_memory_mb(conv1_shape, elem_size)
print(f"  Conv1 Output: {shape_to_string(conv1_shape)} = {c1_min}-{c1_max} MB")

val (c2_min, c2_max) = compute_memory_mb(conv2_shape, elem_size)
print(f"  Conv2 Output: {shape_to_string(conv2_shape)} = {c2_min}-{c2_max} MB")

val (fl_min, fl_max) = compute_memory_mb(flat_shape, elem_size)
print(f"  Flattened: {shape_to_string(flat_shape)} = {fl_min}-{fl_max} MB")

val total_min = in_min + c1_min + c2_min + fl_min
val total_max = in_max + c1_max + c2_max + fl_max

print("")
print(f"Total Memory: {total_min}-{total_max} MB")
print(f"Peak batch=64: {total_max} MB")
print("")

# ============================================================================
# Runtime Shape Verification
# ============================================================================

fn check_dim_valid(actual_dim: i32, expected_dim: Dim) -> bool:
    """Check if a single dimension matches constraints"""
    match expected_dim:
        case Dim.Literal(v):
            actual_dim == v
        case Dim.Named(_, lo, hi):
            actual_dim >= lo and actual_dim <= hi
        case _:
            true

fn verify_runtime_shape(actual: List<i32>, expected: TensorShape) -> bool:
    """Verify actual runtime shape matches expected constraints"""
    if actual.len() != expected.ndim():
        return false

    var i = 0
    while i < actual.len():
        val actual_dim = actual[i]
        val expected_dim = expected.dims[i]

        if not check_dim_valid(actual_dim, expected_dim):
            return false

        i = i + 1

    true

print("Test 4: Runtime Shape Verification")
print("------------------------------------------------------------")

val expected_shape = TensorShape(dims: [
    Dim.Named(name: "batch", lo: 1, hi: 64),
    Dim.Literal(value: 784)
])

# Valid shapes
val valid_shapes = [
    [32, 784],
    [1, 784],
    [64, 784]
]

print("Expected: {shape_to_string(expected_shape)}")
print("")

for actual in valid_shapes:
    val actual_str = "[{actual[0]}, {actual[1]}]"
    if verify_runtime_shape(actual, expected_shape):
        print("OK: {actual_str} matches")
    else:
        print("FAIL: {actual_str} rejected")

# Invalid shapes
val invalid_shapes = [
    [65, 784],
    [32, 512],
    [0, 784]
]

for actual in invalid_shapes:
    val actual_str = "[{actual[0]}, {actual[1]}]"
    if verify_runtime_shape(actual, expected_shape):
        print("FAIL: {actual_str} should have been rejected")
    else:
        print("OK: {actual_str} correctly rejected")

print("")

# ============================================================================
# Summary
# ============================================================================

print("============================================================")
print("  SUMMARY")
print("============================================================")
print("")
print("Demonstrated advanced operations:")
print("  OK: Reshape with element count verification")
print("  OK: Broadcasting rules and shape inference")
print("  OK: Memory profiling for CNN architectures")
print("  OK: Runtime shape constraint verification")
print("")
print("Key Capabilities:")
print("  • Compile-time reshape validation")
print("  • Numpy-style broadcasting semantics")
print("  • Memory estimation with min/max bounds")
print("  • Runtime constraint checking")
print("============================================================")
