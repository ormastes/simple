# Torch Dynamic FFI (DynLoader-based)
#
# Wraps rt_torch_* calls via DynLoader, loading libspl_torch.so on demand.
# Works in compiled mode without static linking to libtorch.
# Array-taking functions (tensor creation with dims) are excluded â€” use ffi.spl for those.
#
# Convention: SIMPLE_SFFI_PATH env var sets the .so search directory (default: build)

use std.ffi.dynamic.{DynLib, DynLoader, sffi_lib_path}

extern fn rt_cstring_to_text(ptr: i64) -> text

val TORCH_LIB_PATH = sffi_lib_path("torch")

fn _dl() -> DynLoader:
    DynLoader.instance()

fn _call0(name: text) -> i64:
    var loader = _dl()
    loader.call0(TORCH_LIB_PATH, name)

fn _call1(name: text, a0: i64) -> i64:
    var loader = _dl()
    loader.call1(TORCH_LIB_PATH, name, a0)

fn _call2(name: text, a0: i64, a1: i64) -> i64:
    var loader = _dl()
    loader.call2(TORCH_LIB_PATH, name, a0, a1)

fn _call3(name: text, a0: i64, a1: i64, a2: i64) -> i64:
    var loader = _dl()
    loader.call3(TORCH_LIB_PATH, name, a0, a1, a2)

fn _call_n(name: text, args: [i64]) -> i64:
    var loader = _dl()
    loader.call(TORCH_LIB_PATH, name, args)

# ============================================================================
# Library Information
# ============================================================================

# Check if PyTorch/libtorch is available at runtime
fn dyn_torch_available() -> bool:
    _call0("rt_torch_available") != 0

# Check if CUDA is available for GPU acceleration
fn dyn_torch_cuda_available() -> bool:
    _call0("rt_torch_cuda_available") != 0

# Get PyTorch version string (returned as C string pointer, converted to text)
fn dyn_torch_version() -> text:
    val ptr = _call0("rt_torch_version")
    rt_cstring_to_text(ptr)

# ============================================================================
# Element-wise Arithmetic Operations (single-handle)
# ============================================================================

# Element-wise negation: -a
fn dyn_torch_tensor_neg(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_neg", handle)

# Element-wise absolute value
fn dyn_torch_tensor_abs(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_abs", handle)

# Element-wise square root
fn dyn_torch_tensor_sqrt(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sqrt", handle)

# Element-wise exponential: e^x
fn dyn_torch_tensor_exp(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_exp", handle)

# Element-wise natural logarithm
fn dyn_torch_tensor_log(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_log", handle)

# ============================================================================
# Activation Functions (single-handle)
# ============================================================================

# ReLU activation: max(0, x)
fn dyn_torch_tensor_relu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_relu", handle)

# Sigmoid activation: 1 / (1 + e^(-x))
fn dyn_torch_tensor_sigmoid(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sigmoid", handle)

# Tanh activation
fn dyn_torch_tensor_tanh(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_tanh", handle)

# GELU activation (Gaussian Error Linear Unit)
fn dyn_torch_tensor_gelu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_gelu", handle)

# ============================================================================
# Linear Algebra / Shape Operations (single-handle)
# ============================================================================

# Matrix transpose (2D only, swap rows/cols)
fn dyn_torch_tensor_t(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_t", handle)

# Flatten to 1D
fn dyn_torch_tensor_flatten(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_flatten", handle)

# Make tensor contiguous in memory
fn dyn_torch_tensor_contiguous(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_contiguous", handle)

# Remove dimensions of size 1
fn dyn_torch_tensor_squeeze(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_squeeze", handle)

# ============================================================================
# Shape Information (single-handle)
# ============================================================================

# Get number of dimensions
fn dyn_torch_tensor_ndim(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_ndim", handle)

# Get total number of elements
fn dyn_torch_tensor_numel(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_numel", handle)

# ============================================================================
# Device Management (single-handle)
# ============================================================================

# Move tensor to CPU
fn dyn_torch_tensor_cpu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_cpu", handle)

# Clone tensor (deep copy)
fn dyn_torch_tensor_clone(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_clone", handle)

# Check if tensor is on CUDA device
fn dyn_torch_tensor_is_cuda(handle: i64) -> bool:
    _call1("rt_torch_torchtensor_is_cuda", handle) != 0

# ============================================================================
# Memory Management (single-handle)
# ============================================================================

# Free tensor handle (release memory)
fn dyn_torch_tensor_free(handle: i64):
    _call1("rt_torch_torchtensor_free", handle)

# ============================================================================
# Autograd Operations (single-handle)
# ============================================================================

# Compute gradients (backward pass)
fn dyn_torch_autograd_backward(handle: i64):
    _call1("rt_torch_autograd_backward", handle)

# Zero out gradients
fn dyn_torch_autograd_zero_grad(handle: i64):
    _call1("rt_torch_autograd_zero_grad", handle)

# Detach tensor from computation graph (no gradient)
fn dyn_torch_autograd_detach(handle: i64) -> i64:
    _call1("rt_torch_autograd_detach", handle)

# Get gradient tensor (returns handle or 0 if no gradient)
fn dyn_torch_autograd_grad(handle: i64) -> i64:
    _call1("rt_torch_autograd_grad", handle)

# Check if tensor requires gradients
fn dyn_torch_autograd_requires_grad(handle: i64) -> bool:
    _call1("rt_torch_autograd_requires_grad", handle) != 0

# ============================================================================
# Element-wise Arithmetic Operations (two-handle)
# ============================================================================

# Element-wise addition: a + b
fn dyn_torch_tensor_add(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_add", handle, other)

# Element-wise subtraction: a - b
fn dyn_torch_tensor_sub(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_sub", handle, other)

# Element-wise multiplication: a * b
fn dyn_torch_tensor_mul(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_mul", handle, other)

# Element-wise division: a / b
fn dyn_torch_tensor_div(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_div", handle, other)

# Matrix multiplication: a @ b
fn dyn_torch_tensor_matmul(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_matmul", handle, other)

# Dot product (1D tensors)
fn dyn_torch_tensor_dot(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_dot", handle, other)

# ============================================================================
# Activation Functions (two-arg: handle + dim)
# ============================================================================

# Softmax along dimension
fn dyn_torch_tensor_softmax(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_softmax", handle, dim)

# Log softmax along dimension
fn dyn_torch_tensor_log_softmax(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_log_softmax", handle, dim)

# ============================================================================
# Shape Manipulation (two-arg: handle + dim)
# ============================================================================

# Add dimension of size 1
fn dyn_torch_tensor_unsqueeze(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_unsqueeze", handle, dim)

# Remove specific dimension of size 1
fn dyn_torch_tensor_squeeze_dim(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_squeeze_dim", handle, dim)

# ============================================================================
# Indexing (three-arg)
# ============================================================================

# Index select (gather specific indices); indices is a tensor handle
fn dyn_torch_tensor_index_select(handle: i64, dim: i64, indices: i64) -> i64:
    _call3("rt_torch_torchtensor_index_select", handle, dim, indices)

# ============================================================================
# Device Management (two-arg: handle + device_id)
# ============================================================================

# Move tensor to CUDA device (device_id passed as i64 to avoid i32 cast issues)
fn dyn_torch_tensor_cuda(handle: i64, device_id: i64) -> i64:
    _call2("rt_torch_torchtensor_cuda", handle, device_id)

# ============================================================================
# Autograd Operations (two-arg)
# ============================================================================

# Enable/disable gradient computation for tensor (bool passed as i64: 1=true, 0=false)
fn dyn_torch_autograd_set_requires_grad(handle: i64, requires_grad: i64):
    _call2("rt_torch_autograd_set_requires_grad", handle, requires_grad)

# ============================================================================
# Linear Algebra Operations (three-arg)
# ============================================================================

# Matrix transpose along two specified dimensions
fn dyn_torch_tensor_transpose(handle: i64, dim0: i64, dim1: i64) -> i64:
    _call3("rt_torch_torchtensor_transpose", handle, dim0, dim1)

# ============================================================================
# Reduction Operations (three-arg: handle, dim, keepdim)
# ============================================================================

# Sum along dimension. keepdim: 1=true, 0=false
fn dyn_torch_tensor_sum_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_sum_dim", handle, dim, keepdim)

# Mean along dimension
fn dyn_torch_tensor_mean_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_mean_dim", handle, dim, keepdim)

# Argmax along dimension
fn dyn_torch_tensor_argmax(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_argmax", handle, dim, keepdim)

# Argmin along dimension
fn dyn_torch_tensor_argmin(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_argmin", handle, dim, keepdim)

# Max along dimension
fn dyn_torch_tensor_max_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_max_dim", handle, dim, keepdim)

# ============================================================================
# Indexing Operations (three-arg)
# ============================================================================

# Gather elements along dim using indices tensor
fn dyn_torch_tensor_gather(handle: i64, dim: i64, indices: i64) -> i64:
    _call3("rt_torch_torchtensor_gather", handle, dim, indices)

# ============================================================================
# Slice Operation (five-arg: handle, dim, start, end, step)
# ============================================================================

fn dyn_torch_tensor_slice(handle: i64, dim: i64, start: i64, end_idx: i64, step: i64) -> i64:
    _call_n("rt_torch_torchtensor_slice", [handle, dim, start, end_idx, step])

# ============================================================================
# NN Operations (three-arg: input, weight, bias)
# ============================================================================

# Linear layer: output = input @ weight^T + bias
fn dyn_torch_nn_linear(input: i64, weight: i64, bias: i64) -> i64:
    _call3("rt_torch_nn_linear", input, weight, bias)

# Embedding lookup: select rows from weight by input indices
fn dyn_torch_nn_embedding(input: i64, weight: i64) -> i64:
    _call2("rt_torch_nn_embedding", input, weight)

# ============================================================================
# Autograd Operations (no-arg)
# ============================================================================

# Begin no-gradient context
fn dyn_torch_autograd_no_grad_begin():
    _call0("rt_torch_autograd_no_grad_begin")

# End no-gradient context
fn dyn_torch_autograd_no_grad_end():
    _call0("rt_torch_autograd_no_grad_end")

# ============================================================================
# CUDA Memory (one-arg: device_id)
# ============================================================================

# Get currently allocated CUDA memory in bytes
fn dyn_torch_cuda_memory_allocated(device_id: i64) -> i64:
    _call1("rt_torch_cuda_memory_allocated", device_id)

# Get peak allocated CUDA memory in bytes
fn dyn_torch_cuda_max_memory_allocated(device_id: i64) -> i64:
    _call1("rt_torch_cuda_max_memory_allocated", device_id)

# Clear CUDA cache
fn dyn_torch_cuda_empty_cache():
    _call0("rt_torch_cuda_empty_cache")

# ============================================================================
# Exports
# ============================================================================

export dyn_torch_available, dyn_torch_cuda_available, dyn_torch_version
export dyn_torch_tensor_neg, dyn_torch_tensor_abs, dyn_torch_tensor_sqrt
export dyn_torch_tensor_exp, dyn_torch_tensor_log
export dyn_torch_tensor_relu, dyn_torch_tensor_sigmoid, dyn_torch_tensor_tanh
export dyn_torch_tensor_gelu, dyn_torch_tensor_t, dyn_torch_tensor_flatten
export dyn_torch_tensor_contiguous, dyn_torch_tensor_squeeze
export dyn_torch_tensor_ndim, dyn_torch_tensor_numel
export dyn_torch_tensor_cpu, dyn_torch_tensor_clone
export dyn_torch_tensor_is_cuda, dyn_torch_tensor_free
export dyn_torch_tensor_add, dyn_torch_tensor_sub, dyn_torch_tensor_mul, dyn_torch_tensor_div
export dyn_torch_tensor_matmul, dyn_torch_tensor_dot
export dyn_torch_tensor_softmax, dyn_torch_tensor_log_softmax
export dyn_torch_tensor_unsqueeze, dyn_torch_tensor_squeeze_dim
export dyn_torch_tensor_index_select, dyn_torch_tensor_cuda, dyn_torch_tensor_transpose
export dyn_torch_autograd_backward, dyn_torch_autograd_zero_grad
export dyn_torch_autograd_detach, dyn_torch_autograd_grad
export dyn_torch_autograd_requires_grad, dyn_torch_autograd_set_requires_grad
export dyn_torch_autograd_no_grad_begin, dyn_torch_autograd_no_grad_end
export dyn_torch_tensor_sum_dim, dyn_torch_tensor_mean_dim
export dyn_torch_tensor_argmax, dyn_torch_tensor_argmin, dyn_torch_tensor_max_dim
export dyn_torch_tensor_gather, dyn_torch_tensor_slice
export dyn_torch_nn_linear, dyn_torch_nn_embedding
export dyn_torch_cuda_memory_allocated, dyn_torch_cuda_max_memory_allocated
export dyn_torch_cuda_empty_cache
