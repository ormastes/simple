# GC TorchBackend Implementation
#
# Implements the common TorchBackend trait using direct FFI calls.
# Same handle-level operations as nogc — the gc/nogc difference is
# in the Tensor wrapper class, not in the backend operations.
#
# Usage:
#   use std.common.torch.interface.{TorchBackend}
#   use std.gc_async_mut.torch.backend  # registers impl
#   bind TorchBackend = GcTorchOps      # static dispatch

use std.common.torch.interface.{TorchBackend}
use std.torch.ffi.{
    rt_torch_available,
    rt_torch_version,
    rt_torch_cuda_available,
    rt_torch_tensor_zeros,
    rt_torch_tensor_ones,
    rt_torch_tensor_randn,
    rt_torch_tensor_rand,
    rt_torch_tensor_full,
    rt_torch_tensor_from_data,
    rt_torch_tensor_eye,
    rt_torch_tensor_empty,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_div,
    rt_torch_torchtensor_matmul,
    rt_torch_torchtensor_neg,
    rt_torch_torchtensor_abs,
    rt_torch_torchtensor_sqrt,
    rt_torch_torchtensor_exp,
    rt_torch_torchtensor_log,
    rt_torch_torchtensor_pow,
    rt_torch_torchtensor_add_scalar,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_relu,
    rt_torch_torchtensor_sigmoid,
    rt_torch_torchtensor_tanh,
    rt_torch_torchtensor_softmax,
    rt_torch_torchtensor_log_softmax,
    rt_torch_torchtensor_leaky_relu,
    rt_torch_torchtensor_gelu,
    rt_torch_torchtensor_shape,
    rt_torch_torchtensor_ndim,
    rt_torch_torchtensor_numel,
    rt_torch_torchtensor_reshape,
    rt_torch_torchtensor_view,
    rt_torch_torchtensor_transpose,
    rt_torch_torchtensor_permute,
    rt_torch_torchtensor_squeeze_dim,
    rt_torch_torchtensor_unsqueeze,
    rt_torch_torchtensor_flatten,
    rt_torch_torchtensor_contiguous,
    rt_torch_torchtensor_sum,
    rt_torch_torchtensor_sum_dim,
    rt_torch_torchtensor_mean,
    rt_torch_torchtensor_mean_dim,
    rt_torch_autograd_backward,
    rt_torch_autograd_zero_grad,
    rt_torch_autograd_set_requires_grad,
    rt_torch_autograd_requires_grad,
    rt_torch_autograd_grad,
    rt_torch_autograd_detach,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_cpu,
    rt_torch_torchtensor_is_cuda,
    rt_torch_nn_linear,
    rt_torch_nn_conv2d,
    rt_torch_nn_max_pool2d,
    rt_torch_nn_batch_norm,
    rt_torch_nn_dropout,
    rt_torch_nn_mse_loss,
    rt_torch_nn_cross_entropy
}

# ============================================================================
# GC Backend — Direct FFI Implementation
# ============================================================================
# At the handle level, gc and nogc backends are identical — both call
# the same libtorch FFI functions. The difference is in the Tensor wrapper:
#   - GC Tensor: owns_handle field, conditional drop(), shared references
#   - NoGC Tensor: no owns_handle, unconditional drop(), move semantics

struct GcTorchOps:
    pass_dn

impl TorchBackend for GcTorchOps:
    fn available() -> bool:
        rt_torch_available()

    fn version() -> text:
        rt_torch_version()

    fn cuda_available() -> bool:
        rt_torch_cuda_available()

    fn create_zeros(dims: [i64]) -> i64:
        rt_torch_tensor_zeros(dims)

    fn create_ones(dims: [i64]) -> i64:
        rt_torch_tensor_ones(dims)

    fn create_randn(dims: [i64]) -> i64:
        rt_torch_tensor_randn(dims)

    fn create_rand(dims: [i64]) -> i64:
        rt_torch_tensor_rand(dims)

    fn create_full(dims: [i64], value: f64) -> i64:
        rt_torch_tensor_full(dims, value)

    fn create_from_data(data: [f64], dims: [i64]) -> i64:
        rt_torch_tensor_from_data(data, dims)

    fn create_eye(n: i64) -> i64:
        rt_torch_tensor_eye(n)

    fn create_empty(dims: [i64]) -> i64:
        rt_torch_tensor_empty(dims)

    fn tensor_add(a: i64, b: i64) -> i64:
        rt_torch_torchtensor_add(a, b)

    fn tensor_sub(a: i64, b: i64) -> i64:
        rt_torch_torchtensor_sub(a, b)

    fn tensor_mul(a: i64, b: i64) -> i64:
        rt_torch_torchtensor_mul(a, b)

    fn tensor_div(a: i64, b: i64) -> i64:
        rt_torch_torchtensor_div(a, b)

    fn tensor_matmul(a: i64, b: i64) -> i64:
        rt_torch_torchtensor_matmul(a, b)

    fn tensor_neg(h: i64) -> i64:
        rt_torch_torchtensor_neg(h)

    fn tensor_abs(h: i64) -> i64:
        rt_torch_torchtensor_abs(h)

    fn tensor_sqrt(h: i64) -> i64:
        rt_torch_torchtensor_sqrt(h)

    fn tensor_exp(h: i64) -> i64:
        rt_torch_torchtensor_exp(h)

    fn tensor_log(h: i64) -> i64:
        rt_torch_torchtensor_log(h)

    fn tensor_pow(h: i64, exponent: f64) -> i64:
        rt_torch_torchtensor_pow(h, exponent)

    fn tensor_add_scalar(h: i64, scalar: f64) -> i64:
        rt_torch_torchtensor_add_scalar(h, scalar)

    fn tensor_mul_scalar(h: i64, scalar: f64) -> i64:
        rt_torch_torchtensor_mul_scalar(h, scalar)

    fn tensor_relu(h: i64) -> i64:
        rt_torch_torchtensor_relu(h)

    fn tensor_sigmoid(h: i64) -> i64:
        rt_torch_torchtensor_sigmoid(h)

    fn tensor_tanh(h: i64) -> i64:
        rt_torch_torchtensor_tanh(h)

    fn tensor_softmax(h: i64, dim: i64) -> i64:
        rt_torch_torchtensor_softmax(h, dim)

    fn tensor_log_softmax(h: i64, dim: i64) -> i64:
        rt_torch_torchtensor_log_softmax(h, dim)

    fn tensor_leaky_relu(h: i64, negative_slope: f64) -> i64:
        rt_torch_torchtensor_leaky_relu(h, negative_slope)

    fn tensor_gelu(h: i64) -> i64:
        rt_torch_torchtensor_gelu(h)

    fn tensor_shape(h: i64) -> [i64]:
        rt_torch_torchtensor_shape(h)

    fn tensor_ndim(h: i64) -> i64:
        rt_torch_torchtensor_ndim(h)

    fn tensor_numel(h: i64) -> i64:
        rt_torch_torchtensor_numel(h)

    fn tensor_reshape(h: i64, dims: [i64]) -> i64:
        rt_torch_torchtensor_reshape(h, dims)

    fn tensor_view(h: i64, dims: [i64]) -> i64:
        rt_torch_torchtensor_view(h, dims)

    fn tensor_transpose(h: i64, dim0: i64, dim1: i64) -> i64:
        rt_torch_torchtensor_transpose(h, dim0, dim1)

    fn tensor_permute(h: i64, dims: [i64]) -> i64:
        rt_torch_torchtensor_permute(h, dims)

    fn tensor_squeeze(h: i64, dim: i64) -> i64:
        rt_torch_torchtensor_squeeze_dim(h, dim)

    fn tensor_unsqueeze(h: i64, dim: i64) -> i64:
        rt_torch_torchtensor_unsqueeze(h, dim)

    fn tensor_flatten(h: i64) -> i64:
        rt_torch_torchtensor_flatten(h)

    fn tensor_contiguous(h: i64) -> i64:
        rt_torch_torchtensor_contiguous(h)

    fn tensor_sum(h: i64) -> f64:
        rt_torch_torchtensor_sum(h)

    fn tensor_sum_dim(h: i64, dim: i64, keepdim: bool) -> i64:
        rt_torch_torchtensor_sum_dim(h, dim, keepdim)

    fn tensor_mean(h: i64) -> f64:
        rt_torch_torchtensor_mean(h)

    fn tensor_mean_dim(h: i64, dim: i64, keepdim: bool) -> i64:
        rt_torch_torchtensor_mean_dim(h, dim, keepdim)

    fn tensor_backward(h: i64):
        rt_torch_autograd_backward(h)

    fn tensor_zero_grad(h: i64):
        rt_torch_autograd_zero_grad(h)

    fn tensor_set_requires_grad(h: i64, value: bool):
        rt_torch_autograd_set_requires_grad(h, value)

    fn tensor_requires_grad(h: i64) -> bool:
        rt_torch_autograd_requires_grad(h)

    fn tensor_grad(h: i64) -> i64:
        rt_torch_autograd_grad(h)

    fn tensor_detach(h: i64) -> i64:
        rt_torch_autograd_detach(h)

    fn no_grad_begin():
        rt_torch_autograd_no_grad_begin()

    fn no_grad_end():
        rt_torch_autograd_no_grad_end()

    fn tensor_clone(h: i64) -> i64:
        rt_torch_torchtensor_clone(h)

    fn tensor_free(h: i64):
        rt_torch_torchtensor_free(h)

    fn tensor_cuda(h: i64, device_id: i64) -> i64:
        rt_torch_torchtensor_cuda(h, 0)

    fn tensor_cpu(h: i64) -> i64:
        rt_torch_torchtensor_cpu(h)

    fn tensor_is_cuda(h: i64) -> bool:
        rt_torch_torchtensor_is_cuda(h)

    fn nn_linear(input: i64, weight: i64, bias: i64) -> i64:
        rt_torch_nn_linear(input, weight, bias)

    fn nn_conv2d(input: i64, weight: i64, bias: i64, stride: [i64], padding: [i64], dilation: [i64], groups: i64) -> i64:
        rt_torch_nn_conv2d(input, weight, bias, stride, padding, dilation, groups)

    fn nn_max_pool2d(input: i64, kernel_size: [i64], stride: [i64], padding: [i64]) -> i64:
        rt_torch_nn_max_pool2d(input, kernel_size, stride, padding)

    fn nn_batch_norm(input: i64, running_mean: i64, running_var: i64, weight: i64, bias: i64, training: bool, momentum: f64, eps: f64) -> i64:
        rt_torch_nn_batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)

    fn nn_dropout(input: i64, p: f64, training: bool) -> i64:
        rt_torch_nn_dropout(input, p, training)

    fn nn_mse_loss(input: i64, target: i64) -> f64:
        rt_torch_nn_mse_loss(input, target)

    fn nn_cross_entropy(input: i64, target: i64) -> f64:
        rt_torch_nn_cross_entropy(input, target)

export GcTorchOps
