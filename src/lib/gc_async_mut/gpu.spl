# GPU Memory Management API
#
# High-level GPU memory management for Simple language.
# Three-tier SFFI pattern:
#   Tier 1: extern fn rt_cuda_* (in compiler/loader/cuda_ffi.spl)
#   Tier 2: fn cuda_* (low-level wrappers in cuda_ffi.spl)
#   Tier 3: GpuPtr, gpu_init, gpu_alloc, etc. (this file)
#
# Provides safe GPU memory management with automatic error checking.

use compiler.loader.cuda_ffi.*

# ============================================================================
# GPU Error Handling
# ============================================================================

struct GpuError:
    """GPU operation error."""
    code: i64
    message: text

    static fn from_code(code: i64) -> GpuError:
        """Create error from CUDA error code."""
        GpuError(
            code: code,
            message: cuda_get_error_string(code)
        )

    fn to_text() -> text:
        "GpuError({self.code}): {self.message}"

# ============================================================================
# GPU Device
# ============================================================================

struct GpuDevice:
    """Handle to a GPU device."""
    id: i64
    handle: i64

    static fn default() -> Result<GpuDevice, GpuError>:
        """Get the default GPU device (device 0)."""
        GpuDevice__get(0)

    static fn get(device_id: i64) -> Result<GpuDevice, GpuError>:
        """Get a GPU device by ID."""
        val handle = cuda_device_get(device_id)
        if handle < 0:
            return Err(GpuError__from_code(handle))
        Ok(GpuDevice(id: device_id, handle: handle))

    static fn count() -> i64:
        """Get number of available GPU devices."""
        cuda_device_count()

    fn name() -> text:
        """Get device name."""
        cuda_device_name(self.handle)

    fn compute_capability() -> (i64, i64):
        """Get compute capability as (major, minor)."""
        val cap = cuda_device_compute_capability(self.handle)
        (cap / 10, cap % 10)

# ============================================================================
# GPU Context
# ============================================================================

struct GpuContext:
    """GPU execution context."""
    handle: i64
    device: GpuDevice

    static fn create(device: GpuDevice) -> Result<GpuContext, GpuError>:
        """Create a GPU context for a device."""
        val handle = cuda_ctx_create(device.handle)
        if handle < 0:
            return Err(GpuError__from_code(handle))
        Ok(GpuContext(handle: handle, device: device))

    fn destroy() -> Result<(), GpuError>:
        """Destroy this GPU context."""
        val result = cuda_ctx_destroy(self.handle)
        if result < 0:
            return Err(GpuError__from_code(result))
        Ok(())

    fn synchronize() -> Result<(), GpuError>:
        """Wait for all operations on this context to complete."""
        val result = cuda_ctx_synchronize()
        if result < 0:
            return Err(GpuError__from_code(result))
        Ok(())

# ============================================================================
# GPU Pointer - Device Memory Handle
# ============================================================================

class GpuPtr:
    """
    Handle to GPU device memory.

    Wraps a device pointer with size tracking for safe memory operations.
    Must be explicitly freed with gpu_free().

    Usage:
        val ptr = gpu_alloc(1024)
        gpu_upload(ptr, host_data, 1024)
        gpu_download(ptr, host_buf, 1024)
        gpu_free(ptr)
    """

    device_ptr: i64
    size: i64
    is_valid: bool

    static fn null() -> GpuPtr:
        """Create a null GPU pointer."""
        GpuPtr(device_ptr: 0, size: 0, is_valid: false)

    fn is_null() -> bool:
        """Check if pointer is null/invalid."""
        self.device_ptr == 0 or not self.is_valid

# ============================================================================
# GPU Module - Compiled Kernel Container
# ============================================================================

struct GpuModule:
    """Handle to a loaded GPU module (compiled PTX)."""
    handle: i64

    static fn load_ptx(ptx_code: text) -> Result<GpuModule, GpuError>:
        """Load a GPU module from PTX source string."""
        val handle = cuda_module_load_data(ptx_code)
        if handle < 0:
            return Err(GpuError__from_code(handle))
        Ok(GpuModule(handle: handle))

    static fn load_file(path: text) -> Result<GpuModule, GpuError>:
        """Load a GPU module from a file path."""
        val handle = cuda_module_load(path)
        if handle < 0:
            return Err(GpuError__from_code(handle))
        Ok(GpuModule(handle: handle))

    fn unload() -> Result<(), GpuError>:
        """Unload this GPU module."""
        val result = cuda_module_unload(self.handle)
        if result < 0:
            return Err(GpuError__from_code(result))
        Ok(())

# ============================================================================
# Launch Configuration
# ============================================================================

struct GpuLaunchConfig:
    """Configuration for launching a GPU kernel."""
    grid_x: i64
    grid_y: i64
    grid_z: i64
    block_x: i64
    block_y: i64
    block_z: i64

    static fn simple(num_blocks: i64, threads_per_block: i64) -> GpuLaunchConfig:
        """Create a 1D launch configuration."""
        GpuLaunchConfig(
            grid_x: num_blocks,
            grid_y: 1,
            grid_z: 1,
            block_x: threads_per_block,
            block_y: 1,
            block_z: 1
        )

    static fn grid_2d(grid_x: i64, grid_y: i64, block_x: i64, block_y: i64) -> GpuLaunchConfig:
        """Create a 2D launch configuration."""
        GpuLaunchConfig(
            grid_x: grid_x,
            grid_y: grid_y,
            grid_z: 1,
            block_x: block_x,
            block_y: block_y,
            block_z: 1
        )

    static fn grid_3d(grid_x: i64, grid_y: i64, grid_z: i64, block_x: i64, block_y: i64, block_z: i64) -> GpuLaunchConfig:
        """Create a 3D launch configuration."""
        GpuLaunchConfig(
            grid_x: grid_x,
            grid_y: grid_y,
            grid_z: grid_z,
            block_x: block_x,
            block_y: block_y,
            block_z: block_z
        )

    fn total_threads() -> i64:
        """Get total number of threads."""
        self.grid_x * self.grid_y * self.grid_z * self.block_x * self.block_y * self.block_z

# ============================================================================
# Top-Level API Functions
# ============================================================================

fn gpu_init() -> Result<(), GpuError>:
    """Initialize the GPU runtime. Must be called before any other GPU operations."""
    val result = cuda_init()
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

fn gpu_alloc(size: i64) -> Result<GpuPtr, GpuError>:
    """Allocate device memory of given size in bytes."""
    val ptr = cuda_mem_alloc(size)
    if ptr < 0:
        return Err(GpuError__from_code(ptr))
    Ok(GpuPtr(device_ptr: ptr, size: size, is_valid: true))

fn gpu_free(gpu_ptr: GpuPtr) -> Result<(), GpuError>:
    """Free device memory."""
    if gpu_ptr.is_null():
        return Ok(())
    val result = cuda_mem_free(gpu_ptr.device_ptr)
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

fn gpu_upload(gpu_ptr: GpuPtr, host_ptr: i64, size: i64) -> Result<(), GpuError>:
    """Copy data from host memory to device memory."""
    val result = cuda_memcpy_htod(gpu_ptr.device_ptr, host_ptr, size)
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

fn gpu_download(gpu_ptr: GpuPtr, host_ptr: i64, size: i64) -> Result<(), GpuError>:
    """Copy data from device memory to host memory."""
    val result = cuda_memcpy_dtoh(host_ptr, gpu_ptr.device_ptr, size)
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

fn gpu_memset(gpu_ptr: GpuPtr, value: i64, size: i64) -> Result<(), GpuError>:
    """Set device memory to a value."""
    val result = cuda_memset(gpu_ptr.device_ptr, value, size)
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

fn gpu_launch(module: GpuModule, func_name: text, config: GpuLaunchConfig, args_ptr: i64) -> Result<(), GpuError>:
    """Launch a GPU kernel with the given configuration."""
    val result = cuda_launch_kernel(
        module.handle,
        func_name,
        config.grid_x,
        config.grid_y,
        config.grid_z,
        config.block_x,
        config.block_y,
        config.block_z,
        args_ptr
    )
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

fn gpu_sync() -> Result<(), GpuError>:
    """Synchronize the device - wait for all pending operations to complete."""
    val result = cuda_sync()
    if result < 0:
        return Err(GpuError__from_code(result))
    Ok(())

# ============================================================================
# Export
# ============================================================================

export GpuError, GpuDevice, GpuContext, GpuPtr, GpuModule, GpuLaunchConfig
export gpu_init, gpu_alloc, gpu_free, gpu_upload, gpu_download
export gpu_memset, gpu_launch, gpu_sync
