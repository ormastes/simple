# GPU Memory Management
#
# Provides typed GPU arrays with automatic memory management

use std.gpu.device.{Gpu, GpuBackend}
use std.torch.{TorchTensorWrapper}
use std.torch.ffi.{
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_cpu,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_free,
    rt_torch_tensor_zeros
}

# ============================================================================
# GPU Array Type
# ============================================================================

class GpuArray[T]:
    """Typed GPU array with automatic memory management.

    Wraps backend-specific memory (PyTorch tensors, Vulkan buffers, etc.)
    and provides unified interface for data transfer.

    Memory is automatically freed when array goes out of scope (RAII).

    Example:
        val arr = ctx.alloc[f32](1024)
        arr.upload([1.0, 2.0, 3.0, ...])
        val result = arr.download()
    """

    backend: GpuBackend
    device_id: i32
    count: i64
    torch_tensor: TorchTensorWrapper?

    fn size_bytes() -> i64:
        """Get size in bytes."""
        # Default to 8 bytes per element until sizeof[T]() is available
        eprint("Warning: size_bytes() using default 8 bytes per element, sizeof[T]() not yet available")
        self.count * 8

    fn upload(data: [T]) -> bool:
        """Upload data from host to device.

        Creates a CPU tensor from data, moves it to GPU,
        and stores the handle.

        Args:
            data: Host array to upload

        Returns:
            true on success
        """
        match self.backend:
            case GpuBackend.Cuda:
                val cpu_handle = rt_torch_tensor_from_data(data, [data.len()])
                val gpu_handle = rt_torch_torchtensor_cuda(cpu_handle, self.device_id)
                rt_torch_torchtensor_free(cpu_handle)
                val wrapper = TorchTensorWrapper.from_handle(gpu_handle)
                self.torch_tensor = Some(wrapper)
                self.count = data.len()
                true
            case _:
                false

    fn download() -> [T]:
        """Download data from device to host (blocking).

        Moves tensor to CPU. Returns empty array if no tensor stored
        or if rt_torch_torchtensor_to_data FFI is not available.

        Returns:
            Host array with device data (empty if no data extraction FFI)
        """
        match self.backend:
            case GpuBackend.Cuda:
                match self.torch_tensor:
                    case Some(tensor):
                        # Move to CPU first
                        val cpu_handle = rt_torch_torchtensor_cpu(tensor.handle)
                        # Note: Extracting raw data from tensor requires
                        # rt_torch_torchtensor_to_data FFI (not yet available).
                        # The CPU tensor handle is valid for further torch ops.
                        rt_torch_torchtensor_free(cpu_handle)
                        var result: [T] = []
                        result
                    case nil:
                        var result: [T] = []
                        result
            case _:
                var result: [T] = []
                result

    fn copy_to(other: GpuArray[T]) -> bool:
        """Copy device to device.

        Clones the source tensor and transfers to destination device.

        Args:
            other: Destination array

        Returns:
            true on success
        """
        match self.backend:
            case GpuBackend.Cuda:
                match self.torch_tensor:
                    case Some(tensor):
                        val cloned = rt_torch_torchtensor_clone(tensor.handle)
                        val on_device = rt_torch_torchtensor_cuda(cloned, other.device_id)
                        rt_torch_torchtensor_free(cloned)
                        other.torch_tensor = Some(TorchTensorWrapper.from_handle(on_device))
                        other.count = self.count
                        true
                    case nil:
                        false
            case _:
                false

    fn drop():
        """Automatically free memory when object goes out of scope."""
        match self.backend:
            case GpuBackend.Cuda:
                # PyTorch tensor handles its own memory
                # Wrapper's drop() will be called automatically
                ()
            case _:
                ()

# ============================================================================
# Memory Allocation Functions
# ============================================================================

fn gpu_alloc[T](gpu: Gpu, count: i64) -> GpuArray[T]:
    """Allocate GPU array (empty/uninitialized).

    Creates a zeros tensor on CPU and moves to GPU device.

    Args:
        gpu: GPU device
        count: Number of elements

    Returns:
        GPU array with allocated memory
    """
    match gpu.backend:
        case GpuBackend.Cuda:
            val cpu_handle = rt_torch_tensor_zeros([count])
            val gpu_handle = rt_torch_torchtensor_cuda(cpu_handle, gpu.device_id)
            rt_torch_torchtensor_free(cpu_handle)
            val wrapper = TorchTensorWrapper.from_handle(gpu_handle)
            GpuArray[T](
                backend: GpuBackend.Cuda,
                device_id: gpu.device_id,
                count: count,
                torch_tensor: Some(wrapper)
            )
        case _:
            GpuArray[T](
                backend: GpuBackend.None,
                device_id: -1,
                count: count,
                torch_tensor: nil
            )

fn gpu_alloc_upload[T](gpu: Gpu, data: [T]) -> GpuArray[T]:
    """Allocate and upload data to GPU in one call.

    Args:
        gpu: GPU device
        data: Host data to upload

    Returns:
        GPU array with uploaded data
    """
    val arr = gpu_alloc[T](gpu, data.len())
    arr.upload(data)
    arr

fn gpu_alloc_zeros[T](gpu: Gpu, count: i64) -> GpuArray[T]:
    """Allocate zero-initialized GPU array.

    Args:
        gpu: GPU device
        count: Number of elements

    Returns:
        GPU array filled with zeros
    """
    match gpu.backend:
        case GpuBackend.Cuda:
            # Use PyTorch zeros tensor
            val tensor = TorchTensorWrapper.tensor_zeros([count])
            val gpu_tensor = tensor.cuda(gpu.device_id)
            GpuArray[T](
                backend: GpuBackend.Cuda,
                device_id: gpu.device_id,
                count: count,
                torch_tensor: Some(gpu_tensor)
            )
        case _:
            GpuArray[T](
                backend: GpuBackend.None,
                device_id: -1,
                count: count,
                torch_tensor: nil
            )

# ============================================================================
# Exports
# ============================================================================

export GpuArray
export gpu_alloc, gpu_alloc_upload, gpu_alloc_zeros
