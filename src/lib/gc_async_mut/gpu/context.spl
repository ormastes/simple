# GPU Context API
#
# Unified interface for GPU operations across backends

use std.gpu.device.{GpuBackend, Gpu, detect_backends, preferred_backend, gpu_cuda, gpu_none}
use std.gpu.memory.{GpuArray, gpu_alloc, gpu_alloc_upload, gpu_alloc_zeros}
use std.torch.{TorchStream}

# ============================================================================
# Context Type
# ============================================================================

class Context:
    """Unified GPU context managing device, memory, and streams.

    Automatically selects best available backend and provides
    backend-agnostic API for all operations.

    Example:
        # Auto-detect best GPU
        val ctx = Context.default()

        # Allocate and upload
        val a = ctx.alloc_upload([1.0, 2.0, 3.0, 4.0])
        val b = ctx.alloc_upload([5.0, 6.0, 7.0, 8.0])

        # Operations (future)
        # ctx.launch(kernel, args: [a, b])

        # Download results
        val result = a.download()

        # Memory auto-freed when ctx goes out of scope
    """

    backend: GpuBackend
    device: Gpu
    default_stream: TorchStream?

    # ========================================================================
    # Constructors
    # ========================================================================

    static fn default() -> Context:
        """Auto-detect best backend and device.

        Checks in order:
        1. CUDA (via PyTorch) - device 0
        2. Vulkan compute - device 0 (future)
        3. CPU fallback

        Returns:
            Context with best available backend
        """
        val backend = preferred_backend()
        match backend:
            case GpuBackend.Cuda:
                Context.new(backend: GpuBackend.Cuda, device: 0)
            case GpuBackend.Vulkan:
                Context.new(backend: GpuBackend.Vulkan, device: 0)
            case GpuBackend.None:
                Context.new(backend: GpuBackend.None, device: -1)

    static fn new(backend: GpuBackend, device: i32) -> Context:
        """Create context for specific backend and device.

        Args:
            backend: GPU backend type
            device: Device ID (0=1st GPU, 1=2nd GPU, etc.)

        Returns:
            Context for specified backend

        Example:
            # Explicit CUDA on 2nd GPU
            val ctx = Context.new(backend: GpuBackend.Cuda, device: 1)
        """
        val gpu = match backend:
            case GpuBackend.Cuda: gpu_cuda(device)
            case GpuBackend.Vulkan:
                gpu_vulkan(device)
            case GpuBackend.None: gpu_none()

        val stream = match backend:
            case GpuBackend.Cuda:
                Some(TorchStream.create(device))
            case _:
                nil

        Context(backend: backend, device: gpu, default_stream: stream)

    # ========================================================================
    # Memory Allocation
    # ========================================================================

    fn alloc[T](count: i64) -> GpuArray[T]:
        """Allocate uninitialized array on device.

        Args:
            count: Number of elements

        Returns:
            GPU array (uninitialized)

        Example:
            val arr = ctx.alloc[f32](1024)
        """
        gpu_alloc[T](self.device, count)

    fn alloc_upload[T](data: [T]) -> GpuArray[T]:
        """Allocate and upload data to device (async).

        Args:
            data: Host array to upload

        Returns:
            GPU array with uploaded data

        Example:
            val arr = ctx.alloc_upload([1.0, 2.0, 3.0, 4.0])
        """
        gpu_alloc_upload[T](self.device, data)

    fn alloc_zeros[T](count: i64) -> GpuArray[T]:
        """Allocate zero-initialized array.

        Args:
            count: Number of elements

        Returns:
            GPU array filled with zeros

        Example:
            val arr = ctx.alloc_zeros[f32](1024)
        """
        gpu_alloc_zeros[T](self.device, count)

    # ========================================================================
    # Synchronization
    # ========================================================================

    fn sync():
        """Wait for all operations to complete (blocking).

        Example:
            ctx.launch(kernel, ...)
            ctx.sync()  # Wait for kernel to finish
        """
        match self.default_stream:
            case Some(s):
                s.sync()
            case nil:
                self.device.sync()

    # ========================================================================
    # Stream Management
    # ========================================================================

    fn create_stream() -> TorchStream:
        """Create new stream for async operations.

        Returns:
            New CUDA stream

        Example:
            val stream1 = ctx.create_stream()
            val stream2 = ctx.create_stream()
            # Use streams for parallel execution
        """
        match self.backend:
            case GpuBackend.Cuda:
                TorchStream.create(self.device.device_id)
            case _:
                eprint("Warning: create_stream() called on non-CUDA backend, returning dummy stream")
                TorchStream.create(0)

    # ========================================================================
    # Device Info
    # ========================================================================

    fn backend_name() -> text:
        """Get backend name.

        Returns:
            Backend name ("CUDA", "Vulkan", "CPU")
        """
        match self.backend:
            case GpuBackend.Cuda: "CUDA"
            case GpuBackend.Vulkan: "Vulkan"
            case GpuBackend.None: "CPU"

    fn device_id() -> i32:
        """Get device ID.

        Returns:
            Device ID, or -1 for CPU
        """
        self.device.device_id

    # ========================================================================
    # Cleanup
    # ========================================================================

    fn drop():
        """Automatically free all resources when context goes out of scope."""
        match self.default_stream:
            case Some(s):
                s.drop()
            case nil:
                ()

# ============================================================================
# Config Integration
# ============================================================================

fn create_context_from_config() -> Context:
    """Create GPU context from global DL config.

    Reads device from `dl.default_device` and creates appropriate context.

    Returns:
        Context matching config settings

    Example:
        use std.src.dl.config.{load_local_config}
        use std.gpu.{create_context_from_config}

        load_local_config()  # Loads dl.config.sdn
        val ctx = create_context_from_config()  # Uses config device
    """
    use std.src.dl.config.{dl, Device}

    match dl.default_device:
        case Device.CPU:
            Context.new(backend: GpuBackend.None, device: -1)
        case Device.CUDA(id):
            Context.new(backend: GpuBackend.Cuda, device: id)

# ============================================================================
# Exports
# ============================================================================

export Context
export create_context_from_config
