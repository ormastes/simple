# GPU Module - GPU operations for graphics and compute workloads
#
# @tag:stdlib
# @tag:api
# @tag:gpu
#
# This module provides GPU device management, context creation, and memory
# operations for both graphics and compute workloads. It supports multiple
# backends including CUDA, Vulkan, and Metal.
#
# Core Features
# -------------
#
# - Device enumeration: Discover and select GPU devices
# - Context management: Create and manage GPU contexts
# - Memory operations: Allocate and transfer GPU memory
# - Backend integration: CUDA, Vulkan, Metal, OpenCL
# - Cross-platform: Works on Linux, Windows, macOS
#
# Public API
# ----------
#
# Device management:
#   use std.gpu.device.{GpuDevice, list_gpus, get_default_gpu}
#
# Context creation:
#   use std.gpu.context.{GpuContext, create_context}
#
# Memory operations:
#   use std.gpu.memory.{GpuMemory, allocate_gpu, copy_to_gpu, copy_from_gpu}
#
# Examples
# --------
#
# Enumerate GPU devices:
#   val gpus = list_gpus()
#   print "Found {gpus.len()} GPU(s)"
#
# Select and initialize GPU:
#   val gpu = get_default_gpu()
#   val context = create_context(gpu)
#
# Allocate and transfer memory:
#   val gpu_memory = allocate_gpu(context, host_data.len() * 8)
#   copy_to_gpu(gpu_memory, host_data)
#   val result = copy_from_gpu(gpu_memory)
#   gpu_memory.free()
#
# Supported Backends
# ------------------
#
# CUDA    - NVIDIA GPUs     - Compute, ML
# Vulkan  - Cross-platform  - Graphics, compute
# Metal   - macOS/iOS       - Graphics, compute
# OpenCL  - Cross-platform  - General compute
#
# Submodules
# ----------
#
# - device: GPU device enumeration and selection
# - context: GPU context creation and management
# - memory: GPU memory allocation and transfer
# - kernel: Kernel compilation and execution
# - stream: Async operation streams
#
# Related Modules
# ---------------
#
# - std.compute: High-level compute operations
# - std.gpu_runtime: GPU runtime integration with PyTorch
# - std.dl: Deep learning utilities

# All submodules are automatically available.
