# Pure Simple Deep Learning Library
# @tag:api
#
# Zero-dependency deep learning with tensor operations, autograd, and neural networks
#
# This module implements a complete deep learning framework in pure Simple,
# without C/C++/Python dependencies. It provides tensor operations, automatic
# differentiation (autograd), and common neural network layers for training models.
#
# Example: Creating and operating on tensors
#
#     use std.pure.{PureTensor, add, matmul}
#
#     # Create 2x2 tensor with specific data
#     val a = PureTensor(shape: [2, 2], data: [1.0, 2.0, 3.0, 4.0])
#     val b = PureTensor(shape: [2, 2], data: [5.0, 6.0, 7.0, 8.0])
#
#     # Element-wise addition
#     val sum = add(a, b)
#     # sum.data = [6.0, 8.0, 10.0, 12.0]
#
#     # Matrix multiplication (2x2 @ 2x2 = 2x2)
#     val product = matmul(a, b)
#     # product.data = [19.0, 22.0, 43.0, 50.0]
#
# Example: Building and training a neural network
#
#     use std.pure.{Linear, ReLU, Variable, backward, train_step}
#
#     # Create simple 2-layer network (2 inputs -> 4 hidden -> 1 output)
#     val layer1 = Linear(in_features: 2, out_features: 4)
#     val activation = ReLU()
#     val layer2 = Linear(in_features: 4, out_features: 1)
#
#     # Training data (XOR problem)
#     val inputs = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]
#     val targets = [0.0, 1.0, 1.0, 0.0]
#
#     # Training loop
#     for epoch in 0..100:
#         for i in 0..inputs.len():
#             val loss = train_step(layer1, inputs[i], targets[i], 0.01)
#             if i % 25 == 0:
#                 print "Epoch {epoch}, Loss: {loss}"

# Core tensor types
export PureTensor, compute_strides
export add, sub, mul, div, matmul, relu, sigmoid, tanh

# Advanced tensor operations
export stack, concat, split, reshape, permute

# Neural network layers
export Linear, ReLU, Sigmoid, Tanh

# Autograd
export Variable, backward

# Autograd for advanced operations
export tensor_stack, tensor_concat, tensor_split, tensor_reshape, tensor_permute

# Training utilities
export train_step, evaluate

use std.pure.tensor (PureTensor, compute_strides)
use std.pure.tensor_ops (add, sub, mul, div, matmul, relu, sigmoid, tanh)
use std.pure.tensor_f64_advanced (stack, concat, split, reshape, permute)
use std.pure.nn (Linear, ReLU, Sigmoid, Tanh)
use std.pure.autograd (Variable, backward)
use std.pure.autograd_advanced (tensor_stack, tensor_concat, tensor_split, tensor_reshape, tensor_permute)
use std.pure.training (train_step, evaluate)
