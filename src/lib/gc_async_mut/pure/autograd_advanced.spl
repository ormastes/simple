# Autograd Support for Advanced Tensor Operations
#
# Gradient computation for stack, concat, split, reshape, and permute

use std.pure.autograd (Tensor, tensor_from_value, backward, OpType)
use std.pure.tensor_f64 (TensorF64, from_data, zeros, ones)
use std.pure.tensor_f64_advanced (stack, concat, split, reshape, permute)

# ============================================================================
# Extended OpType for Advanced Operations
# ============================================================================
# Note: These would be added to the main OpType enum in autograd.spl
# For now, we'll work with wrapper functions that track gradients manually

# ============================================================================
# Stack with Autograd
# ============================================================================

fn tensor_stack(tensors: [Tensor], dim: i64) -> Tensor:
    """Stack tensors along new dimension with gradient tracking.

    Backward pass: Gradient flows back to each input tensor via split.
    grad_inputs[i] = split(grad_output, sizes, dim)[i]
    """
    # Extract values
    var values: [TensorF64] = []
    var any_requires_grad = false
    for t in tensors:
        values.push(t.value)
        if t.requires_grad:
            any_requires_grad = true

    val result_value = stack(values, dim)

    if not any_requires_grad:
        return tensor_from_value(result_value, false)

    # Track operation for backward pass
    var result = tensor_from_value(result_value, true)
    result.op_name = "stack"
    # Note: Full gradient computation requires custom backward logic
    # For production use, this would be integrated into the main autograd system
    result


# ============================================================================
# Concat with Autograd
# ============================================================================

fn tensor_concat(tensors: [Tensor], dim: i64) -> Tensor:
    """Concatenate tensors with gradient tracking.

    Backward pass: Gradient flows back via split along concat dimension.
    grad_inputs = split(grad_output, [t.shape[dim] for t in inputs], dim)
    """
    var values: [TensorF64] = []
    var any_requires_grad = false
    for t in tensors:
        values.push(t.value)
        if t.requires_grad:
            any_requires_grad = true

    val result_value = concat(values, dim)

    if not any_requires_grad:
        return tensor_from_value(result_value, false)

    var result = tensor_from_value(result_value, true)
    result.op_name = "concat"
    result


# ============================================================================
# Split with Autograd
# ============================================================================

fn tensor_split(tensor: Tensor, sizes: [i64], dim: i64) -> [Tensor]:
    """Split tensor with gradient tracking.

    Backward pass: Gradients are concatenated back together.
    grad_input = concat([grad_outputs[i] for i in range(len(sizes))], dim)
    """
    val chunks = split(tensor.value, sizes, dim)

    var result_tensors: [Tensor] = []
    for chunk in chunks:
        if tensor.requires_grad:
            var t = tensor_from_value(chunk, true)
            t.op_name = "split_chunk"
            result_tensors.push(t)
        else:
            result_tensors.push(tensor_from_value(chunk, false))

    result_tensors


# ============================================================================
# Reshape with Autograd
# ============================================================================

fn tensor_reshape(tensor: Tensor, shape: [i64]) -> Tensor:
    """Reshape tensor with gradient tracking.

    Backward pass: Gradient reshaped back to input shape.
    grad_input = reshape(grad_output, input.shape)
    """
    val result_value = reshape(tensor.value, shape)

    if not tensor.requires_grad:
        return tensor_from_value(result_value, false)

    var result = tensor_from_value(result_value, true)
    result.op_name = "reshape"
    result


# ============================================================================
# Permute with Autograd
# ============================================================================

fn tensor_permute(tensor: Tensor, dims: [i64]) -> Tensor:
    """Permute tensor dimensions with gradient tracking.

    Backward pass: Inverse permutation applied to gradient.
    If forward is permute([2,0,1]), backward is permute([1,2,0])
    """
    val result_value = permute(tensor.value, dims)

    if not tensor.requires_grad:
        return tensor_from_value(result_value, false)

    var result = tensor_from_value(result_value, true)
    result.op_name = "permute"
    result


# ============================================================================
# Helper: Compute Inverse Permutation
# ============================================================================

fn inverse_permutation(perm: [i64]) -> [i64]:
    """Compute inverse of a permutation.

    Args:
        perm: Permutation array (e.g., [2, 0, 1])

    Returns:
        Inverse permutation (e.g., [1, 2, 0])

    Example:
        forward: [a, b, c] -> [c, a, b] via [2, 0, 1]
        backward: [c, a, b] -> [a, b, c] via [1, 2, 0]
    """
    var inv: [i64] = []
    var i = 0
    while i < perm.len():
        inv.push(0)
        i = i + 1

    var j = 0
    while j < perm.len():
        inv[perm[j]] = j
        j = j + 1

    inv


# ============================================================================
# Gradient Helpers (for manual backward pass if needed)
# ============================================================================

fn backward_stack(grad_output: TensorF64, input_shapes: [[i64]], dim: i64) -> [TensorF64]:
    """Compute gradients for stack operation.

    Args:
        grad_output: Gradient of stacked output
        input_shapes: Original shapes of input tensors
        dim: Dimension that was stacked along

    Returns:
        List of gradients for each input tensor
    """
    var sizes: [i64] = []
    for shape in input_shapes:
        sizes.push(1)  # Each input contributes 1 slice

    # Split gradient back to inputs
    split(grad_output, sizes, dim)


fn backward_concat(grad_output: TensorF64, input_shapes: [[i64]], dim: i64) -> [TensorF64]:
    """Compute gradients for concat operation.

    Args:
        grad_output: Gradient of concatenated output
        input_shapes: Original shapes of input tensors
        dim: Dimension that was concatenated along

    Returns:
        List of gradients for each input tensor
    """
    var sizes: [i64] = []
    for shape in input_shapes:
        sizes.push(shape[dim])

    # Split gradient back to inputs
    split(grad_output, sizes, dim)


fn backward_reshape(grad_output: TensorF64, input_shape: [i64]) -> TensorF64:
    """Compute gradient for reshape operation.

    Args:
        grad_output: Gradient of reshaped output
        input_shape: Original shape of input tensor

    Returns:
        Gradient reshaped back to input shape
    """
    reshape(grad_output, input_shape)


fn backward_permute(grad_output: TensorF64, forward_dims: [i64]) -> TensorF64:
    """Compute gradient for permute operation.

    Args:
        grad_output: Gradient of permuted output
        forward_dims: Permutation used in forward pass

    Returns:
        Gradient with inverse permutation applied
    """
    val inv_dims = inverse_permutation(forward_dims)
    permute(grad_output, inv_dims)


# ============================================================================
# Exports
# ============================================================================

export tensor_stack, tensor_concat, tensor_split, tensor_reshape, tensor_permute
export backward_stack, backward_concat, backward_reshape, backward_permute
export inverse_permutation
