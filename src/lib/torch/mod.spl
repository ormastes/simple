# torch Simple API (Tier 3)
# Auto-generated API scaffold
#
# This provides an idiomatic Simple API wrapping the FFI bindings.
# Feel free to customize and extend this code.

use lib.torch.ffi.*

# ============================================================================
# Backend Detection
# ============================================================================

fn torch_available() -> bool:
    rt_torch_available()

fn torch_version() -> text:
    rt_torch_version()

# ============================================================================
# TorchTensor Wrapper Class
# ============================================================================

class TorchTensorWrapper:
    """High-level wrapper for TorchTensor.

    Automatically manages memory via RAII pattern.
    """

    handle: i64  # Opaque FFI handle (pointer)
    owns_handle: bool

    # Static factory methods
    static fn tensor_zeros(dims: [i64]) -> TorchTensorWrapper:
        val handle = rt_torch_tensor_zeros(dims)
        TorchTensorWrapper(handle: handle, owns_handle: true)

    static fn tensor_ones(dims: [i64]) -> TorchTensorWrapper:
        val handle = rt_torch_tensor_ones(dims)
        TorchTensorWrapper(handle: handle, owns_handle: true)

    static fn tensor_randn(dims: [i64]) -> TorchTensorWrapper:
        val handle = rt_torch_tensor_randn(dims)
        TorchTensorWrapper(handle: handle, owns_handle: true)


    fn drop():
        """Automatically free memory when object goes out of scope."""
        if self.owns_handle:
            rt_torch_torchtensor_free(self.handle)

    # Methods
    fn add(other: i64) -> i64:
        val result_handle = rt_torch_torchtensor_add(self.handle, other)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn mul(other: i64) -> i64:
        val result_handle = rt_torch_torchtensor_mul(self.handle, other)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn matmul(other: i64) -> i64:
        val result_handle = rt_torch_torchtensor_matmul(self.handle, other)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn ndim() -> i64:
        rt_torch_torchtensor_ndim(self.handle)

    fn numel() -> i64:
        rt_torch_torchtensor_numel(self.handle)

    fn shape() -> [i64]:
        rt_torch_torchtensor_shape(self.handle)

    fn relu() -> i64:
        val result_handle = rt_torch_torchtensor_relu(self.handle)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn sigmoid() -> i64:
        val result_handle = rt_torch_torchtensor_sigmoid(self.handle)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn tanh() -> i64:
        val result_handle = rt_torch_torchtensor_tanh(self.handle)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn cuda(device_id: i32) -> i64:
        val result_handle = rt_torch_torchtensor_cuda(self.handle, device_id)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn cpu() -> i64:
        val result_handle = rt_torch_torchtensor_cpu(self.handle)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)

    fn is_cuda() -> bool:
        rt_torch_torchtensor_is_cuda(self.handle)

    fn to_stream(device_id: i32, stream: i64) -> i64:
        val result_handle = rt_torch_torchtensor_to_stream(self.handle, device_id, stream)
        TorchTensorWrapper(handle: result_handle, owns_handle: true)


# ============================================================================
# TorchStream Wrapper Class
# ============================================================================

class TorchStreamWrapper:
    """High-level wrapper for TorchStream.

    Automatically manages memory via RAII pattern.
    """

    handle: i64  # Opaque FFI handle (pointer)
    owns_handle: bool

    # Static factory methods
    static fn stream_create(device_id: i32) -> TorchStreamWrapper:
        val handle = rt_torch_stream_create(device_id)
        TorchStreamWrapper(handle: handle, owns_handle: true)


    fn drop():
        """Automatically free memory when object goes out of scope."""
        if self.owns_handle:
            rt_torch_stream_free(self.handle)

    # Methods
    fn sync() -> void:
        rt_torch_torchstream_sync(self.handle)

    fn query() -> bool:
        rt_torch_torchstream_query(self.handle)


# ============================================================================
# Exports
# ============================================================================

export TorchTensorWrapper
export TorchStreamWrapper
export TorchStream
export torch_available
export torch_version
export torch_cuda_available

