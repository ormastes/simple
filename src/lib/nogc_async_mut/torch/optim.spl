# NoGC Real Optimizers using Raw Tensor Handles
#
# Same API as gc_async_mut/torch/optim.spl by convention.
# Uses duplicated FFI declarations from nogc ffi.spl.

use std.nogc_async_mut.torch.ffi.{
    rt_torch_tensor_zeros,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_div,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_add_scalar,
    rt_torch_torchtensor_sqrt,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_shape,
    rt_torch_torchtensor_cuda,
    rt_torch_autograd_grad,
    rt_torch_autograd_zero_grad,
    rt_torch_autograd_set_requires_grad,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end
}

export SGDOptimizer, AdamOptimizer

# ============================================================================
# Helper: move tensor to CUDA device 0
# ============================================================================

fn to_cuda(h: i64) -> i64:
    rt_torch_torchtensor_cuda(h, 0)

fn zeros_like_on_cuda(param: i64) -> i64:
    val shape = rt_torch_torchtensor_shape(param)
    val z_cpu = rt_torch_tensor_zeros(shape)
    val z_gpu = to_cuda(z_cpu)
    rt_torch_torchtensor_free(z_cpu)
    z_gpu


# ============================================================================
# SGD Optimizer with Momentum
# ============================================================================

class SGDOptimizer:
    params: [i64]
    lr: f64
    momentum: f64
    velocities: [i64]

    static fn create(params: [i64], lr: f64, momentum: f64) -> SGDOptimizer:
        var vels = []
        var i = 0
        while i < params.len():
            val v = zeros_like_on_cuda(params[i])
            vels = vels + [v]
            i = i + 1
        SGDOptimizer(
            params: params,
            lr: lr,
            momentum: momentum,
            velocities: vels
        )

    me step() -> [i64]:
        rt_torch_autograd_no_grad_begin()
        var new_params = []
        var i = 0
        while i < self.params.len():
            val p = self.params[i]
            val g = rt_torch_autograd_grad(p)
            if g == 0:
                new_params = new_params + [rt_torch_torchtensor_clone(p)]
                rt_torch_torchtensor_free(p)
            else:
                if self.momentum > 0.0:
                    val v_old = self.velocities[i]
                    val v_scaled = rt_torch_torchtensor_mul_scalar(v_old, self.momentum)
                    val v_new = rt_torch_torchtensor_add(v_scaled, g)
                    rt_torch_torchtensor_free(v_scaled)
                    rt_torch_torchtensor_free(v_old)
                    self.velocities[i] = v_new
                    val update = rt_torch_torchtensor_mul_scalar(v_new, self.lr)
                    val p_new = rt_torch_torchtensor_sub(p, update)
                    rt_torch_torchtensor_free(update)
                    rt_torch_torchtensor_free(p)
                    new_params = new_params + [p_new]
                else:
                    val update = rt_torch_torchtensor_mul_scalar(g, self.lr)
                    val p_new = rt_torch_torchtensor_sub(p, update)
                    rt_torch_torchtensor_free(update)
                    rt_torch_torchtensor_free(p)
                    new_params = new_params + [p_new]
            i = i + 1
        rt_torch_autograd_no_grad_end()
        var j = 0
        while j < new_params.len():
            rt_torch_autograd_set_requires_grad(new_params[j], true)
            j = j + 1
        self.params = new_params
        new_params

    fn zero_grads():
        var i = 0
        while i < self.params.len():
            rt_torch_autograd_zero_grad(self.params[i])
            i = i + 1


# ============================================================================
# Adam Optimizer
# ============================================================================

class AdamOptimizer:
    params: [i64]
    lr: f64
    beta1: f64
    beta2: f64
    eps: f64
    m: [i64]
    v: [i64]
    t: i64

    static fn create(params: [i64], lr: f64) -> AdamOptimizer:
        var m_list = []
        var v_list = []
        var i = 0
        while i < params.len():
            val m_z = zeros_like_on_cuda(params[i])
            val v_z = zeros_like_on_cuda(params[i])
            m_list = m_list + [m_z]
            v_list = v_list + [v_z]
            i = i + 1
        AdamOptimizer(
            params: params,
            lr: lr,
            beta1: 0.9,
            beta2: 0.999,
            eps: 0.00000001,
            m: m_list,
            v: v_list,
            t: 0
        )

    static fn create_with_betas(params: [i64], lr: f64, beta1: f64, beta2: f64, eps: f64) -> AdamOptimizer:
        var m_list = []
        var v_list = []
        var i = 0
        while i < params.len():
            val m_z = zeros_like_on_cuda(params[i])
            val v_z = zeros_like_on_cuda(params[i])
            m_list = m_list + [m_z]
            v_list = v_list + [v_z]
            i = i + 1
        AdamOptimizer(
            params: params,
            lr: lr,
            beta1: beta1,
            beta2: beta2,
            eps: eps,
            m: m_list,
            v: v_list,
            t: 0
        )

    me step() -> [i64]:
        self.t = self.t + 1
        rt_torch_autograd_no_grad_begin()

        val bc1 = 1.0 - pow_f64(self.beta1, self.t)
        val bc2 = 1.0 - pow_f64(self.beta2, self.t)

        var new_params = []
        var i = 0
        while i < self.params.len():
            val p = self.params[i]
            val g = rt_torch_autograd_grad(p)
            if g == 0:
                new_params = new_params + [rt_torch_torchtensor_clone(p)]
                rt_torch_torchtensor_free(p)
            else:
                val m_old = self.m[i]
                val m_scaled = rt_torch_torchtensor_mul_scalar(m_old, self.beta1)
                val g_scaled = rt_torch_torchtensor_mul_scalar(g, 1.0 - self.beta1)
                val m_new = rt_torch_torchtensor_add(m_scaled, g_scaled)
                rt_torch_torchtensor_free(m_scaled)
                rt_torch_torchtensor_free(g_scaled)
                rt_torch_torchtensor_free(m_old)
                self.m[i] = m_new

                val v_old = self.v[i]
                val v_scaled = rt_torch_torchtensor_mul_scalar(v_old, self.beta2)
                val g_sq = rt_torch_torchtensor_mul(g, g)
                val g_sq_scaled = rt_torch_torchtensor_mul_scalar(g_sq, 1.0 - self.beta2)
                rt_torch_torchtensor_free(g_sq)
                val v_new = rt_torch_torchtensor_add(v_scaled, g_sq_scaled)
                rt_torch_torchtensor_free(v_scaled)
                rt_torch_torchtensor_free(g_sq_scaled)
                rt_torch_torchtensor_free(v_old)
                self.v[i] = v_new

                val m_hat = rt_torch_torchtensor_mul_scalar(m_new, 1.0 / bc1)
                val v_hat = rt_torch_torchtensor_mul_scalar(v_new, 1.0 / bc2)

                val v_sqrt = rt_torch_torchtensor_sqrt(v_hat)
                rt_torch_torchtensor_free(v_hat)
                val v_sqrt_eps = rt_torch_torchtensor_add_scalar(v_sqrt, self.eps)
                rt_torch_torchtensor_free(v_sqrt)
                val update = rt_torch_torchtensor_div(m_hat, v_sqrt_eps)
                rt_torch_torchtensor_free(m_hat)
                rt_torch_torchtensor_free(v_sqrt_eps)
                val lr_update = rt_torch_torchtensor_mul_scalar(update, self.lr)
                rt_torch_torchtensor_free(update)
                val p_new = rt_torch_torchtensor_sub(p, lr_update)
                rt_torch_torchtensor_free(lr_update)
                rt_torch_torchtensor_free(p)
                new_params = new_params + [p_new]
            i = i + 1

        rt_torch_autograd_no_grad_end()
        var j = 0
        while j < new_params.len():
            rt_torch_autograd_set_requires_grad(new_params[j], true)
            j = j + 1
        self.params = new_params
        new_params

    fn zero_grads():
        var i = 0
        while i < self.params.len():
            rt_torch_autograd_zero_grad(self.params[i])
            i = i + 1


fn pow_f64(base: f64, exp: i64) -> f64:
    var result = 1.0
    var i = 0
    while i < exp:
        result = result * base
        i = i + 1
    result
