# NoGC Torch Training Components
#
# Same API as gc_async_mut/torch/torch_training.spl by convention.
# Adjusted for unique ownership (no owns_handle on Stream).

use std.nogc_async_mut.torch.mod.{Tensor, Linear, Conv2d}
use std.nogc_async_mut.torch.ffi.{
    rt_torch_stream_create,
    rt_torch_torchstream_sync,
    rt_torch_torchstream_query,
    rt_torch_torchstream_free,
    rt_torch_nn_mse_loss,
    rt_torch_nn_cross_entropy,
    rt_torch_autograd_grad,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_add_scalar,
    rt_torch_torchtensor_div,
    rt_torch_torchtensor_sqrt,
    rt_torch_torchtensor_free,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_mean
}

# ============================================================================
# Loss Functions
# ============================================================================

class MSELoss:
    static fn create() -> MSELoss:
        MSELoss()

    fn forward(pred: Tensor, target: Tensor) -> Tensor:
        """Compute MSE loss: mean((pred - target)^2).

        Uses PyTorch FFI for efficient computation.
        """
        val loss_val = rt_torch_nn_mse_loss(pred.handle, target.handle)
        Tensor.from_handle(rt_torch_tensor_from_data([loss_val], [1]))


class CrossEntropyLoss:
    static fn create() -> CrossEntropyLoss:
        CrossEntropyLoss()

    fn forward(logits: Tensor, targets: Tensor) -> Tensor:
        """Compute cross-entropy loss for classification.

        Uses PyTorch FFI for efficient computation.
        """
        val loss_val = rt_torch_nn_cross_entropy(logits.handle, targets.handle)
        Tensor.from_handle(rt_torch_tensor_from_data([loss_val], [1]))


# ============================================================================
# Optimizers
# ============================================================================

class SGD:
    parameters: [Tensor]
    lr: i64
    momentum: i64
    velocities: [Tensor]

    static fn create(parameters: [Tensor], lr: i64, momentum: i64) -> SGD:
        var velocities_list = []
        var i = 0
        while i < parameters.len():
            val param = parameters[i]
            val velocity = Tensor.zeros(param.shape())
            velocities_list = velocities_list + [velocity]
            i = i + 1
        SGD(
            parameters: parameters,
            lr: lr,
            momentum: momentum,
            velocities: velocities_list
        )

    fn step():
        """Update parameters using SGD with momentum.

        velocity = momentum * velocity + lr * grad
        param = param - velocity
        """
        var i = 0
        while i < self.parameters.len():
            val param = self.parameters[i]
            val grad_handle = rt_torch_autograd_grad(param.handle)
            if grad_handle != 0:
                # velocity = momentum * velocity + lr * grad
                val v_scaled = rt_torch_torchtensor_mul_scalar(self.velocities[i].handle, self.momentum)
                val g_scaled = rt_torch_torchtensor_mul_scalar(grad_handle, self.lr)
                val v_new_handle = rt_torch_torchtensor_add(v_scaled, g_scaled)
                rt_torch_torchtensor_free(v_scaled)
                rt_torch_torchtensor_free(g_scaled)
                # Update stored velocity
                rt_torch_torchtensor_free(self.velocities[i].handle)
                self.velocities[i] = Tensor.from_handle(v_new_handle)
                # param = param - velocity
                val new_param = rt_torch_torchtensor_sub(param.handle, v_new_handle)
                rt_torch_torchtensor_free(param.handle)
                self.parameters[i] = Tensor.from_handle(new_param)
            i = i + 1

    fn zero_grad():
        var i = 0
        while i < self.parameters.len():
            self.parameters[i].zero_grad()
            i = i + 1


class Adam:
    parameters: [Tensor]
    lr: i64
    beta1: i64
    beta2: i64
    eps: i64
    m: [Tensor]
    v: [Tensor]
    t: i64

    static fn create(parameters: [Tensor], lr: i64, beta1: i64, beta2: i64) -> Adam:
        var m_list = []
        var v_list = []
        var i = 0
        while i < parameters.len():
            val param = parameters[i]
            val m_tensor = Tensor.zeros(param.shape())
            val v_tensor = Tensor.zeros(param.shape())
            m_list = m_list + [m_tensor]
            v_list = v_list + [v_tensor]
            i = i + 1
        Adam(
            parameters: parameters,
            lr: lr,
            beta1: beta1,
            beta2: beta2,
            eps: 0,
            m: m_list,
            v: v_list,
            t: 0
        )

    me step():
        """Update parameters using Adam algorithm.

        m = beta1 * m + (1 - beta1) * grad
        v = beta2 * v + (1 - beta2) * grad^2
        m_hat = m / (1 - beta1^t)
        v_hat = v / (1 - beta2^t)
        param = param - lr * m_hat / (sqrt(v_hat) + eps)
        """
        self.t = self.t + 1

        var i = 0
        while i < self.parameters.len():
            val param = self.parameters[i]
            val grad_handle = rt_torch_autograd_grad(param.handle)
            if grad_handle != 0:
                # m = beta1 * m + (1 - beta1) * grad
                val m_old_scaled = rt_torch_torchtensor_mul_scalar(self.m[i].handle, self.beta1)
                val g_scaled = rt_torch_torchtensor_mul_scalar(grad_handle, 1 - self.beta1)
                val m_new = rt_torch_torchtensor_add(m_old_scaled, g_scaled)
                rt_torch_torchtensor_free(m_old_scaled)
                rt_torch_torchtensor_free(g_scaled)
                rt_torch_torchtensor_free(self.m[i].handle)
                self.m[i] = Tensor.from_handle(m_new)

                # v = beta2 * v + (1 - beta2) * grad^2
                val v_old_scaled = rt_torch_torchtensor_mul_scalar(self.v[i].handle, self.beta2)
                val grad_sq = rt_torch_torchtensor_mul(grad_handle, grad_handle)
                val g2_scaled = rt_torch_torchtensor_mul_scalar(grad_sq, 1 - self.beta2)
                val v_new = rt_torch_torchtensor_add(v_old_scaled, g2_scaled)
                rt_torch_torchtensor_free(v_old_scaled)
                rt_torch_torchtensor_free(grad_sq)
                rt_torch_torchtensor_free(g2_scaled)
                rt_torch_torchtensor_free(self.v[i].handle)
                self.v[i] = Tensor.from_handle(v_new)

                # Bias correction: m_hat = m / (1 - beta1^t), v_hat = v / (1 - beta2^t)
                var beta1_t = 1.0
                var beta2_t = 1.0
                var k = 0
                while k < self.t:
                    beta1_t = beta1_t * self.beta1
                    beta2_t = beta2_t * self.beta2
                    k = k + 1
                val m_hat = rt_torch_torchtensor_mul_scalar(m_new, 1.0 / (1.0 - beta1_t))
                val v_hat = rt_torch_torchtensor_mul_scalar(v_new, 1.0 / (1.0 - beta2_t))

                # param = param - lr * m_hat / (sqrt(v_hat) + eps)
                val v_sqrt = rt_torch_torchtensor_sqrt(v_hat)
                val denom = rt_torch_torchtensor_add_scalar(v_sqrt, self.eps)
                val update = rt_torch_torchtensor_div(m_hat, denom)
                val update_scaled = rt_torch_torchtensor_mul_scalar(update, self.lr)
                val new_param = rt_torch_torchtensor_sub(param.handle, update_scaled)

                rt_torch_torchtensor_free(m_hat)
                rt_torch_torchtensor_free(v_hat)
                rt_torch_torchtensor_free(v_sqrt)
                rt_torch_torchtensor_free(denom)
                rt_torch_torchtensor_free(update)
                rt_torch_torchtensor_free(update_scaled)
                rt_torch_torchtensor_free(param.handle)
                self.parameters[i] = Tensor.from_handle(new_param)
            i = i + 1

    fn zero_grad():
        var i = 0
        while i < self.parameters.len():
            self.parameters[i].zero_grad()
            i = i + 1


class RMSprop:
    parameters: [Tensor]
    lr: i64
    alpha: i64
    eps: i64
    v: [Tensor]

    static fn create(parameters: [Tensor], lr: i64, alpha: i64, eps: i64) -> RMSprop:
        var v_list = []
        var i = 0
        while i < parameters.len():
            val param = parameters[i]
            val v_tensor = Tensor.zeros(param.shape())
            v_list = v_list + [v_tensor]
            i = i + 1
        RMSprop(
            parameters: parameters,
            lr: lr,
            alpha: alpha,
            eps: eps,
            v: v_list
        )

    fn step():
        """Update parameters using RMSprop.

        v = alpha * v + (1 - alpha) * grad^2
        param = param - lr * grad / (sqrt(v) + eps)
        """
        var i = 0
        while i < self.parameters.len():
            val param = self.parameters[i]
            val grad_handle = rt_torch_autograd_grad(param.handle)
            if grad_handle != 0:
                # v = alpha * v + (1 - alpha) * grad^2
                val v_old_scaled = rt_torch_torchtensor_mul_scalar(self.v[i].handle, self.alpha)
                val grad_sq = rt_torch_torchtensor_mul(grad_handle, grad_handle)
                val g2_scaled = rt_torch_torchtensor_mul_scalar(grad_sq, 1 - self.alpha)
                val v_new = rt_torch_torchtensor_add(v_old_scaled, g2_scaled)
                rt_torch_torchtensor_free(v_old_scaled)
                rt_torch_torchtensor_free(grad_sq)
                rt_torch_torchtensor_free(g2_scaled)
                rt_torch_torchtensor_free(self.v[i].handle)
                self.v[i] = Tensor.from_handle(v_new)

                # param = param - lr * grad / (sqrt(v) + eps)
                val v_sqrt = rt_torch_torchtensor_sqrt(v_new)
                val denom = rt_torch_torchtensor_add_scalar(v_sqrt, self.eps)
                val update = rt_torch_torchtensor_div(grad_handle, denom)
                val update_scaled = rt_torch_torchtensor_mul_scalar(update, self.lr)
                val new_param = rt_torch_torchtensor_sub(param.handle, update_scaled)

                rt_torch_torchtensor_free(v_sqrt)
                rt_torch_torchtensor_free(denom)
                rt_torch_torchtensor_free(update)
                rt_torch_torchtensor_free(update_scaled)
                rt_torch_torchtensor_free(param.handle)
                self.parameters[i] = Tensor.from_handle(new_param)
            i = i + 1

    fn zero_grad():
        var i = 0
        while i < self.parameters.len():
            self.parameters[i].zero_grad()
            i = i + 1


# ============================================================================
# CUDA Stream Wrapper (Unique Ownership)
# ============================================================================

class Stream:
    # No owns_handle — unique owner always owns
    handle: i64
    device_id: i64

    static fn create(device_id: i64) -> Stream:
        val device_i32 = 0
        val handle = rt_torch_stream_create(device_i32)
        Stream(handle: handle, device_id: device_id)

    fn drop():
        # Unconditional free — unique ownership
        rt_torch_torchstream_free(self.handle)

    fn synchronize():
        rt_torch_torchstream_sync(self.handle)

    fn query() -> bool:
        rt_torch_torchstream_query(self.handle)


# ============================================================================
# Sequential Container
# ============================================================================

class Sequential:
    layers_linear: [Linear]
    layers_conv2d: [Conv2d]

    static fn create() -> Sequential:
        Sequential(layers_linear: [], layers_conv2d: [])

    me add_layer_linear(layer: Linear):
        self.layers_linear = self.layers_linear + [layer]

    me add_layer_conv2d(layer: Conv2d):
        self.layers_conv2d = self.layers_conv2d + [layer]

    fn forward(x: Tensor) -> Tensor:
        var output = x
        var i = 0
        while i < self.layers_linear.len():
            output = self.layers_linear[i].forward(output)
            i = i + 1
        var j = 0
        while j < self.layers_conv2d.len():
            output = self.layers_conv2d[j].forward(output)
            j = j + 1
        output

    fn parameters() -> [Tensor]:
        var all_params = []
        var i = 0
        while i < self.layers_linear.len():
            val layer_params = self.layers_linear[i].parameters()
            all_params = all_params + layer_params
            i = i + 1
        var j = 0
        while j < self.layers_conv2d.len():
            val layer_params = self.layers_conv2d[j].parameters()
            all_params = all_params + layer_params
            j = j + 1
        all_params


# ============================================================================
# Utility Functions
# ============================================================================

fn no_grad(f: fn()):
    f()

fn set_seed(seed: i64):
    0

fn manual_seed(seed: i64):
    set_seed(seed)

export MSELoss, CrossEntropyLoss
export SGD, Adam, RMSprop
export Stream, Sequential
export no_grad, set_seed, manual_seed
