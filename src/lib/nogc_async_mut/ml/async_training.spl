# Async ML Training Utilities
#
# Training loops that combine ML operations with async-compatible
# iteration patterns. Uses pure training primitives underneath.

use std.pure.training.{mse_loss, SGD, Adam, TrainingHistory}
use std.pure.tensor.{PureTensor, tensor_from_data}
use std.pure.data.dataset.{ArrayDataset, LabeledDataset, LabeledSample}
use std.pure.data.dataloader.{DataLoader, LabeledDataLoader, LabeledBatch, create_dataloader, create_dataloader_labeled}

# ============================================================================
# Training Epoch
# ============================================================================

fn train_epoch(model: any, dataloader: LabeledDataLoader, optimizer: any, loss_fn: fn(PureTensor<f64>, PureTensor<f64>) -> f64) -> f64:
    """Run one training epoch over all batches.

    Iterates through all batches in the dataloader, computing forward pass,
    loss, and optimizer step for each batch.

    Args:
        model: Model with .forward(x) method
        dataloader: LabeledDataLoader providing batches
        optimizer: Optimizer with .step() and .zero_grad() methods
        loss_fn: Loss function (e.g., mse_loss)

    Returns:
        Average loss over all batches in the epoch
    """
    dataloader.reset()
    var total_loss = 0.0
    var num_batches = 0

    var batch = dataloader.next_batch()
    while batch.features.len() > 0:
        # Zero gradients
        optimizer.zero_grad()

        # Create tensors from batch data
        val x = tensor_from_data(batch.features, [batch.features.len()])
        val y = tensor_from_data(batch.labels, [batch.labels.len()])

        # Forward pass
        val pred = model.forward(x)

        # Compute loss
        val loss = loss_fn(pred, y)
        total_loss = total_loss + loss

        # Optimizer step
        optimizer.step()

        num_batches = num_batches + 1
        batch = dataloader.next_batch()

    if num_batches > 0:
        total_loss / num_batches
    else:
        0.0

# ============================================================================
# Model Evaluation
# ============================================================================

fn evaluate_model(model: any, dataloader: LabeledDataLoader, loss_fn: fn(PureTensor<f64>, PureTensor<f64>) -> f64) -> f64:
    """Evaluate model on a dataset without gradient updates.

    Args:
        model: Model with .forward(x) method
        dataloader: LabeledDataLoader providing evaluation batches
        loss_fn: Loss function for computing evaluation metric

    Returns:
        Average loss over all evaluation batches
    """
    dataloader.reset()
    var total_loss = 0.0
    var num_batches = 0

    var batch = dataloader.next_batch()
    while batch.features.len() > 0:
        val x = tensor_from_data(batch.features, [batch.features.len()])
        val y = tensor_from_data(batch.labels, [batch.labels.len()])

        val pred = model.forward(x)
        val loss = loss_fn(pred, y)
        total_loss = total_loss + loss

        num_batches = num_batches + 1
        batch = dataloader.next_batch()

    if num_batches > 0:
        total_loss / num_batches
    else:
        0.0

# ============================================================================
# Full Training Loop
# ============================================================================

fn train_loop(model: any, train_dl: LabeledDataLoader, val_dl: LabeledDataLoader, optimizer: any, loss_fn: fn(PureTensor<f64>, PureTensor<f64>) -> f64, epochs: i64) -> [f64]:
    """Run full training loop with validation.

    Trains for the specified number of epochs, recording validation
    loss after each epoch.

    Args:
        model: Model with .forward(x) method
        train_dl: Training dataloader
        val_dl: Validation dataloader
        optimizer: Optimizer with .step() and .zero_grad() methods
        loss_fn: Loss function
        epochs: Number of training epochs

    Returns:
        Array of validation losses (one per epoch)
    """
    var val_losses: [f64] = []

    var epoch = 0
    while epoch < epochs:
        # Training phase
        val train_loss = train_epoch(model, train_dl, optimizer, loss_fn)

        # Validation phase
        val val_loss = evaluate_model(model, val_dl, loss_fn)
        val_losses.push(val_loss)

        epoch = epoch + 1

    val_losses

# ============================================================================
# Exports
# ============================================================================

export train_epoch, evaluate_model, train_loop
