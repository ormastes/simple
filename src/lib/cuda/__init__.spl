# CUDA Module (Library)
# @tag:internal
#
# CUDA integration utilities (library-level).
# Provides CUDA runtime support for GPU kernel execution.
#
# This module wraps CUDA runtime APIs for device memory management,
# kernel launches, and stream synchronization. It's used internally
# by the GPU backend and torch module, not directly by user code.
#
# Internal API (not part of public API):
#   - use std.cuda.ffi.{cuda_malloc, cuda_free, cuda_memcpy}
#   - use std.cuda.ffi.{cuda_launch_kernel, cuda_stream_create}
#
# Example: Allocating GPU memory
#
#     use std.cuda.ffi.{cuda_malloc, cuda_free, cuda_memcpy}
#
#     # Allocate 1024 bytes on GPU
#     val device_ptr = cuda_malloc(1024)
#
#     # Copy data from host to device
#     val host_data = [1, 2, 3, 4, 5]
#     cuda_memcpy(device_ptr, host_data, 5 * 8, CudaMemcpyKind.HostToDevice)
#
#     # ... perform GPU operations ...
#
#     # Free GPU memory
#     cuda_free(device_ptr)
#
# Example: Launching a CUDA kernel
#
#     use std.cuda.ffi.{cuda_launch_kernel}
#
#     # Launch kernel with 256 threads per block, 4 blocks
#     # kernel_ptr is the compiled CUDA kernel function pointer
#     cuda_launch_kernel(
#         kernel_ptr,
#         grid_dim: [4, 1, 1],    # 4 blocks
#         block_dim: [256, 1, 1],  # 256 threads per block
#         args: [device_input, device_output, n],
#         shared_mem_bytes: 0
#     )
#
# Example: Asynchronous execution with streams
#
#     use std.cuda.ffi.{cuda_stream_create, cuda_stream_sync}
#
#     # Create CUDA stream for async operations
#     val stream = cuda_stream_create()
#
#     # Launch multiple kernels on stream (execute in parallel)
#     cuda_launch_kernel_async(kernel1, grid1, block1, args1, stream)
#     cuda_launch_kernel_async(kernel2, grid2, block2, args2, stream)
#
#     # Wait for all operations on stream to complete
#     cuda_stream_sync(stream)

# All submodules are automatically available.
# INTERNAL: This module is used by GPU backend, not user-facing.
