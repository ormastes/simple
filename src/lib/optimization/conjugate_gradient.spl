# Conjugate Gradient Method Module
# Nonlinear conjugate gradient optimization

fn conjugate_gradient(f: fn(List<f64>) -> f64, grad_f: fn(List<f64>) -> List<f64>,
                     x0: List<f64>, config: OptimizationConfig) -> OptimizationResult:
    var x = vector_copy(x0)
    var gradient = grad_f(x)
    var direction = vector_scale(gradient, -1.0)
    var iteration = 0
    var converged = false
    var prev_value = f(x)

    while iteration < config.max_iterations:
        val grad_norm = vector_norm(gradient)

        if grad_norm < config.gradient_tolerance:
            converged = true
            return OptimizationResult(
                solution: x,
                objective_value: prev_value,
                iterations: iteration,
                converged: true,
                gradient_norm: grad_norm,
                message: "Converged: gradient norm below tolerance"
            )

        val ls_result = backtracking_line_search(f, x, direction, gradient,
                                                  1.0, 0.8, 0.0001)

        val new_x = ls_result.new_point
        val new_value = ls_result.new_value
        val new_gradient = grad_f(new_x)

        # Fletcher-Reeves formula
        val beta_num = vector_norm_squared(new_gradient)
        val beta_den = vector_norm_squared(gradient)
        var beta = 0.0
        if beta_den > 1e-10:
            beta = beta_num / beta_den

        val scaled_direction = vector_scale(direction, beta)
        val neg_gradient = vector_scale(new_gradient, -1.0)
        direction = vector_add(neg_gradient, scaled_direction)

        x = new_x
        gradient = new_gradient
        prev_value = new_value
        iteration = iteration + 1

    OptimizationResult(
        solution: x,
        objective_value: f(x),
        iterations: iteration,
        converged: converged,
        gradient_norm: vector_norm(gradient),
        message: "Maximum iterations reached"
    )

fn gradient_descent(f: fn(List<f64>) -> f64, grad_f: fn(List<f64>) -> List<f64>,
                   x0: List<f64>, config: OptimizationConfig) -> OptimizationResult:
    var x = vector_copy(x0)
    var iteration = 0
    var converged = false
    var prev_value = f(x)

    while iteration < config.max_iterations:
        val gradient = grad_f(x)
        val grad_norm = vector_norm(gradient)

        if grad_norm < config.gradient_tolerance:
            converged = true
            return OptimizationResult(
                solution: x,
                objective_value: prev_value,
                iterations: iteration,
                converged: true,
                gradient_norm: grad_norm,
                message: "Converged: gradient norm below tolerance"
            )

        val direction = vector_scale(gradient, -1.0)
        val ls_result = backtracking_line_search(f, x, direction, gradient,
                                                  config.learning_rate, 0.8, 0.0001)

        val new_x = ls_result.new_point
        val new_value = ls_result.new_value

        if check_value_convergence(prev_value, new_value, config.value_tolerance):
            converged = true
            return OptimizationResult(
                solution: new_x,
                objective_value: new_value,
                iterations: iteration,
                converged: true,
                gradient_norm: grad_norm,
                message: "Converged: function value change below tolerance"
            )

        x = new_x
        prev_value = new_value
        iteration = iteration + 1

    val final_gradient = grad_f(x)
    OptimizationResult(
        solution: x,
        objective_value: f(x),
        iterations: iteration,
        converged: converged,
        gradient_norm: vector_norm(final_gradient),
        message: "Maximum iterations reached"
    )

fn sgd_optimize(grad_f: fn(List<f64>, i64) -> List<f64>, x0: List<f64>,
               num_samples: i64, config: OptimizationConfig) -> OptimizationResult:
    var x = vector_copy(x0)
    var iteration = 0
    var epoch = 0
    var converged = false

    while epoch < config.max_iterations:
        var sample_idx = 0
        while sample_idx < num_samples:
            val gradient = grad_f(x, sample_idx)
            val grad_norm = vector_norm(gradient)

            if grad_norm < config.gradient_tolerance:
                converged = true
                return OptimizationResult(
                    solution: x,
                    objective_value: 0.0,
                    iterations: iteration,
                    converged: true,
                    gradient_norm: grad_norm,
                    message: "Converged: gradient norm below tolerance"
                )

            val scaled_gradient = vector_scale(gradient, config.learning_rate)
            x = vector_subtract(x, scaled_gradient)

            iteration = iteration + 1
            sample_idx = sample_idx + 1

        epoch = epoch + 1

    OptimizationResult(
        solution: x,
        objective_value: 0.0,
        iterations: iteration,
        converged: converged,
        gradient_norm: 0.0,
        message: "Maximum iterations reached"
    )

fn adam_initialize(n: i64, beta1: f64, beta2: f64, epsilon: f64) -> AdamState:
    AdamState(
        m: vector_zeros(n),
        v: vector_zeros(n),
        beta1: beta1,
        beta2: beta2,
        epsilon: epsilon,
        t: 0
    )

fn adam_update(state: AdamState, gradient: List<f64>) -> AdamState:
    val new_t = state.t + 1
    var new_m = []
    var new_v = []

    var i = 0
    while i < gradient.length():
        val m_i = state.beta1 * state.m[i] + (1.0 - state.beta1) * gradient[i]
        val v_i = state.beta2 * state.v[i] + (1.0 - state.beta2) * gradient[i] * gradient[i]
        new_m.append(m_i)
        new_v.append(v_i)
        i = i + 1

    AdamState(
        m: new_m,
        v: new_v,
        beta1: state.beta1,
        beta2: state.beta2,
        epsilon: state.epsilon,
        t: new_t
    )

fn adam_get_update(state: AdamState, learning_rate: f64) -> List<f64>:
    val beta1_t = pow(state.beta1, float(state.t))
    val beta2_t = pow(state.beta2, float(state.t))

    val update = []
    var i = 0
    while i < state.m.length():
        val m_hat = state.m[i] / (1.0 - beta1_t)
        val v_hat = state.v[i] / (1.0 - beta2_t)
        val update_i = learning_rate * m_hat / (sqrt(v_hat) + state.epsilon)
        update.append(update_i)
        i = i + 1
    update

fn adam_optimize(grad_f: fn(List<f64>, i64) -> List<f64>, x0: List<f64>,
                num_samples: i64, config: OptimizationConfig) -> OptimizationResult:
    var x = vector_copy(x0)
    var state = adam_initialize(x.length(), 0.9, 0.999, 1e-8)
    var iteration = 0
    var epoch = 0
    var converged = false

    while epoch < config.max_iterations:
        var sample_idx = 0
        while sample_idx < num_samples:
            val gradient = grad_f(x, sample_idx)
            val grad_norm = vector_norm(gradient)

            if grad_norm < config.gradient_tolerance:
                converged = true
                return OptimizationResult(
                    solution: x,
                    objective_value: 0.0,
                    iterations: iteration,
                    converged: true,
                    gradient_norm: grad_norm,
                    message: "Converged: gradient norm below tolerance"
                )

            state = adam_update(state, gradient)
            val update = adam_get_update(state, config.learning_rate)
            x = vector_subtract(x, update)

            iteration = iteration + 1
            sample_idx = sample_idx + 1

        epoch = epoch + 1

    OptimizationResult(
        solution: x,
        objective_value: 0.0,
        iterations: iteration,
        converged: converged,
        gradient_norm: 0.0,
        message: "Maximum iterations reached"
    )

fn rmsprop_initialize(n: i64, decay_rate: f64, epsilon: f64) -> RMSpropState:
    RMSpropState(
        cache: vector_zeros(n),
        decay_rate: decay_rate,
        epsilon: epsilon
    )

fn rmsprop_update(state: RMSpropState, x: List<f64>, gradient: List<f64>,
                 learning_rate: f64) -> List<f64>:
    var new_cache = []
    var new_x = []

    var i = 0
    while i < gradient.length():
        val cache_i = state.decay_rate * state.cache[i] + (1.0 - state.decay_rate) * gradient[i] * gradient[i]
        new_cache.append(cache_i)
        val x_i = x[i] - learning_rate * gradient[i] / (sqrt(cache_i) + state.epsilon)
        new_x.append(x_i)
        i = i + 1

    new_x

fn rmsprop_optimize(grad_f: fn(List<f64>, i64) -> List<f64>, x0: List<f64>,
                   num_samples: i64, config: OptimizationConfig) -> OptimizationResult:
    var x = vector_copy(x0)
    var state = rmsprop_initialize(x.length(), 0.9, 1e-8)
    var iteration = 0
    var epoch = 0
    var converged = false

    while epoch < config.max_iterations:
        var sample_idx = 0
        while sample_idx < num_samples:
            val gradient = grad_f(x, sample_idx)
            val grad_norm = vector_norm(gradient)

            if grad_norm < config.gradient_tolerance:
                converged = true
                return OptimizationResult(
                    solution: x,
                    objective_value: 0.0,
                    iterations: iteration,
                    converged: true,
                    gradient_norm: grad_norm,
                    message: "Converged: gradient norm below tolerance"
                )

            x = rmsprop_update(state, x, gradient, config.learning_rate)

            iteration = iteration + 1
            sample_idx = sample_idx + 1

        epoch = epoch + 1

    OptimizationResult(
        solution: x,
        objective_value: 0.0,
        iterations: iteration,
        converged: converged,
        gradient_norm: 0.0,
        message: "Maximum iterations reached"
    )

fn adagrad_initialize(n: i64, epsilon: f64) -> AdaGradState:
    AdaGradState(
        cache: vector_zeros(n),
        epsilon: epsilon
    )

fn adagrad_update(state: AdaGradState, x: List<f64>, gradient: List<f64>,
                 learning_rate: f64) -> List<f64>:
    var new_cache = []
    var new_x = []

    var i = 0
    while i < gradient.length():
        val cache_i = state.cache[i] + gradient[i] * gradient[i]
        new_cache.append(cache_i)
        val x_i = x[i] - learning_rate * gradient[i] / (sqrt(cache_i) + state.epsilon)
        new_x.append(x_i)
        i = i + 1

    new_x

fn adagrad_optimize(grad_f: fn(List<f64>, i64) -> List<f64>, x0: List<f64>,
                   num_samples: i64, config: OptimizationConfig) -> OptimizationResult:
    var x = vector_copy(x0)
    var state = adagrad_initialize(x.length(), 1e-8)
    var iteration = 0
    var epoch = 0
    var converged = false

    while epoch < config.max_iterations:
        var sample_idx = 0
        while sample_idx < num_samples:
            val gradient = grad_f(x, sample_idx)
            val grad_norm = vector_norm(gradient)

            if grad_norm < config.gradient_tolerance:
                converged = true
                return OptimizationResult(
                    solution: x,
                    objective_value: 0.0,
                    iterations: iteration,
                    converged: true,
                    gradient_norm: grad_norm,
                    message: "Converged: gradient norm below tolerance"
                )

            x = adagrad_update(state, x, gradient, config.learning_rate)

            iteration = iteration + 1
            sample_idx = sample_idx + 1

        epoch = epoch + 1

    OptimizationResult(
        solution: x,
        objective_value: 0.0,
        iterations: iteration,
        converged: converged,
        gradient_norm: 0.0,
        message: "Maximum iterations reached"
    )

fn lr_constant(initial_lr: f64, epoch: i64) -> f64:
    initial_lr

fn lr_exponential_decay(initial_lr: f64, epoch: i64, decay_rate: f64) -> f64:
    initial_lr * pow(decay_rate, float(epoch))

fn lr_step_decay(initial_lr: f64, epoch: i64, step_size: i64, gamma: f64) -> f64:
    val num_steps = epoch / step_size
    initial_lr * pow(gamma, float(num_steps))

fn lr_inverse_time_decay(initial_lr: f64, epoch: i64, decay_rate: f64) -> f64:
    initial_lr / (1.0 + decay_rate * float(epoch))

fn lr_cosine_annealing(initial_lr: f64, epoch: i64, total_epochs: i64, min_lr: f64) -> f64:
    val pi = 3.141592653589793
    val cos_inner = pi * float(epoch) / float(total_epochs)
    val cos_val = cos(cos_inner)
    min_lr + (initial_lr - min_lr) * (1.0 + cos_val) / 2.0

fn lr_warmup_cosine(initial_lr: f64, epoch: i64, warmup_epochs: i64,
                   total_epochs: i64, min_lr: f64) -> f64:
    if epoch < warmup_epochs:
        return initial_lr * float(epoch) / float(warmup_epochs)

    val adjusted_epoch = epoch - warmup_epochs
    val adjusted_total = total_epochs - warmup_epochs
    lr_cosine_annealing(initial_lr, adjusted_epoch, adjusted_total, min_lr)
