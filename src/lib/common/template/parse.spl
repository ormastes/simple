# Template Engine Parser Module
# Tokenization and AST parsing

import template.types
import template.utilities

# ============================================================================
# TOKENIZER
# ============================================================================

fn tokenize(template: text) -> [[text]]:
    var tokens: [[text]] = []
    var pos = 0
    val len = template.len()

    while pos < len:
        # Check for {{
        if pos + 1 < len:
            val c1 = char_at_safe(template, pos)
            val c2 = char_at_safe(template, pos + 1)
            if c1 == "{" and c2 == "{":
                # Check for {{{
                if pos + 2 < len:
                    val c3 = char_at_safe(template, pos + 2)
                    if c3 == "{":
                        tokens.push(make_token("RAW_VAR_OPEN", "{{{", pos))
                        pos = pos + 3
                    else:
                        # Check for {{! or {{!--
                        if c3 == "!":
                            if pos + 3 < len:
                                val c4 = char_at_safe(template, pos + 3)
                                if c4 == "-":
                                    if pos + 4 < len:
                                        val c5 = char_at_safe(template, pos + 4)
                                        if c5 == "-":
                                            tokens.push(make_token("COMMENT_OPEN", "{{!--", pos))
                                            pos = pos + 5
                                        else:
                                            tokens.push(make_token("COMMENT_OPEN", "{{!", pos))
                                            pos = pos + 3
                                    else:
                                        tokens.push(make_token("COMMENT_OPEN", "{{!", pos))
                                        pos = pos + 3
                                else:
                                    tokens.push(make_token("COMMENT_OPEN", "{{!", pos))
                                    pos = pos + 3
                            else:
                                tokens.push(make_token("COMMENT_OPEN", "{{!", pos))
                                pos = pos + 3
                        else:
                            # Check for {{# or {{/ or {{> or {{~
                            if c3 == "#":
                                tokens.push(make_token("BLOCK_OPEN", "{{#", pos))
                                pos = pos + 3
                            else:
                                if c3 == "/":
                                    tokens.push(make_token("BLOCK_CLOSE", "{{/", pos))
                                    pos = pos + 3
                                else:
                                    if c3 == ">":
                                        tokens.push(make_token("PARTIAL", "{{>", pos))
                                        pos = pos + 3
                                    else:
                                        if c3 == "~":
                                            tokens.push(make_token("VAR_OPEN_TRIM", "{{~", pos))
                                            pos = pos + 3
                                        else:
                                            tokens.push(make_token("VAR_OPEN", "{{", pos))
                                            pos = pos + 2
                else:
                    tokens.push(make_token("VAR_OPEN", "{{", pos))
                    pos = pos + 2
            else:
                # Check for }}
                if c1 == "}" and c2 == "}":
                    # Check for }}}
                    if pos + 2 < len:
                        val c3 = char_at_safe(template, pos + 2)
                        if c3 == "}":
                            tokens.push(make_token("RAW_VAR_CLOSE", "}}}", pos))
                            pos = pos + 3
                        else:
                            # Check for ~}}
                            if pos > 0:
                                val prev = char_at_safe(template, pos - 1)
                                if prev == "~":
                                    tokens.push(make_token("VAR_CLOSE_TRIM", "~}}", pos))
                                    pos = pos + 2
                                else:
                                    tokens.push(make_token("VAR_CLOSE", "}}", pos))
                                    pos = pos + 2
                            else:
                                tokens.push(make_token("VAR_CLOSE", "}}", pos))
                                pos = pos + 2
                    else:
                        tokens.push(make_token("VAR_CLOSE", "}}", pos))
                        pos = pos + 2
                else:
                    # Check for --}}
                    if c1 == "-" and c2 == "-":
                        if pos + 3 < len:
                            val c3 = char_at_safe(template, pos + 2)
                            val c4 = char_at_safe(template, pos + 3)
                            if c3 == "}" and c4 == "}":
                                tokens.push(make_token("COMMENT_CLOSE", "--}}", pos))
                                pos = pos + 4
                            else:
                                tokens.push(make_token("TEXT", c1, pos))
                                pos = pos + 1
                        else:
                            tokens.push(make_token("TEXT", c1, pos))
                            pos = pos + 1
                    else:
                        # Regular text - collect until next special
                        var text_start = pos
                        var text_end = pos
                        while text_end < len:
                            val tc = char_at_safe(template, text_end)
                            val is_special = tc == "{" or tc == "}"
                            if is_special:
                                text_end = len
                            else:
                                text_end = text_end + 1
                        if text_end > text_start:
                            val text = substr_safe(template, text_start, text_end)
                            tokens.push(make_token("TEXT", text, text_start))
                        pos = text_end
        else:
            # Single character at end
            val c = char_at_safe(template, pos)
            tokens.push(make_token("TEXT", c, pos))
            pos = pos + 1

    tokens

# ============================================================================
# EXPRESSION PARSING
# ============================================================================

fn parse_expr(expr: text) -> [text]:
    val trimmed = str_trim(expr)
    # For now, simple variable path parsing
    # Returns [type, name, arg1, arg2]
    if str_contains(trimmed, "."):
        # Property access
        val parts = str_split(trimmed, ".")
        if parts.len() >= 2:
            return make_node("PATH", parts[0], parts[1], "")
    make_node("VAR", trimmed, "", "")

fn parse_block_expr(expr: text) -> [text]:
    val trimmed = str_trim(expr)
    val parts = str_split(trimmed, " ")
    if parts.len() > 0:
        val cmd = parts[0]
        if parts.len() > 1:
            return make_node("BLOCK", cmd, parts[1], "")
        return make_node("BLOCK", cmd, "", "")
    make_node("BLOCK", "", "", "")

# ============================================================================
# PARSER
# ============================================================================

fn parse(tokens: [[text]]) -> [[text]]:
    var nodes: [[text]] = []
    var i = 0
    val len = tokens.len()

    while i < len:
        val tok = tokens[i]
        val ttype = token_type(tok)
        val tval = token_value(tok)

        if ttype == "TEXT":
            nodes.push(make_node("TEXT", tval, "", ""))
            i = i + 1
        else:
            if ttype == "VAR_OPEN" or ttype == "VAR_OPEN_TRIM":
                # Find closing }}
                var j = i + 1
                var content_parts = []
                while j < len:
                    val t = tokens[j]
                    val tt = token_type(t)
                    if tt == "VAR_CLOSE" or tt == "VAR_CLOSE_TRIM":
                        j = len
                    else:
                        content_parts.append(token_value(t))
                        j = j + 1
                val content = content_parts.join("")
                nodes.push(make_node("VAR", str_trim(content), "", ""))
                i = j + 1
            else:
                if ttype == "RAW_VAR_OPEN":
                    # Find closing }}}
                    var j = i + 1
                    var content_parts = []
                    while j < len:
                        val t = tokens[j]
                        if token_type(t) == "RAW_VAR_CLOSE":
                            j = len
                        else:
                            content_parts.append(token_value(t))
                            j = j + 1
                    val content = content_parts.join("")
                    nodes.push(make_node("RAW_VAR", str_trim(content), "", ""))
                    i = j + 1
                else:
                    if ttype == "BLOCK_OPEN":
                        # Parse block {{#if}}, {{#each}}, etc.
                        var j = i + 1
                        var content_parts = []
                        while j < len:
                            val t = tokens[j]
                            if token_type(t) == "VAR_CLOSE":
                                j = len
                            else:
                                content_parts.append(token_value(t))
                                j = j + 1
                        val content = content_parts.join("")
                        val block_info = parse_block_expr(content)
                        nodes.push(block_info)
                        i = j + 1
                    else:
                        if ttype == "BLOCK_CLOSE":
                            # {{/if}}, {{/each}}
                            var j = i + 1
                            var content_parts = []
                            while j < len:
                                val t = tokens[j]
                                if token_type(t) == "VAR_CLOSE":
                                    j = len
                                else:
                                    content_parts.append(token_value(t))
                                    j = j + 1
                            val content = content_parts.join("")
                            nodes.push(make_node("END_BLOCK", str_trim(content), "", ""))
                            i = j + 1
                        else:
                            if ttype == "PARTIAL":
                                # {{> name}}
                                var j = i + 1
                                var content_parts = []
                                while j < len:
                                    val t = tokens[j]
                                    if token_type(t) == "VAR_CLOSE":
                                        j = len
                                    else:
                                        content_parts.append(token_value(t))
                                        j = j + 1
                                val content = content_parts.join("")
                                nodes.push(make_node("PARTIAL", str_trim(content), "", ""))
                                i = j + 1
                            else:
                                if ttype == "COMMENT_OPEN":
                                    # Skip until comment close
                                    var j = i + 1
                                    while j < len:
                                        val t = tokens[j]
                                        if token_type(t) == "COMMENT_CLOSE" or token_type(t) == "VAR_CLOSE":
                                            j = len
                                        j = j + 1
                                    i = j + 1
                                else:
                                    i = i + 1

    nodes

fn compile_template(template: text) -> [[text]]:
    val tokens = tokenize(template)
    parse(tokens)

# ============================================================================
# EXPORTS
# ============================================================================

export tokenize, parse_expr, parse_block_expr, parse, compile_template
