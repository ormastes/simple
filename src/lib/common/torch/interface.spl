# Torch Backend Interface
#
# Usable trait that both gc_async_mut and nogc_sync_mut torch backends implement.
# Operates at the handle level (i64 opaque handles) for portability.
#
# Usage:
#   use std.common.torch.interface.{TorchBackend}
#   use std.nogc_sync_mut.torch.backend  # provides impl
#
#   # Static dispatch (compile-time, zero overhead):
#   bind TorchBackend = NogcTorchOps
#
#   # Or dynamic dispatch (runtime, vtable):
#   fn train(backend: TorchBackend): ...
#
# The high-level Tensor class in each mode (gc/nogc) wraps these handle
# operations with mode-specific memory management:
#   - gc<T>:              Tensor has owns_handle field, conditional drop()
#   - unique_pointer<T>:  Tensor always owns, unconditional drop()

# ============================================================================
# Core Backend Trait — handle-level operations
# ============================================================================

trait TorchBackend:
    # --- Library Info ---
    fn available() -> bool
    fn version() -> text
    fn cuda_available() -> bool

    # --- Tensor Creation ---
    fn create_zeros(dims: [i64]) -> i64
    fn create_ones(dims: [i64]) -> i64
    fn create_randn(dims: [i64]) -> i64
    fn create_rand(dims: [i64]) -> i64
    fn create_full(dims: [i64], value: f64) -> i64
    fn create_from_data(data: [f64], dims: [i64]) -> i64
    fn create_eye(n: i64) -> i64
    fn create_empty(dims: [i64]) -> i64

    # --- Arithmetic ---
    fn tensor_add(a: i64, b: i64) -> i64
    fn tensor_sub(a: i64, b: i64) -> i64
    fn tensor_mul(a: i64, b: i64) -> i64
    fn tensor_div(a: i64, b: i64) -> i64
    fn tensor_matmul(a: i64, b: i64) -> i64
    fn tensor_neg(h: i64) -> i64
    fn tensor_abs(h: i64) -> i64
    fn tensor_sqrt(h: i64) -> i64
    fn tensor_exp(h: i64) -> i64
    fn tensor_log(h: i64) -> i64
    fn tensor_pow(h: i64, exponent: f64) -> i64
    fn tensor_add_scalar(h: i64, scalar: f64) -> i64
    fn tensor_mul_scalar(h: i64, scalar: f64) -> i64

    # --- Activations ---
    fn tensor_relu(h: i64) -> i64
    fn tensor_sigmoid(h: i64) -> i64
    fn tensor_tanh(h: i64) -> i64
    fn tensor_softmax(h: i64, dim: i64) -> i64
    fn tensor_log_softmax(h: i64, dim: i64) -> i64
    fn tensor_leaky_relu(h: i64, negative_slope: f64) -> i64
    fn tensor_gelu(h: i64) -> i64

    # --- Properties ---
    fn tensor_shape(h: i64) -> [i64]
    fn tensor_ndim(h: i64) -> i64
    fn tensor_numel(h: i64) -> i64

    # --- Shape Manipulation ---
    fn tensor_reshape(h: i64, dims: [i64]) -> i64
    fn tensor_view(h: i64, dims: [i64]) -> i64
    fn tensor_transpose(h: i64, dim0: i64, dim1: i64) -> i64
    fn tensor_permute(h: i64, dims: [i64]) -> i64
    fn tensor_squeeze(h: i64, dim: i64) -> i64
    fn tensor_unsqueeze(h: i64, dim: i64) -> i64
    fn tensor_flatten(h: i64) -> i64
    fn tensor_contiguous(h: i64) -> i64

    # --- Reduction ---
    fn tensor_sum(h: i64) -> f64
    fn tensor_sum_dim(h: i64, dim: i64, keepdim: bool) -> i64
    fn tensor_mean(h: i64) -> f64
    fn tensor_mean_dim(h: i64, dim: i64, keepdim: bool) -> i64

    # --- Autograd ---
    fn tensor_backward(h: i64)
    fn tensor_zero_grad(h: i64)
    fn tensor_set_requires_grad(h: i64, value: bool)
    fn tensor_requires_grad(h: i64) -> bool
    fn tensor_grad(h: i64) -> i64
    fn tensor_detach(h: i64) -> i64
    fn no_grad_begin()
    fn no_grad_end()

    # --- Memory ---
    fn tensor_clone(h: i64) -> i64
    fn tensor_free(h: i64)

    # --- Device ---
    fn tensor_cuda(h: i64, device_id: i64) -> i64
    fn tensor_cpu(h: i64) -> i64
    fn tensor_is_cuda(h: i64) -> bool

    # --- Neural Network ---
    fn nn_linear(input: i64, weight: i64, bias: i64) -> i64
    fn nn_conv2d(input: i64, weight: i64, bias: i64, stride: [i64], padding: [i64], dilation: [i64], groups: i64) -> i64
    fn nn_max_pool2d(input: i64, kernel_size: [i64], stride: [i64], padding: [i64]) -> i64
    fn nn_batch_norm(input: i64, running_mean: i64, running_var: i64, weight: i64, bias: i64, training: bool, momentum: f64, eps: f64) -> i64
    fn nn_dropout(input: i64, p: f64, training: bool) -> i64

    # --- Loss ---
    fn nn_mse_loss(input: i64, target: i64) -> f64
    fn nn_cross_entropy(input: i64, target: i64) -> f64


# ============================================================================
# Layer Interface Trait
# ============================================================================
# Convention: all neural network layers implement these methods.
# Since Simple traits use handle-level params, layer traits use i64.

trait LayerForward:
    fn forward(input: i64) -> i64

trait HasParameters:
    fn parameter_handles() -> [i64]


# ============================================================================
# Memory Model Summary
# ============================================================================
#
# GC Mode (gc_async_mut/torch):
#   - Default: gc<T> — shared, reference-counted
#   - Tensor has `owns_handle: bool` field
#   - drop() conditional: frees only if owns_handle is true
#   - Passing tensor to function shares reference (no ownership transfer)
#   - sub()/div() need workarounds to manage shared ownership of intermediaries
#
# NoGC Mode (nogc_sync_mut/torch):
#   - Default: unique_pointer<T> — single owner, RAII
#   - Tensor has only `handle: i64` field
#   - drop() unconditional: always frees
#   - Passing tensor to function transfers ownership (move semantics)
#   - sub()/div() use direct FFI calls
#   - Must clone() if tensor needed after passing to function
#

export TorchBackend
export LayerForward
export HasParameters
