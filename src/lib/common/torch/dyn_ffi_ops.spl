# Torch Dynamic FFI — Operations
#
# All public dyn_torch_* functions that wrap libspl_torch.so calls via DynLoader.
# Works in compiled mode without static linking to libtorch.
# Array-taking functions (tensor creation with dims) are excluded — use ffi.spl for those.
#
# See dyn_ffi.spl for infrastructure (call helpers, extern decls, constants).

use std.common.torch.dyn_ffi.{
    rt_cstring_to_text,
    spl_f64_to_bits,
    spl_bits_to_f64,
    spl_str_ptr,
    _call0,
    _call1,
    _call2,
    _call3,
    _call_n
}

# ============================================================================
# Library Information
# ============================================================================

# Check if PyTorch/libtorch is available at runtime
fn dyn_torch_available() -> bool:
    _call0("rt_torch_available") != 0

# Check if CUDA is available for GPU acceleration
fn dyn_torch_cuda_available() -> bool:
    _call0("rt_torch_cuda_available") != 0

# Get PyTorch version string (returned as C string pointer, converted to text)
fn dyn_torch_version() -> text:
    val ptr = _call0("rt_torch_version")
    rt_cstring_to_text(ptr)

# ============================================================================
# Element-wise Arithmetic Operations (single-handle)
# ============================================================================

# Element-wise negation: -a
fn dyn_torch_tensor_neg(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_neg", handle)

# Element-wise absolute value
fn dyn_torch_tensor_abs(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_abs", handle)

# Element-wise square root
fn dyn_torch_tensor_sqrt(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sqrt", handle)

# Element-wise exponential: e^x
fn dyn_torch_tensor_exp(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_exp", handle)

# Element-wise natural logarithm
fn dyn_torch_tensor_log(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_log", handle)

# ============================================================================
# Activation Functions (single-handle)
# ============================================================================

# ReLU activation: max(0, x)
fn dyn_torch_tensor_relu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_relu", handle)

# Sigmoid activation: 1 / (1 + e^(-x))
fn dyn_torch_tensor_sigmoid(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sigmoid", handle)

# Tanh activation
fn dyn_torch_tensor_tanh(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_tanh", handle)

# GELU activation (Gaussian Error Linear Unit)
fn dyn_torch_tensor_gelu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_gelu", handle)

# ============================================================================
# Linear Algebra / Shape Operations (single-handle)
# ============================================================================

# Matrix transpose (2D only, swap rows/cols)
fn dyn_torch_tensor_t(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_t", handle)

# Flatten to 1D
fn dyn_torch_tensor_flatten(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_flatten", handle)

# Make tensor contiguous in memory
fn dyn_torch_tensor_contiguous(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_contiguous", handle)

# Remove dimensions of size 1
fn dyn_torch_tensor_squeeze(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_squeeze", handle)

# ============================================================================
# Shape Information (single-handle)
# ============================================================================

# Get number of dimensions
fn dyn_torch_tensor_ndim(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_ndim", handle)

# Get total number of elements
fn dyn_torch_tensor_numel(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_numel", handle)

# ============================================================================
# Device Management (single-handle)
# ============================================================================

# Move tensor to CPU
fn dyn_torch_tensor_cpu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_cpu", handle)

# Clone tensor (deep copy)
fn dyn_torch_tensor_clone(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_clone", handle)

# Check if tensor is on CUDA device
fn dyn_torch_tensor_is_cuda(handle: i64) -> bool:
    _call1("rt_torch_torchtensor_is_cuda", handle) != 0

# ============================================================================
# Memory Management (single-handle)
# ============================================================================

# Free tensor handle (release memory)
fn dyn_torch_tensor_free(handle: i64):
    _call1("rt_torch_torchtensor_free", handle)

# ============================================================================
# Autograd Operations (single-handle)
# ============================================================================

# Compute gradients (backward pass)
fn dyn_torch_autograd_backward(handle: i64):
    _call1("rt_torch_autograd_backward", handle)

# Zero out gradients
fn dyn_torch_autograd_zero_grad(handle: i64):
    _call1("rt_torch_autograd_zero_grad", handle)

# Detach tensor from computation graph (no gradient)
fn dyn_torch_autograd_detach(handle: i64) -> i64:
    _call1("rt_torch_autograd_detach", handle)

# Get gradient tensor (returns handle or 0 if no gradient)
fn dyn_torch_autograd_grad(handle: i64) -> i64:
    _call1("rt_torch_autograd_grad", handle)

# Check if tensor requires gradients
fn dyn_torch_autograd_requires_grad(handle: i64) -> bool:
    _call1("rt_torch_autograd_requires_grad", handle) != 0

# ============================================================================
# Element-wise Arithmetic Operations (two-handle)
# ============================================================================

# Element-wise addition: a + b
fn dyn_torch_tensor_add(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_add", handle, other)

# Element-wise subtraction: a - b
fn dyn_torch_tensor_sub(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_sub", handle, other)

# Element-wise multiplication: a * b
fn dyn_torch_tensor_mul(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_mul", handle, other)

# Element-wise division: a / b
fn dyn_torch_tensor_div(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_div", handle, other)

# Matrix multiplication: a @ b
fn dyn_torch_tensor_matmul(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_matmul", handle, other)

# Dot product (1D tensors)
fn dyn_torch_tensor_dot(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_dot", handle, other)

# ============================================================================
# Activation Functions (two-arg: handle + dim)
# ============================================================================

# Softmax along dimension
fn dyn_torch_tensor_softmax(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_softmax", handle, dim)

# Log softmax along dimension
fn dyn_torch_tensor_log_softmax(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_log_softmax", handle, dim)

# ============================================================================
# Shape Manipulation (two-arg: handle + dim)
# ============================================================================

# Add dimension of size 1
fn dyn_torch_tensor_unsqueeze(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_unsqueeze", handle, dim)

# Remove specific dimension of size 1
fn dyn_torch_tensor_squeeze_dim(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_squeeze_dim", handle, dim)

# ============================================================================
# Indexing (three-arg)
# ============================================================================

# Index select (gather specific indices); indices is a tensor handle
fn dyn_torch_tensor_index_select(handle: i64, dim: i64, indices: i64) -> i64:
    _call3("rt_torch_torchtensor_index_select", handle, dim, indices)

# ============================================================================
# Device Management (two-arg: handle + device_id)
# ============================================================================

# Move tensor to CUDA device (device_id passed as i64 to avoid i32 cast issues)
fn dyn_torch_tensor_cuda(handle: i64, device_id: i64) -> i64:
    _call2("rt_torch_torchtensor_cuda", handle, device_id)

# ============================================================================
# Autograd Operations (two-arg)
# ============================================================================

# Enable/disable gradient computation for tensor (bool passed as i64: 1=true, 0=false)
fn dyn_torch_autograd_set_requires_grad(handle: i64, requires_grad: i64):
    _call2("rt_torch_autograd_set_requires_grad", handle, requires_grad)

# ============================================================================
# Linear Algebra Operations (three-arg)
# ============================================================================

# Matrix transpose along two specified dimensions
fn dyn_torch_tensor_transpose(handle: i64, dim0: i64, dim1: i64) -> i64:
    _call3("rt_torch_torchtensor_transpose", handle, dim0, dim1)

# ============================================================================
# Reduction Operations (three-arg: handle, dim, keepdim)
# ============================================================================

# Sum along dimension. keepdim: 1=true, 0=false
fn dyn_torch_tensor_sum_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_sum_dim", handle, dim, keepdim)

# Mean along dimension
fn dyn_torch_tensor_mean_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_mean_dim", handle, dim, keepdim)

# Argmax along dimension
fn dyn_torch_tensor_argmax(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_argmax", handle, dim, keepdim)

# Argmin along dimension
fn dyn_torch_tensor_argmin(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_argmin", handle, dim, keepdim)

# Max along dimension
fn dyn_torch_tensor_max_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_max_dim", handle, dim, keepdim)

# ============================================================================
# Indexing Operations (three-arg)
# ============================================================================

# Gather elements along dim using indices tensor
fn dyn_torch_tensor_gather(handle: i64, dim: i64, indices: i64) -> i64:
    _call3("rt_torch_torchtensor_gather", handle, dim, indices)

# ============================================================================
# Slice Operation (five-arg: handle, dim, start, end, step)
# ============================================================================

fn dyn_torch_tensor_slice(handle: i64, dim: i64, start: i64, end_idx: i64, step: i64) -> i64:
    _call_n("rt_torch_torchtensor_slice", [handle, dim, start, end_idx, step])

# ============================================================================
# NN Operations (three-arg: input, weight, bias)
# ============================================================================

# Linear layer: output = input @ weight^T + bias
fn dyn_torch_nn_linear(input: i64, weight: i64, bias: i64) -> i64:
    _call3("rt_torch_nn_linear", input, weight, bias)

# Embedding lookup: select rows from weight by input indices
fn dyn_torch_nn_embedding(input: i64, weight: i64) -> i64:
    _call2("rt_torch_nn_embedding", input, weight)

# ============================================================================
# Autograd Operations (no-arg)
# ============================================================================

# Begin no-gradient context
fn dyn_torch_autograd_no_grad_begin():
    _call0("rt_torch_autograd_no_grad_begin")

# End no-gradient context
fn dyn_torch_autograd_no_grad_end():
    _call0("rt_torch_autograd_no_grad_end")

# ============================================================================
# CUDA Memory (one-arg: device_id)
# ============================================================================

# Get currently allocated CUDA memory in bytes
fn dyn_torch_cuda_memory_allocated(device_id: i64) -> i64:
    _call1("rt_torch_cuda_memory_allocated", device_id)

# Get peak allocated CUDA memory in bytes
fn dyn_torch_cuda_max_memory_allocated(device_id: i64) -> i64:
    _call1("rt_torch_cuda_max_memory_allocated", device_id)

# Clear CUDA cache
fn dyn_torch_cuda_empty_cache():
    _call0("rt_torch_cuda_empty_cache")

# ============================================================================
# Linear Algebra (single-handle)
# ============================================================================

# Matrix inverse
fn dyn_torch_tensor_inverse(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_inverse", handle)

# Singular Value Decomposition (returns handle to tuple)
fn dyn_torch_tensor_svd(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_svd", handle)

# Eigenvalues and eigenvectors (returns handle to tuple)
fn dyn_torch_tensor_eig(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_eig", handle)

# ============================================================================
# Tensor Creation (single i64 arg)
# ============================================================================

# Create identity matrix of size n x n
fn dyn_torch_tensor_eye(n: i64) -> i64:
    _call1("rt_torch_tensor_eye", n)

# ============================================================================
# Reduction (missing: min_dim)
# ============================================================================

# Min along dimension
fn dyn_torch_tensor_min_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_min_dim", handle, dim, keepdim)

# ============================================================================
# Device Info
# ============================================================================

# Get device ID (-1 for CPU, >= 0 for CUDA)
fn dyn_torch_tensor_device(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_device", handle)

# Move tensor to device with CUDA stream
fn dyn_torch_tensor_to_stream(handle: i64, device_id: i64, stream: i64) -> i64:
    _call3("rt_torch_torchtensor_to_stream", handle, device_id, stream)

# ============================================================================
# CUDA Stream Operations
# ============================================================================

# Create CUDA stream on device
fn dyn_torch_stream_create(device_id: i64) -> i64:
    _call1("rt_torch_stream_create", device_id)

# Synchronize CUDA stream (wait for all ops to complete)
fn dyn_torch_stream_sync(handle: i64):
    _call1("rt_torch_torchstream_sync", handle)

# Query if stream is idle (true = all ops complete)
fn dyn_torch_stream_query(handle: i64) -> bool:
    _call1("rt_torch_torchstream_query", handle) != 0

# Free CUDA stream handle
fn dyn_torch_stream_free(handle: i64):
    _call1("rt_torch_torchstream_free", handle)

# ============================================================================
# Reduction Operations returning f64 (via bit-cast)
# ============================================================================

# Sum all elements → f64
fn dyn_torch_tensor_sum(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_sum_bits", handle)
    spl_bits_to_f64(bits)

# Mean of all elements → f64
fn dyn_torch_tensor_mean(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_mean_bits", handle)
    spl_bits_to_f64(bits)

# Maximum element → f64
fn dyn_torch_tensor_max(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_max_bits", handle)
    spl_bits_to_f64(bits)

# Minimum element → f64
fn dyn_torch_tensor_min(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_min_bits", handle)
    spl_bits_to_f64(bits)

# Frobenius norm → f64
fn dyn_torch_tensor_norm(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_norm_bits", handle)
    spl_bits_to_f64(bits)

# Determinant → f64
fn dyn_torch_tensor_det(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_det_bits", handle)
    spl_bits_to_f64(bits)

# Standard deviation → f64
fn dyn_torch_tensor_std(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_std_bits", handle)
    spl_bits_to_f64(bits)

# Variance → f64
fn dyn_torch_tensor_var(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_var_bits", handle)
    spl_bits_to_f64(bits)

# ============================================================================
# Loss Functions returning f64 (via bit-cast)
# ============================================================================

# Mean Squared Error loss → f64
fn dyn_torch_nn_mse_loss(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_mse_loss_bits", input, target)
    spl_bits_to_f64(bits)

# Cross Entropy loss → f64
fn dyn_torch_nn_cross_entropy(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_cross_entropy_bits", input, target)
    spl_bits_to_f64(bits)

# Binary Cross Entropy loss → f64
fn dyn_torch_nn_binary_cross_entropy(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_binary_cross_entropy_bits", input, target)
    spl_bits_to_f64(bits)

# Negative Log Likelihood loss → f64
fn dyn_torch_nn_nll_loss(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_nll_loss_bits", input, target)
    spl_bits_to_f64(bits)

# ============================================================================
# Element-wise Operations with f64 input (via bit-cast)
# ============================================================================

# Power: tensor ** exponent
fn dyn_torch_tensor_pow(handle: i64, exponent: f64) -> i64:
    _call2("rt_torch_torchtensor_pow_bits", handle, spl_f64_to_bits(exponent))

# Scalar addition: tensor + scalar
fn dyn_torch_tensor_add_scalar(handle: i64, scalar: f64) -> i64:
    _call2("rt_torch_torchtensor_add_scalar_bits", handle, spl_f64_to_bits(scalar))

# Scalar multiplication: tensor * scalar
fn dyn_torch_tensor_mul_scalar(handle: i64, scalar: f64) -> i64:
    _call2("rt_torch_torchtensor_mul_scalar_bits", handle, spl_f64_to_bits(scalar))

# ============================================================================
# Activation with f64 input (via bit-cast)
# ============================================================================

# Leaky ReLU: max(negative_slope * x, x)
fn dyn_torch_tensor_leaky_relu(handle: i64, negative_slope: f64) -> i64:
    _call2("rt_torch_torchtensor_leaky_relu_bits", handle, spl_f64_to_bits(negative_slope))

# ============================================================================
# Tensor Creation with f64 input (via bit-cast)
# ============================================================================

# Evenly spaced values in [start, end) with step
fn dyn_torch_tensor_arange(start: f64, end_val: f64, step: f64) -> i64:
    _call3("rt_torch_tensor_arange_bits", spl_f64_to_bits(start), spl_f64_to_bits(end_val), spl_f64_to_bits(step))

# Evenly spaced values with count steps
fn dyn_torch_tensor_linspace(start: f64, end_val: f64, steps: i64) -> i64:
    _call3("rt_torch_tensor_linspace_bits", spl_f64_to_bits(start), spl_f64_to_bits(end_val), steps)

# ============================================================================
# NN Operations with f64 input (via bit-cast)
# ============================================================================

# Dropout: randomly zero elements during training
fn dyn_torch_nn_dropout(input: i64, p: f64, training: bool) -> i64:
    var t = 0
    if training:
        t = 1
    _call3("rt_torch_nn_dropout_bits", input, spl_f64_to_bits(p), t)

# ============================================================================
# P2: Fixed-dimension Tensor Creation
# ============================================================================

# --- zeros ---
fn dyn_torch_tensor_zeros_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_zeros_1d", d0)
fn dyn_torch_tensor_zeros_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_zeros_2d", d0, d1)
fn dyn_torch_tensor_zeros_3d(d0: i64, d1: i64, d2: i64) -> i64:
    _call3("rt_torch_tensor_zeros_3d", d0, d1, d2)
fn dyn_torch_tensor_zeros_4d(d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_tensor_zeros_4d", [d0, d1, d2, d3])

# --- ones ---
fn dyn_torch_tensor_ones_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_ones_1d", d0)
fn dyn_torch_tensor_ones_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_ones_2d", d0, d1)
fn dyn_torch_tensor_ones_3d(d0: i64, d1: i64, d2: i64) -> i64:
    _call3("rt_torch_tensor_ones_3d", d0, d1, d2)
fn dyn_torch_tensor_ones_4d(d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_tensor_ones_4d", [d0, d1, d2, d3])

# --- randn ---
fn dyn_torch_tensor_randn_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_randn_1d", d0)
fn dyn_torch_tensor_randn_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_randn_2d", d0, d1)
fn dyn_torch_tensor_randn_3d(d0: i64, d1: i64, d2: i64) -> i64:
    _call3("rt_torch_tensor_randn_3d", d0, d1, d2)
fn dyn_torch_tensor_randn_4d(d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_tensor_randn_4d", [d0, d1, d2, d3])

# --- rand ---
fn dyn_torch_tensor_rand_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_rand_1d", d0)
fn dyn_torch_tensor_rand_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_rand_2d", d0, d1)
fn dyn_torch_tensor_rand_3d(d0: i64, d1: i64, d2: i64) -> i64:
    _call3("rt_torch_tensor_rand_3d", d0, d1, d2)
fn dyn_torch_tensor_rand_4d(d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_tensor_rand_4d", [d0, d1, d2, d3])

# --- empty ---
fn dyn_torch_tensor_empty_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_empty_1d", d0)
fn dyn_torch_tensor_empty_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_empty_2d", d0, d1)
fn dyn_torch_tensor_empty_3d(d0: i64, d1: i64, d2: i64) -> i64:
    _call3("rt_torch_tensor_empty_3d", d0, d1, d2)
fn dyn_torch_tensor_empty_4d(d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_tensor_empty_4d", [d0, d1, d2, d3])

# --- full (value is f64, bit-cast via spl_f64_to_bits) ---
fn dyn_torch_tensor_full_1d(d0: i64, value: f64) -> i64:
    _call2("rt_torch_tensor_full_1d", d0, spl_f64_to_bits(value))
fn dyn_torch_tensor_full_2d(d0: i64, d1: i64, value: f64) -> i64:
    _call3("rt_torch_tensor_full_2d", d0, d1, spl_f64_to_bits(value))
fn dyn_torch_tensor_full_3d(d0: i64, d1: i64, d2: i64, value: f64) -> i64:
    _call_n("rt_torch_tensor_full_3d", [d0, d1, d2, spl_f64_to_bits(value)])
fn dyn_torch_tensor_full_4d(d0: i64, d1: i64, d2: i64, d3: i64, value: f64) -> i64:
    _call_n("rt_torch_tensor_full_4d", [d0, d1, d2, d3, spl_f64_to_bits(value)])

# ============================================================================
# P2: Fixed-dimension Shape Operations
# ============================================================================

# --- reshape ---
fn dyn_torch_tensor_reshape_1d(handle: i64, d0: i64) -> i64:
    _call2("rt_torch_torchtensor_reshape_1d", handle, d0)
fn dyn_torch_tensor_reshape_2d(handle: i64, d0: i64, d1: i64) -> i64:
    _call3("rt_torch_torchtensor_reshape_2d", handle, d0, d1)
fn dyn_torch_tensor_reshape_3d(handle: i64, d0: i64, d1: i64, d2: i64) -> i64:
    _call_n("rt_torch_torchtensor_reshape_3d", [handle, d0, d1, d2])
fn dyn_torch_tensor_reshape_4d(handle: i64, d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_torchtensor_reshape_4d", [handle, d0, d1, d2, d3])

# --- view ---
fn dyn_torch_tensor_view_1d(handle: i64, d0: i64) -> i64:
    _call2("rt_torch_torchtensor_view_1d", handle, d0)
fn dyn_torch_tensor_view_2d(handle: i64, d0: i64, d1: i64) -> i64:
    _call3("rt_torch_torchtensor_view_2d", handle, d0, d1)
fn dyn_torch_tensor_view_3d(handle: i64, d0: i64, d1: i64, d2: i64) -> i64:
    _call_n("rt_torch_torchtensor_view_3d", [handle, d0, d1, d2])
fn dyn_torch_tensor_view_4d(handle: i64, d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_torchtensor_view_4d", [handle, d0, d1, d2, d3])

# --- permute ---
fn dyn_torch_tensor_permute_2d(handle: i64, d0: i64, d1: i64) -> i64:
    _call3("rt_torch_torchtensor_permute_2d", handle, d0, d1)
fn dyn_torch_tensor_permute_3d(handle: i64, d0: i64, d1: i64, d2: i64) -> i64:
    _call_n("rt_torch_torchtensor_permute_3d", [handle, d0, d1, d2])
fn dyn_torch_tensor_permute_4d(handle: i64, d0: i64, d1: i64, d2: i64, d3: i64) -> i64:
    _call_n("rt_torch_torchtensor_permute_4d", [handle, d0, d1, d2, d3])

# ============================================================================
# P2: Fixed-count Cat/Stack
# ============================================================================

# --- cat (concatenate tensors along dimension) ---
fn dyn_torch_tensor_cat_2(t0: i64, t1: i64, dim: i64) -> i64:
    _call3("rt_torch_torchtensor_cat_2", t0, t1, dim)
fn dyn_torch_tensor_cat_3(t0: i64, t1: i64, t2: i64, dim: i64) -> i64:
    _call_n("rt_torch_torchtensor_cat_3", [t0, t1, t2, dim])
fn dyn_torch_tensor_cat_4(t0: i64, t1: i64, t2: i64, t3: i64, dim: i64) -> i64:
    _call_n("rt_torch_torchtensor_cat_4", [t0, t1, t2, t3, dim])

# --- stack (stack tensors along new dimension) ---
fn dyn_torch_tensor_stack_2(t0: i64, t1: i64, dim: i64) -> i64:
    _call3("rt_torch_torchtensor_stack_2", t0, t1, dim)
fn dyn_torch_tensor_stack_3(t0: i64, t1: i64, t2: i64, dim: i64) -> i64:
    _call_n("rt_torch_torchtensor_stack_3", [t0, t1, t2, dim])
fn dyn_torch_tensor_stack_4(t0: i64, t1: i64, t2: i64, t3: i64, dim: i64) -> i64:
    _call_n("rt_torch_torchtensor_stack_4", [t0, t1, t2, t3, dim])

# ============================================================================
# P2: Shape Query
# ============================================================================

# Get the size of a specific dimension (returns -1 if dim_idx out of range)
fn dyn_torch_tensor_shape_dim(handle: i64, dim_idx: i64) -> i64:
    _call2("rt_torch_torchtensor_shape_dim", handle, dim_idx)

# ============================================================================
# P2: NN Operations with fixed arrays + f64 bit-cast
# ============================================================================

# Batch normalization (all scalar: momentum and eps are f64 bit-cast)
fn dyn_torch_nn_batch_norm(input: i64, running_mean: i64, running_var: i64, weight: i64, bias: i64, training: bool, momentum: f64, eps: f64) -> i64:
    var t = 0
    if training:
        t = 1
    _call_n("rt_torch_nn_batch_norm_bits", [input, running_mean, running_var, weight, bias, t, spl_f64_to_bits(momentum), spl_f64_to_bits(eps)])

# Layer normalization (1D normalized shape + f64 eps)
fn dyn_torch_nn_layer_norm_1d(input: i64, norm_d0: i64, weight: i64, bias: i64, eps: f64) -> i64:
    _call_n("rt_torch_nn_layer_norm_1d", [input, norm_d0, weight, bias, spl_f64_to_bits(eps)])

# Layer normalization (2D normalized shape + f64 eps)
fn dyn_torch_nn_layer_norm_2d(input: i64, norm_d0: i64, norm_d1: i64, weight: i64, bias: i64, eps: f64) -> i64:
    _call_n("rt_torch_nn_layer_norm_2d", [input, norm_d0, norm_d1, weight, bias, spl_f64_to_bits(eps)])

# Conv2d with explicit stride/padding/dilation (2D each)
fn dyn_torch_nn_conv2d(input: i64, weight: i64, bias: i64, stride_h: i64, stride_w: i64, pad_h: i64, pad_w: i64, dil_h: i64, dil_w: i64, groups: i64) -> i64:
    _call_n("rt_torch_nn_conv2d_simple", [input, weight, bias, stride_h, stride_w, pad_h, pad_w, dil_h, dil_w, groups])

# Max pooling 2D (kernel_size, stride, padding as h,w pairs)
fn dyn_torch_nn_max_pool2d(input: i64, kh: i64, kw: i64, sh: i64, sw: i64, ph: i64, pw: i64) -> i64:
    _call_n("rt_torch_nn_max_pool2d_simple", [input, kh, kw, sh, sw, ph, pw])

# Average pooling 2D (kernel_size, stride, padding as h,w pairs)
fn dyn_torch_nn_avg_pool2d(input: i64, kh: i64, kw: i64, sh: i64, sw: i64, ph: i64, pw: i64) -> i64:
    _call_n("rt_torch_nn_avg_pool2d_simple", [input, kh, kw, sh, sw, ph, pw])

# ============================================================================
# Trigonometric Functions (Group 1)
# ============================================================================

# Element-wise sine
fn dyn_torch_tensor_sin(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sin", handle)

# Element-wise cosine
fn dyn_torch_tensor_cos(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_cos", handle)

# Element-wise tangent
fn dyn_torch_tensor_tan(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_tan", handle)

# Element-wise arc sine
fn dyn_torch_tensor_asin(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_asin", handle)

# Element-wise arc cosine
fn dyn_torch_tensor_acos(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_acos", handle)

# Element-wise atan2(y, x)
fn dyn_torch_tensor_atan2(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_atan2", handle, other)

# ============================================================================
# Integer Tensor Creation (Group 2)
# ============================================================================

# Create int64 arange tensor
fn dyn_torch_tensor_arange_int(start: i64, end_val: i64, step: i64) -> i64:
    _call3("rt_torch_tensor_arange_int", start, end_val, step)

# Create int64 zeros (1D)
fn dyn_torch_tensor_zeros_int_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_zeros_int_1d", d0)

# Create int64 zeros (2D)
fn dyn_torch_tensor_zeros_int_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_zeros_int_2d", d0, d1)

# Create int64 ones (1D)
fn dyn_torch_tensor_ones_int_1d(d0: i64) -> i64:
    _call1("rt_torch_tensor_ones_int_1d", d0)

# Create int64 ones (2D)
fn dyn_torch_tensor_ones_int_2d(d0: i64, d1: i64) -> i64:
    _call2("rt_torch_tensor_ones_int_2d", d0, d1)

# Create int64 full (1D)
fn dyn_torch_tensor_full_int_1d(d0: i64, value: i64) -> i64:
    _call2("rt_torch_tensor_full_int_1d", d0, value)

# Create int64 full (2D)
fn dyn_torch_tensor_full_int_2d(d0: i64, d1: i64, value: i64) -> i64:
    _call3("rt_torch_tensor_full_int_2d", d0, d1, value)

# Cast tensor to float64
fn dyn_torch_tensor_to_float(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_to_float", handle)

# Cast tensor to int64
fn dyn_torch_tensor_to_int(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_to_int", handle)

# Cast tensor to float32
fn dyn_torch_tensor_to_float32(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_to_float32", handle)

# ============================================================================
# Tensor Serialization (Group 3)
# ============================================================================

# Save tensor to file (path via string pointer)
fn dyn_torch_tensor_save(handle: i64, path: text):
    _call2("rt_torch_tensor_save_dyn", handle, spl_str_ptr(path))

# Load tensor from file (path via string pointer)
fn dyn_torch_tensor_load(path: text) -> i64:
    _call1("rt_torch_tensor_load_dyn", spl_str_ptr(path))

# ============================================================================
# Safetensors Loading (Group 4)
# ============================================================================

# Open safetensors file
fn dyn_torch_safetensors_open(path: text) -> i64:
    _call1("rt_torch_safetensors_open_dyn", spl_str_ptr(path))

# Close safetensors file
fn dyn_torch_safetensors_close(handle: i64):
    _call1("rt_torch_safetensors_close_dyn", handle)

# Get number of tensors
fn dyn_torch_safetensors_num_tensors(handle: i64) -> i64:
    _call1("rt_torch_safetensors_num_tensors_dyn", handle)

# List tensor names (newline-delimited)
fn dyn_torch_safetensors_list_names(handle: i64) -> text:
    val ptr = _call1("rt_torch_safetensors_list_names_dyn", handle)
    rt_cstring_to_text(ptr)

# Get tensor by name
fn dyn_torch_safetensors_get_tensor(sf_handle: i64, name: text) -> i64:
    _call2("rt_torch_safetensors_get_tensor_dyn", sf_handle, spl_str_ptr(name))

# ============================================================================
# Exports
# ============================================================================

export dyn_torch_available, dyn_torch_cuda_available, dyn_torch_version
export dyn_torch_tensor_neg, dyn_torch_tensor_abs, dyn_torch_tensor_sqrt
export dyn_torch_tensor_exp, dyn_torch_tensor_log
export dyn_torch_tensor_relu, dyn_torch_tensor_sigmoid, dyn_torch_tensor_tanh
export dyn_torch_tensor_gelu, dyn_torch_tensor_t, dyn_torch_tensor_flatten
export dyn_torch_tensor_contiguous, dyn_torch_tensor_squeeze
export dyn_torch_tensor_ndim, dyn_torch_tensor_numel
export dyn_torch_tensor_cpu, dyn_torch_tensor_clone
export dyn_torch_tensor_is_cuda, dyn_torch_tensor_free
export dyn_torch_tensor_add, dyn_torch_tensor_sub, dyn_torch_tensor_mul, dyn_torch_tensor_div
export dyn_torch_tensor_matmul, dyn_torch_tensor_dot
export dyn_torch_tensor_softmax, dyn_torch_tensor_log_softmax
export dyn_torch_tensor_unsqueeze, dyn_torch_tensor_squeeze_dim
export dyn_torch_tensor_index_select, dyn_torch_tensor_cuda, dyn_torch_tensor_transpose
export dyn_torch_autograd_backward, dyn_torch_autograd_zero_grad
export dyn_torch_autograd_detach, dyn_torch_autograd_grad
export dyn_torch_autograd_requires_grad, dyn_torch_autograd_set_requires_grad
export dyn_torch_autograd_no_grad_begin, dyn_torch_autograd_no_grad_end
export dyn_torch_tensor_sum_dim, dyn_torch_tensor_mean_dim
export dyn_torch_tensor_argmax, dyn_torch_tensor_argmin, dyn_torch_tensor_max_dim
export dyn_torch_tensor_gather, dyn_torch_tensor_slice
export dyn_torch_nn_linear, dyn_torch_nn_embedding
export dyn_torch_cuda_memory_allocated, dyn_torch_cuda_max_memory_allocated
export dyn_torch_cuda_empty_cache
export dyn_torch_tensor_inverse, dyn_torch_tensor_svd, dyn_torch_tensor_eig
export dyn_torch_tensor_eye, dyn_torch_tensor_min_dim
export dyn_torch_tensor_device, dyn_torch_tensor_to_stream
export dyn_torch_stream_create, dyn_torch_stream_sync
export dyn_torch_stream_query, dyn_torch_stream_free
# P1: f64-returning reductions
export dyn_torch_tensor_sum, dyn_torch_tensor_mean
export dyn_torch_tensor_max, dyn_torch_tensor_min
export dyn_torch_tensor_norm, dyn_torch_tensor_det
export dyn_torch_tensor_std, dyn_torch_tensor_var
# P1: f64-returning losses
export dyn_torch_nn_mse_loss, dyn_torch_nn_cross_entropy
export dyn_torch_nn_binary_cross_entropy, dyn_torch_nn_nll_loss
# P1: f64-input element-wise ops
export dyn_torch_tensor_pow, dyn_torch_tensor_add_scalar, dyn_torch_tensor_mul_scalar
# P1: f64-input activations
export dyn_torch_tensor_leaky_relu
# P1: f64-input tensor creation
export dyn_torch_tensor_arange, dyn_torch_tensor_linspace
# P1: f64-input NN ops
export dyn_torch_nn_dropout
# P1: bit-cast helpers (re-export for user code)
export spl_f64_to_bits, spl_bits_to_f64
# P2: tensor creation (fixed-dim)
export dyn_torch_tensor_zeros_1d, dyn_torch_tensor_zeros_2d
export dyn_torch_tensor_zeros_3d, dyn_torch_tensor_zeros_4d
export dyn_torch_tensor_ones_1d, dyn_torch_tensor_ones_2d
export dyn_torch_tensor_ones_3d, dyn_torch_tensor_ones_4d
export dyn_torch_tensor_randn_1d, dyn_torch_tensor_randn_2d
export dyn_torch_tensor_randn_3d, dyn_torch_tensor_randn_4d
export dyn_torch_tensor_rand_1d, dyn_torch_tensor_rand_2d
export dyn_torch_tensor_rand_3d, dyn_torch_tensor_rand_4d
export dyn_torch_tensor_empty_1d, dyn_torch_tensor_empty_2d
export dyn_torch_tensor_empty_3d, dyn_torch_tensor_empty_4d
export dyn_torch_tensor_full_1d, dyn_torch_tensor_full_2d
export dyn_torch_tensor_full_3d, dyn_torch_tensor_full_4d
# P2: shape ops (fixed-dim)
export dyn_torch_tensor_reshape_1d, dyn_torch_tensor_reshape_2d
export dyn_torch_tensor_reshape_3d, dyn_torch_tensor_reshape_4d
export dyn_torch_tensor_view_1d, dyn_torch_tensor_view_2d
export dyn_torch_tensor_view_3d, dyn_torch_tensor_view_4d
export dyn_torch_tensor_permute_2d, dyn_torch_tensor_permute_3d, dyn_torch_tensor_permute_4d
# P2: cat/stack (fixed count)
export dyn_torch_tensor_cat_2, dyn_torch_tensor_cat_3, dyn_torch_tensor_cat_4
export dyn_torch_tensor_stack_2, dyn_torch_tensor_stack_3, dyn_torch_tensor_stack_4
# P2: shape query
export dyn_torch_tensor_shape_dim
# P2: NN ops (fixed arrays)
export dyn_torch_nn_batch_norm
export dyn_torch_nn_layer_norm_1d, dyn_torch_nn_layer_norm_2d
export dyn_torch_nn_conv2d, dyn_torch_nn_max_pool2d, dyn_torch_nn_avg_pool2d
# Trigonometric functions (Group 1)
export dyn_torch_tensor_sin, dyn_torch_tensor_cos, dyn_torch_tensor_tan
export dyn_torch_tensor_asin, dyn_torch_tensor_acos, dyn_torch_tensor_atan2
# Integer tensor creation (Group 2)
export dyn_torch_tensor_arange_int
export dyn_torch_tensor_zeros_int_1d, dyn_torch_tensor_zeros_int_2d
export dyn_torch_tensor_ones_int_1d, dyn_torch_tensor_ones_int_2d
export dyn_torch_tensor_full_int_1d, dyn_torch_tensor_full_int_2d
export dyn_torch_tensor_to_float, dyn_torch_tensor_to_int, dyn_torch_tensor_to_float32
# Tensor serialization (Group 3)
export dyn_torch_tensor_save, dyn_torch_tensor_load
# Safetensors loading (Group 4)
export dyn_torch_safetensors_open, dyn_torch_safetensors_close
export dyn_torch_safetensors_num_tensors, dyn_torch_safetensors_list_names
export dyn_torch_safetensors_get_tensor
