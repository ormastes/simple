# Torch Dynamic FFI — Core Operations
#
# Core dyn_torch_* functions that wrap libspl_torch.so calls via DynLoader.
# Works in compiled mode without static linking to libtorch.
# Contains: library info, arithmetic, activations, shape, device management,
#           autograd, reductions (f64), loss functions, f64-input ops.
#
# Extended operations (fixed-dim creation/shape, cat/stack, trig, int creation,
# serialization, safetensors) are in dyn_ffi_ops_ext.spl.
#
# See dyn_ffi.spl for infrastructure (call helpers, extern decls, constants).

use std.common.torch.dyn_ffi.{
    rt_cstring_to_text,
    spl_f64_to_bits,
    spl_bits_to_f64,
    spl_str_ptr,
    _call0,
    _call1,
    _call2,
    _call3,
    _call_n
}
use std.common.torch.dyn_ffi_ops_ext.*

# ============================================================================
# Library Information
# ============================================================================

# Check if PyTorch/libtorch is available at runtime
fn dyn_torch_available() -> bool:
    _call0("rt_torch_available") != 0

# Check if CUDA is available for GPU acceleration
fn dyn_torch_cuda_available() -> bool:
    _call0("rt_torch_cuda_available") != 0

# Get PyTorch version string (returned as C string pointer, converted to text)
fn dyn_torch_version() -> text:
    val ptr = _call0("rt_torch_version")
    rt_cstring_to_text(ptr)

# ============================================================================
# Element-wise Arithmetic Operations (single-handle)
# ============================================================================

# Element-wise negation: -a
fn dyn_torch_tensor_neg(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_neg", handle)

# Element-wise absolute value
fn dyn_torch_tensor_abs(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_abs", handle)

# Element-wise square root
fn dyn_torch_tensor_sqrt(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sqrt", handle)

# Element-wise exponential: e^x
fn dyn_torch_tensor_exp(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_exp", handle)

# Element-wise natural logarithm
fn dyn_torch_tensor_log(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_log", handle)

# ============================================================================
# Activation Functions (single-handle)
# ============================================================================

# ReLU activation: max(0, x)
fn dyn_torch_tensor_relu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_relu", handle)

# Sigmoid activation: 1 / (1 + e^(-x))
fn dyn_torch_tensor_sigmoid(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_sigmoid", handle)

# Tanh activation
fn dyn_torch_tensor_tanh(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_tanh", handle)

# GELU activation (Gaussian Error Linear Unit)
fn dyn_torch_tensor_gelu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_gelu", handle)

# ============================================================================
# Linear Algebra / Shape Operations (single-handle)
# ============================================================================

# Matrix transpose (2D only, swap rows/cols)
fn dyn_torch_tensor_t(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_t", handle)

# Flatten to 1D
fn dyn_torch_tensor_flatten(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_flatten", handle)

# Make tensor contiguous in memory
fn dyn_torch_tensor_contiguous(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_contiguous", handle)

# Remove dimensions of size 1
fn dyn_torch_tensor_squeeze(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_squeeze", handle)

# ============================================================================
# Shape Information (single-handle)
# ============================================================================

# Get number of dimensions
fn dyn_torch_tensor_ndim(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_ndim", handle)

# Get total number of elements
fn dyn_torch_tensor_numel(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_numel", handle)

# ============================================================================
# Device Management (single-handle)
# ============================================================================

# Move tensor to CPU
fn dyn_torch_tensor_cpu(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_cpu", handle)

# Clone tensor (deep copy)
fn dyn_torch_tensor_clone(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_clone", handle)

# Check if tensor is on CUDA device
fn dyn_torch_tensor_is_cuda(handle: i64) -> bool:
    _call1("rt_torch_torchtensor_is_cuda", handle) != 0

# ============================================================================
# Memory Management (single-handle)
# ============================================================================

# Free tensor handle (release memory)
fn dyn_torch_tensor_free(handle: i64):
    _call1("rt_torch_torchtensor_free", handle)

# ============================================================================
# Autograd Operations (single-handle)
# ============================================================================

# Compute gradients (backward pass)
fn dyn_torch_autograd_backward(handle: i64):
    _call1("rt_torch_autograd_backward", handle)

# Zero out gradients
fn dyn_torch_autograd_zero_grad(handle: i64):
    _call1("rt_torch_autograd_zero_grad", handle)

# Detach tensor from computation graph (no gradient)
fn dyn_torch_autograd_detach(handle: i64) -> i64:
    _call1("rt_torch_autograd_detach", handle)

# Get gradient tensor (returns handle or 0 if no gradient)
fn dyn_torch_autograd_grad(handle: i64) -> i64:
    _call1("rt_torch_autograd_grad", handle)

# Check if tensor requires gradients
fn dyn_torch_autograd_requires_grad(handle: i64) -> bool:
    _call1("rt_torch_autograd_requires_grad", handle) != 0

# ============================================================================
# Element-wise Arithmetic Operations (two-handle)
# ============================================================================

# Element-wise addition: a + b
fn dyn_torch_tensor_add(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_add", handle, other)

# Element-wise subtraction: a - b
fn dyn_torch_tensor_sub(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_sub", handle, other)

# Element-wise multiplication: a * b
fn dyn_torch_tensor_mul(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_mul", handle, other)

# Element-wise division: a / b
fn dyn_torch_tensor_div(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_div", handle, other)

# Matrix multiplication: a @ b
fn dyn_torch_tensor_matmul(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_matmul", handle, other)

# Dot product (1D tensors)
fn dyn_torch_tensor_dot(handle: i64, other: i64) -> i64:
    _call2("rt_torch_torchtensor_dot", handle, other)

# ============================================================================
# Activation Functions (two-arg: handle + dim)
# ============================================================================

# Softmax along dimension
fn dyn_torch_tensor_softmax(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_softmax", handle, dim)

# Log softmax along dimension
fn dyn_torch_tensor_log_softmax(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_log_softmax", handle, dim)

# ============================================================================
# Shape Manipulation (two-arg: handle + dim)
# ============================================================================

# Add dimension of size 1
fn dyn_torch_tensor_unsqueeze(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_unsqueeze", handle, dim)

# Remove specific dimension of size 1
fn dyn_torch_tensor_squeeze_dim(handle: i64, dim: i64) -> i64:
    _call2("rt_torch_torchtensor_squeeze_dim", handle, dim)

# ============================================================================
# Indexing (three-arg)
# ============================================================================

# Index select (gather specific indices); indices is a tensor handle
fn dyn_torch_tensor_index_select(handle: i64, dim: i64, indices: i64) -> i64:
    _call3("rt_torch_torchtensor_index_select", handle, dim, indices)

# ============================================================================
# Device Management (two-arg: handle + device_id)
# ============================================================================

# Move tensor to CUDA device (device_id passed as i64 to avoid i32 cast issues)
fn dyn_torch_tensor_cuda(handle: i64, device_id: i64) -> i64:
    _call2("rt_torch_torchtensor_cuda", handle, device_id)

# ============================================================================
# Autograd Operations (two-arg)
# ============================================================================

# Enable/disable gradient computation for tensor (bool passed as i64: 1=true, 0=false)
fn dyn_torch_autograd_set_requires_grad(handle: i64, requires_grad: i64):
    _call2("rt_torch_autograd_set_requires_grad", handle, requires_grad)

# ============================================================================
# Linear Algebra Operations (three-arg)
# ============================================================================

# Matrix transpose along two specified dimensions
fn dyn_torch_tensor_transpose(handle: i64, dim0: i64, dim1: i64) -> i64:
    _call3("rt_torch_torchtensor_transpose", handle, dim0, dim1)

# ============================================================================
# Reduction Operations (three-arg: handle, dim, keepdim)
# ============================================================================

# Sum along dimension. keepdim: 1=true, 0=false
fn dyn_torch_tensor_sum_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_sum_dim", handle, dim, keepdim)

# Mean along dimension
fn dyn_torch_tensor_mean_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_mean_dim", handle, dim, keepdim)

# Argmax along dimension
fn dyn_torch_tensor_argmax(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_argmax", handle, dim, keepdim)

# Argmin along dimension
fn dyn_torch_tensor_argmin(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_argmin", handle, dim, keepdim)

# Max along dimension
fn dyn_torch_tensor_max_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_max_dim", handle, dim, keepdim)

# ============================================================================
# Indexing Operations (three-arg)
# ============================================================================

# Gather elements along dim using indices tensor
fn dyn_torch_tensor_gather(handle: i64, dim: i64, indices: i64) -> i64:
    _call3("rt_torch_torchtensor_gather", handle, dim, indices)

# ============================================================================
# Slice Operation (five-arg: handle, dim, start, end, step)
# ============================================================================

fn dyn_torch_tensor_slice(handle: i64, dim: i64, start: i64, end_idx: i64, step: i64) -> i64:
    _call_n("rt_torch_torchtensor_slice", [handle, dim, start, end_idx, step])

# ============================================================================
# NN Operations (three-arg: input, weight, bias)
# ============================================================================

# Linear layer: output = input @ weight^T + bias
fn dyn_torch_nn_linear(input: i64, weight: i64, bias: i64) -> i64:
    _call3("rt_torch_nn_linear", input, weight, bias)

# Embedding lookup: select rows from weight by input indices
fn dyn_torch_nn_embedding(input: i64, weight: i64) -> i64:
    _call2("rt_torch_nn_embedding", input, weight)

# ============================================================================
# Autograd Operations (no-arg)
# ============================================================================

# Begin no-gradient context
fn dyn_torch_autograd_no_grad_begin():
    _call0("rt_torch_autograd_no_grad_begin")

# End no-gradient context
fn dyn_torch_autograd_no_grad_end():
    _call0("rt_torch_autograd_no_grad_end")

# ============================================================================
# CUDA Memory (one-arg: device_id)
# ============================================================================

# Get currently allocated CUDA memory in bytes
fn dyn_torch_cuda_memory_allocated(device_id: i64) -> i64:
    _call1("rt_torch_cuda_memory_allocated", device_id)

# Get peak allocated CUDA memory in bytes
fn dyn_torch_cuda_max_memory_allocated(device_id: i64) -> i64:
    _call1("rt_torch_cuda_max_memory_allocated", device_id)

# Clear CUDA cache
fn dyn_torch_cuda_empty_cache():
    _call0("rt_torch_cuda_empty_cache")

# ============================================================================
# Linear Algebra (single-handle)
# ============================================================================

# Matrix inverse
fn dyn_torch_tensor_inverse(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_inverse", handle)

# Singular Value Decomposition (returns handle to tuple)
fn dyn_torch_tensor_svd(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_svd", handle)

# Eigenvalues and eigenvectors (returns handle to tuple)
fn dyn_torch_tensor_eig(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_eig", handle)

# ============================================================================
# Tensor Creation (single i64 arg)
# ============================================================================

# Create identity matrix of size n x n
fn dyn_torch_tensor_eye(n: i64) -> i64:
    _call1("rt_torch_tensor_eye", n)

# ============================================================================
# Reduction (missing: min_dim)
# ============================================================================

# Min along dimension
fn dyn_torch_tensor_min_dim(handle: i64, dim: i64, keepdim: i64) -> i64:
    _call3("rt_torch_torchtensor_min_dim", handle, dim, keepdim)

# ============================================================================
# Device Info
# ============================================================================

# Get device ID (-1 for CPU, >= 0 for CUDA)
fn dyn_torch_tensor_device(handle: i64) -> i64:
    _call1("rt_torch_torchtensor_device", handle)

# Move tensor to device with CUDA stream
fn dyn_torch_tensor_to_stream(handle: i64, device_id: i64, stream: i64) -> i64:
    _call3("rt_torch_torchtensor_to_stream", handle, device_id, stream)

# ============================================================================
# CUDA Stream Operations
# ============================================================================

# Create CUDA stream on device
fn dyn_torch_stream_create(device_id: i64) -> i64:
    _call1("rt_torch_stream_create", device_id)

# Synchronize CUDA stream (wait for all ops to complete)
fn dyn_torch_stream_sync(handle: i64):
    _call1("rt_torch_torchstream_sync", handle)

# Query if stream is idle (true = all ops complete)
fn dyn_torch_stream_query(handle: i64) -> bool:
    _call1("rt_torch_torchstream_query", handle) != 0

# Free CUDA stream handle
fn dyn_torch_stream_free(handle: i64):
    _call1("rt_torch_torchstream_free", handle)

# ============================================================================
# Reduction Operations returning f64 (via bit-cast)
# ============================================================================

# Sum all elements → f64
fn dyn_torch_tensor_sum(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_sum_bits", handle)
    spl_bits_to_f64(bits)

# Mean of all elements → f64
fn dyn_torch_tensor_mean(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_mean_bits", handle)
    spl_bits_to_f64(bits)

# Maximum element → f64
fn dyn_torch_tensor_max(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_max_bits", handle)
    spl_bits_to_f64(bits)

# Minimum element → f64
fn dyn_torch_tensor_min(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_min_bits", handle)
    spl_bits_to_f64(bits)

# Frobenius norm → f64
fn dyn_torch_tensor_norm(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_norm_bits", handle)
    spl_bits_to_f64(bits)

# Determinant → f64
fn dyn_torch_tensor_det(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_det_bits", handle)
    spl_bits_to_f64(bits)

# Standard deviation → f64
fn dyn_torch_tensor_std(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_std_bits", handle)
    spl_bits_to_f64(bits)

# Variance → f64
fn dyn_torch_tensor_var(handle: i64) -> f64:
    val bits = _call1("rt_torch_torchtensor_var_bits", handle)
    spl_bits_to_f64(bits)

# ============================================================================
# Loss Functions returning f64 (via bit-cast)
# ============================================================================

# Mean Squared Error loss → f64
fn dyn_torch_nn_mse_loss(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_mse_loss_bits", input, target)
    spl_bits_to_f64(bits)

# Cross Entropy loss → f64
fn dyn_torch_nn_cross_entropy(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_cross_entropy_bits", input, target)
    spl_bits_to_f64(bits)

# Binary Cross Entropy loss → f64
fn dyn_torch_nn_binary_cross_entropy(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_binary_cross_entropy_bits", input, target)
    spl_bits_to_f64(bits)

# Negative Log Likelihood loss → f64
fn dyn_torch_nn_nll_loss(input: i64, target: i64) -> f64:
    val bits = _call2("rt_torch_nn_nll_loss_bits", input, target)
    spl_bits_to_f64(bits)

# ============================================================================
# Element-wise Operations with f64 input (via bit-cast)
# ============================================================================

# Power: tensor ** exponent
fn dyn_torch_tensor_pow(handle: i64, exponent: f64) -> i64:
    _call2("rt_torch_torchtensor_pow_bits", handle, spl_f64_to_bits(exponent))

# Scalar addition: tensor + scalar
fn dyn_torch_tensor_add_scalar(handle: i64, scalar: f64) -> i64:
    _call2("rt_torch_torchtensor_add_scalar_bits", handle, spl_f64_to_bits(scalar))

# Scalar multiplication: tensor * scalar
fn dyn_torch_tensor_mul_scalar(handle: i64, scalar: f64) -> i64:
    _call2("rt_torch_torchtensor_mul_scalar_bits", handle, spl_f64_to_bits(scalar))

# ============================================================================
# Activation with f64 input (via bit-cast)
# ============================================================================

# Leaky ReLU: max(negative_slope * x, x)
fn dyn_torch_tensor_leaky_relu(handle: i64, negative_slope: f64) -> i64:
    _call2("rt_torch_torchtensor_leaky_relu_bits", handle, spl_f64_to_bits(negative_slope))

# ============================================================================
# Tensor Creation with f64 input (via bit-cast)
# ============================================================================

# Evenly spaced values in [start, end) with step
fn dyn_torch_tensor_arange(start: f64, end_val: f64, step: f64) -> i64:
    _call3("rt_torch_tensor_arange_bits", spl_f64_to_bits(start), spl_f64_to_bits(end_val), spl_f64_to_bits(step))

# Evenly spaced values with count steps
fn dyn_torch_tensor_linspace(start: f64, end_val: f64, steps: i64) -> i64:
    _call3("rt_torch_tensor_linspace_bits", spl_f64_to_bits(start), spl_f64_to_bits(end_val), steps)

# ============================================================================
# NN Operations with f64 input (via bit-cast)
# ============================================================================

# Dropout: randomly zero elements during training
fn dyn_torch_nn_dropout(input: i64, p: f64, training: bool) -> i64:
    var t = 0
    if training:
        t = 1
    _call3("rt_torch_nn_dropout_bits", input, spl_f64_to_bits(p), t)

# ============================================================================
# Exports
# ============================================================================

export dyn_torch_available, dyn_torch_cuda_available, dyn_torch_version
export dyn_torch_tensor_neg, dyn_torch_tensor_abs, dyn_torch_tensor_sqrt
export dyn_torch_tensor_exp, dyn_torch_tensor_log
export dyn_torch_tensor_relu, dyn_torch_tensor_sigmoid, dyn_torch_tensor_tanh
export dyn_torch_tensor_gelu, dyn_torch_tensor_t, dyn_torch_tensor_flatten
export dyn_torch_tensor_contiguous, dyn_torch_tensor_squeeze
export dyn_torch_tensor_ndim, dyn_torch_tensor_numel
export dyn_torch_tensor_cpu, dyn_torch_tensor_clone
export dyn_torch_tensor_is_cuda, dyn_torch_tensor_free
export dyn_torch_tensor_add, dyn_torch_tensor_sub, dyn_torch_tensor_mul, dyn_torch_tensor_div
export dyn_torch_tensor_matmul, dyn_torch_tensor_dot
export dyn_torch_tensor_softmax, dyn_torch_tensor_log_softmax
export dyn_torch_tensor_unsqueeze, dyn_torch_tensor_squeeze_dim
export dyn_torch_tensor_index_select, dyn_torch_tensor_cuda, dyn_torch_tensor_transpose
export dyn_torch_autograd_backward, dyn_torch_autograd_zero_grad
export dyn_torch_autograd_detach, dyn_torch_autograd_grad
export dyn_torch_autograd_requires_grad, dyn_torch_autograd_set_requires_grad
export dyn_torch_autograd_no_grad_begin, dyn_torch_autograd_no_grad_end
export dyn_torch_tensor_sum_dim, dyn_torch_tensor_mean_dim
export dyn_torch_tensor_argmax, dyn_torch_tensor_argmin, dyn_torch_tensor_max_dim
export dyn_torch_tensor_gather, dyn_torch_tensor_slice
export dyn_torch_nn_linear, dyn_torch_nn_embedding
export dyn_torch_cuda_memory_allocated, dyn_torch_cuda_max_memory_allocated
export dyn_torch_cuda_empty_cache
export dyn_torch_tensor_inverse, dyn_torch_tensor_svd, dyn_torch_tensor_eig
export dyn_torch_tensor_eye, dyn_torch_tensor_min_dim
export dyn_torch_tensor_device, dyn_torch_tensor_to_stream
export dyn_torch_stream_create, dyn_torch_stream_sync
export dyn_torch_stream_query, dyn_torch_stream_free
# P1: f64-returning reductions
export dyn_torch_tensor_sum, dyn_torch_tensor_mean
export dyn_torch_tensor_max, dyn_torch_tensor_min
export dyn_torch_tensor_norm, dyn_torch_tensor_det
export dyn_torch_tensor_std, dyn_torch_tensor_var
# P1: f64-returning losses
export dyn_torch_nn_mse_loss, dyn_torch_nn_cross_entropy
export dyn_torch_nn_binary_cross_entropy, dyn_torch_nn_nll_loss
# P1: f64-input element-wise ops
export dyn_torch_tensor_pow, dyn_torch_tensor_add_scalar, dyn_torch_tensor_mul_scalar
# P1: f64-input activations
export dyn_torch_tensor_leaky_relu
# P1: f64-input tensor creation
export dyn_torch_tensor_arange, dyn_torch_tensor_linspace
# P1: f64-input NN ops
export dyn_torch_nn_dropout
# P1: bit-cast helpers (re-export for user code)
export spl_f64_to_bits, spl_bits_to_f64
