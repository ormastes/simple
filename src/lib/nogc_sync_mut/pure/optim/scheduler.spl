# Pure Simple Learning Rate Schedulers
#
# Common LR scheduling strategies
# Zero external dependencies
#
# NOTE: This module uses plain f64 values, no tensor imports needed.

# ============================================================================
# Helper: Cosine approximation via Taylor series (6 terms)
# ============================================================================

fn _sched_cos(x: f64) -> f64:
    """Approximate cos(x) using 6-term Taylor series.

    cos(x) = 1 - x^2/2! + x^4/4! - x^6/6! + x^8/8! - x^10/10! + x^12/12!

    Args:
        x - Input angle in radians

    Returns:
        Approximate cosine value
    """
    val x2 = x * x
    val x4 = x2 * x2
    val x6 = x4 * x2
    val x8 = x6 * x2
    val x10 = x8 * x2
    val x12 = x10 * x2
    1.0 - x2 / 2.0 + x4 / 24.0 - x6 / 720.0 + x8 / 40320.0 - x10 / 3628800.0 + x12 / 479001600.0

# ============================================================================
# ReduceLROnPlateau
# ============================================================================

class ReduceLROnPlateau:
    """Reduce learning rate when a metric has stopped improving.

    When the metric stops improving for 'patience' steps,
    the learning rate is multiplied by 'factor'.

    Example:
        val sched = ReduceLROnPlateau.create(0.01, 0.5, 3, 0.0001)
        # After each epoch:
        sched.step(current_loss)
        val lr = sched.get_lr()
    """
    current_lr: f64
    factor: f64
    patience: i64
    min_lr: f64
    best_metric: f64
    steps_without_improve: i64

    static fn create(initial_lr: f64, factor: f64, patience: i64, min_lr: f64) -> ReduceLROnPlateau:
        """Create ReduceLROnPlateau scheduler.

        Args:
            initial_lr - Starting learning rate
            factor - Factor to multiply LR by on plateau (e.g. 0.5)
            patience - Number of steps without improvement before reducing
            min_lr - Minimum learning rate floor

        Returns:
            ReduceLROnPlateau instance
        """
        ReduceLROnPlateau(
            current_lr: initial_lr,
            factor: factor,
            patience: patience,
            min_lr: min_lr,
            best_metric: 999999.0,
            steps_without_improve: 0
        )

    me step(metric: f64):
        """Update scheduler with new metric value.

        If metric improves, reset patience counter.
        If not improved for patience steps, reduce LR.

        Args:
            metric - Current metric value (lower is better)
        """
        if metric < self.best_metric:
            self.best_metric = metric
            self.steps_without_improve = 0
        else:
            self.steps_without_improve = self.steps_without_improve + 1
            if self.steps_without_improve >= self.patience:
                val new_lr = self.current_lr * self.factor
                self.current_lr = if new_lr > self.min_lr: new_lr else: self.min_lr
                self.steps_without_improve = 0

    fn get_lr() -> f64:
        """Get current learning rate."""
        self.current_lr

    fn to_string() -> text:
        """String representation."""
        "ReduceLROnPlateau(lr={self.current_lr}, factor={self.factor}, patience={self.patience}, min_lr={self.min_lr})"

# ============================================================================
# CyclicLR
# ============================================================================

class CyclicLR:
    """Cyclic Learning Rate scheduler with triangular wave.

    Learning rate oscillates between base_lr and max_lr in a
    triangular pattern with period = 2 * step_size_up.

    Example:
        val sched = CyclicLR.create(0.001, 0.01, 10)
        # After each batch:
        sched.step()
        val lr = sched.get_lr()
    """
    base_lr: f64
    max_lr: f64
    step_size_up: i64
    current_step: i64

    static fn create(base_lr: f64, max_lr: f64, step_size_up: i64) -> CyclicLR:
        """Create CyclicLR scheduler.

        Args:
            base_lr - Minimum learning rate
            max_lr - Maximum learning rate
            step_size_up - Number of steps in ascending half of cycle

        Returns:
            CyclicLR instance
        """
        CyclicLR(
            base_lr: base_lr,
            max_lr: max_lr,
            step_size_up: step_size_up,
            current_step: 0
        )

    me step():
        """Advance scheduler by one step."""
        self.current_step = self.current_step + 1

    fn get_lr() -> f64:
        """Get current learning rate using triangular wave.

        Returns:
            Current learning rate between base_lr and max_lr
        """
        val cycle_len = 2 * self.step_size_up
        val pos = self.current_step % cycle_len
        # Ascending phase
        if pos < self.step_size_up:
            val progress = pos * 1.0 / self.step_size_up
            self.base_lr + (self.max_lr - self.base_lr) * progress
        # Descending phase
        else:
            val progress = (pos - self.step_size_up) * 1.0 / self.step_size_up
            self.max_lr - (self.max_lr - self.base_lr) * progress

    fn to_string() -> text:
        """String representation."""
        "CyclicLR(base_lr={self.base_lr}, max_lr={self.max_lr}, step_size_up={self.step_size_up}, step={self.current_step})"

# ============================================================================
# WarmupLR
# ============================================================================

class WarmupLR:
    """Linear warmup learning rate scheduler.

    Linearly increases LR from 0 to target_lr over warmup_steps,
    then holds at target_lr.

    Example:
        val sched = WarmupLR.create(0.01, 100)
        # After each step:
        sched.step()
        val lr = sched.get_lr()
    """
    target_lr: f64
    warmup_steps: i64
    current_step: i64

    static fn create(target_lr: f64, warmup_steps: i64) -> WarmupLR:
        """Create WarmupLR scheduler.

        Args:
            target_lr - Target learning rate after warmup
            warmup_steps - Number of warmup steps

        Returns:
            WarmupLR instance
        """
        WarmupLR(
            target_lr: target_lr,
            warmup_steps: warmup_steps,
            current_step: 0
        )

    me step():
        """Advance scheduler by one step."""
        self.current_step = self.current_step + 1

    fn get_lr() -> f64:
        """Get current learning rate.

        Returns:
            Linearly interpolated LR during warmup, target_lr after
        """
        if self.current_step >= self.warmup_steps:
            self.target_lr
        else:
            self.target_lr * self.current_step / self.warmup_steps

    fn to_string() -> text:
        """String representation."""
        "WarmupLR(target_lr={self.target_lr}, warmup_steps={self.warmup_steps}, step={self.current_step})"

# ============================================================================
# LinearLR
# ============================================================================

class LinearLR:
    """Linear learning rate scheduler.

    Linearly interpolates between start_factor * base_lr and
    end_factor * base_lr over total_iters steps.

    Example:
        val sched = LinearLR.create(0.01, 1.0, 0.1, 100)
        sched.step()
        val lr = sched.get_lr()
    """
    base_lr: f64
    start_factor: f64
    end_factor: f64
    total_iters: i64
    current_step: i64

    static fn create(base_lr: f64, start_factor: f64, end_factor: f64, total_iters: i64) -> LinearLR:
        """Create LinearLR scheduler.

        Args:
            base_lr - Base learning rate
            start_factor - Starting multiplicative factor
            end_factor - Ending multiplicative factor
            total_iters - Total number of iterations for interpolation

        Returns:
            LinearLR instance
        """
        LinearLR(
            base_lr: base_lr,
            start_factor: start_factor,
            end_factor: end_factor,
            total_iters: total_iters,
            current_step: 0
        )

    me step():
        """Advance scheduler by one step."""
        self.current_step = self.current_step + 1

    fn get_lr() -> f64:
        """Get current learning rate.

        Returns:
            Linearly interpolated LR based on current step
        """
        if self.current_step >= self.total_iters:
            self.base_lr * self.end_factor
        else:
            val progress = self.current_step * 1.0 / self.total_iters
            val factor = self.start_factor + (self.end_factor - self.start_factor) * progress
            self.base_lr * factor

    fn to_string() -> text:
        """String representation."""
        "LinearLR(base_lr={self.base_lr}, start={self.start_factor}, end={self.end_factor}, iters={self.total_iters}, step={self.current_step})"

# ============================================================================
# OneCycleLR
# ============================================================================

class OneCycleLR:
    """One Cycle learning rate scheduler.

    Three phases:
    - Phase 1 (0-30%): Warmup from min_lr to max_lr
    - Phase 2 (30-70%): Cosine decay from max_lr to min_lr
    - Phase 3 (70-100%): Final decay from min_lr to min_lr/10

    Example:
        val sched = OneCycleLR.create(0.01, 100, 0.0001)
        sched.step()
        val lr = sched.get_lr()
    """
    max_lr: f64
    total_steps: i64
    current_step: i64
    min_lr: f64

    static fn create(max_lr: f64, total_steps: i64, min_lr: f64) -> OneCycleLR:
        """Create OneCycleLR scheduler.

        Args:
            max_lr - Maximum learning rate (peak of cycle)
            total_steps - Total number of training steps
            min_lr - Minimum learning rate

        Returns:
            OneCycleLR instance
        """
        OneCycleLR(
            max_lr: max_lr,
            total_steps: total_steps,
            current_step: 0,
            min_lr: min_lr
        )

    me step():
        """Advance scheduler by one step."""
        self.current_step = self.current_step + 1

    fn get_lr() -> f64:
        """Get current learning rate based on one-cycle policy.

        Returns:
            Current learning rate
        """
        val progress = self.current_step * 1.0 / self.total_steps

        # Phase 1: Warmup (0% to 30%)
        if progress < 0.3:
            val phase_progress = progress / 0.3
            self.min_lr + (self.max_lr - self.min_lr) * phase_progress

        # Phase 2: Cosine decay (30% to 70%)
        else:
            if progress < 0.7:
                val phase_progress = (progress - 0.3) / 0.4
                # Cosine from max_lr to min_lr
                val pi = 3.14159265358979
                val cos_val = _sched_cos(pi * phase_progress)
                self.min_lr + (self.max_lr - self.min_lr) * (1.0 + cos_val) / 2.0

            # Phase 3: Final decay (70% to 100%)
            else:
                val phase_progress = (progress - 0.7) / 0.3
                val final_lr = self.min_lr / 10.0
                self.min_lr + (final_lr - self.min_lr) * phase_progress

    fn to_string() -> text:
        """String representation."""
        "OneCycleLR(max_lr={self.max_lr}, total_steps={self.total_steps}, min_lr={self.min_lr}, step={self.current_step})"

# ============================================================================
# Factory Functions
# ============================================================================

fn reduce_lr_on_plateau_create(initial_lr: f64, factor: f64, patience: i64, min_lr: f64) -> ReduceLROnPlateau:
    """Create ReduceLROnPlateau scheduler (factory function)."""
    ReduceLROnPlateau.create(initial_lr, factor, patience, min_lr)

fn cyclic_lr_create(base_lr: f64, max_lr: f64, step_size_up: i64) -> CyclicLR:
    """Create CyclicLR scheduler (factory function)."""
    CyclicLR.create(base_lr, max_lr, step_size_up)

fn warmup_lr_create(target_lr: f64, warmup_steps: i64) -> WarmupLR:
    """Create WarmupLR scheduler (factory function)."""
    WarmupLR.create(target_lr, warmup_steps)

fn linear_lr_create(base_lr: f64, start_factor: f64, end_factor: f64, total_iters: i64) -> LinearLR:
    """Create LinearLR scheduler (factory function)."""
    LinearLR.create(base_lr, start_factor, end_factor, total_iters)

fn one_cycle_lr_create(max_lr: f64, total_steps: i64, min_lr: f64) -> OneCycleLR:
    """Create OneCycleLR scheduler (factory function)."""
    OneCycleLR.create(max_lr, total_steps, min_lr)

# ============================================================================
# Exports
# ============================================================================

export ReduceLROnPlateau, CyclicLR, WarmupLR, LinearLR, OneCycleLR
export reduce_lr_on_plateau_create, cyclic_lr_create, warmup_lr_create
export linear_lr_create, one_cycle_lr_create
export _sched_cos
