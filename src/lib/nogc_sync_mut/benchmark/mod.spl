# Performance Benchmarking Utilities (Facade)
#
# Comprehensive benchmarking framework for the Simple language.
# Pure Simple implementation with real nanosecond timing via FFI.

# Import all submodules
import types
import measure
import stats
import compare
import report
import utilities
import benchmark_config
import benchmark_stats

# Re-export from types.spl
export create_benchmark_suite, add_benchmark, configure_suite
export BENCHMARK_DEFAULT_WARMUP, BENCHMARK_DEFAULT_ITERATIONS
export BENCHMARK_DEFAULT_OUTLIER_THRESHOLD, BENCHMARK_REGRESSION_THRESHOLD

# Re-export from measure.spl
export get_timestamp, time_function, time_iterations

# Re-export from stats.spl
export calculate_mean, calculate_median, calculate_variance, calculate_stddev
export calculate_sum, calculate_stats
export percentile, percentile_50, percentile_95, percentile_99
export quartiles, interquartile_range
export min_value, max_value, range_value, coefficient_of_variation
export is_outlier, detect_outliers, remove_outliers, outlier_count
export sort_values
export get_stat_mean, get_stat_median, get_stat_stddev
export get_stat_min, get_stat_max, get_stat_p95, get_stat_p99, get_stat_cv

# Re-export from compare.spl
export compare_benchmarks, speedup_ratio, is_significant_difference
export compare_to_baseline
export detect_regression, regression_percentage, is_performance_improvement

# Re-export from report.spl
export format_time, format_percentage, format_result, format_comparison
export format_stats_row
export print_result, print_comparison, print_summary
export print_comparison_table, print_regression_report
export export_result_text, export_comparison_text, export_suite_text
export export_result_csv_header, export_result_csv_row, export_results_csv

# Re-export from utilities.spl
export create_result, get_result_stats, get_result_name
export get_result_timings, get_result_iterations
export benchmark_count, get_benchmark_names
export find_fastest_result, find_slowest_result
export create_suite_summary
export estimate_memory_usage, track_allocations, memory_overhead

# Re-export from benchmark_config.spl
export BenchmarkConfig

# Re-export from benchmark_stats.spl
export BenchmarkStats

# =============================================================================
# Top-level benchmark function
# =============================================================================

use benchmark_config.{BenchmarkConfig, benchmark_config_default}
use benchmark_stats.{BenchmarkStats, calculate_stats as calc_bstats}
use measure.{get_timestamp, time_function}

fn benchmark(name: text, fn_to_bench: fn(), config: BenchmarkConfig?) -> BenchmarkStats:
    val cfg = config ?? benchmark_config_default()

    # Warmup
    var w = 0
    while w < cfg.warmup_iterations:
        fn_to_bench()
        w = w + 1

    # Collect samples (each sample = average of measurement_iterations runs)
    var samples: [f64] = []
    var s = 0
    while s < cfg.sample_size:
        var total_ns: i64 = 0
        var m = 0
        while m < cfg.measurement_iterations:
            total_ns = total_ns + time_function(fn_to_bench)
            m = m + 1
        val avg_ns = total_ns.to_f64() / cfg.measurement_iterations.to_f64()
        samples.push(avg_ns)
        s = s + 1

    calc_bstats(samples, cfg.outlier_threshold)

export benchmark
