# Pure Simple Tensor F64 Operations
# Concrete implementations for TensorF64 (interpreter-compatible)

use lib.pure.tensor_f64 (TensorF64, from_data)

# Element-wise operations

fn add(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise addition."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] + b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn sub(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise subtraction."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] - b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn mul(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise multiplication."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] * b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn mul_scalar(t: TensorF64, scalar: f64) -> TensorF64:
    """Multiply tensor by scalar."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        result_data.push(t.data[i] * scalar)
        i = i + 1
    from_data(result_data, t.shape)

fn add_scalar(t: TensorF64, scalar: f64) -> TensorF64:
    """Add scalar to tensor."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        result_data.push(t.data[i] + scalar)
        i = i + 1
    from_data(result_data, t.shape)

# Matrix operations

fn matmul(a: TensorF64, b: TensorF64) -> TensorF64:
    """Matrix multiplication: C = A @ B"""
    val m = a.shape[0]
    val k = a.shape[1]
    val n = b.shape[1]

    var result_data: [f64] = []
    var i = 0
    while i < m * n:
        result_data.push(0.0)
        i = i + 1

    var row = 0
    while row < m:
        var col = 0
        while col < n:
            var sum = 0.0
            var inner = 0
            while inner < k:
                val a_idx = row * k + inner
                val b_idx = inner * n + col
                sum = sum + a.data[a_idx] * b.data[b_idx]
                inner = inner + 1
            val c_idx = row * n + col
            result_data[c_idx] = sum
            col = col + 1
        row = row + 1

    from_data(result_data, [m, n])

fn transpose(a: TensorF64) -> TensorF64:
    """Transpose a 2D matrix."""
    if a.shape.len() != 2:
        # For non-2D tensors, return as-is (could error instead)
        return a

    val rows = a.shape[0]
    val cols = a.shape[1]

    var result_data: [f64] = []
    var i = 0
    while i < rows * cols:
        result_data.push(0.0)
        i = i + 1

    var row = 0
    while row < rows:
        var col = 0
        while col < cols:
            val src_idx = row * cols + col
            val dst_idx = col * rows + row
            result_data[dst_idx] = a.data[src_idx]
            col = col + 1
        row = row + 1

    from_data(result_data, [cols, rows])

# Activation functions

fn relu(x: TensorF64) -> TensorF64:
    """ReLU activation: max(0, x)"""
    var result_data: [f64] = []
    for v in x.data:
        val activated = if v > 0.0: v else: 0.0
        result_data.push(activated)
    from_data(result_data, x.shape)

fn sigmoid(x: TensorF64) -> TensorF64:
    """Sigmoid: 1 / (1 + exp(-x))

    Uses approximation for simplicity.
    """
    var result_data: [f64] = []
    for v in x.data:
        # Approximation: 1 / (1 + exp(-x)) â‰ˆ 0.5 + 0.25*x for small x
        val sig_val = 0.5 + 0.25 * v
        val clamped = if sig_val < 0.0: 0.0 elif sig_val > 1.0: 1.0 else: sig_val
        result_data.push(clamped)
    from_data(result_data, x.shape)

fn tanh(x: TensorF64) -> TensorF64:
    """Tanh activation (linear approximation)"""
    var result_data: [f64] = []
    for v in x.data:
        val tanh_val = v * 0.5
        val clamped = if tanh_val < -1.0: -1.0 elif tanh_val > 1.0: 1.0 else: tanh_val
        result_data.push(clamped)
    from_data(result_data, x.shape)

export add, sub, mul, mul_scalar, add_scalar
export matmul, transpose
export relu, sigmoid, tanh
