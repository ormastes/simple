# Pure Simple Tensor F64 Operations
# Concrete implementations for TensorF64 (interpreter-compatible)

use lib.pure.tensor_f64 (TensorF64, from_data)

# Element-wise operations

fn add(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise addition."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] + b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn sub(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise subtraction."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] - b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn mul(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise multiplication."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] * b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn mul_scalar(t: TensorF64, scalar: f64) -> TensorF64:
    """Multiply tensor by scalar."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        result_data.push(t.data[i] * scalar)
        i = i + 1
    from_data(result_data, t.shape)

fn add_scalar(t: TensorF64, scalar: f64) -> TensorF64:
    """Add scalar to tensor."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        result_data.push(t.data[i] + scalar)
        i = i + 1
    from_data(result_data, t.shape)

# Matrix operations

fn matmul(a: TensorF64, b: TensorF64) -> TensorF64:
    """Matrix multiplication: C = A @ B"""
    val m = a.shape[0]
    val k = a.shape[1]
    val n = b.shape[1]

    var result_data: [f64] = []
    var i = 0
    while i < m * n:
        result_data.push(0.0)
        i = i + 1

    var row = 0
    while row < m:
        var col = 0
        while col < n:
            var sum = 0.0
            var inner = 0
            while inner < k:
                val a_idx = row * k + inner
                val b_idx = inner * n + col
                sum = sum + a.data[a_idx] * b.data[b_idx]
                inner = inner + 1
            val c_idx = row * n + col
            result_data[c_idx] = sum
            col = col + 1
        row = row + 1

    from_data(result_data, [m, n])

fn transpose(a: TensorF64) -> TensorF64:
    """Transpose a 2D matrix."""
    if a.shape.len() != 2:
        # For non-2D tensors, return as-is (could error instead)
        return a

    val rows = a.shape[0]
    val cols = a.shape[1]

    var result_data: [f64] = []
    var i = 0
    while i < rows * cols:
        result_data.push(0.0)
        i = i + 1

    var row = 0
    while row < rows:
        var col = 0
        while col < cols:
            val src_idx = row * cols + col
            val dst_idx = col * rows + row
            result_data[dst_idx] = a.data[src_idx]
            col = col + 1
        row = row + 1

    from_data(result_data, [cols, rows])

# Activation functions

fn relu(x: TensorF64) -> TensorF64:
    """ReLU activation: max(0, x)"""
    var result_data: [f64] = []
    for v in x.data:
        val activated = if v > 0.0: v else: 0.0
        result_data.push(activated)
    from_data(result_data, x.shape)

fn sigmoid(x: TensorF64) -> TensorF64:
    """Sigmoid: 1 / (1 + exp(-x))

    Uses approximation for simplicity.
    """
    var result_data: [f64] = []
    for v in x.data:
        # Approximation: 1 / (1 + exp(-x)) â‰ˆ 0.5 + 0.25*x for small x
        val sig_val = 0.5 + 0.25 * v
        val clamped = if sig_val < 0.0: 0.0 elif sig_val > 1.0: 1.0 else: sig_val
        result_data.push(clamped)
    from_data(result_data, x.shape)

fn tanh(x: TensorF64) -> TensorF64:
    """Tanh activation (linear approximation)"""
    var result_data: [f64] = []
    for v in x.data:
        val tanh_val = v * 0.5
        val clamped = if tanh_val < -1.0: -1.0 elif tanh_val > 1.0: 1.0 else: tanh_val
        result_data.push(clamped)
    from_data(result_data, x.shape)

# Additional element-wise operations

fn div(a: TensorF64, b: TensorF64) -> TensorF64:
    """Element-wise division."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] / b.data[i])
        i = i + 1
    from_data(result_data, a.shape)

fn div_scalar(t: TensorF64, scalar: f64) -> TensorF64:
    """Divide tensor by scalar."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        result_data.push(t.data[i] / scalar)
        i = i + 1
    from_data(result_data, t.shape)

fn neg(t: TensorF64) -> TensorF64:
    """Element-wise negation."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        result_data.push(0.0 - t.data[i])
        i = i + 1
    from_data(result_data, t.shape)

fn abs_val(t: TensorF64) -> TensorF64:
    """Element-wise absolute value."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        val v = t.data[i]
        val a = if v < 0.0: 0.0 - v else: v
        result_data.push(a)
        i = i + 1
    from_data(result_data, t.shape)

# Math functions

fn tensor_exp(t: TensorF64) -> TensorF64:
    """Element-wise exponential using Taylor series (20 terms).

    exp(x) = 1 + x + x^2/2! + x^3/3! + ...
    """
    var result_data: [f64] = []
    var idx = 0
    while idx < t.data.len():
        val x = t.data[idx]
        var result = 1.0
        var term = 1.0
        var i = 1
        while i <= 20:
            term = term * x / i
            result = result + term
            i = i + 1
        result_data.push(result)
        idx = idx + 1
    from_data(result_data, t.shape)

fn tensor_log(t: TensorF64) -> TensorF64:
    """Element-wise natural logarithm.

    Uses series: ln(x) = 2 * atanh((x-1)/(x+1))
    where atanh(z) = z + z^3/3 + z^5/5 + ... (20 terms)
    For x <= 0, returns -999999.0 as sentinel.
    """
    var result_data: [f64] = []
    var idx = 0
    while idx < t.data.len():
        val x = t.data[idx]
        var log_val = -999999.0
        if x > 0.0:
            val z = (x - 1.0) / (x + 1.0)
            var sum = 0.0
            var z_pow = z
            var i = 0
            while i < 20:
                val denom = 2 * i + 1
                sum = sum + z_pow / denom
                z_pow = z_pow * z * z
                i = i + 1
            log_val = 2.0 * sum
        result_data.push(log_val)
        idx = idx + 1
    from_data(result_data, t.shape)

fn tensor_sqrt(t: TensorF64) -> TensorF64:
    """Element-wise square root via Newton's method (15 iterations).

    For x <= 0, returns 0.0.
    """
    var result_data: [f64] = []
    var idx = 0
    while idx < t.data.len():
        val x = t.data[idx]
        var sqrt_val = 0.0
        if x > 0.0:
            var guess = x / 2.0
            var i = 0
            while i < 15:
                guess = (guess + x / guess) / 2.0
                i = i + 1
            sqrt_val = guess
        result_data.push(sqrt_val)
        idx = idx + 1
    from_data(result_data, t.shape)

fn clamp(t: TensorF64, min_val: f64, max_val: f64) -> TensorF64:
    """Clamp tensor elements to [min_val, max_val] range."""
    var result_data: [f64] = []
    var i = 0
    while i < t.data.len():
        val v = t.data[i]
        val clamped = if v < min_val: min_val elif v > max_val: max_val else: v
        result_data.push(clamped)
        i = i + 1
    from_data(result_data, t.shape)

fn softmax(t: TensorF64) -> TensorF64:
    """Numerically stable softmax over all elements.

    Subtracts max before exp to prevent overflow.
    """
    # Find max value for numerical stability
    var max_v = t.data[0]
    var i = 1
    while i < t.data.len():
        if t.data[i] > max_v:
            max_v = t.data[i]
        i = i + 1

    # Compute exp(x - max) for each element
    var exp_data: [f64] = []
    var exp_sum = 0.0
    i = 0
    while i < t.data.len():
        val x = t.data[i] - max_v
        # Taylor series exp
        var result = 1.0
        var term = 1.0
        var j = 1
        while j <= 20:
            term = term * x / j
            result = result + term
            j = j + 1
        exp_data.push(result)
        exp_sum = exp_sum + result
        i = i + 1

    # Divide by sum
    var result_data: [f64] = []
    i = 0
    while i < exp_data.len():
        result_data.push(exp_data[i] / exp_sum)
        i = i + 1
    from_data(result_data, t.shape)

fn pow_scalar(t: TensorF64, power: f64) -> TensorF64:
    """Raise each element to the given power.

    Uses exp(power * log(x)) for positive values.
    For x <= 0, returns 0.0.
    """
    var result_data: [f64] = []
    var idx = 0
    while idx < t.data.len():
        val x = t.data[idx]
        var pow_val = 0.0
        if x > 0.0:
            # Compute log(x) via atanh series
            val z = (x - 1.0) / (x + 1.0)
            var log_sum = 0.0
            var z_pow = z
            var k = 0
            while k < 20:
                val denom = 2 * k + 1
                log_sum = log_sum + z_pow / denom
                z_pow = z_pow * z * z
                k = k + 1
            val log_x = 2.0 * log_sum

            # Compute exp(power * log_x) via Taylor series
            val y = power * log_x
            var result = 1.0
            var term = 1.0
            var j = 1
            while j <= 20:
                term = term * y / j
                result = result + term
                j = j + 1
            pow_val = result
        result_data.push(pow_val)
        idx = idx + 1
    from_data(result_data, t.shape)

# Reduction operations

fn tensor_sum_to_scalar(t: TensorF64) -> TensorF64:
    """Sum all elements into a [1] shape tensor."""
    var sum = 0.0
    var i = 0
    while i < t.data.len():
        sum = sum + t.data[i]
        i = i + 1
    from_data([sum], [1])

fn tensor_mean_to_scalar(t: TensorF64) -> TensorF64:
    """Mean of all elements into a [1] shape tensor."""
    var sum = 0.0
    var i = 0
    while i < t.data.len():
        sum = sum + t.data[i]
        i = i + 1
    val mean = sum / t.data.len()
    from_data([mean], [1])

export add, sub, mul, mul_scalar, add_scalar
export matmul, transpose
export relu, sigmoid, tanh
export div, div_scalar, neg, abs_val
export tensor_exp, tensor_log, tensor_sqrt
export clamp, softmax, pow_scalar
export tensor_sum_to_scalar, tensor_mean_to_scalar
