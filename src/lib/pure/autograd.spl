# Pure Simple Autograd System
# Automatic differentiation with reverse-mode (backpropagation)
# REDESIGNED: No inline functions, uses operation enum instead

export Tensor, backward
export tensor_add, tensor_sub, tensor_mul, tensor_matmul, tensor_relu
export tensor_sum, tensor_mean, tensor_mul_scalar
export detach, requires_grad

use lib.pure.tensor (PureTensor, zeros_like, ones_like)
use lib.pure.tensor_ops (add, sub, mul, matmul, transpose, mul_scalar, add_scalar)

# ============================================================================
# Operation Type (replaces function pointers)
# ============================================================================

enum OpType:
    Add
    Sub
    Mul
    MatMul
    Relu
    Sum
    Mean
    MulScalar

# ============================================================================
# Tensor with Gradient Tracking
# ============================================================================

class Tensor:
    value: PureTensor<f64>
    grad: PureTensor<f64>?
    requires_grad: bool
    op_type: OpType?
    inputs: [Tensor]?
    op_name: text

    static fn from_data(data: [f64], shape: [i64], requires_grad: bool = false) -> Tensor:
        Tensor(
            value: PureTensor.from_data(data, shape),
            grad: nil,
            requires_grad: requires_grad,
            op_type: nil,
            inputs: nil,
            op_name: "leaf"
        )

    static fn from_value(value: PureTensor<f64>, requires_grad: bool = false) -> Tensor:
        Tensor(
            value: value,
            grad: nil,
            requires_grad: requires_grad,
            op_type: nil,
            inputs: nil,
            op_name: "leaf"
        )

    static fn zeros(shape: [i64], requires_grad: bool = false) -> Tensor:
        Tensor.from_value(PureTensor.zeros(shape), requires_grad)

    static fn ones(shape: [i64], requires_grad: bool = false) -> Tensor:
        Tensor.from_value(PureTensor.ones(shape), requires_grad)

    me zero_grad():
        self.grad = nil

    me backward_step(grad_output: PureTensor<f64>):
        if self.requires_grad:
            if self.grad.?:
                self.grad = Some(add(self.grad.unwrap(), grad_output))
            else:
                self.grad = Some(grad_output)

    fn shape() -> [i64]:
        self.value.shape

    fn numel() -> i64:
        self.value.numel()

    fn to_string() -> text:
        if self.requires_grad:
            "Tensor({self.value.to_string()}, grad_fn={self.op_name})"
        else:
            "Tensor({self.value.to_string()})"


# ============================================================================
# Backward Pass
# ============================================================================

fn backward(t: Tensor):
    val grad_output = PureTensor.ones(t.shape())
    t.backward_step(grad_output)

    if t.inputs.? and t.op_type.?:
        propagate_grads(t, grad_output)


fn propagate_grads(t: Tensor, grad_out: PureTensor<f64>):
    val inputs = t.inputs.unwrap()
    val op = t.op_type.unwrap()

    # Compute input gradients based on operation type
    match op:
        OpType.Add:
            # da/dc = 1, db/dc = 1
            if inputs[0].requires_grad:
                inputs[0].backward_step(grad_out)
                if inputs[0].inputs.?:
                    propagate_grads(inputs[0], grad_out)

            if inputs[1].requires_grad:
                inputs[1].backward_step(grad_out)
                if inputs[1].inputs.?:
                    propagate_grads(inputs[1], grad_out)

        OpType.Sub:
            # da/dc = 1, db/dc = -1
            if inputs[0].requires_grad:
                inputs[0].backward_step(grad_out)
                if inputs[0].inputs.?:
                    propagate_grads(inputs[0], grad_out)

            if inputs[1].requires_grad:
                val neg_grad = mul_scalar(grad_out, -1.0)
                inputs[1].backward_step(neg_grad)
                if inputs[1].inputs.?:
                    propagate_grads(inputs[1], neg_grad)

        OpType.Mul:
            # da/dc = b, db/dc = a
            if inputs[0].requires_grad:
                val grad_a = mul(grad_out, inputs[1].value)
                inputs[0].backward_step(grad_a)
                if inputs[0].inputs.?:
                    propagate_grads(inputs[0], grad_a)

            if inputs[1].requires_grad:
                val grad_b = mul(grad_out, inputs[0].value)
                inputs[1].backward_step(grad_b)
                if inputs[1].inputs.?:
                    propagate_grads(inputs[1], grad_b)

        OpType.MatMul:
            # da/dc = grad @ b^T, db/dc = a^T @ grad
            if inputs[0].requires_grad:
                val grad_a = matmul(grad_out, transpose(inputs[1].value))
                inputs[0].backward_step(grad_a)
                if inputs[0].inputs.?:
                    propagate_grads(inputs[0], grad_a)

            if inputs[1].requires_grad:
                val grad_b = matmul(transpose(inputs[0].value), grad_out)
                inputs[1].backward_step(grad_b)
                if inputs[1].inputs.?:
                    propagate_grads(inputs[1], grad_b)

        OpType.Relu:
            # d(relu)/dx = 1 if x > 0 else 0
            var grad_x_data: [f64] = []
            var i = 0
            while i < inputs[0].value.data.len():
                if inputs[0].value.data[i] > 0.0:
                    grad_x_data.push(grad_out.data[i])
                else:
                    grad_x_data.push(0.0)
                i = i + 1

            val grad_x = PureTensor.from_data(grad_x_data, inputs[0].value.shape)
            inputs[0].backward_step(grad_x)
            if inputs[0].inputs.?:
                propagate_grads(inputs[0], grad_x)

        OpType.Sum:
            # Broadcast gradient back to input shape
            if inputs[0].requires_grad:
                inputs[0].backward_step(grad_out)
                if inputs[0].inputs.?:
                    propagate_grads(inputs[0], grad_out)

        OpType.Mean:
            # Gradient distributed evenly
            val n = inputs[0].value.data.len()
            val grad_scaled = mul_scalar(grad_out, 1.0 / n)
            if inputs[0].requires_grad:
                inputs[0].backward_step(grad_scaled)
                if inputs[0].inputs.?:
                    propagate_grads(inputs[0], grad_scaled)

        OpType.MulScalar:
            # Handled separately (stores scalar in operation context)
            pass


# ============================================================================
# Operations with Autograd
# ============================================================================

fn tensor_add(a: Tensor, b: Tensor) -> Tensor:
    val result_value = add(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Add)
    result.inputs = Some([a, b])
    result.op_name = "add"
    result


fn tensor_sub(a: Tensor, b: Tensor) -> Tensor:
    val result_value = sub(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Sub)
    result.inputs = Some([a, b])
    result.op_name = "sub"
    result


fn tensor_mul(a: Tensor, b: Tensor) -> Tensor:
    val result_value = mul(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Mul)
    result.inputs = Some([a, b])
    result.op_name = "mul"
    result


fn tensor_matmul(a: Tensor, b: Tensor) -> Tensor:
    val result_value = matmul(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.MatMul)
    result.inputs = Some([a, b])
    result.op_name = "matmul"
    result


fn tensor_relu(x: Tensor) -> Tensor:
    # ReLU: max(0, x)
    var result_data: [f64] = []
    for v in x.value.data:
        result_data.push(if v > 0.0: v else: 0.0)

    val result_value = PureTensor.from_data(result_data, x.value.shape)

    if not x.requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Relu)
    result.inputs = Some([x])
    result.op_name = "relu"
    result


fn tensor_sum(x: Tensor) -> Tensor:
    var total = 0.0
    for v in x.value.data:
        total = total + v

    val result_value = PureTensor.from_data([total], [1])

    if not x.requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Sum)
    result.inputs = Some([x])
    result.op_name = "sum"
    result


fn tensor_mean(x: Tensor) -> Tensor:
    var total = 0.0
    for v in x.value.data:
        total = total + v

    val mean_val = total / x.value.data.len()
    val result_value = PureTensor.from_data([mean_val], [1])

    if not x.requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Mean)
    result.inputs = Some([x])
    result.op_name = "mean"
    result


fn tensor_mul_scalar(x: Tensor, scalar: f64) -> Tensor:
    val result_value = mul_scalar(x.value, scalar)

    if not x.requires_grad:
        return Tensor.from_value(result_value, requires_grad: false)

    var result = Tensor.from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.MulScalar)
    result.inputs = Some([x])
    result.op_name = "mul_scalar"
    result


fn detach(x: Tensor) -> Tensor:
    Tensor.from_value(x.value, requires_grad: false)


fn requires_grad(x: Tensor, requires_grad: bool) -> Tensor:
    Tensor.from_value(x.value, requires_grad)
