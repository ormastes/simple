# Pure Simple Embedding Layer
#
# Implements learnable embedding lookup table
# Maps integer indices to dense vectors
# Zero external dependencies

use std.pure.tensor_f64 (TensorF64, from_data, zeros, randn)
use std.pure.autograd (Tensor, tensor_from_value)

# ============================================================================
# Embedding Layer
# ============================================================================

class Embedding:
    """Embedding lookup table: indices -> dense vectors

    Args:
        num_embeddings: Size of the embedding dictionary (vocabulary size)
        embedding_dim: Dimension of each embedding vector

    Shape:
        Input: (batch_size,) or (batch_size, seq_len) of integer indices
        Output: (batch_size, embedding_dim) or (batch_size, seq_len, embedding_dim)

    Attributes:
        weight: Embedding table of shape (num_embeddings, embedding_dim)

    Example:
        val emb = Embedding.create(num_embeddings: 1000, embedding_dim: 128)
        val indices = tensor_from_data([5.0, 10.0, 3.0], [3], requires_grad: false)
        val output = emb.forward(indices)  # Shape: [3, 128]
    """
    weight: Tensor
    num_embeddings: i64
    embedding_dim: i64
    training: bool

    static fn create(num_embeddings: i64, embedding_dim: i64) -> Embedding:
        """Create Embedding layer with random initialization.

        Weight initialization: randn([num_embeddings, embedding_dim]) * 0.1
        """
        # Initialize embedding table with small random values
        val weight_value = randn([num_embeddings, embedding_dim])

        # Scale down initial weights
        var scaled_data: [f64] = []
        for v in weight_value.data:
            scaled_data.push(v * 0.1)

        val weight_tensor = tensor_from_value(
            from_data(scaled_data, [num_embeddings, embedding_dim]),
            requires_grad: true
        )

        Embedding(
            weight: weight_tensor,
            num_embeddings: num_embeddings,
            embedding_dim: embedding_dim,
            training: true
        )

    fn forward(indices: Tensor) -> Tensor:
        """Forward pass: lookup embeddings by indices.

        Args:
            indices: Tensor of integer indices (as f64)
                    Shape: (batch_size,) or (batch_size, seq_len)

        Returns:
            Tensor of embeddings
            Shape: (batch_size, embedding_dim) or (batch_size, seq_len, embedding_dim)
        """
        val input_shape = indices.shape()
        val batch_size = input_shape[0]

        # Handle 1D input: (batch_size,) -> (batch_size, embedding_dim)
        if input_shape.len() == 1:
            var output_data: [f64] = []

            # For each index in batch
            var i = 0
            while i < batch_size:
                # Get index (convert from f64 to i64)
                val idx = indices.value.data[i]
                val idx_int = idx

                # Bounds check
                if idx_int < 0.0 or idx_int >= self.num_embeddings:
                    # Out of bounds - return zeros
                    var j = 0
                    while j < self.embedding_dim:
                        output_data.push(0.0)
                        j = j + 1
                else:
                    # Lookup embedding vector
                    var j = 0
                    while j < self.embedding_dim:
                        val offset = idx_int * self.embedding_dim + j
                        output_data.push(self.weight.value.data[offset])
                        j = j + 1

                i = i + 1

            val output_value = from_data(output_data, [batch_size, self.embedding_dim])
            tensor_from_value(output_value, requires_grad: self.training)

        # Handle 2D input: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)
        else:
            val seq_len = input_shape[1]
            var output_data: [f64] = []

            # For each item in batch
            var batch_idx = 0
            while batch_idx < batch_size:
                # For each position in sequence
                var seq_idx = 0
                while seq_idx < seq_len:
                    # Get index
                    val idx = indices.value.data[batch_idx * seq_len + seq_idx]
                    val idx_int = idx

                    # Bounds check
                    if idx_int < 0.0 or idx_int >= self.num_embeddings:
                        # Out of bounds - return zeros
                        var j = 0
                        while j < self.embedding_dim:
                            output_data.push(0.0)
                            j = j + 1
                    else:
                        # Lookup embedding vector
                        var j = 0
                        while j < self.embedding_dim:
                            val offset = idx_int * self.embedding_dim + j
                            output_data.push(self.weight.value.data[offset])
                            j = j + 1

                    seq_idx = seq_idx + 1

                batch_idx = batch_idx + 1

            val output_value = from_data(output_data, [batch_size, seq_len, self.embedding_dim])
            tensor_from_value(output_value, requires_grad: self.training)

    fn backward(grad_output: Tensor):
        """Backward pass: scatter-add gradients to embedding table.

        For each index that was looked up in forward(), add the corresponding
        gradient to that row of the weight table.

        Note: Full backward pass requires autograd integration.
        This is a placeholder for the gradient computation logic.
        """
        # Backward pass logic would scatter gradients back to weight.grad
        # based on which indices were accessed during forward pass.
        # This requires tracking indices and accumulating gradients.
        # Implementation blocked by autograd limitations.
        pass_do_nothing

    fn parameters() -> [Tensor]:
        """Get list of parameters (just weight)."""
        [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Embedding(num_embeddings={self.num_embeddings}, embedding_dim={self.embedding_dim})"

# ============================================================================
# Factory Function (for backwards compatibility)
# ============================================================================

fn embedding_create(num_embeddings: i64, embedding_dim: i64) -> Embedding:
    """Create Embedding layer (legacy API)."""
    Embedding.create(num_embeddings, embedding_dim)

# ============================================================================
# Exports
# ============================================================================

export Embedding, embedding_create
