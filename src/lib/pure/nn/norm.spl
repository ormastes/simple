# Pure Simple Normalization Layers
#
# Implements BatchNorm2d, GroupNorm, and InstanceNorm2d
# Zero external dependencies
#
# NOTE: This module uses generics (PureTensor<f64>) and will only work
# in compiled mode, not in the interpreter (runtime parser limitation).

use std.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros, tensor_ones}

# ============================================================================
# Math Helpers
# ============================================================================

fn _norm_sqrt(x: f64) -> f64:
    """Square root via Newton's method."""
    if x <= 0.0:
        return 0.0
    var guess = x
    var i = 0
    while i < 20:
        guess = (guess + x / guess) / 2.0
        i = i + 1
    guess

# ============================================================================
# BatchNorm2d
# ============================================================================

class BatchNorm2d:
    """2D Batch Normalization over channel dimension.

    Normalizes input across the batch dimension for each channel.
    During training: uses batch statistics.
    During evaluation: uses running statistics.

    Input shape: [batch_size, num_features, height, width]
    Output shape: same as input

    Args:
        num_features - Number of channels (C)
        eps - Small constant for numerical stability
        momentum - Running mean/variance momentum factor
    """
    num_features: i64
    eps: f64
    momentum: f64
    training: bool
    # Learnable parameters
    weight: PureTensor<f64>    # gamma (scale), shape [num_features]
    bias: PureTensor<f64>      # beta (shift), shape [num_features]
    # Running statistics
    running_mean: PureTensor<f64>
    running_var: PureTensor<f64>

    static fn create(num_features: i64, eps: f64, momentum: f64) -> BatchNorm2d:
        """Create BatchNorm2d layer.

        Args:
            num_features - Number of channels
            eps - Epsilon for stability (typical: 0.00001)
            momentum - Momentum for running stats (typical: 0.1)

        Returns:
            BatchNorm2d layer instance
        """
        BatchNorm2d(
            num_features: num_features,
            eps: eps,
            momentum: momentum,
            training: true,
            weight: tensor_ones([num_features]),
            bias: tensor_zeros([num_features]),
            running_mean: tensor_zeros([num_features]),
            running_var: tensor_ones([num_features])
        )

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass: normalize across batch dimension per channel.

        Args:
            x - Input tensor [batch, channels, height, width]

        Returns:
            Normalized tensor (same shape)
        """
        val batch_size = x.shape[0]
        val channels = x.shape[1]
        val height = x.shape[2]
        val width = x.shape[3]
        val spatial = height * width

        var result_data: [f64] = []
        # Pre-fill with zeros
        val total = batch_size * channels * spatial
        var idx = 0
        while idx < total:
            result_data.push(0.0)
            idx = idx + 1

        var c = 0
        while c < channels:
            var chan_mean = 0.0
            var chan_var = 0.0

            if self.training:
                # Compute batch mean for this channel
                var sum_val = 0.0
                var b = 0
                while b < batch_size:
                    var s = 0
                    while s < spatial:
                        val data_idx = (b * channels + c) * spatial + s
                        sum_val = sum_val + x.data[data_idx]
                        s = s + 1
                    b = b + 1

                chan_mean = sum_val / (batch_size * spatial)

                # Compute batch variance for this channel
                var var_sum = 0.0
                b = 0
                while b < batch_size:
                    var s = 0
                    while s < spatial:
                        val data_idx = (b * channels + c) * spatial + s
                        val diff = x.data[data_idx] - chan_mean
                        var_sum = var_sum + diff * diff
                        s = s + 1
                    b = b + 1

                chan_var = var_sum / (batch_size * spatial)

                # Update running statistics
                val rm = self.running_mean.data[c]
                val rv = self.running_var.data[c]
                self.running_mean.data[c] = (1.0 - self.momentum) * rm + self.momentum * chan_mean
                self.running_var.data[c] = (1.0 - self.momentum) * rv + self.momentum * chan_var
            else:
                # Use running statistics in eval mode
                chan_mean = self.running_mean.data[c]
                chan_var = self.running_var.data[c]

            # Normalize: (x - mean) / sqrt(var + eps) * weight + bias
            val inv_std = 1.0 / _norm_sqrt(chan_var + self.eps)
            val gamma = self.weight.data[c]
            val beta = self.bias.data[c]

            var b = 0
            while b < batch_size:
                var s = 0
                while s < spatial:
                    val data_idx = (b * channels + c) * spatial + s
                    val normalized = (x.data[data_idx] - chan_mean) * inv_std
                    result_data[data_idx] = normalized * gamma + beta
                    s = s + 1
                b = b + 1

            c = c + 1

        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        """Return learnable parameters: [weight, bias]."""
        [self.weight, self.bias]

    me train_mode():
        """Set layer to training mode."""
        self.training = true

    me eval_mode():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "BatchNorm2d(num_features={self.num_features}, eps={self.eps}, momentum={self.momentum})"

# ============================================================================
# GroupNorm
# ============================================================================

class GroupNorm:
    """Group Normalization.

    Divides channels into groups and normalizes within each group.
    Does not depend on batch size (works with batch_size=1).

    Input shape: [batch_size, num_channels, height, width]
    Output shape: same as input

    Args:
        num_groups - Number of groups to divide channels into
        num_channels - Number of channels (must be divisible by num_groups)
        eps - Small constant for numerical stability
    """
    num_groups: i64
    num_channels: i64
    eps: f64
    training: bool
    # Learnable parameters
    weight: PureTensor<f64>    # gamma, shape [num_channels]
    bias: PureTensor<f64>      # beta, shape [num_channels]

    static fn create(num_groups: i64, num_channels: i64, eps: f64) -> GroupNorm:
        """Create GroupNorm layer.

        Args:
            num_groups - Number of groups
            num_channels - Number of channels (must be divisible by num_groups)
            eps - Epsilon for stability (typical: 0.00001)

        Returns:
            GroupNorm layer instance
        """
        GroupNorm(
            num_groups: num_groups,
            num_channels: num_channels,
            eps: eps,
            training: true,
            weight: tensor_ones([num_channels]),
            bias: tensor_zeros([num_channels])
        )

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass: normalize within channel groups.

        Args:
            x - Input tensor [batch, channels, height, width]

        Returns:
            Normalized tensor (same shape)
        """
        val batch_size = x.shape[0]
        val channels = x.shape[1]
        val height = x.shape[2]
        val width = x.shape[3]
        val spatial = height * width
        val channels_per_group = channels / self.num_groups

        var result_data: [f64] = []
        val total = batch_size * channels * spatial
        var idx = 0
        while idx < total:
            result_data.push(0.0)
            idx = idx + 1

        var b = 0
        while b < batch_size:
            var g = 0
            while g < self.num_groups:
                # Compute mean and variance for this group
                val c_start = g * channels_per_group
                val c_end = c_start + channels_per_group

                var sum_val = 0.0
                var count = 0
                var c = c_start
                while c < c_end:
                    var s = 0
                    while s < spatial:
                        val data_idx = (b * channels + c) * spatial + s
                        sum_val = sum_val + x.data[data_idx]
                        count = count + 1
                        s = s + 1
                    c = c + 1

                val group_mean = sum_val / count

                var var_sum = 0.0
                c = c_start
                while c < c_end:
                    var s = 0
                    while s < spatial:
                        val data_idx = (b * channels + c) * spatial + s
                        val diff = x.data[data_idx] - group_mean
                        var_sum = var_sum + diff * diff
                        s = s + 1
                    c = c + 1

                val group_var = var_sum / count
                val inv_std = 1.0 / _norm_sqrt(group_var + self.eps)

                # Normalize and apply affine
                c = c_start
                while c < c_end:
                    val gamma = self.weight.data[c]
                    val beta = self.bias.data[c]
                    var s = 0
                    while s < spatial:
                        val data_idx = (b * channels + c) * spatial + s
                        val normalized = (x.data[data_idx] - group_mean) * inv_std
                        result_data[data_idx] = normalized * gamma + beta
                        s = s + 1
                    c = c + 1

                g = g + 1
            b = b + 1

        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        """Return learnable parameters: [weight, bias]."""
        [self.weight, self.bias]

    me train_mode():
        """Set layer to training mode."""
        self.training = true

    me eval_mode():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "GroupNorm(num_groups={self.num_groups}, num_channels={self.num_channels}, eps={self.eps})"

# ============================================================================
# InstanceNorm2d
# ============================================================================

class InstanceNorm2d:
    """2D Instance Normalization.

    Normalizes each channel of each sample independently.
    Equivalent to GroupNorm with num_groups = num_channels.

    Input shape: [batch_size, num_features, height, width]
    Output shape: same as input

    Args:
        num_features - Number of channels
        eps - Small constant for numerical stability
    """
    num_features: i64
    eps: f64
    training: bool
    # Learnable parameters (optional affine)
    weight: PureTensor<f64>    # gamma, shape [num_features]
    bias: PureTensor<f64>      # beta, shape [num_features]

    static fn create(num_features: i64, eps: f64) -> InstanceNorm2d:
        """Create InstanceNorm2d layer.

        Args:
            num_features - Number of channels
            eps - Epsilon for stability (typical: 0.00001)

        Returns:
            InstanceNorm2d layer instance
        """
        InstanceNorm2d(
            num_features: num_features,
            eps: eps,
            training: true,
            weight: tensor_ones([num_features]),
            bias: tensor_zeros([num_features])
        )

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass: normalize each channel of each sample.

        Args:
            x - Input tensor [batch, channels, height, width]

        Returns:
            Normalized tensor (same shape)
        """
        val batch_size = x.shape[0]
        val channels = x.shape[1]
        val height = x.shape[2]
        val width = x.shape[3]
        val spatial = height * width

        var result_data: [f64] = []
        val total = batch_size * channels * spatial
        var idx = 0
        while idx < total:
            result_data.push(0.0)
            idx = idx + 1

        var b = 0
        while b < batch_size:
            var c = 0
            while c < channels:
                # Compute mean for this (batch, channel) pair
                var sum_val = 0.0
                var s = 0
                while s < spatial:
                    val data_idx = (b * channels + c) * spatial + s
                    sum_val = sum_val + x.data[data_idx]
                    s = s + 1

                val inst_mean = sum_val / spatial

                # Compute variance
                var var_sum = 0.0
                s = 0
                while s < spatial:
                    val data_idx = (b * channels + c) * spatial + s
                    val diff = x.data[data_idx] - inst_mean
                    var_sum = var_sum + diff * diff
                    s = s + 1

                val inst_var = var_sum / spatial
                val inv_std = 1.0 / _norm_sqrt(inst_var + self.eps)

                # Normalize and apply affine
                val gamma = self.weight.data[c]
                val beta = self.bias.data[c]
                s = 0
                while s < spatial:
                    val data_idx = (b * channels + c) * spatial + s
                    val normalized = (x.data[data_idx] - inst_mean) * inv_std
                    result_data[data_idx] = normalized * gamma + beta
                    s = s + 1

                c = c + 1
            b = b + 1

        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        """Return learnable parameters: [weight, bias]."""
        [self.weight, self.bias]

    me train_mode():
        """Set layer to training mode."""
        self.training = true

    me eval_mode():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "InstanceNorm2d(num_features={self.num_features}, eps={self.eps})"

# ============================================================================
# Factory Functions
# ============================================================================

fn batchnorm2d_create(num_features: i64, eps: f64, momentum: f64) -> BatchNorm2d:
    """Create BatchNorm2d layer (factory function).

    Args:
        num_features - Number of channels
        eps - Epsilon for stability
        momentum - Running stats momentum

    Returns:
        BatchNorm2d instance
    """
    BatchNorm2d.create(num_features, eps, momentum)

fn groupnorm_create(num_groups: i64, num_channels: i64, eps: f64) -> GroupNorm:
    """Create GroupNorm layer (factory function).

    Args:
        num_groups - Number of groups
        num_channels - Number of channels
        eps - Epsilon for stability

    Returns:
        GroupNorm instance
    """
    GroupNorm.create(num_groups, num_channels, eps)

fn instancenorm2d_create(num_features: i64, eps: f64) -> InstanceNorm2d:
    """Create InstanceNorm2d layer (factory function).

    Args:
        num_features - Number of channels
        eps - Epsilon for stability

    Returns:
        InstanceNorm2d instance
    """
    InstanceNorm2d.create(num_features, eps)

# ============================================================================
# Exports
# ============================================================================

export BatchNorm2d, GroupNorm, InstanceNorm2d
export batchnorm2d_create, groupnorm_create, instancenorm2d_create
