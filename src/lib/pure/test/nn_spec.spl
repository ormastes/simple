# Tests for Pure Simple Neural Network Layers

use std.pure.tensor (PureTensor)
use std.pure.autograd (Tensor, backward, tensor_mean)
use std.pure.nn (
    Linear, ReLU, Sigmoid, Tanh, Softmax, Dropout,
    Sequential, count_parameters, zero_grad
)

describe "Neural Network Layers":
    describe "Linear Layer":
        it "creates linear layer with correct shapes":
            val layer = Linear.create(10, 5, bias: true)
            assert layer.in_features == 10
            assert layer.out_features == 5
            assert layer.weight.shape() == [5, 10]
            assert layer.bias.?.shape() == [5]

        it "creates linear layer without bias":
            val layer = Linear.create(10, 5, bias: false)
            assert layer.weight.shape() == [5, 10]
            assert not layer.bias.?

        it "performs forward pass":
            val layer = Linear.create(3, 2, bias: true)
            val input = Tensor.from_data([1.0, 2.0, 3.0], [1, 3], requires_grad: false)
            val output = layer.forward(input)

            assert output.shape() == [1, 2]

        it "returns correct parameters with bias":
            val layer = Linear.create(10, 5, bias: true)
            val params = layer.parameters()

            assert params.len() == 2  # weight + bias
            assert params[0].shape() == [5, 10]  # weight
            assert params[1].shape() == [5]       # bias

        it "returns correct parameters without bias":
            val layer = Linear.create(10, 5, bias: false)
            val params = layer.parameters()

            assert params.len() == 1  # only weight
            assert params[0].shape() == [5, 10]

        it "switches between train and eval modes":
            val layer = Linear.create(10, 5)
            assert layer.training == true

            layer.eval()
            assert layer.training == false

            layer.train()
            assert layer.training == true

        it "has string representation":
            val layer = Linear.create(784, 128, bias: true)
            val s = layer.to_string()
            assert s.contains("Linear")
            assert s.contains("784")
            assert s.contains("128")

    describe "ReLU Layer":
        it "creates ReLU layer":
            val relu = ReLU.create()
            assert relu.training == true

        it "applies ReLU activation":
            val relu = ReLU.create()
            val input = Tensor.from_data([-2.0, -1.0, 0.0, 1.0, 2.0], [5], requires_grad: true)
            val output = relu.forward(input)

            # ReLU: max(0, x)
            assert output.value.data[0] == 0.0  # -2 -> 0
            assert output.value.data[1] == 0.0  # -1 -> 0
            assert output.value.data[2] == 0.0  #  0 -> 0
            assert output.value.data[3] == 1.0  #  1 -> 1
            assert output.value.data[4] == 2.0  #  2 -> 2

        it "has no parameters":
            val relu = ReLU.create()
            val params = relu.parameters()
            assert params.len() == 0

        it "has string representation":
            val relu = ReLU.create()
            val s = relu.to_string()
            assert s == "ReLU()"

    describe "Sigmoid Layer":
        it "creates Sigmoid layer":
            val sigmoid = Sigmoid.create()
            assert sigmoid.training == true

        it "applies sigmoid activation":
            val sigmoid = Sigmoid.create()
            val input = Tensor.from_data([0.0], [1], requires_grad: false)
            val output = sigmoid.forward(input)

            # sigmoid(0) = 0.5
            # Note: approximate check due to floating point
            assert output.value.data[0] > 0.49
            assert output.value.data[0] < 0.51

        it "has no parameters":
            val sigmoid = Sigmoid.create()
            assert sigmoid.parameters().len() == 0

    describe "Tanh Layer":
        it "creates Tanh layer":
            val tanh = Tanh.create()
            assert tanh.training == true

        it "applies tanh activation":
            val tanh = Tanh.create()
            val input = Tensor.from_data([0.0], [1], requires_grad: false)
            val output = tanh.forward(input)

            # tanh(0) = 0.0
            assert output.value.data[0] > -0.01
            assert output.value.data[0] < 0.01

        it "has no parameters":
            val tanh = Tanh.create()
            assert tanh.parameters().len() == 0

    describe "Softmax Layer":
        it "creates Softmax layer":
            val softmax = Softmax.create(dim: -1)
            assert softmax.dim == -1

        it "applies softmax activation":
            val softmax = Softmax.create()
            val input = Tensor.from_data([1.0, 2.0, 3.0], [3], requires_grad: false)
            val output = softmax.forward(input)

            # Softmax: exp(x_i) / sum(exp(x_j))
            # Output should sum to 1.0
            var sum = 0.0
            for val in output.value.data:
                sum = sum + val

            assert sum > 0.99
            assert sum < 1.01

        it "has no parameters":
            val softmax = Softmax.create()
            assert softmax.parameters().len() == 0

    describe "Dropout Layer":
        it "creates Dropout layer":
            val dropout = Dropout.create(p: 0.5)
            assert dropout.p == 0.5
            assert dropout.training == true

        it "returns input unchanged in eval mode":
            val dropout = Dropout.create(p: 0.5)
            dropout.eval()

            val input = Tensor.from_data([1.0, 2.0, 3.0], [3], requires_grad: false)
            val output = dropout.forward(input)

            assert output.value.data == input.value.data

        it "has no parameters":
            val dropout = Dropout.create()
            assert dropout.parameters().len() == 0

    describe "Sequential Container":
        it "creates sequential model":
            val model = Sequential.create([
                Linear.create(10, 5),
                ReLU.create(),
                Linear.create(5, 2)
            ])

            assert model.layers.len() == 3

        it "performs forward pass through layers":
            val model = Sequential.create([
                Linear.create(3, 4),
                ReLU.create(),
                Linear.create(4, 2)
            ])

            val input = Tensor.from_data([1.0, 2.0, 3.0], [1, 3], requires_grad: false)
            val output = model.forward(input)

            # Input [1, 3] -> Linear -> [1, 4] -> ReLU -> [1, 4] -> Linear -> [1, 2]
            assert output.shape() == [1, 2]

        it "collects parameters from all layers":
            val model = Sequential.create([
                Linear.create(10, 5, bias: true),   # 2 params (W, b)
                ReLU.create(),                       # 0 params
                Linear.create(5, 2, bias: true)     # 2 params (W, b)
            ])

            val params = model.parameters()
            assert params.len() == 4  # 2 + 0 + 2

        it "sets all layers to train mode":
            val model = Sequential.create([
                Linear.create(10, 5),
                ReLU.create()
            ])

            model.eval()
            model.train()

            assert model.layers[0].training == true
            assert model.layers[1].training == true

        it "sets all layers to eval mode":
            val model = Sequential.create([
                Linear.create(10, 5),
                ReLU.create()
            ])

            model.eval()

            assert model.layers[0].training == false
            assert model.layers[1].training == false

        it "has string representation":
            val model = Sequential.create([
                Linear.create(2, 3),
                ReLU.create()
            ])

            val s = model.to_string()
            assert s.contains("Sequential")
            assert s.contains("Linear")
            assert s.contains("ReLU")

    describe "Helper Functions":
        it "counts total parameters":
            val model = Sequential.create([
                Linear.create(10, 5, bias: true),   # 10*5 + 5 = 55
                ReLU.create(),                       # 0
                Linear.create(5, 2, bias: true)     # 5*2 + 2 = 12
            ])

            val total = count_parameters(model)
            assert total == 67  # 55 + 0 + 12

        it "zeros all gradients":
            val model = Sequential.create([
                Linear.create(3, 2, bias: true)
            ])

            # Create some gradients
            val input = Tensor.from_data([1.0, 2.0, 3.0], [1, 3], requires_grad: false)
            val output = model.forward(input)
            val loss = tensor_mean(output)
            backward(loss)

            # Zero them
            zero_grad(model)

            # Check all parameters have no gradient
            for param in model.parameters():
                assert not param.grad.?

    describe "End-to-End Model":
        it "builds and runs a simple MLP":
            # 2-layer MLP: input -> hidden -> output
            val model = Sequential.create([
                Linear.create(4, 8),    # 4 -> 8
                ReLU.create(),
                Linear.create(8, 3),    # 8 -> 3
                Softmax.create()
            ])

            val input = Tensor.from_data([1.0, 2.0, 3.0, 4.0], [1, 4], requires_grad: false)
            val output = model.forward(input)

            # Output should be [1, 3] (3 classes)
            assert output.shape() == [1, 3]

            # Softmax output should sum to 1
            var sum = 0.0
            for val in output.value.data:
                sum = sum + val
            assert sum > 0.99
            assert sum < 1.01

        it "builds XOR network architecture":
            # Classic XOR problem: 2 inputs, 4 hidden, 1 output
            val model = Sequential.create([
                Linear.create(2, 4),
                ReLU.create(),
                Linear.create(4, 1),
                Sigmoid.create()
            ])

            val input = Tensor.from_data([0.0, 1.0], [1, 2], requires_grad: false)
            val output = model.forward(input)

            # Output should be [1, 1] (single output)
            assert output.shape() == [1, 1]

            # Sigmoid output should be in (0, 1)
            assert output.value.data[0] > 0.0
            assert output.value.data[0] < 1.0
