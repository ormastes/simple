# Pure Simple Training Metrics
#
# Classification and regression metrics for model evaluation
# Zero external dependencies
#
# NOTE: This module uses generics (PureTensor<f64>) and will only work
# in compiled mode, not in the interpreter (runtime parser limitation).

use std.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros}

# ============================================================================
# Classification Metrics
# ============================================================================

fn accuracy(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute classification accuracy.

    Compares predicted labels (or thresholded probabilities) to targets.
    Values > 0.5 are treated as class 1, otherwise class 0.

    Args:
        predictions - Predicted values (probabilities or labels)
        targets - Ground truth labels (0.0 or 1.0)

    Returns:
        Accuracy as fraction (0.0 to 1.0)
    """
    var correct = 0
    var total = predictions.data.len()
    var i = 0
    while i < total:
        val pred_label = if predictions.data[i] > 0.5: 1.0 else: 0.0
        val target_label = if targets.data[i] > 0.5: 1.0 else: 0.0
        if pred_label == target_label:
            correct = correct + 1
        i = i + 1
    if total == 0:
        0.0
    else:
        correct * 1.0 / total

fn precision_score(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute precision: TP / (TP + FP).

    Args:
        predictions - Predicted labels (thresholded at 0.5)
        targets - Ground truth labels (0.0 or 1.0)

    Returns:
        Precision score (0.0 to 1.0), 0.0 if no positive predictions
    """
    var true_positives = 0
    var false_positives = 0
    var i = 0
    while i < predictions.data.len():
        val pred_label = if predictions.data[i] > 0.5: 1.0 else: 0.0
        val target_label = if targets.data[i] > 0.5: 1.0 else: 0.0
        if pred_label == 1.0:
            if target_label == 1.0:
                true_positives = true_positives + 1
            else:
                false_positives = false_positives + 1
        i = i + 1
    val denominator = true_positives + false_positives
    if denominator == 0:
        0.0
    else:
        true_positives * 1.0 / denominator

fn recall_score(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute recall: TP / (TP + FN).

    Args:
        predictions - Predicted labels (thresholded at 0.5)
        targets - Ground truth labels (0.0 or 1.0)

    Returns:
        Recall score (0.0 to 1.0), 0.0 if no actual positives
    """
    var true_positives = 0
    var false_negatives = 0
    var i = 0
    while i < predictions.data.len():
        val pred_label = if predictions.data[i] > 0.5: 1.0 else: 0.0
        val target_label = if targets.data[i] > 0.5: 1.0 else: 0.0
        if target_label == 1.0:
            if pred_label == 1.0:
                true_positives = true_positives + 1
            else:
                false_negatives = false_negatives + 1
        i = i + 1
    val denominator = true_positives + false_negatives
    if denominator == 0:
        0.0
    else:
        true_positives * 1.0 / denominator

fn f1_score(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute F1 score: 2 * precision * recall / (precision + recall).

    The harmonic mean of precision and recall.

    Args:
        predictions - Predicted labels (thresholded at 0.5)
        targets - Ground truth labels (0.0 or 1.0)

    Returns:
        F1 score (0.0 to 1.0), 0.0 if both precision and recall are 0
    """
    val p = precision_score(predictions, targets)
    val r = recall_score(predictions, targets)
    val denominator = p + r
    if denominator < 0.000001:
        0.0
    else:
        2.0 * p * r / denominator

fn confusion_matrix_values(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> [i64]:
    """Compute confusion matrix values for binary classification.

    Returns [TP, FP, FN, TN] as a flat array.

    Args:
        predictions - Predicted labels (thresholded at 0.5)
        targets - Ground truth labels (0.0 or 1.0)

    Returns:
        Array of [true_positives, false_positives, false_negatives, true_negatives]
    """
    var tp = 0
    var fp = 0
    var fn_count = 0
    var tn = 0
    var i = 0
    while i < predictions.data.len():
        val pred_label = if predictions.data[i] > 0.5: 1.0 else: 0.0
        val target_label = if targets.data[i] > 0.5: 1.0 else: 0.0
        if pred_label == 1.0 and target_label == 1.0:
            tp = tp + 1
        if pred_label == 1.0 and target_label == 0.0:
            fp = fp + 1
        if pred_label == 0.0 and target_label == 1.0:
            fn_count = fn_count + 1
        if pred_label == 0.0 and target_label == 0.0:
            tn = tn + 1
        i = i + 1
    [tp, fp, fn_count, tn]

fn top_k_accuracy(predictions: PureTensor<f64>, targets: PureTensor<f64>, k: i64, num_classes: i64) -> f64:
    """Compute top-k accuracy for multi-class classification.

    Each sample in predictions is a row of num_classes scores.
    Targets contain the correct class index (as f64) for each sample.
    A sample is correct if the true class is among the top-k predictions.

    Args:
        predictions - Prediction scores [num_samples * num_classes] flattened
        targets - True class indices [num_samples], each value in [0, num_classes)
        k - Number of top predictions to consider
        num_classes - Number of classes per sample

    Returns:
        Top-k accuracy (0.0 to 1.0)
    """
    val num_samples = targets.data.len()
    if num_samples == 0:
        return 0.0

    var correct = 0
    var s = 0
    while s < num_samples:
        val true_class = targets.data[s]
        val true_class_int = int(true_class)

        # Find top-k indices for this sample by repeated argmax
        var used: [i64] = []
        var found = false
        var rank = 0
        while rank < k:
            var best_idx = -1
            var best_score = -999999.0
            var c = 0
            while c < num_classes:
                val idx = s * num_classes + c
                val score = predictions.data[idx]
                # Check if already used
                var is_used = false
                var u = 0
                while u < used.len():
                    if used[u] == c:
                        is_used = true
                    u = u + 1
                if (not is_used and score > best_score):
                    best_score = score
                    best_idx = c
                c = c + 1
            if best_idx >= 0:
                used.push(best_idx)
                if best_idx == true_class_int:
                    found = true
            rank = rank + 1

        if found:
            correct = correct + 1
        s = s + 1

    correct * 1.0 / num_samples

# ============================================================================
# Regression Metrics
# ============================================================================

fn mean_absolute_error(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute Mean Absolute Error (MAE).

    MAE = mean(|predictions - targets|)

    Args:
        predictions - Predicted values
        targets - Ground truth values

    Returns:
        MAE value (>= 0.0)
    """
    var abs_sum = 0.0
    var i = 0
    while i < predictions.data.len():
        val diff = predictions.data[i] - targets.data[i]
        val abs_diff = if diff < 0.0: -diff else: diff
        abs_sum = abs_sum + abs_diff
        i = i + 1
    val n = predictions.data.len()
    if n == 0:
        0.0
    else:
        abs_sum / n

fn mean_squared_error(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute Mean Squared Error (MSE).

    MSE = mean((predictions - targets)^2)

    Args:
        predictions - Predicted values
        targets - Ground truth values

    Returns:
        MSE value (>= 0.0)
    """
    var sq_sum = 0.0
    var i = 0
    while i < predictions.data.len():
        val diff = predictions.data[i] - targets.data[i]
        sq_sum = sq_sum + diff * diff
        i = i + 1
    val n = predictions.data.len()
    if n == 0:
        0.0
    else:
        sq_sum / n

fn root_mean_squared_error(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute Root Mean Squared Error (RMSE).

    RMSE = sqrt(MSE)

    Args:
        predictions - Predicted values
        targets - Ground truth values

    Returns:
        RMSE value (>= 0.0)
    """
    val mse = mean_squared_error(predictions, targets)
    # Newton's method for sqrt
    if mse < 0.000001:
        return 0.0
    var guess = mse / 2.0
    var iter = 0
    while iter < 50:
        guess = (guess + mse / guess) / 2.0
        iter = iter + 1
    guess

fn r2_score(predictions: PureTensor<f64>, targets: PureTensor<f64>) -> f64:
    """Compute R-squared (coefficient of determination).

    R^2 = 1 - SS_res / SS_tot
    where SS_res = sum((y - y_hat)^2), SS_tot = sum((y - y_mean)^2)

    Args:
        predictions - Predicted values
        targets - Ground truth values

    Returns:
        R^2 value (1.0 = perfect, 0.0 = baseline, negative = worse than mean)
    """
    val n = targets.data.len()
    if n == 0:
        return 0.0

    # Compute target mean
    var target_sum = 0.0
    var i = 0
    while i < n:
        target_sum = target_sum + targets.data[i]
        i = i + 1
    val target_mean = target_sum / n

    # Compute SS_res and SS_tot
    var ss_res = 0.0
    var ss_tot = 0.0
    i = 0
    while i < n:
        val diff_pred = targets.data[i] - predictions.data[i]
        ss_res = ss_res + diff_pred * diff_pred
        val diff_mean = targets.data[i] - target_mean
        ss_tot = ss_tot + diff_mean * diff_mean
        i = i + 1

    if ss_tot < 0.000001:
        # All targets are the same value
        if ss_res < 0.000001:
            1.0  # Perfect prediction of constant
        else:
            0.0
    else:
        1.0 - ss_res / ss_tot

# ============================================================================
# Exports
# ============================================================================

export accuracy, precision_score, recall_score, f1_score
export confusion_matrix_values, top_k_accuracy
export mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score
