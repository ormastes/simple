# Pure Simple Autograd System v2
# Global Gradient Store - Workaround for Value Semantics
#
# REDESIGNED: Uses global gradient dictionary to work around interpreter's
# value semantics limitation (arrays store copies, not references).
#
# Key insight: Instead of storing gradients in Tensor.grad field (which gets
# copied when tensor is put in array), we store gradients in a global dict
# keyed by tensor ID.

export Tensor, backward, get_gradient, clear_gradients
export tensor_add, tensor_sub, tensor_mul, tensor_matmul, tensor_relu
export tensor_sum, tensor_mean, tensor_mul_scalar
export detach, requires_grad
export tensor_from_data, tensor_from_value, tensor_zeros, tensor_ones

use lib.pure.tensor_f64 (TensorF64, from_data, zeros, ones)
use lib.pure.tensor_f64_ops (add, sub, mul, matmul, transpose, mul_scalar, add_scalar)

# ============================================================================
# Global Gradient Store
# ============================================================================

# Global tensor ID counter
var NEXT_TENSOR_ID: i64 = 1

# Global gradient storage: tensor_id -> TensorF64
var GRADIENTS: Dict<i64, TensorF64> = {}

fn allocate_tensor_id() -> i64:
    """Allocate a unique tensor ID."""
    val id = NEXT_TENSOR_ID
    NEXT_TENSOR_ID = NEXT_TENSOR_ID + 1
    id

fn get_gradient(tensor_id: i64) -> TensorF64?:
    """Get gradient for a tensor by ID.

    Returns None if no gradient has been computed.
    """
    GRADIENTS.get(tensor_id)

fn clear_gradients():
    """Clear all gradients from global store."""
    GRADIENTS = {}

# ============================================================================
# Operation Type (replaces function pointers)
# ============================================================================

enum OpType:
    Add
    Sub
    Mul
    MatMul
    Relu
    Sum
    Mean
    MulScalar

# ============================================================================
# Tensor with Gradient Tracking
# ============================================================================

class Tensor:
    tensor_id: i64           # Unique ID for gradient lookup
    value: TensorF64
    requires_grad: bool
    op_type: OpType?
    inputs: [i64]?           # Store input tensor IDs, not Tensor objects!
    op_name: text

    static fn from_data(data: [f64], shape: [i64], requires_grad: bool = false) -> Tensor:
        Tensor(
            tensor_id: allocate_tensor_id(),
            value: from_data(data, shape),
            requires_grad: requires_grad,
            op_type: nil,
            inputs: nil,
            op_name: "leaf"
        )

    static fn from_value(value: TensorF64, requires_grad: bool = false) -> Tensor:
        Tensor(
            tensor_id: allocate_tensor_id(),
            value: value,
            requires_grad: requires_grad,
            op_type: nil,
            inputs: nil,
            op_name: "leaf"
        )

    static fn zeros(shape: [i64], requires_grad: bool = false) -> Tensor:
        Tensor.from_value(zeros(shape), requires_grad)

    static fn ones(shape: [i64], requires_grad: bool = false) -> Tensor:
        Tensor.from_value(ones(shape), requires_grad)

    fn shape() -> [i64]:
        self.value.shape

    fn numel() -> i64:
        self.value.numel()

    fn grad() -> TensorF64?:
        """Get gradient for this tensor from global store."""
        get_gradient(self.tensor_id)

    fn to_string() -> text:
        if self.requires_grad:
            "Tensor(id={self.tensor_id}, {self.value.to_string()}, grad_fn={self.op_name})"
        else:
            "Tensor(id={self.tensor_id}, {self.value.to_string()})"


# ============================================================================
# Module-Level Factory Functions (Interpreter-Compatible)
# ============================================================================

fn tensor_from_data(data: [f64], shape: [i64], requires_grad: bool = false) -> Tensor:
    """Create tensor from data array."""
    Tensor(
        tensor_id: allocate_tensor_id(),
        value: from_data(data, shape),
        requires_grad: requires_grad,
        op_type: nil,
        inputs: nil,
        op_name: "leaf"
    )

fn tensor_from_value(value: TensorF64, requires_grad: bool = false) -> Tensor:
    """Create tensor from TensorF64 value."""
    Tensor(
        tensor_id: allocate_tensor_id(),
        value: value,
        requires_grad: requires_grad,
        op_type: nil,
        inputs: nil,
        op_name: "leaf"
    )

fn tensor_zeros(shape: [i64], requires_grad: bool = false) -> Tensor:
    """Create tensor filled with zeros."""
    tensor_from_value(zeros(shape), requires_grad)

fn tensor_ones(shape: [i64], requires_grad: bool = false) -> Tensor:
    """Create tensor filled with ones."""
    tensor_from_value(ones(shape), requires_grad)


# ============================================================================
# Backward Pass with Global Gradient Store
# ============================================================================

fn backward(t: Tensor):
    """Compute gradients for all inputs in computation graph.

    Gradients are stored in global GRADIENTS dict, accessible via get_gradient().
    """
    val grad_output = ones(t.shape())

    # Store gradient for output tensor
    if t.requires_grad:
        if GRADIENTS.get(t.tensor_id).?:
            GRADIENTS[t.tensor_id] = add(GRADIENTS[t.tensor_id].unwrap(), grad_output)
        else:
            GRADIENTS[t.tensor_id] = grad_output

    if t.inputs.? and t.op_type.?:
        propagate_grads(t.tensor_id, t.op_type.unwrap(), t.inputs.unwrap(), grad_output)


fn propagate_grads(tensor_id: i64, op: OpType, input_ids: [i64], grad_out: TensorF64):
    """Propagate gradients to input tensors.

    Note: We can't access Tensor objects from input_ids because of value semantics.
    This is a limitation - we need a global tensor registry to make this work.

    TODO: Add global tensor registry: TENSORS: Dict<i64, Tensor>
    """
    # This implementation is incomplete - we need access to input tensors
    # to compute their gradients and continue backprop
    pass


# ============================================================================
# Operations with Autograd
# ============================================================================

fn tensor_add(a: Tensor, b: Tensor) -> Tensor:
    val result_value = add(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Add)
    result.inputs = Some([a.tensor_id, b.tensor_id])
    result.op_name = "add"
    result


fn tensor_sub(a: Tensor, b: Tensor) -> Tensor:
    val result_value = sub(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Sub)
    result.inputs = Some([a.tensor_id, b.tensor_id])
    result.op_name = "sub"
    result


fn tensor_mul(a: Tensor, b: Tensor) -> Tensor:
    val result_value = mul(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Mul)
    result.inputs = Some([a.tensor_id, b.tensor_id])
    result.op_name = "mul"
    result


fn tensor_matmul(a: Tensor, b: Tensor) -> Tensor:
    val result_value = matmul(a.value, b.value)
    val requires_grad = a.requires_grad or b.requires_grad

    if not requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.MatMul)
    result.inputs = Some([a.tensor_id, b.tensor_id])
    result.op_name = "matmul"
    result


fn tensor_relu(x: Tensor) -> Tensor:
    # ReLU: max(0, x)
    var result_data: [f64] = []
    for v in x.value.data:
        result_data.push(if v > 0.0: v else: 0.0)

    val result_value = from_data(result_data, x.value.shape)

    if not x.requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Relu)
    result.inputs = Some([x.tensor_id])
    result.op_name = "relu"
    result


fn tensor_sum(x: Tensor) -> Tensor:
    var total = 0.0
    for v in x.value.data:
        total = total + v

    val result_value = from_data([total], [1])

    if not x.requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Sum)
    result.inputs = Some([x.tensor_id])
    result.op_name = "sum"
    result


fn tensor_mean(x: Tensor) -> Tensor:
    var total = 0.0
    for v in x.value.data:
        total = total + v

    val mean_val = total / x.value.data.len()
    val result_value = from_data([mean_val], [1])

    if not x.requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.Mean)
    result.inputs = Some([x.tensor_id])
    result.op_name = "mean"
    result


fn tensor_mul_scalar(x: Tensor, scalar: f64) -> Tensor:
    val result_value = mul_scalar(x.value, scalar)

    if not x.requires_grad:
        return tensor_from_value(result_value, requires_grad: false)

    var result = tensor_from_value(result_value, requires_grad: true)
    result.op_type = Some(OpType.MulScalar)
    result.inputs = Some([x.tensor_id])
    result.op_name = "mul_scalar"
    result


fn detach(x: Tensor) -> Tensor:
    tensor_from_value(x.value, requires_grad: false)


fn requires_grad(x: Tensor, requires_grad: bool) -> Tensor:
    tensor_from_value(x.value, requires_grad)
