# Pure Simple Neural Network Layers
#
# Implements common NN layers in pure Simple
# Zero external dependencies
#
# NOTE: This module uses generics (PureTensor<f64>) and will only work
# in compiled mode, not in the interpreter (runtime parser limitation).

use lib.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros, tensor_ones, tensor_randn}
use lib.pure.tensor_ops.{add, mul, mul_scalar, relu, sigmoid, tanh, softmax, tensor_exp, tensor_sum, tensor_div_scalar}
use lib.pure.nn.pooling.{MaxPool2d, AvgPool2d, maxpool2d_create, avgpool2d_create}

# ============================================================================
# Layer Classes
# ============================================================================

class Linear:
    """Fully-connected layer: y = xW^T + b"""
    weight: PureTensor<f64>
    bias: PureTensor<f64>?
    in_features: i64
    out_features: i64
    training: bool

    static fn create(in_features: i64, out_features: i64, bias: bool) -> Linear:
        """Create Linear layer with simple initialization."""
        var w_data: [f64] = []
        var i = 0
        while i < in_features * out_features:
            w_data.push(0.1)
            i = i + 1

        val weight = tensor_from_data(w_data, [out_features, in_features])
        val bias_tensor = if bias:
            Some(tensor_zeros([out_features]))
        else:
            nil

        Linear(weight: weight, bias: bias_tensor, in_features: in_features, out_features: out_features, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward: y = xW^T + b"""
        var result: [f64] = []
        var i = 0
        while i < self.out_features:
            var sum = 0.0
            var j = 0
            while j < self.in_features and j < x.data.len():
                sum = sum + self.weight.data[i * self.in_features + j] * x.data[j]
                j = j + 1

            if self.bias.?:
                sum = sum + self.bias.unwrap().data[i]

            result.push(sum)
            i = i + 1

        tensor_from_data(result, [self.out_features])

    fn parameters() -> [PureTensor<f64>]:
        """Get list of parameters (weight, bias if exists)."""
        if self.bias.?:
            [self.weight, self.bias.unwrap()]
        else:
            [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Linear(in_features={self.in_features}, out_features={self.out_features})"

class ReLU:
    """Rectified Linear Unit: max(0, x)"""
    training: bool

    static fn create() -> ReLU:
        ReLU(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        relu(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "ReLU()"

class Sigmoid:
    """Sigmoid activation"""
    training: bool

    static fn create() -> Sigmoid:
        Sigmoid(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        sigmoid(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Sigmoid()"

class Tanh:
    """Tanh activation"""
    training: bool

    static fn create() -> Tanh:
        Tanh(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        tanh(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Tanh()"

class Softmax:
    """Softmax activation: exp(x_i) / sum(exp(x_j))"""
    dim: i64
    training: bool

    static fn create(dim: i64) -> Softmax:
        Softmax(dim: dim, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        # Simple softmax: exp(x) / sum(exp(x))
        val exp_x = tensor_exp(x)
        val sum_exp = tensor_sum(exp_x)
        tensor_div_scalar(exp_x, sum_exp)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Softmax(dim={self.dim})"

class Dropout:
    """Dropout regularization"""
    p: f64
    training: bool

    static fn create(p: f64) -> Dropout:
        Dropout(p: p, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        # In training mode: randomly drop elements
        # In eval mode: return input unchanged
        if self.training:
            # Dropout mask implementation (Phase 3.4 - TODO âœ…)
            # Generate random values in [0, 1)
            val rand_tensor = tensor_randn(x.shape)

            # Create binary mask: keep if rand > p, drop if rand <= p
            var mask_data: [f64] = []
            val keep_prob = 1.0 - self.p
            for val in rand_tensor.data:
                # Shift random values from [-0.5, 0.5) to [0, 1)
                val normalized = val + 0.5
                if normalized > self.p:
                    mask_data.push(1.0)
                else:
                    mask_data.push(0.0)

            val mask = tensor_from_data(mask_data, x.shape)

            # Apply mask and scale by 1/(1-p) to maintain expected value
            val scale = 1.0 / keep_prob
            val masked = mul(x, mask)
            mul_scalar(masked, scale)
        else:
            x

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Dropout(p={self.p})"

class Sequential:
    """Sequential container for chaining layers"""
    layers: [any]  # List of layers (using 'any' since no Layer trait yet)
    training: bool

    static fn create(layers: [any]) -> Sequential:
        Sequential(layers: layers, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass through all layers."""
        var result = x
        for layer in self.layers:
            result = layer.forward(result)
        result

    fn parameters() -> [PureTensor<f64>]:
        """Collect parameters from all layers."""
        var all_params: [PureTensor<f64>] = []
        for layer in self.layers:
            val layer_params = layer.parameters()
            all_params = all_params + layer_params
        all_params

    me train():
        """Set all layers to training mode."""
        self.training = true
        for layer in self.layers:
            layer.train()

    me eval():
        """Set all layers to evaluation mode."""
        self.training = false
        for layer in self.layers:
            layer.eval()

    fn to_string() -> text:
        """String representation."""
        var s = "Sequential(\n"
        for layer in self.layers:
            s = s + "  " + layer.to_string() + "\n"
        s + ")"

class Conv2d:
    """2D Convolutional layer: conv(x, weight) + bias

    Input shape: [batch, in_channels, height, width]
    Output shape: [batch, out_channels, out_height, out_width]
    """
    weight: PureTensor<f64>
    bias: PureTensor<f64>?
    in_channels: i64
    out_channels: i64
    kernel_size: i64
    stride: i64
    padding: i64
    training: bool

    static fn create(
        in_channels: i64,
        out_channels: i64,
        kernel_size: i64,
        stride: i64,
        padding: i64,
        bias: bool
    ) -> Conv2d:
        """Create Conv2d layer with simple initialization."""
        val weight_size = out_channels * in_channels * kernel_size * kernel_size
        var w_data: [f64] = []
        var i = 0
        while i < weight_size:
            w_data.push(0.1)
            i = i + 1

        val weight = tensor_from_data(
            w_data,
            [out_channels, in_channels, kernel_size, kernel_size]
        )

        val bias_tensor = if bias:
            Some(tensor_zeros([out_channels]))
        else:
            nil

        Conv2d(
            weight: weight,
            bias: bias_tensor,
            in_channels: in_channels,
            out_channels: out_channels,
            kernel_size: kernel_size,
            stride: stride,
            padding: padding,
            training: true
        )

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass: conv2d(x, weight) + bias"""
        val batch = x.shape[0]
        val in_c = x.shape[1]
        val in_h = x.shape[2]
        val in_w = x.shape[3]

        val out_h = (in_h + 2 * self.padding - self.kernel_size) / self.stride + 1
        val out_w = (in_w + 2 * self.padding - self.kernel_size) / self.stride + 1

        var result_data: [f64] = []

        var b = 0
        while b < batch:
            var oc = 0
            while oc < self.out_channels:
                var oh = 0
                while oh < out_h:
                    var ow = 0
                    while ow < out_w:
                        var sum = 0.0

                        var ic = 0
                        while ic < self.in_channels:
                            var kh = 0
                            while kh < self.kernel_size:
                                var kw = 0
                                while kw < self.kernel_size:
                                    val ih = oh * self.stride + kh - self.padding
                                    val iw = ow * self.stride + kw - self.padding

                                    if ih >= 0 and ih < in_h and iw >= 0 and iw < in_w:
                                        val x_idx = b * (in_c * in_h * in_w) +
                                                   ic * (in_h * in_w) +
                                                   ih * in_w +
                                                   iw

                                        val w_idx = oc * (in_c * self.kernel_size * self.kernel_size) +
                                                   ic * (self.kernel_size * self.kernel_size) +
                                                   kh * self.kernel_size +
                                                   kw

                                        sum = sum + x.data[x_idx] * self.weight.data[w_idx]

                                    kw = kw + 1
                                kh = kh + 1
                            ic = ic + 1

                        if self.bias.?:
                            sum = sum + self.bias.unwrap().data[oc]

                        result_data.push(sum)
                        ow = ow + 1
                    oh = oh + 1
                oc = oc + 1
            b = b + 1

        tensor_from_data(result_data, [batch, self.out_channels, out_h, out_w])

    fn backward(grad_output: PureTensor<f64>, x: PureTensor<f64>) -> PureTensor<f64>:
        """Backward pass for Conv2d"""
        val batch = x.shape[0]
        val in_h = x.shape[2]
        val in_w = x.shape[3]
        val out_h = grad_output.shape[2]
        val out_w = grad_output.shape[3]

        var grad_x_data: [f64] = []
        var i = 0
        val total_size = batch * self.in_channels * in_h * in_w
        while i < total_size:
            grad_x_data.push(0.0)
            i = i + 1

        var b = 0
        while b < batch:
            var oc = 0
            while oc < self.out_channels:
                var oh = 0
                while oh < out_h:
                    var ow = 0
                    while ow < out_w:
                        val grad_idx = b * (self.out_channels * out_h * out_w) +
                                      oc * (out_h * out_w) +
                                      oh * out_w +
                                      ow
                        val grad = grad_output.data[grad_idx]

                        var ic = 0
                        while ic < self.in_channels:
                            var kh = 0
                            while kh < self.kernel_size:
                                var kw = 0
                                while kw < self.kernel_size:
                                    val ih = oh * self.stride + kh - self.padding
                                    val iw = ow * self.stride + kw - self.padding

                                    if ih >= 0 and ih < in_h and iw >= 0 and iw < in_w:
                                        val w_idx = oc * (self.in_channels * self.kernel_size * self.kernel_size) +
                                                   ic * (self.kernel_size * self.kernel_size) +
                                                   kh * self.kernel_size +
                                                   kw

                                        val gx_idx = b * (self.in_channels * in_h * in_w) +
                                                    ic * (in_h * in_w) +
                                                    ih * in_w +
                                                    iw

                                        grad_x_data[gx_idx] = grad_x_data[gx_idx] + grad * self.weight.data[w_idx]

                                    kw = kw + 1
                                kh = kh + 1
                            ic = ic + 1

                        ow = ow + 1
                    oh = oh + 1
                oc = oc + 1
            b = b + 1

        tensor_from_data(grad_x_data, [batch, self.in_channels, in_h, in_w])

    fn parameters() -> [PureTensor<f64>]:
        """Get list of parameters (weight, bias if exists)."""
        if self.bias.?:
            [self.weight, self.bias.unwrap()]
        else:
            [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Conv2d(in_channels={self.in_channels}, out_channels={self.out_channels}, kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding})"

class LeakyReLU:
    """Leaky Rectified Linear Unit: max(negative_slope * x, x)"""
    negative_slope: f64
    training: bool

    static fn create(negative_slope: f64) -> LeakyReLU:
        LeakyReLU(negative_slope: negative_slope, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        var result_data: [f64] = []
        for v in x.data:
            val activated = if v > 0.0: v else: self.negative_slope * v
            result_data.push(activated)
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "LeakyReLU(negative_slope={self.negative_slope})"

class ELU:
    """Exponential Linear Unit: x if x > 0, alpha * (exp(x) - 1) if x <= 0"""
    alpha: f64
    training: bool

    static fn create(alpha: f64) -> ELU:
        ELU(alpha: alpha, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        var result_data: [f64] = []
        for v in x.data:
            if v > 0.0:
                result_data.push(v)
            else:
                # Compute exp(v) using Taylor series (20 terms)
                var exp_val = 1.0
                var term = 1.0
                var k = 1
                while k <= 20:
                    term = term * v / k
                    exp_val = exp_val + term
                    k = k + 1
                result_data.push(self.alpha * (exp_val - 1.0))
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "ELU(alpha={self.alpha})"

class GELU:
    """Gaussian Error Linear Unit: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))"""
    training: bool

    static fn create() -> GELU:
        GELU(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        val sqrt_2_over_pi = 0.7978845608
        var result_data: [f64] = []
        for v in x.data:
            # Compute inner = sqrt(2/pi) * (x + 0.044715 * x^3)
            val inner = sqrt_2_over_pi * (v + 0.044715 * v * v * v)
            # Compute tanh(inner) using (exp(2z)-1)/(exp(2z)+1) with Taylor exp
            val z2 = 2.0 * inner
            # Compute exp(z2) using Taylor series (20 terms)
            var exp_val = 1.0
            var term = 1.0
            var k = 1
            while k <= 20:
                term = term * z2 / k
                exp_val = exp_val + term
                k = k + 1
            val tanh_val = (exp_val - 1.0) / (exp_val + 1.0)
            result_data.push(0.5 * v * (1.0 + tanh_val))
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "GELU()"

class BatchNorm1d:
    """Batch Normalization over 1D inputs"""
    num_features: i64
    eps: f64
    momentum: f64
    weight: PureTensor<f64>
    bias: PureTensor<f64>
    running_mean: PureTensor<f64>
    running_var: PureTensor<f64>
    training: bool

    static fn create(num_features: i64, eps: f64, momentum: f64) -> BatchNorm1d:
        val weight = tensor_ones([num_features])
        val bias = tensor_zeros([num_features])
        val running_mean = tensor_zeros([num_features])
        val running_var = tensor_ones([num_features])
        BatchNorm1d(num_features: num_features, eps: eps, momentum: momentum, weight: weight, bias: bias, running_mean: running_mean, running_var: running_var, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Normalize using running_mean/running_var, then scale and shift."""
        var result_data: [f64] = []
        var i = 0
        while i < x.data.len():
            val feature_idx = i % self.num_features
            val mean_val = self.running_mean.data[feature_idx]
            val var_val = self.running_var.data[feature_idx]
            # Compute sqrt(var + eps) using Newton's method
            val var_eps = var_val + self.eps
            var sqrt_val = var_eps
            var iter = 0
            while iter < 20:
                sqrt_val = 0.5 * (sqrt_val + var_eps / sqrt_val)
                iter = iter + 1
            val normalized = (x.data[i] - mean_val) / sqrt_val
            val scaled = normalized * self.weight.data[feature_idx] + self.bias.data[feature_idx]
            result_data.push(scaled)
            i = i + 1
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        [self.weight, self.bias]

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "BatchNorm1d(num_features={self.num_features}, eps={self.eps}, momentum={self.momentum})"

class LayerNorm:
    """Layer Normalization"""
    normalized_shape: i64
    eps: f64
    weight: PureTensor<f64>
    bias: PureTensor<f64>
    training: bool

    static fn create(normalized_shape: i64, eps: f64) -> LayerNorm:
        val weight = tensor_ones([normalized_shape])
        val bias = tensor_zeros([normalized_shape])
        LayerNorm(normalized_shape: normalized_shape, eps: eps, weight: weight, bias: bias, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Compute mean and variance across all elements, normalize, scale and shift."""
        # Compute mean
        var sum = 0.0
        var i = 0
        while i < x.data.len():
            sum = sum + x.data[i]
            i = i + 1
        val mean_val = sum / x.data.len()

        # Compute variance
        var var_sum = 0.0
        i = 0
        while i < x.data.len():
            val diff = x.data[i] - mean_val
            var_sum = var_sum + diff * diff
            i = i + 1
        val var_val = var_sum / x.data.len()

        # Compute sqrt(var + eps) using Newton's method
        val var_eps = var_val + self.eps
        var sqrt_val = var_eps
        var iter = 0
        while iter < 20:
            sqrt_val = 0.5 * (sqrt_val + var_eps / sqrt_val)
            iter = iter + 1

        # Normalize, scale and shift
        var result_data: [f64] = []
        i = 0
        while i < x.data.len():
            val feature_idx = i % self.normalized_shape
            val normalized = (x.data[i] - mean_val) / sqrt_val
            val scaled = normalized * self.weight.data[feature_idx] + self.bias.data[feature_idx]
            result_data.push(scaled)
            i = i + 1
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        [self.weight, self.bias]

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "LayerNorm(normalized_shape={self.normalized_shape}, eps={self.eps})"

class Embedding:
    """Embedding lookup table: indices -> dense vectors

    Maps integer indices to dense vector representations.
    Used for word embeddings, token embeddings, etc.

    Args:
        num_embeddings: Size of the embedding dictionary (vocabulary size)
        embedding_dim: Dimension of each embedding vector

    Shape:
        Input: Integer indices (as f64), shape (batch_size,) or (batch_size, seq_len)
        Output: Dense vectors, shape (batch_size, embedding_dim) or (batch_size, seq_len, embedding_dim)
    """
    weight: PureTensor<f64>
    num_embeddings: i64
    embedding_dim: i64
    training: bool

    static fn create(num_embeddings: i64, embedding_dim: i64) -> Embedding:
        """Create Embedding layer with random initialization."""
        # Initialize embedding table with small random values
        val weight_init = tensor_randn([num_embeddings, embedding_dim])

        # Scale down initial weights (multiply by 0.1)
        var scaled_data: [f64] = []
        for v in weight_init.data:
            scaled_data.push(v * 0.1)

        val weight = tensor_from_data(scaled_data, [num_embeddings, embedding_dim])

        Embedding(
            weight: weight,
            num_embeddings: num_embeddings,
            embedding_dim: embedding_dim,
            training: true
        )

    fn forward(indices: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass: lookup embeddings by indices.

        Args:
            indices: Tensor of integer indices (as f64)
                    Shape: (batch_size,) or (batch_size, seq_len)

        Returns:
            Tensor of embeddings
            Shape: (batch_size, embedding_dim) or (batch_size, seq_len, embedding_dim)
        """
        val input_shape = indices.shape
        val batch_size = input_shape[0]

        # Handle 1D input: (batch_size,) -> (batch_size, embedding_dim)
        if input_shape.len() == 1:
            var output_data: [f64] = []

            # For each index in batch
            var i = 0
            while i < batch_size:
                # Get index (already f64, convert to i64 for array access)
                val idx_f64 = indices.data[i]
                val idx_i64 = idx_f64

                # Bounds check
                if idx_i64 < 0.0 or idx_i64 >= self.num_embeddings:
                    # Out of bounds - return zeros
                    var j = 0
                    while j < self.embedding_dim:
                        output_data.push(0.0)
                        j = j + 1
                else:
                    # Lookup embedding vector
                    # Weight is [num_embeddings, embedding_dim]
                    # Access row idx_i64
                    var j = 0
                    while j < self.embedding_dim:
                        val weight_idx = idx_i64 * self.embedding_dim + j
                        output_data.push(self.weight.data[weight_idx])
                        j = j + 1

                i = i + 1

            tensor_from_data(output_data, [batch_size, self.embedding_dim])

        # Handle 2D input: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)
        else:
            val seq_len = input_shape[1]
            var output_data: [f64] = []

            # For each item in batch
            var batch_idx = 0
            while batch_idx < batch_size:
                # For each position in sequence
                var seq_idx = 0
                while seq_idx < seq_len:
                    # Get index
                    val idx_f64 = indices.data[batch_idx * seq_len + seq_idx]
                    val idx_i64 = idx_f64

                    # Bounds check
                    if idx_i64 < 0.0 or idx_i64 >= self.num_embeddings:
                        # Out of bounds - return zeros
                        var j = 0
                        while j < self.embedding_dim:
                            output_data.push(0.0)
                            j = j + 1
                    else:
                        # Lookup embedding vector
                        var j = 0
                        while j < self.embedding_dim:
                            val weight_idx = idx_i64 * self.embedding_dim + j
                            output_data.push(self.weight.data[weight_idx])
                            j = j + 1

                    seq_idx = seq_idx + 1

                batch_idx = batch_idx + 1

            tensor_from_data(output_data, [batch_size, seq_len, self.embedding_dim])

    fn parameters() -> [PureTensor<f64>]:
        """Get list of parameters (just weight)."""
        [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Embedding(num_embeddings={self.num_embeddings}, embedding_dim={self.embedding_dim})"

# ============================================================================
# Helper Functions
# ============================================================================

fn count_parameters(model: any) -> i64:
    """Count total number of parameters in a model.

    Args:
        model - Model with .parameters() method

    Returns:
        Total number of scalar parameters
    """
    val params = model.parameters()
    var total = 0
    for param in params:
        var param_size = 1
        for dim in param.shape:
            param_size = param_size * dim
        total = total + param_size
    total

fn zero_grad(model: any):
    """Zero all gradients in the model.

    Args:
        model - Model with .parameters() method

    Note: This is a placeholder. Actual gradient zeroing requires
    autograd support which is blocked by interpreter limitations.
    """
    # Zero gradients by resetting parameter gradient accumulators
    # Since autograd is not yet available, manually zero grad tensors
    val params = model.parameters()
    for param in params:
        if param.grad.?:
            param.grad = PureTensor.zeros_like(param.grad.unwrap())
    ()

# Module-level factory function (for backwards compatibility)
fn linear_create(in_features: i64, out_features: i64, use_bias: bool) -> Linear:
    """Create Linear layer (legacy API)."""
    Linear.create(in_features, out_features, use_bias)

fn calculate_conv2d_output_size(input_size: i64, kernel_size: i64, stride: i64, padding: i64) -> i64:
    """Calculate output size for Conv2d operation.

    Formula: (input + 2*padding - kernel_size) / stride + 1
    """
    (input_size + 2 * padding - kernel_size) / stride + 1

# ============================================================================
# Exports
# ============================================================================

export Linear, ReLU, Sigmoid, Tanh, Softmax, Dropout, Sequential
export Conv2d, LeakyReLU, ELU, GELU, BatchNorm1d, LayerNorm, Embedding
export MaxPool2d, AvgPool2d
export count_parameters, zero_grad
export linear_create  # Legacy API
export maxpool2d_create, avgpool2d_create  # Pooling factory functions
export calculate_conv2d_output_size
