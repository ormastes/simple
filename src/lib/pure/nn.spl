# Pure Simple Neural Network Layers
#
# Implements common NN layers in pure Simple
# Zero external dependencies
#
# NOTE: This module uses generics (PureTensor<f64>) and will only work
# in compiled mode, not in the interpreter (runtime parser limitation).

use lib.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros, tensor_ones, tensor_randn}
use lib.pure.tensor_ops.{add, mul, mul_scalar, relu, sigmoid, tanh, softmax, tensor_exp, tensor_sum, tensor_div_scalar}

# ============================================================================
# Layer Classes
# ============================================================================

class Linear:
    """Fully-connected layer: y = xW^T + b"""
    weight: PureTensor<f64>
    bias: PureTensor<f64>?
    in_features: i64
    out_features: i64
    training: bool

    static fn create(in_features: i64, out_features: i64, bias: bool) -> Linear:
        """Create Linear layer with simple initialization."""
        var w_data: [f64] = []
        var i = 0
        while i < in_features * out_features:
            w_data.push(0.1)
            i = i + 1

        val weight = tensor_from_data(w_data, [out_features, in_features])
        val bias_tensor = if bias:
            Some(tensor_zeros([out_features]))
        else:
            nil

        Linear(weight: weight, bias: bias_tensor, in_features: in_features, out_features: out_features, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward: y = xW^T + b"""
        var result: [f64] = []
        var i = 0
        while i < self.out_features:
            var sum = 0.0
            var j = 0
            while j < self.in_features and j < x.data.len():
                sum = sum + self.weight.data[i * self.in_features + j] * x.data[j]
                j = j + 1

            if self.bias.?:
                sum = sum + self.bias.unwrap().data[i]

            result.push(sum)
            i = i + 1

        tensor_from_data(result, [self.out_features])

    fn parameters() -> [PureTensor<f64>]:
        """Get list of parameters (weight, bias if exists)."""
        if self.bias.?:
            [self.weight, self.bias.unwrap()]
        else:
            [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Linear(in_features={self.in_features}, out_features={self.out_features})"

class ReLU:
    """Rectified Linear Unit: max(0, x)"""
    training: bool

    static fn create() -> ReLU:
        ReLU(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        relu(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "ReLU()"

class Sigmoid:
    """Sigmoid activation"""
    training: bool

    static fn create() -> Sigmoid:
        Sigmoid(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        sigmoid(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Sigmoid()"

class Tanh:
    """Tanh activation"""
    training: bool

    static fn create() -> Tanh:
        Tanh(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        tanh(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Tanh()"

class Softmax:
    """Softmax activation: exp(x_i) / sum(exp(x_j))"""
    dim: i64
    training: bool

    static fn create(dim: i64) -> Softmax:
        Softmax(dim: dim, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        # Simple softmax: exp(x) / sum(exp(x))
        val exp_x = tensor_exp(x)
        val sum_exp = tensor_sum(exp_x)
        tensor_div_scalar(exp_x, sum_exp)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Softmax(dim={self.dim})"

class Dropout:
    """Dropout regularization"""
    p: f64
    training: bool

    static fn create(p: f64) -> Dropout:
        Dropout(p: p, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        # In training mode: randomly drop elements
        # In eval mode: return input unchanged
        if self.training:
            # Dropout mask implementation (Phase 3.4 - TODO âœ…)
            # Generate random values in [0, 1)
            val rand_tensor = tensor_randn(x.shape)

            # Create binary mask: keep if rand > p, drop if rand <= p
            var mask_data: [f64] = []
            val keep_prob = 1.0 - self.p
            for val in rand_tensor.data:
                # Shift random values from [-0.5, 0.5) to [0, 1)
                val normalized = val + 0.5
                if normalized > self.p:
                    mask_data.push(1.0)
                else:
                    mask_data.push(0.0)

            val mask = tensor_from_data(mask_data, x.shape)

            # Apply mask and scale by 1/(1-p) to maintain expected value
            val scale = 1.0 / keep_prob
            val masked = mul(x, mask)
            mul_scalar(masked, scale)
        else:
            x

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Dropout(p={self.p})"

class Sequential:
    """Sequential container for chaining layers"""
    layers: [any]  # List of layers (using 'any' since no Layer trait yet)
    training: bool

    static fn create(layers: [any]) -> Sequential:
        Sequential(layers: layers, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass through all layers."""
        var result = x
        for layer in self.layers:
            result = layer.forward(result)
        result

    fn parameters() -> [PureTensor<f64>]:
        """Collect parameters from all layers."""
        var all_params: [PureTensor<f64>] = []
        for layer in self.layers:
            val layer_params = layer.parameters()
            all_params = all_params + layer_params
        all_params

    me train():
        """Set all layers to training mode."""
        self.training = true
        for layer in self.layers:
            layer.train()

    me eval():
        """Set all layers to evaluation mode."""
        self.training = false
        for layer in self.layers:
            layer.eval()

    fn to_string() -> text:
        """String representation."""
        var s = "Sequential(\n"
        for layer in self.layers:
            s = s + "  " + layer.to_string() + "\n"
        s + ")"

class LeakyReLU:
    """Leaky Rectified Linear Unit: max(negative_slope * x, x)"""
    negative_slope: f64
    training: bool

    static fn create(negative_slope: f64) -> LeakyReLU:
        LeakyReLU(negative_slope: negative_slope, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        var result_data: [f64] = []
        for v in x.data:
            val activated = if v > 0.0: v else: self.negative_slope * v
            result_data.push(activated)
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "LeakyReLU(negative_slope={self.negative_slope})"

class ELU:
    """Exponential Linear Unit: x if x > 0, alpha * (exp(x) - 1) if x <= 0"""
    alpha: f64
    training: bool

    static fn create(alpha: f64) -> ELU:
        ELU(alpha: alpha, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        var result_data: [f64] = []
        for v in x.data:
            if v > 0.0:
                result_data.push(v)
            else:
                # Compute exp(v) using Taylor series (20 terms)
                var exp_val = 1.0
                var term = 1.0
                var k = 1
                while k <= 20:
                    term = term * v / k
                    exp_val = exp_val + term
                    k = k + 1
                result_data.push(self.alpha * (exp_val - 1.0))
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "ELU(alpha={self.alpha})"

class GELU:
    """Gaussian Error Linear Unit: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))"""
    training: bool

    static fn create() -> GELU:
        GELU(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        val sqrt_2_over_pi = 0.7978845608
        var result_data: [f64] = []
        for v in x.data:
            # Compute inner = sqrt(2/pi) * (x + 0.044715 * x^3)
            val inner = sqrt_2_over_pi * (v + 0.044715 * v * v * v)
            # Compute tanh(inner) using (exp(2z)-1)/(exp(2z)+1) with Taylor exp
            val z2 = 2.0 * inner
            # Compute exp(z2) using Taylor series (20 terms)
            var exp_val = 1.0
            var term = 1.0
            var k = 1
            while k <= 20:
                term = term * z2 / k
                exp_val = exp_val + term
                k = k + 1
            val tanh_val = (exp_val - 1.0) / (exp_val + 1.0)
            result_data.push(0.5 * v * (1.0 + tanh_val))
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "GELU()"

class BatchNorm1d:
    """Batch Normalization over 1D inputs"""
    num_features: i64
    eps: f64
    momentum: f64
    weight: PureTensor<f64>
    bias: PureTensor<f64>
    running_mean: PureTensor<f64>
    running_var: PureTensor<f64>
    training: bool

    static fn create(num_features: i64, eps: f64, momentum: f64) -> BatchNorm1d:
        val weight = tensor_ones([num_features])
        val bias = tensor_zeros([num_features])
        val running_mean = tensor_zeros([num_features])
        val running_var = tensor_ones([num_features])
        BatchNorm1d(num_features: num_features, eps: eps, momentum: momentum, weight: weight, bias: bias, running_mean: running_mean, running_var: running_var, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Normalize using running_mean/running_var, then scale and shift."""
        var result_data: [f64] = []
        var i = 0
        while i < x.data.len():
            val feature_idx = i % self.num_features
            val mean_val = self.running_mean.data[feature_idx]
            val var_val = self.running_var.data[feature_idx]
            # Compute sqrt(var + eps) using Newton's method
            val var_eps = var_val + self.eps
            var sqrt_val = var_eps
            var iter = 0
            while iter < 20:
                sqrt_val = 0.5 * (sqrt_val + var_eps / sqrt_val)
                iter = iter + 1
            val normalized = (x.data[i] - mean_val) / sqrt_val
            val scaled = normalized * self.weight.data[feature_idx] + self.bias.data[feature_idx]
            result_data.push(scaled)
            i = i + 1
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        [self.weight, self.bias]

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "BatchNorm1d(num_features={self.num_features}, eps={self.eps}, momentum={self.momentum})"

class LayerNorm:
    """Layer Normalization"""
    normalized_shape: i64
    eps: f64
    weight: PureTensor<f64>
    bias: PureTensor<f64>
    training: bool

    static fn create(normalized_shape: i64, eps: f64) -> LayerNorm:
        val weight = tensor_ones([normalized_shape])
        val bias = tensor_zeros([normalized_shape])
        LayerNorm(normalized_shape: normalized_shape, eps: eps, weight: weight, bias: bias, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Compute mean and variance across all elements, normalize, scale and shift."""
        # Compute mean
        var sum = 0.0
        var i = 0
        while i < x.data.len():
            sum = sum + x.data[i]
            i = i + 1
        val mean_val = sum / x.data.len()

        # Compute variance
        var var_sum = 0.0
        i = 0
        while i < x.data.len():
            val diff = x.data[i] - mean_val
            var_sum = var_sum + diff * diff
            i = i + 1
        val var_val = var_sum / x.data.len()

        # Compute sqrt(var + eps) using Newton's method
        val var_eps = var_val + self.eps
        var sqrt_val = var_eps
        var iter = 0
        while iter < 20:
            sqrt_val = 0.5 * (sqrt_val + var_eps / sqrt_val)
            iter = iter + 1

        # Normalize, scale and shift
        var result_data: [f64] = []
        i = 0
        while i < x.data.len():
            val feature_idx = i % self.normalized_shape
            val normalized = (x.data[i] - mean_val) / sqrt_val
            val scaled = normalized * self.weight.data[feature_idx] + self.bias.data[feature_idx]
            result_data.push(scaled)
            i = i + 1
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        [self.weight, self.bias]

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "LayerNorm(normalized_shape={self.normalized_shape}, eps={self.eps})"

# ============================================================================
# Helper Functions
# ============================================================================

fn count_parameters(model: any) -> i64:
    """Count total number of parameters in a model.

    Args:
        model - Model with .parameters() method

    Returns:
        Total number of scalar parameters
    """
    val params = model.parameters()
    var total = 0
    for param in params:
        var param_size = 1
        for dim in param.shape:
            param_size = param_size * dim
        total = total + param_size
    total

fn zero_grad(model: any):
    """Zero all gradients in the model.

    Args:
        model - Model with .parameters() method

    Note: This is a placeholder. Actual gradient zeroing requires
    autograd support which is blocked by interpreter limitations.
    """
    # Zero gradients by resetting parameter gradient accumulators
    # Since autograd is not yet available, manually zero grad tensors
    val params = model.parameters()
    for param in params:
        if param.grad.?:
            param.grad = PureTensor.zeros_like(param.grad.unwrap())
    ()

# Module-level factory function (for backwards compatibility)
fn linear_create(in_features: i64, out_features: i64, use_bias: bool) -> Linear:
    """Create Linear layer (legacy API)."""
    Linear.create(in_features, out_features, use_bias)

# ============================================================================
# Exports
# ============================================================================

export Linear, ReLU, Sigmoid, Tanh, Softmax, Dropout, Sequential
export LeakyReLU, ELU, GELU, BatchNorm1d, LayerNorm
export count_parameters, zero_grad
export linear_create  # Legacy API
