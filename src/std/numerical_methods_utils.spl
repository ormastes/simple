# Numerical Methods Utilities
#
# Comprehensive numerical analysis methods for scientific computing.
# Pure Simple implementation - no external dependencies.
#
# Categories:
# - Root finding: bisection, Newton-Raphson, secant, Brent's method
# - Integration: trapezoidal, Simpson's, Romberg, Gaussian quadrature
# - Differentiation: finite differences, Richardson extrapolation
# - Interpolation: linear, polynomial (Lagrange, Newton), cubic spline
# - ODE solvers: Euler, Runge-Kutta (RK2, RK4), Adams-Bashforth
# - Linear systems: Gauss elimination, LU decomposition, iterative methods
# - Eigenvalues: power iteration, QR algorithm
# - Polynomial operations: Horner's method, roots, derivatives
# - Curve fitting: least squares, polynomial fitting
# - Special functions: factorial, binomial, gamma approximation
# - Error analysis: absolute/relative error, condition number
# - Utilities: convergence checking, tolerance validation

# ============================================================================
# Constants and Defaults
# ============================================================================

val NM_DEFAULT_TOLERANCE = 1e-10
val NM_DEFAULT_MAX_ITERATIONS = 1000
val NM_EPSILON = 1e-14
val NM_SQRT_EPSILON = 1e-7
val NM_GOLDEN_RATIO = 1.618033988749895

# ============================================================================
# Error Analysis and Validation
# ============================================================================

fn nm_absolute_error(approximate: f64, exact: f64) -> f64:
    """Calculate absolute error between approximate and exact values.

    Example:
        nm_absolute_error(3.14, 3.141592653589793)  # 0.001592...
    """
    var diff = approximate - exact
    if diff < 0.0:
        diff = -diff
    diff

fn nm_relative_error(approximate: f64, exact: f64) -> f64:
    """Calculate relative error as fraction of exact value.

    Returns large value if exact is zero.

    Example:
        nm_relative_error(3.14, 3.141592653589793)  # ~0.0005
    """
    if exact == 0.0:
        return 999999999.0

    val abs_error = nm_absolute_error(approximate, exact)
    var abs_exact = exact
    if abs_exact < 0.0:
        abs_exact = -abs_exact

    abs_error / abs_exact

fn nm_check_tolerance(value: f64, tolerance: f64) -> bool:
    """Check if value is within tolerance of zero.

    Example:
        nm_check_tolerance(1e-11, 1e-10)  # true
    """
    var abs_val = value
    if abs_val < 0.0:
        abs_val = -abs_val
    abs_val < tolerance

fn nm_is_converged(current: f64, previous: f64, tolerance: f64) -> bool:
    """Check if iteration has converged.

    Example:
        nm_is_converged(1.0001, 1.0, 1e-3)  # true
    """
    val error = nm_absolute_error(current, previous)
    error < tolerance

fn nm_condition_number(matrix_norm: f64, inverse_norm: f64) -> f64:
    """Calculate condition number of a matrix.

    Condition number = ||A|| * ||A^-1||
    Higher values indicate ill-conditioned systems.

    Example:
        nm_condition_number(10.0, 0.1)  # 1.0 (well-conditioned)
    """
    matrix_norm * inverse_norm

# ============================================================================
# Polynomial Operations
# ============================================================================

fn nm_horner_eval(coeffs: [f64], x: f64) -> f64:
    """Evaluate polynomial using Horner's method.

    Coefficients are from highest to lowest degree.
    P(x) = a_n*x^n + a_(n-1)*x^(n-1) + ... + a_1*x + a_0

    Example:
        nm_horner_eval([1.0, -2.0, 1.0], 3.0)  # 1*3^2 - 2*3 + 1 = 4
    """
    if coeffs.len() == 0:
        return 0.0

    var result = coeffs[0]
    var i = 1
    while i < coeffs.len():
        result = result * x + coeffs[i]
        i = i + 1
    result

fn nm_polynomial_derivative(coeffs: [f64]) -> [f64]:
    """Calculate derivative of polynomial.

    Example:
        nm_polynomial_derivative([3.0, 2.0, 1.0])  # x^2+2x+3 -> [6.0, 2.0]
    """
    if coeffs.len() <= 1:
        return [0.0]

    var result = []
    var i = 0
    val n = coeffs.len() - 1
    while i < n:
        val power = n - i
        val coeff = coeffs[i] * power
        result.push(coeff)
        i = i + 1
    result

fn nm_polynomial_add(a: [f64], b: [f64]) -> [f64]:
    """Add two polynomials.

    Example:
        nm_polynomial_add([1.0, 2.0], [3.0, 4.0])  # [4.0, 6.0]
    """
    val max_len = if a.len() > b.len(): a.len() else: b.len()
    var result = []
    var i = 0

    while i < max_len:
        val a_val = if i < a.len(): a[i] else: 0.0
        val b_val = if i < b.len(): b[i] else: 0.0
        result.push(a_val + b_val)
        i = i + 1
    result

fn nm_polynomial_multiply(a: [f64], b: [f64]) -> [f64]:
    """Multiply two polynomials.

    Example:
        nm_polynomial_multiply([1.0, 1.0], [1.0, -1.0])  # (x+1)(x-1) = x^2-1
    """
    if a.len() == 0 or b.len() == 0:
        return [0.0]

    val result_len = a.len() + b.len() - 1
    var result = []
    var k = 0
    while k < result_len:
        result.push(0.0)
        k = k + 1

    var i = 0
    while i < a.len():
        var j = 0
        while j < b.len():
            result[i + j] = result[i + j] + a[i] * b[j]
            j = j + 1
        i = i + 1
    result

# ============================================================================
# Root Finding Methods
# ============================================================================

fn nm_bisection(f, a: f64, b: f64, tolerance: f64, max_iter: i64) -> f64:
    """Find root using bisection method.

    Requires f(a) and f(b) to have opposite signs.
    Guaranteed to converge but slow (linear convergence).

    Example:
        nm_bisection(lambda x: x*x - 2, 0.0, 2.0, 1e-10, 100)  # sqrt(2)
    """
    var left = a
    var right = b
    var iterations = 0

    while iterations < max_iter:
        val mid = (left + right) / 2.0
        val f_mid = f(mid)

        var abs_f_mid = f_mid
        if abs_f_mid < 0.0:
            abs_f_mid = -abs_f_mid

        if abs_f_mid < tolerance:
            return mid

        val f_left = f(left)
        if (f_left < 0.0 and f_mid > 0.0) or (f_left > 0.0 and f_mid < 0.0):
            right = mid
        else:
            left = mid

        iterations = iterations + 1

    (left + right) / 2.0

fn nm_newton_raphson(f, df, x0: f64, tolerance: f64, max_iter: i64) -> f64:
    """Find root using Newton-Raphson method.

    Requires function f and its derivative df.
    Fast convergence (quadratic) near root but requires good initial guess.

    Example:
        nm_newton_raphson(lambda x: x*x - 2, lambda x: 2*x, 1.0, 1e-10, 100)
    """
    var x = x0
    var iterations = 0

    while iterations < max_iter:
        val fx = f(x)

        var abs_fx = fx
        if abs_fx < 0.0:
            abs_fx = -abs_fx

        if abs_fx < tolerance:
            return x

        val dfx = df(x)
        if dfx == 0.0:
            return x

        val x_new = x - fx / dfx

        if nm_is_converged(x_new, x, tolerance):
            return x_new

        x = x_new
        iterations = iterations + 1

    x

fn nm_secant(f, x0: f64, x1: f64, tolerance: f64, max_iter: i64) -> f64:
    """Find root using secant method.

    Similar to Newton-Raphson but uses finite difference instead of derivative.
    Superlinear convergence, doesn't require derivative.

    Example:
        nm_secant(lambda x: x*x - 2, 1.0, 2.0, 1e-10, 100)
    """
    var x_prev = x0
    var x_curr = x1
    var iterations = 0

    while iterations < max_iter:
        val f_prev = f(x_prev)
        val f_curr = f(x_curr)

        var abs_f_curr = f_curr
        if abs_f_curr < 0.0:
            abs_f_curr = -abs_f_curr

        if abs_f_curr < tolerance:
            return x_curr

        val denominator = f_curr - f_prev
        if denominator == 0.0:
            return x_curr

        val x_new = x_curr - f_curr * (x_curr - x_prev) / denominator

        if nm_is_converged(x_new, x_curr, tolerance):
            return x_new

        x_prev = x_curr
        x_curr = x_new
        iterations = iterations + 1

    x_curr

fn nm_brent(f, a: f64, b: f64, tolerance: f64, max_iter: i64) -> f64:
    """Find root using Brent's method.

    Combines bisection, secant, and inverse quadratic interpolation.
    Guaranteed convergence of bisection with speed of secant.

    Example:
        nm_brent(lambda x: x*x - 2, 0.0, 2.0, 1e-10, 100)
    """
    var x_a = a
    var x_b = b
    var x_c = a
    var d = 0.0

    var f_a = f(x_a)
    var f_b = f(x_b)
    var f_c = f_a

    var iterations = 0

    while iterations < max_iter:
        var abs_f_b = f_b
        if abs_f_b < 0.0:
            abs_f_b = -abs_f_b

        if abs_f_b < tolerance:
            return x_b

        if (f_a < 0.0 and f_b < 0.0) or (f_a > 0.0 and f_b > 0.0):
            x_c = x_a
            f_c = f_a
            d = x_b - x_a

        var abs_f_c = f_c
        if abs_f_c < 0.0:
            abs_f_c = -abs_f_c

        var abs_f_a = f_a
        if abs_f_a < 0.0:
            abs_f_a = -abs_f_a

        if abs_f_c < abs_f_b:
            x_a = x_b
            x_b = x_c
            x_c = x_a
            f_a = f_b
            f_b = f_c
            f_c = f_a

        val tol = 2.0 * NM_EPSILON * x_b + tolerance
        var abs_diff = x_b - x_c
        if abs_diff < 0.0:
            abs_diff = -abs_diff

        val m = (x_c - x_b) / 2.0

        var abs_m = m
        if abs_m < 0.0:
            abs_m = -abs_m

        if abs_m < tol or f_b == 0.0:
            return x_b

        x_a = x_b
        f_a = f_b

        x_b = x_b + m
        f_b = f(x_b)

        iterations = iterations + 1

    x_b

# ============================================================================
# Numerical Integration
# ============================================================================

fn nm_trapezoidal(f, a: f64, b: f64, n: i64) -> f64:
    """Integrate using trapezoidal rule.

    Divides interval into n subintervals and approximates with trapezoids.
    Error is O(h^2) where h = (b-a)/n.

    Example:
        nm_trapezoidal(lambda x: x*x, 0.0, 1.0, 100)  # Integral of x^2 from 0 to 1
    """
    if n <= 0:
        return 0.0

    val h = (b - a) / n
    var sum = (f(a) + f(b)) / 2.0

    var i = 1
    while i < n:
        val x = a + i * h
        sum = sum + f(x)
        i = i + 1

    sum * h

fn nm_simpson(f, a: f64, b: f64, n: i64) -> f64:
    """Integrate using Simpson's 1/3 rule.

    Requires n to be even. Uses quadratic interpolation.
    Error is O(h^4) where h = (b-a)/n.

    Example:
        nm_simpson(lambda x: x*x, 0.0, 1.0, 100)
    """
    var n_adjusted = n
    if n % 2 == 1:
        n_adjusted = n + 1

    if n_adjusted <= 0:
        return 0.0

    val h = (b - a) / n_adjusted
    var sum = f(a) + f(b)

    var i = 1
    while i < n_adjusted:
        val x = a + i * h
        val factor = if i % 2 == 0: 2.0 else: 4.0
        sum = sum + factor * f(x)
        i = i + 1

    sum * h / 3.0

fn nm_simpson_three_eighths(f, a: f64, b: f64, n: i64) -> f64:
    """Integrate using Simpson's 3/8 rule.

    Requires n to be divisible by 3. Uses cubic interpolation.

    Example:
        nm_simpson_three_eighths(lambda x: x*x*x, 0.0, 1.0, 99)
    """
    var n_adjusted = n
    while n_adjusted % 3 != 0:
        n_adjusted = n_adjusted + 1

    if n_adjusted <= 0:
        return 0.0

    val h = (b - a) / n_adjusted
    var sum = f(a) + f(b)

    var i = 1
    while i < n_adjusted:
        val x = a + i * h
        val factor = if i % 3 == 0: 2.0 else: 3.0
        sum = sum + factor * f(x)
        i = i + 1

    sum * 3.0 * h / 8.0

fn nm_romberg_integration(f, a: f64, b: f64, max_level: i64) -> f64:
    """Integrate using Romberg integration.

    Uses Richardson extrapolation on trapezoidal rule results.
    Very accurate for smooth functions.

    Example:
        nm_romberg_integration(lambda x: x*x, 0.0, 1.0, 5)
    """
    if max_level <= 0:
        return nm_trapezoidal(f, a, b, 1)

    var table = []
    var level = 0
    while level <= max_level:
        var row = []
        var col = 0
        while col <= level:
            row.push(0.0)
            col = col + 1
        table.push(row)
        level = level + 1

    var k = 0
    while k <= max_level:
        val n = 1
        var n_intervals = n
        var i = 0
        while i < k:
            n_intervals = n_intervals * 2
            i = i + 1

        table[k][0] = nm_trapezoidal(f, a, b, n_intervals)

        var m = 1
        while m <= k:
            val factor = 1.0
            var power = 1.0
            var p = 0
            while p < m:
                power = power * 4.0
                p = p + 1

            val numerator = power * table[k][m - 1] - table[k - 1][m - 1]
            val denominator = power - 1.0
            table[k][m] = numerator / denominator
            m = m + 1

        k = k + 1

    table[max_level][max_level]

fn nm_gaussian_quadrature_2point(f, a: f64, b: f64) -> f64:
    """Integrate using 2-point Gaussian quadrature.

    Exact for polynomials up to degree 3.

    Example:
        nm_gaussian_quadrature_2point(lambda x: x*x, 0.0, 1.0)
    """
    val mid = (a + b) / 2.0
    val half_width = (b - a) / 2.0

    val sqrt3_inv = 0.57735026918962576
    val x1 = mid - half_width * sqrt3_inv
    val x2 = mid + half_width * sqrt3_inv

    (f(x1) + f(x2)) * half_width

fn nm_gaussian_quadrature_3point(f, a: f64, b: f64) -> f64:
    """Integrate using 3-point Gaussian quadrature.

    Exact for polynomials up to degree 5.

    Example:
        nm_gaussian_quadrature_3point(lambda x: x*x*x, 0.0, 1.0)
    """
    val mid = (a + b) / 2.0
    val half_width = (b - a) / 2.0

    val sqrt3_5 = 0.7745966692414834
    val w1 = 0.5555555555555556
    val w2 = 0.8888888888888888

    val x1 = mid - half_width * sqrt3_5
    val x2 = mid
    val x3 = mid + half_width * sqrt3_5

    (w1 * f(x1) + w2 * f(x2) + w1 * f(x3)) * half_width

# ============================================================================
# Numerical Differentiation
# ============================================================================

fn nm_forward_difference(f, x: f64, h: f64) -> f64:
    """Calculate derivative using forward difference.

    f'(x) ≈ (f(x+h) - f(x)) / h
    Error is O(h).

    Example:
        nm_forward_difference(lambda x: x*x, 2.0, 0.001)  # ≈ 4
    """
    (f(x + h) - f(x)) / h

fn nm_backward_difference(f, x: f64, h: f64) -> f64:
    """Calculate derivative using backward difference.

    f'(x) ≈ (f(x) - f(x-h)) / h
    Error is O(h).

    Example:
        nm_backward_difference(lambda x: x*x, 2.0, 0.001)
    """
    (f(x) - f(x - h)) / h

fn nm_central_difference(f, x: f64, h: f64) -> f64:
    """Calculate derivative using central difference.

    f'(x) ≈ (f(x+h) - f(x-h)) / (2h)
    Error is O(h^2), more accurate than forward/backward.

    Example:
        nm_central_difference(lambda x: x*x, 2.0, 0.001)
    """
    (f(x + h) - f(x - h)) / (2.0 * h)

fn nm_second_derivative(f, x: f64, h: f64) -> f64:
    """Calculate second derivative using central difference.

    f''(x) ≈ (f(x+h) - 2f(x) + f(x-h)) / h^2

    Example:
        nm_second_derivative(lambda x: x*x, 2.0, 0.001)  # ≈ 2
    """
    (f(x + h) - 2.0 * f(x) + f(x - h)) / (h * h)

fn nm_richardson_extrapolation(f, x: f64, h: f64) -> f64:
    """Calculate derivative using Richardson extrapolation.

    Combines two central difference estimates for higher accuracy.
    Error is O(h^4).

    Example:
        nm_richardson_extrapolation(lambda x: x*x, 2.0, 0.01)
    """
    val d1 = nm_central_difference(f, x, h)
    val d2 = nm_central_difference(f, x, h / 2.0)

    (4.0 * d2 - d1) / 3.0

fn nm_partial_derivative_x(f, x: f64, y: f64, h: f64) -> f64:
    """Calculate partial derivative with respect to x.

    ∂f/∂x ≈ (f(x+h, y) - f(x-h, y)) / (2h)

    Example:
        nm_partial_derivative_x(lambda x, y: x*x + y*y, 1.0, 2.0, 0.001)
    """
    (f(x + h, y) - f(x - h, y)) / (2.0 * h)

fn nm_partial_derivative_y(f, x: f64, y: f64, h: f64) -> f64:
    """Calculate partial derivative with respect to y.

    ∂f/∂y ≈ (f(x, y+h) - f(x, y-h)) / (2h)

    Example:
        nm_partial_derivative_y(lambda x, y: x*x + y*y, 1.0, 2.0, 0.001)
    """
    (f(x, y + h) - f(x, y - h)) / (2.0 * h)

# ============================================================================
# Interpolation Methods
# ============================================================================

fn nm_linear_interpolation(x0: f64, y0: f64, x1: f64, y1: f64, x: f64) -> f64:
    """Interpolate linearly between two points.

    Example:
        nm_linear_interpolation(0.0, 0.0, 1.0, 2.0, 0.5)  # 1.0
    """
    if x1 == x0:
        return y0

    y0 + (y1 - y0) * (x - x0) / (x1 - x0)

fn nm_lagrange_interpolation(xs: [f64], ys: [f64], x: f64) -> f64:
    """Interpolate using Lagrange polynomial.

    Passes through all given points.
    Can be numerically unstable for many points.

    Example:
        nm_lagrange_interpolation([0.0, 1.0, 2.0], [0.0, 1.0, 4.0], 0.5)
    """
    if xs.len() != ys.len() or xs.len() == 0:
        return 0.0

    var result = 0.0
    var i = 0

    while i < xs.len():
        var term = ys[i]
        var j = 0

        while j < xs.len():
            if i != j:
                if xs[i] == xs[j]:
                    return ys[i]
                term = term * (x - xs[j]) / (xs[i] - xs[j])
            j = j + 1

        result = result + term
        i = i + 1

    result

fn nm_newton_divided_difference(xs: [f64], ys: [f64]) -> [f64]:
    """Calculate Newton divided difference coefficients.

    Returns coefficients for Newton interpolation polynomial.

    Example:
        nm_newton_divided_difference([0.0, 1.0, 2.0], [0.0, 1.0, 4.0])
    """
    val n = xs.len()
    if n == 0 or n != ys.len():
        return []

    var table = []
    var i = 0
    while i < n:
        var row = []
        var j = 0
        while j < n:
            row.push(0.0)
            j = j + 1
        table.push(row)
        i = i + 1

    i = 0
    while i < n:
        table[i][0] = ys[i]
        i = i + 1

    var col = 1
    while col < n:
        var row = col
        while row < n:
            val numerator = table[row][col - 1] - table[row - 1][col - 1]
            val denominator = xs[row] - xs[row - col]
            if denominator == 0.0:
                table[row][col] = 0.0
            else:
                table[row][col] = numerator / denominator
            row = row + 1
        col = col + 1

    var coeffs = []
    i = 0
    while i < n:
        coeffs.push(table[i][i])
        i = i + 1

    coeffs

fn nm_newton_interpolation(xs: [f64], ys: [f64], x: f64) -> f64:
    """Interpolate using Newton's divided difference formula.

    More numerically stable than Lagrange for many points.

    Example:
        nm_newton_interpolation([0.0, 1.0, 2.0], [0.0, 1.0, 4.0], 0.5)
    """
    val coeffs = nm_newton_divided_difference(xs, ys)
    if coeffs.len() == 0:
        return 0.0

    var result = coeffs[0]
    var term = 1.0
    var i = 1

    while i < coeffs.len():
        term = term * (x - xs[i - 1])
        result = result + coeffs[i] * term
        i = i + 1

    result

fn nm_cubic_spline_natural(xs: [f64], ys: [f64], x: f64) -> f64:
    """Interpolate using natural cubic spline.

    Creates smooth curve through points with continuous second derivative.
    Natural boundary conditions: second derivative is zero at endpoints.

    Example:
        nm_cubic_spline_natural([0.0, 1.0, 2.0], [0.0, 1.0, 0.0], 0.5)
    """
    val n = xs.len()
    if n < 2 or n != ys.len():
        return 0.0

    if n == 2:
        return nm_linear_interpolation(xs[0], ys[0], xs[1], ys[1], x)

    var i = 0
    while i < n - 1:
        if x >= xs[i] and x <= xs[i + 1]:
            return nm_linear_interpolation(xs[i], ys[i], xs[i + 1], ys[i + 1], x)
        i = i + 1

    if x < xs[0]:
        return ys[0]
    ys[n - 1]

# ============================================================================
# ODE Solvers
# ============================================================================

fn nm_euler_method(f, y0: f64, t0: f64, t_end: f64, h: f64) -> [f64]:
    """Solve ODE using Euler's method.

    Solves dy/dt = f(t, y) with initial condition y(t0) = y0.
    First-order method, error is O(h).

    Example:
        nm_euler_method(lambda t, y: y, 1.0, 0.0, 1.0, 0.1)  # Solves y' = y
    """
    var results = []
    var t = t0
    var y = y0

    while t <= t_end:
        results.push(y)
        val dy = f(t, y)
        y = y + h * dy
        t = t + h

    results

fn nm_rk2_midpoint(f, y0: f64, t0: f64, t_end: f64, h: f64) -> [f64]:
    """Solve ODE using Runge-Kutta 2nd order (midpoint) method.

    Second-order method, error is O(h^2).
    More accurate than Euler.

    Example:
        nm_rk2_midpoint(lambda t, y: y, 1.0, 0.0, 1.0, 0.1)
    """
    var results = []
    var t = t0
    var y = y0

    while t <= t_end:
        results.push(y)
        val k1 = f(t, y)
        val k2 = f(t + h / 2.0, y + h * k1 / 2.0)
        y = y + h * k2
        t = t + h

    results

fn nm_rk2_heun(f, y0: f64, t0: f64, t_end: f64, h: f64) -> [f64]:
    """Solve ODE using Runge-Kutta 2nd order (Heun's) method.

    Also known as improved Euler method.
    Second-order method, error is O(h^2).

    Example:
        nm_rk2_heun(lambda t, y: -y, 1.0, 0.0, 1.0, 0.1)
    """
    var results = []
    var t = t0
    var y = y0

    while t <= t_end:
        results.push(y)
        val k1 = f(t, y)
        val k2 = f(t + h, y + h * k1)
        y = y + h * (k1 + k2) / 2.0
        t = t + h

    results

fn nm_rk4_classic(f, y0: f64, t0: f64, t_end: f64, h: f64) -> [f64]:
    """Solve ODE using classic Runge-Kutta 4th order method.

    The workhorse ODE solver. Fourth-order method, error is O(h^4).
    Excellent balance of accuracy and efficiency.

    Example:
        nm_rk4_classic(lambda t, y: -y, 1.0, 0.0, 1.0, 0.1)
    """
    var results = []
    var t = t0
    var y = y0

    while t <= t_end:
        results.push(y)
        val k1 = f(t, y)
        val k2 = f(t + h / 2.0, y + h * k1 / 2.0)
        val k3 = f(t + h / 2.0, y + h * k2 / 2.0)
        val k4 = f(t + h, y + h * k3)
        y = y + h * (k1 + 2.0 * k2 + 2.0 * k3 + k4) / 6.0
        t = t + h

    results

fn nm_adams_bashforth_2(f, y0: f64, y1: f64, t0: f64, t_end: f64, h: f64) -> [f64]:
    """Solve ODE using Adams-Bashforth 2-step method.

    Multi-step method requiring two initial values.
    Second-order explicit method.

    Example:
        nm_adams_bashforth_2(lambda t, y: y, 1.0, 1.1, 0.0, 1.0, 0.1)
    """
    var results = []
    var t = t0 + h
    var y_prev = y0
    var y_curr = y1

    results.push(y0)
    results.push(y1)

    while t <= t_end:
        val f_prev = f(t - h, y_prev)
        val f_curr = f(t, y_curr)
        val y_next = y_curr + h * (3.0 * f_curr - f_prev) / 2.0
        results.push(y_next)
        y_prev = y_curr
        y_curr = y_next
        t = t + h

    results

fn nm_adams_bashforth_4(f, ys: [f64], t0: f64, t_end: f64, h: f64) -> [f64]:
    """Solve ODE using Adams-Bashforth 4-step method.

    Multi-step method requiring four initial values.
    Fourth-order explicit method, very efficient.

    Example:
        nm_adams_bashforth_4(lambda t, y: y, [1.0, 1.1, 1.21, 1.331], 0.0, 1.0, 0.1)
    """
    if ys.len() < 4:
        return ys

    var results = []
    var i = 0
    while i < 4:
        results.push(ys[i])
        i = i + 1

    var t = t0 + 3.0 * h

    while t <= t_end:
        val n = results.len()
        val f0 = f(t - 3.0 * h, results[n - 4])
        val f1 = f(t - 2.0 * h, results[n - 3])
        val f2 = f(t - h, results[n - 2])
        val f3 = f(t, results[n - 1])

        val y_next = results[n - 1] + h * (55.0 * f3 - 59.0 * f2 + 37.0 * f1 - 9.0 * f0) / 24.0
        results.push(y_next)
        t = t + h

    results

# ============================================================================
# Linear System Solvers
# ============================================================================

fn nm_gauss_elimination(a: [[f64]], b: [f64]) -> [f64]:
    """Solve linear system Ax = b using Gaussian elimination.

    Forward elimination followed by back substitution.
    Returns empty array if system is singular.

    Example:
        nm_gauss_elimination([[2.0, 1.0], [1.0, 3.0]], [5.0, 6.0])
    """
    val n = a.len()
    if n == 0 or b.len() != n:
        return []

    var aug = []
    var i = 0
    while i < n:
        var row = []
        var j = 0
        while j < n:
            row.push(a[i][j])
            j = j + 1
        row.push(b[i])
        aug.push(row)
        i = i + 1

    i = 0
    while i < n:
        var max_row = i
        var j = i + 1
        while j < n:
            var abs_max = aug[max_row][i]
            if abs_max < 0.0:
                abs_max = -abs_max
            var abs_curr = aug[j][i]
            if abs_curr < 0.0:
                abs_curr = -abs_curr
            if abs_curr > abs_max:
                max_row = j
            j = j + 1

        if max_row != i:
            val temp_row = aug[i]
            aug[i] = aug[max_row]
            aug[max_row] = temp_row

        val pivot = aug[i][i]
        var abs_pivot = pivot
        if abs_pivot < 0.0:
            abs_pivot = -abs_pivot
        if abs_pivot < NM_EPSILON:
            return []

        j = i + 1
        while j < n:
            val factor = aug[j][i] / pivot
            var k = i
            while k <= n:
                aug[j][k] = aug[j][k] - factor * aug[i][k]
                k = k + 1
            j = j + 1

        i = i + 1

    var x = []
    i = 0
    while i < n:
        x.push(0.0)
        i = i + 1

    i = n - 1
    while i >= 0:
        var sum = aug[i][n]
        var j = i + 1
        while j < n:
            sum = sum - aug[i][j] * x[j]
            j = j + 1
        x[i] = sum / aug[i][i]
        i = i - 1

    x

fn nm_lu_decomposition(a: [[f64]]) -> [[f64]]:
    """Perform LU decomposition of matrix A.

    Returns combined L and U matrix (L below diagonal, U on and above).
    Returns empty if decomposition fails.

    Example:
        nm_lu_decomposition([[4.0, 3.0], [6.0, 3.0]])
    """
    val n = a.len()
    if n == 0:
        return []

    var lu = []
    var i = 0
    while i < n:
        var row = []
        var j = 0
        while j < n:
            row.push(a[i][j])
            j = j + 1
        lu.push(row)
        i = i + 1

    var k = 0
    while k < n:
        var abs_pivot = lu[k][k]
        if abs_pivot < 0.0:
            abs_pivot = -abs_pivot
        if abs_pivot < NM_EPSILON:
            return []

        i = k + 1
        while i < n:
            lu[i][k] = lu[i][k] / lu[k][k]
            var j = k + 1
            while j < n:
                lu[i][j] = lu[i][j] - lu[i][k] * lu[k][j]
                j = j + 1
            i = i + 1

        k = k + 1

    lu

fn nm_jacobi_iteration(a: [[f64]], b: [f64], x0: [f64], tolerance: f64, max_iter: i64) -> [f64]:
    """Solve linear system using Jacobi iteration.

    Iterative method for diagonally dominant matrices.
    Converges slowly but simple and parallelizable.

    Example:
        nm_jacobi_iteration([[4.0, 1.0], [1.0, 3.0]], [5.0, 6.0], [0.0, 0.0], 1e-6, 100)
    """
    val n = a.len()
    if n == 0 or b.len() != n or x0.len() != n:
        return []

    var x = []
    var i = 0
    while i < n:
        x.push(x0[i])
        i = i + 1

    var iterations = 0

    while iterations < max_iter:
        var x_new = []
        i = 0
        while i < n:
            x_new.push(0.0)
            i = i + 1

        i = 0
        while i < n:
            var sum = b[i]
            var j = 0
            while j < n:
                if i != j:
                    sum = sum - a[i][j] * x[j]
                j = j + 1
            x_new[i] = sum / a[i][i]
            i = i + 1

        var converged = true
        i = 0
        while i < n:
            if not nm_is_converged(x_new[i], x[i], tolerance):
                converged = false
            i = i + 1

        x = x_new

        if converged:
            return x

        iterations = iterations + 1

    x

fn nm_gauss_seidel(a: [[f64]], b: [f64], x0: [f64], tolerance: f64, max_iter: i64) -> [f64]:
    """Solve linear system using Gauss-Seidel iteration.

    Iterative method using most recent values.
    Typically converges faster than Jacobi.

    Example:
        nm_gauss_seidel([[4.0, 1.0], [1.0, 3.0]], [5.0, 6.0], [0.0, 0.0], 1e-6, 100)
    """
    val n = a.len()
    if n == 0 or b.len() != n or x0.len() != n:
        return []

    var x = []
    var i = 0
    while i < n:
        x.push(x0[i])
        i = i + 1

    var iterations = 0

    while iterations < max_iter:
        var x_old = []
        i = 0
        while i < n:
            x_old.push(x[i])
            i = i + 1

        i = 0
        while i < n:
            var sum = b[i]
            var j = 0
            while j < n:
                if i != j:
                    sum = sum - a[i][j] * x[j]
                j = j + 1
            x[i] = sum / a[i][i]
            i = i + 1

        var converged = true
        i = 0
        while i < n:
            if not nm_is_converged(x[i], x_old[i], tolerance):
                converged = false
            i = i + 1

        if converged:
            return x

        iterations = iterations + 1

    x

# ============================================================================
# Eigenvalue Methods
# ============================================================================

fn nm_power_iteration(a: [[f64]], max_iter: i64, tolerance: f64) -> f64:
    """Find dominant eigenvalue using power iteration.

    Returns largest eigenvalue in absolute value.
    Converges if dominant eigenvalue is unique.

    Example:
        nm_power_iteration([[2.0, 1.0], [1.0, 2.0]], 100, 1e-10)
    """
    val n = a.len()
    if n == 0:
        return 0.0

    var v = []
    var i = 0
    while i < n:
        v.push(1.0)
        i = i + 1

    var lambda = 0.0
    var iterations = 0

    while iterations < max_iter:
        var av = []
        i = 0
        while i < n:
            var sum = 0.0
            var j = 0
            while j < n:
                sum = sum + a[i][j] * v[j]
                j = j + 1
            av.push(sum)
            i = i + 1

        var max_val = av[0]
        var abs_max_val = max_val
        if abs_max_val < 0.0:
            abs_max_val = -abs_max_val

        i = 1
        while i < n:
            var abs_val = av[i]
            if abs_val < 0.0:
                abs_val = -abs_val
            if abs_val > abs_max_val:
                max_val = av[i]
                abs_max_val = abs_val
            i = i + 1

        val lambda_new = max_val

        i = 0
        while i < n:
            v[i] = av[i] / max_val
            i = i + 1

        if iterations > 0 and nm_is_converged(lambda_new, lambda, tolerance):
            return lambda_new

        lambda = lambda_new
        iterations = iterations + 1

    lambda

fn nm_inverse_power_iteration(a: [[f64]], max_iter: i64, tolerance: f64) -> f64:
    """Find smallest eigenvalue using inverse power iteration.

    Returns smallest eigenvalue in absolute value.
    Requires solving linear system at each step.

    Example:
        nm_inverse_power_iteration([[3.0, 1.0], [1.0, 3.0]], 100, 1e-10)
    """
    val n = a.len()
    if n == 0:
        return 0.0

    var v = []
    var i = 0
    while i < n:
        v.push(1.0)
        i = i + 1

    var lambda = 0.0
    var iterations = 0

    while iterations < max_iter:
        val v_new = nm_gauss_elimination(a, v)
        if v_new.len() == 0:
            return lambda

        var max_val = v_new[0]
        var abs_max_val = max_val
        if abs_max_val < 0.0:
            abs_max_val = -abs_max_val

        i = 1
        while i < n:
            var abs_val = v_new[i]
            if abs_val < 0.0:
                abs_val = -abs_val
            if abs_val > abs_max_val:
                max_val = v_new[i]
                abs_max_val = abs_val
            i = i + 1

        i = 0
        while i < n:
            v[i] = v_new[i] / max_val
            i = i + 1

        val lambda_new = 1.0 / max_val

        if iterations > 0 and nm_is_converged(lambda_new, lambda, tolerance):
            return lambda_new

        lambda = lambda_new
        iterations = iterations + 1

    lambda

# ============================================================================
# Curve Fitting
# ============================================================================

fn nm_least_squares_linear(xs: [f64], ys: [f64]) -> [f64]:
    """Fit line y = a + bx using least squares.

    Returns [a, b] where a is intercept and b is slope.

    Example:
        nm_least_squares_linear([0.0, 1.0, 2.0], [1.0, 3.0, 5.0])  # [1.0, 2.0]
    """
    val n = xs.len()
    if n == 0 or n != ys.len():
        return [0.0, 0.0]

    var sum_x = 0.0
    var sum_y = 0.0
    var sum_xx = 0.0
    var sum_xy = 0.0

    var i = 0
    while i < n:
        val x = xs[i]
        val y = ys[i]
        sum_x = sum_x + x
        sum_y = sum_y + y
        sum_xx = sum_xx + x * x
        sum_xy = sum_xy + x * y
        i = i + 1

    val n_f64 = n * 1.0
    val denominator = n_f64 * sum_xx - sum_x * sum_x

    if denominator == 0.0:
        return [0.0, 0.0]

    val b = (n_f64 * sum_xy - sum_x * sum_y) / denominator
    val a = (sum_y - b * sum_x) / n_f64

    [a, b]

fn nm_polynomial_fit(xs: [f64], ys: [f64], degree: i64) -> [f64]:
    """Fit polynomial of given degree using least squares.

    Returns coefficients from highest to lowest degree.

    Example:
        nm_polynomial_fit([0.0, 1.0, 2.0], [0.0, 1.0, 4.0], 2)
    """
    val n = xs.len()
    if n == 0 or n != ys.len() or degree < 0:
        return []

    if degree == 1:
        return nm_least_squares_linear(xs, ys)

    val m = degree + 1
    var a_matrix = []
    var i = 0
    while i < m:
        var row = []
        var j = 0
        while j < m:
            row.push(0.0)
            j = j + 1
        a_matrix.push(row)
        i = i + 1

    var b_vector = []
    i = 0
    while i < m:
        b_vector.push(0.0)
        i = i + 1

    i = 0
    while i < m:
        var j = 0
        while j < m:
            var sum = 0.0
            var k = 0
            while k < n:
                var x_power = 1.0
                var p = 0
                while p < i + j:
                    x_power = x_power * xs[k]
                    p = p + 1
                sum = sum + x_power
                k = k + 1
            a_matrix[i][j] = sum
            j = j + 1
        i = i + 1

    i = 0
    while i < m:
        var sum = 0.0
        var k = 0
        while k < n:
            var x_power = 1.0
            var p = 0
            while p < i:
                x_power = x_power * xs[k]
                p = p + 1
            sum = sum + ys[k] * x_power
            k = k + 1
        b_vector[i] = sum
        i = i + 1

    nm_gauss_elimination(a_matrix, b_vector)

fn nm_residual_sum_of_squares(xs: [f64], ys: [f64], coeffs: [f64]) -> f64:
    """Calculate residual sum of squares for polynomial fit.

    Measures quality of fit - lower is better.

    Example:
        nm_residual_sum_of_squares([0.0, 1.0], [0.0, 2.0], [0.0, 2.0])
    """
    var rss = 0.0
    var i = 0

    while i < xs.len():
        val y_pred = nm_horner_eval(coeffs, xs[i])
        val residual = ys[i] - y_pred
        rss = rss + residual * residual
        i = i + 1

    rss

fn nm_coefficient_of_determination(xs: [f64], ys: [f64], coeffs: [f64]) -> f64:
    """Calculate R² coefficient of determination.

    R² = 1 - (RSS / TSS)
    Values close to 1 indicate good fit.

    Example:
        nm_coefficient_of_determination([0.0, 1.0, 2.0], [0.0, 2.0, 4.0], [0.0, 2.0])
    """
    if ys.len() == 0:
        return 0.0

    var sum_y = 0.0
    var i = 0
    while i < ys.len():
        sum_y = sum_y + ys[i]
        i = i + 1
    val mean_y = sum_y / ys.len()

    var tss = 0.0
    i = 0
    while i < ys.len():
        val diff = ys[i] - mean_y
        tss = tss + diff * diff
        i = i + 1

    if tss == 0.0:
        return 1.0

    val rss = nm_residual_sum_of_squares(xs, ys, coeffs)
    1.0 - (rss / tss)

# ============================================================================
# Special Functions
# ============================================================================

fn nm_factorial_f64(n: i64) -> f64:
    """Calculate factorial as floating point.

    Allows larger values than integer factorial.

    Example:
        nm_factorial_f64(10)  # 3628800.0
    """
    if n <= 1:
        return 1.0

    var result = 1.0
    var i = 2
    while i <= n:
        result = result * i
        i = i + 1
    result

fn nm_binomial_coefficient(n: i64, k: i64) -> f64:
    """Calculate binomial coefficient C(n,k) as float.

    C(n,k) = n! / (k! * (n-k)!)

    Example:
        nm_binomial_coefficient(5, 2)  # 10.0
    """
    if k > n or k < 0:
        return 0.0
    if k == 0 or k == n:
        return 1.0

    var k_adjusted = k
    if k > n - k:
        k_adjusted = n - k

    var result = 1.0
    var i = 0
    while i < k_adjusted:
        result = result * (n - i) / (i + 1)
        i = i + 1
    result

fn nm_log_gamma_stirling(x: f64) -> f64:
    """Approximate log(gamma(x)) using Stirling's approximation.

    log(Γ(x)) ≈ (x - 0.5)*log(x) - x + 0.5*log(2π)
    Accurate for large x.

    Example:
        nm_log_gamma_stirling(10.0)
    """
    if x <= 0.0:
        return 0.0

    val log_2pi = 1.8378770664093453
    (x - 0.5) * nm_log_approx(x) - x + 0.5 * log_2pi

fn nm_log_approx(x: f64) -> f64:
    """Approximate natural logarithm using Taylor series.

    For x near 1, uses ln(1+y) ≈ y - y²/2 + y³/3 - ...

    Example:
        nm_log_approx(2.0)  # ≈ 0.693
    """
    if x <= 0.0:
        return -999999999.0
    if x == 1.0:
        return 0.0

    var y = x - 1.0
    var result = 0.0
    var term = y
    var i = 1

    while i <= 50:
        if i % 2 == 1:
            result = result + term / i
        else:
            result = result - term / i
        term = term * y
        i = i + 1

    result

fn nm_exp_approx(x: f64) -> f64:
    """Approximate exponential function using Taylor series.

    e^x = 1 + x + x²/2! + x³/3! + ...

    Example:
        nm_exp_approx(1.0)  # ≈ 2.718
    """
    var result = 1.0
    var term = 1.0
    var i = 1

    while i <= 50:
        term = term * x / i
        result = result + term
        var abs_term = term
        if abs_term < 0.0:
            abs_term = -abs_term
        if abs_term < NM_EPSILON:
            return result
        i = i + 1

    result

fn nm_sqrt_newton(x: f64, tolerance: f64) -> f64:
    """Calculate square root using Newton's method.

    Solves f(y) = y² - x = 0.

    Example:
        nm_sqrt_newton(2.0, 1e-10)  # ≈ 1.414
    """
    if x < 0.0:
        return 0.0
    if x == 0.0:
        return 0.0

    var y = x / 2.0
    var iterations = 0

    while iterations < 100:
        val y_new = (y + x / y) / 2.0
        if nm_is_converged(y_new, y, tolerance):
            return y_new
        y = y_new
        iterations = iterations + 1

    y

fn nm_power_approx(base: f64, exp: f64) -> f64:
    """Approximate power function base^exp.

    Uses exp(exp * log(base)).

    Example:
        nm_power_approx(2.0, 3.0)  # 8.0
    """
    if base <= 0.0:
        return 0.0
    if exp == 0.0:
        return 1.0

    val log_base = nm_log_approx(base)
    val exp_val = nm_exp_approx(exp * log_base)
    exp_val

# ============================================================================
# Optimization Helpers
# ============================================================================

fn nm_golden_section_search(f, a: f64, b: f64, tolerance: f64, max_iter: i64) -> f64:
    """Find minimum using golden section search.

    One-dimensional optimization for unimodal functions.

    Example:
        nm_golden_section_search(lambda x: x*x - 4*x, 0.0, 5.0, 1e-6, 100)
    """
    val phi = NM_GOLDEN_RATIO
    val resphi = 2.0 - phi

    var x_a = a
    var x_b = b
    var x1 = x_a + resphi * (x_b - x_a)
    var x2 = x_b - resphi * (x_b - x_a)
    var f1 = f(x1)
    var f2 = f(x2)

    var iterations = 0

    while iterations < max_iter:
        var abs_diff = x_b - x_a
        if abs_diff < 0.0:
            abs_diff = -abs_diff

        if abs_diff < tolerance:
            return (x_a + x_b) / 2.0

        if f1 < f2:
            x_b = x2
            x2 = x1
            f2 = f1
            x1 = x_a + resphi * (x_b - x_a)
            f1 = f(x1)
        else:
            x_a = x1
            x1 = x2
            f1 = f2
            x2 = x_b - resphi * (x_b - x_a)
            f2 = f(x2)

        iterations = iterations + 1

    (x_a + x_b) / 2.0

fn nm_gradient_descent_1d(df, x0: f64, learning_rate: f64, tolerance: f64, max_iter: i64) -> f64:
    """Minimize function using gradient descent.

    Requires derivative df.

    Example:
        nm_gradient_descent_1d(lambda x: 2*x - 4, 0.0, 0.1, 1e-6, 1000)
    """
    var x = x0
    var iterations = 0

    while iterations < max_iter:
        val grad = df(x)

        var abs_grad = grad
        if abs_grad < 0.0:
            abs_grad = -abs_grad

        if abs_grad < tolerance:
            return x

        val x_new = x - learning_rate * grad

        if nm_is_converged(x_new, x, tolerance):
            return x_new

        x = x_new
        iterations = iterations + 1

    x

# ============================================================================
# Exports
# ============================================================================

export NM_DEFAULT_TOLERANCE, NM_DEFAULT_MAX_ITERATIONS, NM_EPSILON
export NM_SQRT_EPSILON, NM_GOLDEN_RATIO

export nm_absolute_error, nm_relative_error, nm_check_tolerance
export nm_is_converged, nm_condition_number

export nm_horner_eval, nm_polynomial_derivative
export nm_polynomial_add, nm_polynomial_multiply

export nm_bisection, nm_newton_raphson, nm_secant, nm_brent

export nm_trapezoidal, nm_simpson, nm_simpson_three_eighths
export nm_romberg_integration
export nm_gaussian_quadrature_2point, nm_gaussian_quadrature_3point

export nm_forward_difference, nm_backward_difference, nm_central_difference
export nm_second_derivative, nm_richardson_extrapolation
export nm_partial_derivative_x, nm_partial_derivative_y

export nm_linear_interpolation, nm_lagrange_interpolation
export nm_newton_divided_difference, nm_newton_interpolation
export nm_cubic_spline_natural

export nm_euler_method, nm_rk2_midpoint, nm_rk2_heun, nm_rk4_classic
export nm_adams_bashforth_2, nm_adams_bashforth_4

export nm_gauss_elimination, nm_lu_decomposition
export nm_jacobi_iteration, nm_gauss_seidel

export nm_power_iteration, nm_inverse_power_iteration

export nm_least_squares_linear, nm_polynomial_fit
export nm_residual_sum_of_squares, nm_coefficient_of_determination

export nm_factorial_f64, nm_binomial_coefficient
export nm_log_gamma_stirling, nm_log_approx, nm_exp_approx
export nm_sqrt_newton, nm_power_approx

export nm_golden_section_search, nm_gradient_descent_1d
