# GPU Runtime-Compatible API
# Simplified version that works with runtime parser
#
# This is a subset of the full GPU API that avoids:
# - struct with separate impl blocks
# - complex generics
# - advanced type features

use lib.torch.{TorchTensorWrapper, TorchStream, torch_available, torch_cuda_available}

# ============================================================================
# Backend Detection (Functions Only)
# ============================================================================

fn gpu_available() -> bool:
    """Check if GPU is available."""
    torch_cuda_available()

fn gpu_backend_name() -> text:
    """Get GPU backend name."""
    if torch_cuda_available():
        "CUDA"
    else:
        "CPU"

fn gpu_device_count() -> i32:
    """Get number of GPU devices."""
    if torch_cuda_available():
        1  # TODO: Query actual device count
    else:
        0

# ============================================================================
# Simple Tensor Operations (Runtime Compatible)
# ============================================================================

fn gpu_tensor_zeros(rows: i64, cols: i64) -> i64:
    """Create zero tensor and return handle.

    Returns handle (use with other gpu_tensor_* functions).
    """
    val t = TorchTensorWrapper.tensor_zeros([rows, cols])
    t.handle

fn gpu_tensor_ones(rows: i64, cols: i64) -> i64:
    """Create ones tensor and return handle."""
    val t = TorchTensorWrapper.tensor_ones([rows, cols])
    t.handle

fn gpu_tensor_to_cuda(tensor_handle: i64, device_id: i32) -> i64:
    """Move tensor to GPU.

    Args:
        tensor_handle: Tensor handle from gpu_tensor_*
        device_id: GPU device (0 = first GPU, 1 = second, etc.)

    Returns:
        New handle for GPU tensor
    """
    val t = TorchTensorWrapper(handle: tensor_handle, owns_handle: false)
    val gpu_t = t.cuda(device_id)
    gpu_t.handle

fn gpu_tensor_is_cuda(tensor_handle: i64) -> bool:
    """Check if tensor is on GPU."""
    val t = TorchTensorWrapper(handle: tensor_handle, owns_handle: false)
    t.is_cuda()

fn gpu_tensor_numel(tensor_handle: i64) -> i64:
    """Get number of elements in tensor."""
    val t = TorchTensorWrapper(handle: tensor_handle, owns_handle: false)
    t.numel()

# ============================================================================
# Stream Operations (Runtime Compatible)
# ============================================================================

fn gpu_stream_create(device_id: i32) -> i64:
    """Create CUDA stream for async operations.

    Returns:
        Stream handle
    """
    val stream = TorchStream.create(device_id)
    stream.handle

fn gpu_stream_sync(stream_handle: i64):
    """Wait for stream to complete (blocking)."""
    val stream = TorchStream(handle: stream_handle, owns_handle: false)
    stream.sync()

fn gpu_stream_query(stream_handle: i64) -> bool:
    """Check if stream is complete (non-blocking)."""
    val stream = TorchStream(handle: stream_handle, owns_handle: false)
    stream.query()

# ============================================================================
# High-Level Helpers
# ============================================================================

fn gpu_alloc_zeros(rows: i64, cols: i64, use_gpu: bool, device_id: i32) -> i64:
    """Allocate zero tensor, optionally on GPU.

    Args:
        rows: Number of rows
        cols: Number of columns
        use_gpu: If true, allocate on GPU
        device_id: Which GPU to use (if use_gpu=true)

    Returns:
        Tensor handle
    """
    val t_handle = gpu_tensor_zeros(rows, cols)

    if use_gpu and gpu_available():
        gpu_tensor_to_cuda(t_handle, device_id)
    else:
        t_handle

fn gpu_alloc_ones(rows: i64, cols: i64, use_gpu: bool, device_id: i32) -> i64:
    """Allocate ones tensor, optionally on GPU."""
    val t_handle = gpu_tensor_ones(rows, cols)

    if use_gpu and gpu_available():
        gpu_tensor_to_cuda(t_handle, device_id)
    else:
        t_handle

# ============================================================================
# Context-Style API (Function-Based)
# ============================================================================

fn gpu_ctx_info():
    """Print GPU context information."""
    print "GPU Backend: {gpu_backend_name()}"
    print "CUDA Available: {gpu_available()}"

    if gpu_available():
        print "Device Count: {gpu_device_count()}"
        print "Status: Ready"
    else:
        print "Status: CPU fallback"

fn gpu_ctx_alloc_zeros(rows: i64, cols: i64, device_id: i32) -> i64:
    """Context-style: allocate zeros on GPU.

    Simplified version of ctx.alloc_zeros[f32](rows * cols)
    """
    gpu_alloc_zeros(rows, cols, use_gpu: true, device_id: device_id)

fn gpu_ctx_alloc_ones(rows: i64, cols: i64, device_id: i32) -> i64:
    """Context-style: allocate ones on GPU."""
    gpu_alloc_ones(rows, cols, use_gpu: true, device_id: device_id)

# ============================================================================
# Async Pipeline Helpers
# ============================================================================

fn gpu_async_upload_batch(rows: i64, cols: i64, device_id: i32, stream_handle: i64) -> i64:
    """Upload batch to GPU asynchronously.

    Returns:
        Tensor handle (on GPU)
    """
    val t = gpu_tensor_zeros(rows, cols)
    gpu_tensor_to_cuda(t, device_id)

fn gpu_async_wait(stream_handle: i64):
    """Wait for async operations to complete."""
    gpu_stream_sync(stream_handle)

# ============================================================================
# Exports
# ============================================================================

export gpu_available
export gpu_backend_name
export gpu_device_count
export gpu_ctx_info

export gpu_tensor_zeros
export gpu_tensor_ones
export gpu_tensor_to_cuda
export gpu_tensor_is_cuda
export gpu_tensor_numel

export gpu_stream_create
export gpu_stream_sync
export gpu_stream_query

export gpu_alloc_zeros
export gpu_alloc_ones

export gpu_ctx_alloc_zeros
export gpu_ctx_alloc_ones

export gpu_async_upload_batch
export gpu_async_wait
