Simple Language SIMD GPU Extensions (Specification Update)

Version: 2025 Update – Adding GPU SIMD support with CUDA & AMD compatibility

Introduction

This update extends the Simple language with SIMD (Single Instruction, Multiple Data) features for GPU computing, aligning with both NVIDIA’s CUDA model and AMD’s GPU model. The goal is to provide a concise yet powerful syntax for writing data-parallel GPU kernels, while keeping the language grammar simple for one-pass parsing. Key additions include an indexer trait for custom indexing, a @simd annotation for GPU kernels, implicit thread indexing (similar to CUDA threads/blocks or OpenCL work-items/groups), neighbor element accessors, optional bounds-check suppression, and Rust-inspired error handling. These features enable writing GPU code that is expressive, strongly-typed, GC-free, and portable across NVIDIA and AMD platforms, without verbose boilerplate.

 Figure: A @simd function (kernel) executes on the GPU in parallel across many lightweight threads, each operating on different data. In Simple, marking a function or class with @simd makes it a kernel that can launch thousands of GPU threads, analogous to a CUDA __global__ function being executed by many CUDA threads simultaneously. The runtime distributes threads into groups (blocks/work-groups) that run the same kernel code on different data elements.

Design Goals:

One-Pass Parsable Grammar: The extended syntax is designed to be easy to parse in a single pass (LL(1)/LALR grammar) without backtracking. This is achieved by introducing clear, unambiguous keywords/annotations (like indexer and @simd) and requiring forward declarations where necessary. By simplifying grammar constructs, we avoid context-sensitive parses – “the grammar should be limited or simplified” for single-pass compilation.

Performance & Safety: GPU kernels run asynchronously and in parallel. The spec ensures that by default, @simd code is memory-safe (with optional bounds checks), strongly typed (no implicit casts or dynamic typing), and free of garbage collection pauses. Developers can opt-out of certain safety checks (e.g., range checking) when needed for performance-critical code.

Portability: The abstractions (e.g., thread groups, indexers, streams) are high-level and vendor-neutral. Code written with these features can target NVIDIA GPUs (CUDA) or AMD GPUs (HIP/OpenCL) without source changes. The compiler or runtime will map them to the appropriate CUDA or AMD/HIP constructs behind the scenes.

Reduced Boilerplate: Common GPU boilerplate (device/host qualifiers, launch configuration, index calculations, etc.) is handled implicitly. Simple’s compiler infers device vs. host context and provides default thread indexing and launch parameters, allowing the programmer to focus on kernel logic. Defaults are chosen to be sensible (e.g. private thread-local variables in kernels, automatic grid sizing to cover data lengths, etc.) but can be overridden for fine control.


Below, we detail the new syntax and semantics introduced in this update, followed by examples and a comparison table to existing CUDA/C++ concepts.

Grammar Updates for GPU Extensions

The following grammar snippets show the additions to support @simd annotations, indexer traits, and related features. The grammar remains LL(1)-friendly and avoids ambiguities, enabling one-pass parsing. New keywords (indexer, threadlocal, etc.) and annotations (@simd, @skip_index_range_check) are introduced in a way that does not conflict with existing syntax.

<Program>       ::= <Declaration>* ;

<Declaration>   ::= <ClassDecl> 
                  | <FunctionDecl> 
                  | <VarDecl> ;

<AttributeList> ::= { <Attribute> } ;        -- Zero or more annotations
<Attribute>     ::= '@' <Identifier> [ '(' <AttrArgs> ')' ] ;

<ClassDecl>     ::= [<AttributeList>] 'class' <ClassName> [ <IndexerSpec> ] 
                    '{' <ClassBody> '}' ;
<IndexerSpec>   ::= 'indexer' '(' <IndexParams> ')' 
                    [ '->' <Type> ] ;        -- Optional: specify indexer param types and return type

<ClassBody>     ::= <ClassMember>* ;
<ClassMember>   ::= <FieldDecl> | <MethodDecl> | <ConstructorDecl> | ... ;

<FieldDecl>     ::= [<AttributeList>] <Type> <FieldName> [ '=' <Expr> ] ';' ;
<MethodDecl>    ::= [<AttributeList>] <Type> <MethodName> '(' <ParamList> ')' <Block> ;

<FunctionDecl>  ::= [<AttributeList>] <Type> <FuncName> '(' <ParamList> ')' <Block> ;

<ParamList>     ::= <Param> { ',' <Param> } | <Empty> ;
<Param>         ::= <Type> <ParamName> ;

<Expr>          ::= ...                       -- (augmented to include new operations, see below)
<PostfixExpr>   ::= <ExprPrimary> { <PostfixOp> } ;
<PostfixOp>     ::= '[' <Expr> { ',' <Expr> } ']'   -- Indexer usage (may have multiple indices/params)
                  | '.' <Identifier>              -- Property/neighbor access (e.g. .left_neighbor)

-- Other grammar rules unchanged ...

Notes on Grammar:

Attributes/Annotations: The AttributeList nonterminal allows attaching annotations (like @simd) to classes, functions, or fields. For example, a function declaration may begin with @simd (and optional parameters) to mark it as a GPU kernel. Multiple attributes can be used in sequence. This is analogous to decorators or annotations in other languages. The grammar ensures that an Attribute (starting with @) is recognized distinctly from other expressions, keeping parsing unambiguous.

Class Indexer Specification: The <IndexerSpec> in a class declaration introduces an indexer trait for the class. It has the form indexer(IndexType [ , ExtraParams ] ) -> ReturnType. This means the class can be indexed like an array. For example: class Image indexer(Int, stride=1) -> Pixel { ... } would declare that Image objects can be indexed by an Int (with an optional stride parameter defaulting to 1) to get a Pixel. The grammar is designed so that the indexer(...) clause immediately after the class name is recognized in one pass. If no return type is specified after ->, it can be inferred (e.g., from a field type or a getter method).

Field Indexer Attribute: Alternatively, a field declaration can be annotated to act as the class’s indexer target. For instance: @indexer Float32[] data; inside a class means the class forwards indexing operations to the data field (which in this case is an array of Float32). In the grammar, this is handled by an Attribute on a <FieldDecl>. Only one field per class can be the default indexer (additional indexable fields can be supported by using explicit field names or different indexer methods).

Function and Method Declarations: Both global functions and class methods can be annotated with @simd. The grammar allows @simd before the return type. Optional parameters in the annotation (e.g., @simd(grid_size=..., group_size=...)) are captured as AttrArgs in the grammar. These parameters control launch configuration and other kernel properties (details below). The parser treats @simd(...) as a single unit preceding the function signature.

Postfix Expressions (Indexing and Neighbor Access): We extend the definition of postfix operations to include multi-parameter indexing and neighbor-property access. An index operation obj[expr1, expr2] can be used for multi-dimensional indexing or to pass an index and an optional parameter (e.g., stride or a neighbor enum). The language interprets a single comma-separated list inside [...] as arguments to the indexer. For example, buffer[i, 2] could mean “index buffer at i with stride 2” if the indexer is defined accordingly. Additionally, property access via dot . now includes special neighbor keywords when applicable. If an object or expression has an indexer, then <Expr> . left_neighbor and .right_neighbor are valid postfix ops, resolved as neighbor-index operations (more on semantics below). Grammatically, they are just <Identifier> tokens following a dot, so they integrate with the existing member access syntax without ambiguity. There are no new keywords for neighbor directions (they are treated as identifiers or enum variants), preserving a simple grammar.


Overall, these grammar additions maintain a structure where the parser can decide the meaning in one pass. For example, encountering @simd signals an upcoming function/class, and seeing indexer( after a class name signals the indexer specification. There’s no overlap between these new constructs and existing ones (e.g., indexer was not previously a keyword), so conflicts are avoided. This careful design meets the one-pass parse requirement, as no multi-pass or backtracking is needed to interpret the new syntax.

Indexer Trait for Custom Indexing

The indexer feature allows classes (or their fields) to be indexed like arrays with customizable behavior. This is inspired by concepts like operator[] in C++ and “indexer properties” in languages like C# or Python’s __getitem__, but is adapted for Simple’s syntax and one-pass compilation model.

Class-Level Indexer: Declaring indexer in a class header indicates that the class implements an indexing interface. For example:

class Matrix indexer(Int row, Int col) -> Float {
    // ... internal storage, e.g., a 2D array field ...
    fun get(Int row, Int col) -> Float { ... }
    fun set(Int row, Int col, Float value) { ... }
}

This means you can use matrix[i, j] to get or set elements of Matrix. The compiler desugars such usage to calls to the get/set methods or equivalent logic. The Indexer trait conceptually includes methods for retrieval and assignment. (In a formal sense, the language could define a built-in Indexer<IndexTypes...> interface that classes can implement.) Indexer parameters can be multiple (as above, for multi-dimensional indices) or single. The grammar also allows optional parameters with default values – e.g., indexer(Int index, Int stride = 1) -> T – enabling stride-based indexing (see below) or other modifiers.

Field-Level Indexer Forwarding: A simpler way to make a class indexable is by marking one of its fields with @indexer. This indicates that indexing the class should forward to that field. For example:

class AudioBuffer {
    @indexer Array<Sample> samples;
    // ... other fields or methods ...
}

If Array<Sample> (Simple’s built-in array) already supports indexing, then audioBuffer[i] effectively returns audioBuffer.samples[i]. The compiler implicitly generates the boilerplate to forward the index operator to the samples field. Only one field can be the default indexer for a class (you can still expose others via explicit methods or properties). If multiple fields are marked, the first (or an explicitly flagged one) is used as the default. This forwarding mechanism reduces boilerplate and encourages composition-over-inheritance – a class can wrap a container and expose its contents easily. It’s analogous to getter/setter forwarding in some languages (where methods or properties on an inner object are exposed by the outer object).

Default Indexers and Parameter Overrides: The language provides default indexers for common types. For example, built-in array types might have a default indexer that takes an Int and returns the element type. By implementing the indexer trait, user-defined collections can integrate seamlessly. Optional parameters on indexers (with default values) allow behavior like custom strides or boundary handling without needing entirely separate methods. For instance, if a class defines indexer(Int i, Int stride = 1), a user can write obj[i] (uses default stride=1) or obj[i, 2] (override stride to 2 for that access). This is done in a type-safe way – the extra parameter must be declared in the indexer interface. The compiler resolves at compile time how to apply the stride or extra parameter. This approach gives flexibility similar to Python’s slicing or C++ strided iterator patterns, but baked into the language syntax.

Trait and Implicit Conversions: Internally, we can think of an Indexer trait that types can conform to. This trait might look like:

trait Indexer<I..., V> { 
    fun __get__(I...) -> V;
    fun __set__(I..., V);
}

The compiler treats any type with an indexer spec as implementing this trait. This design also means different index types can be allowed. For example, an object could choose to allow any type that can convert to Int as an index. (In fact, similar ideas exist in Mojo language, where types conforming to an Indexer trait are implicitly convertible to Int, allowing generic index handling). For Simple, an indexer could even accept an enum or a custom type, as long as it’s defined. By default, normal integer types are expected for indices, but this system is extensible (e.g., to allow a special type for wrapped-around indices or negative indices mapped in some way). The key point is that any class can be made indexable by either implementing the trait or designating an indexer field, providing a uniform obj[...] syntax for element access across the language.

Bounds Checking: By default, index operations perform range checking to ensure safety (throwing an error or panicking if out-of-range). However, in high-performance GPU code, these checks can be skipped when the developer assures indices are valid. We introduce an attribute @skip_index_range_check to disable bounds checks within a certain scope (e.g., for a whole class or for specific methods). See the section on Overflow/Range Checks below for details.


@simd Annotation for GPU Kernels

The @simd annotation marks classes or functions that are meant to execute on GPU in parallel. It is the cornerstone for writing GPU kernels in Simple. Here’s how it works:

Function-level @simd: When applied to a function, @simd designates that function as a GPU kernel. It’s analogous to CUDA’s __global__ functions or OpenCL’s __kernel functions. Such a function, when called, will be launched on the GPU (asynchronously by default) rather than executed on the CPU. For example:

@simd(grid_size = N, group_size = 128)
fun vectorAdd(Float[] a, Float[] b, Float[] out) {
    // Each thread computes one element: out[i] = a[i] + b[i]
    let i = this.index();               // implicit global index for this thread (0 <= i < N)
    if i < N {                          // check bounds (if N not multiple of group size)
        out[i] = a[i] + b[i];
    }
}

In this example, vectorAdd is a kernel that will run on the device. We’ve provided optional parameters in the annotation: grid_size = N indicates the total number of threads (work-items) to launch, and group_size = 128 indicates threads are organized in groups of 128 (like CUDA threads-per-block of 128). If these parameters are omitted, the compiler/runtime can choose defaults or infer them (for instance, using out.length as grid size and an implementation-defined optimal group size). Inside the function, this.index() is used to get the thread’s global index (similar to CUDA’s blockIdx.x * blockDim.x + threadIdx.x or OpenCL’s get_global_id(0)). The code then performs the vector addition for that index. The if i < N is a typical boundary check to ensure threads with index beyond array bounds do nothing. (Such checks can be optimized away or omitted with @skip_index_range_check if we ensure grid_size == N exactly.)

When this function is called from host code, it triggers a kernel launch on the GPU rather than running serially. The call is asynchronous – it will enqueue the kernel for execution and the CPU thread can continue or wait for results as needed.

Class-level @simd: Marking a class with @simd indicates that the class is intended to be used as a GPU kernel or GPU data structure. All methods of a @simd class are treated as device functions by default (unless marked otherwise), and one of them (often a method named run or a special operator) can be invoked as the kernel entry point. For example:

@simd(thread_groups = 64)  // e.g., suggest 64 threads per group (block)
class BlurFilter indexer(Int) {
    @indexer Float[] image;
    Int width;
    // Constructor omitted for brevity

    @simd  // This method is a kernel
    fun apply() {
        let i = this.index();            // global index
        if i < image.length {
            // Each thread computes one output pixel as average of a pixel and its neighbors
            image[i] = (image.left_neighbor + image[i] + image.right_neighbor) / 3.0;
        }
    }
}

In this snippet, BlurFilter is a @simd class that contains an array image. We marked the class with @simd(thread_groups = 64), meaning whenever its GPU methods run, they use 64 threads per group by default (the actual grid size might be determined by the data length). The class defines an apply() method also marked @simd – this is the kernel to blur an image by averaging each pixel with its left and right neighbors. Notice how inside apply(), we can use image.left_neighbor and image.right_neighbor conveniently (more on neighbor access below). The this.index() gives each thread a unique pixel index to process. By marking the whole class @simd, we implied that apply() and potentially other methods (or even the indexing operation) are device-side operations. One could create an instance of BlurFilter on the host (with the image data), then call blurFilter.apply(), which would launch the kernel on the GPU to mutate blurFilter.image in place.

Using @simd on classes allows grouping related data and methods together for GPU execution, similar to how CUDA uses functors or how SYCL and HIP handle kernels via lambda or functor objects. It also can be used to allocate memory in GPU space by default (for instance, the image array could be allocated on device memory when BlurFilter is constructed, since it’s in a @simd class).

Optional Parameters in @simd: The annotation can take parameters to fine-tune the kernel launch or behavior. Common parameters include:

grid_size (or total_threads): total number of threads to launch. This can be 1D or a tuple for 2D/3D launches (e.g., grid_size=(N, M) for 2D). If omitted, the compiler might require the user to specify it at call time or infer it from data sizes.

group_size (or thread_groups): number of threads per group (block/work-group). Can also be 1D or tuple. If omitted, a default (like 128 or 256 threads) or an architecture-specific optimal value might be chosen.

stream: an identifier or number indicating on which stream/queue to run this kernel (if not on the default stream). For example, @simd(stream = 2) could tag that by default this kernel runs on a particular stream. This can be overridden at launch. (See Streams section.)

dim: if needed, explicitly specify dimension of the index (1, 2, or 3D). By default, dimension is inferred from how many indices are used in this.index(dim) or how grid_size is specified.

any other vendor-specific tuning (if we expose them) would also go here, but the goal is to keep it minimal and portable.


These parameters give similar control as CUDA’s triple angle bracket syntax (kernel<<<gridDim, blockDim, ...>>>) but in a declarative way. For example, @simd(grid_size=N, group_size=128) on a function is equivalent to launching that kernel with an N-size grid and 128 threads per block in CUDA terms. If a parameter is not applicable on the target (e.g., specifying too large group size for an AMD GPU), the runtime or compiler can adjust or warn, maintaining cross-platform compatibility.

Device vs Host Execution: A key design decision is that code inside a @simd function is intended to run on the GPU device. Thus, not all operations are allowed there (for instance, you typically can’t perform I/O or allocate arbitrary objects with GC on the device). The compiler enforces that only GPU-supported operations are used (e.g., arithmetic, memory accesses to device memory, etc.). By default, any function not marked @simd is a normal host function. Within a @simd class, one could still have host-side helper methods (for example, to prepare data on the CPU), but they should be marked explicitly (perhaps with an annotation like @host, which we could introduce). If not marked, methods in a @simd class might be assumed to be device methods. This implicit behavior reduces clutter – you don’t have to tag every method with __device__ as in CUDA, it’s inferred by context. Similarly, data in a @simd class can default to residing on GPU memory. The compiler will manage separate host and device versions if needed (similar to how CUDA separates host code and device code).

Asynchronous Execution: Calling a @simd function returns immediately on the host, while the GPU runs the kernel in the background. This means if you do:

vectorAdd(a, b, out);
// ... some other code ...
wait();  // hypothetical function to synchronize or implicitly at program end

the vectorAdd kernel will run in parallel to the CPU code following it. Simple may provide an abstraction for synchronization (like a Future or a Device.sync() call or await on a kernel result). By default, if the program tries to use results from out on the host, it should ensure the kernel finished (the language or runtime might insert an implicit sync when accessing device data on host to preserve correctness, or require the programmer to call an explicit wait). The asynchronous model aligns with how CUDA kernels launch by default (non-blocking), which is crucial for overlapping computation and data transfer.

Interoperability: Marking functions/classes as @simd does not tie them strictly to NVIDIA or AMD – the compiler will generate the appropriate device code (PTX for CUDA, or AMDGCN code for AMD, or SPIR-V for a generic path) depending on the chosen target. From the programmer’s perspective, the same code works on both. For example, the vectorAdd or BlurFilter.apply kernel above would work on AMD GPUs as well, where this.index() would correspond to get_global_id(0) under the hood, and thread grouping corresponds to OpenCL work-groups. The specification ensures that concepts like thread indices and groups are abstracted (via this.index(), thread_group object, etc.) so that vendor-specific differences are hidden.


In summary, @simd provides a clear way to write GPU kernels in Simple: you annotate a function (or method) and the compiler handles the rest (launch configuration, device compilation, etc.), freeing you from low-level details while still allowing control when needed.

Thread Groups and Indexing Model

Thread and Thread Group Hierarchy: When a @simd kernel runs, the execution model follows a hierarchy similar to CUDA’s threads/blocks/grid and OpenCL’s work-items/work-groups. All threads running a given kernel are conceptually arranged in groups that can execute concurrently and possibly synchronize with each other. Simple’s model introduces a thread group abstraction to represent this.

Implicit Indices: Every @simd function has access to intrinsic indices for the current thread and its group. As seen in examples, this.index() returns the global index (often as an integer or a tuple for multi-dimensional launches) of the thread. Similarly, one could obtain the group index (the index of the thread’s group within the grid) and the thread index within its group. We could provide these via methods or properties, such as:

this.thread_index() – index of the thread within its group (analogous to CUDA threadIdx or OpenCL get_local_id).

this.group_index() – index of the group (analogous to CUDA blockIdx or OpenCL get_group_id).

this.group_size() – number of threads in a group (same as blockDim in CUDA).

this.grid_size() – total number of threads (could be derivable from group_size * number_of_groups).


These can be methods on an implicit context (this in a @simd function could be considered a special object that has these properties), or as standalone built-in variables. The spec leans toward a method-style to avoid introducing too many global names. In any case, the values are provided by the runtime. For example, this.index() might internally do group_index * group_size + thread_index to compute the global linear index (just as CUDA does with blockIdx.x * blockDim.x + threadIdx.x). For multi-dimensional grids, more complex calculations are handled (the compiler can generate code for that based on launch parameters).

thread_group abstraction: The language optionally provides a thread_group type that encapsulates group-level functionality. A thread_group instance might be implicitly available (e.g., this.group or a global group variable) within @simd code. It could have:

Properties like .id (same as group_index() above), .size (same as group_size), maybe .dim for multi-dim sizes.

A method barrier() to synchronize all threads in the group. This would act like CUDA’s __syncthreads() – all threads in that group must reach the barrier before any can proceed. This is useful when threads need to share data via shared memory.

Perhaps methods for collective operations (like reductions) in the future, but not necessary in the core spec.


If a kernel doesn’t need to use these, the overhead is none (it’s optional). But if you do need to coordinate threads or access group info in a structured way, thread_group provides it. This concept maps directly to CUDA’s thread blocks and OpenCL’s work-groups, which *“group threads that can synchronize and share memory”*.

Shared Memory and Group Collectives: While not explicitly asked, it’s worth noting that if the user wants to use fast on-chip memory shared by a thread group (CUDA’s shared memory / AMD’s LDS), the language could allow a special qualifier for a variable (e.g., thread_group var sharedArr: Float[128]; inside a kernel would allocate an array in shared memory accessible by all threads in that group). This goes along with thread_group.barrier() to ensure all writes are visible. This is an advanced performance feature and might be considered beyond the minimal spec; however, the presence of a thread_group abstraction naturally leads to thinking about group-shared resources. (If not in this spec, it can be an extension that the compiler maps to __shared__ memory on CUDA or local address space on OpenCL.)

Grid (Whole Kernel) Level: Threads cannot synchronize across different groups during a single kernel launch (no cross-block sync in CUDA, and same in Simple). If a global synchronization is needed, it typically means ending one kernel and starting another. The Simple spec follows this: a barrier() only syncs within a thread_group. We do not provide a global barrier within a single kernel (as it’s not supported on GPU hardware except in limited forms). If truly needed, a @simd(thread_group = all) could theoretically mean one group equals the whole grid for small sizes, but that’s just running in one group essentially.

Example of Thread Group Usage: Consider computing a dot product where partial sums are reduced within each block (group):

@simd(group_size = 256)
fun dotProduct(Float[] a, Float[] b, Float* result) {
    extern __shared__ Float partialSums[];        // example syntax for shared memory (size = group_size)
    let tid = this.thread_index();
    let gid = this.index();
    // Each thread computes one product
    Float val = 0.0;
    if gid < a.length { 
        val = a[gid] * b[gid]; 
    }
    partialSums[tid] = val;
    thread_group.barrier();                      // wait for all threads to write
    // Reduction in shared memory (simple loop for one thread)
    if tid == 0 {
        Float groupSum = 0.0;
        for Int i in range(0, thread_group.size) {
            groupSum += partialSums[i];
        }
        atomic_add(result, groupSum);  // atomic add to global result
    }
}

In this pseudo-code, we see thread_group.size and thread_group.barrier() in action. This uses some hypothetical syntax (extern __shared__) to allocate a shared array partialSums for the group – the spec could define a cleaner way, but as an illustration, it parallels CUDA’s extern shared memory usage. After the barrier, one thread (tid 0) accumulates the partial results of that group and adds it to a global result using an atomic. This example shows how the thread group concept maps to actual hardware notions (shared memory and barrier). It’s advanced, but demonstrates the power of combining thread group abstraction with low-level control when needed.

Mapping to Hardware: On NVIDIA, a thread_group corresponds to a CUDA block, which runs on one streaming multiprocessor (SM), and threads within it can synchronize and share fast memory. On AMD, it’s a work-group on a compute unit. The Simple language doesn’t require the programmer to know the term “warp” or “wavefront” (subgroup of threads that execute in lock-step) – that’s an implementation detail. However, the name @simd itself hints at SIMD execution: indeed, GPUs execute threads in SIMD groups (warps of 32 on NVIDIA, wavefronts of 64 on AMD). This is transparent to the programmer; they just write scalar code for each thread. The compiler and GPU handle the vectorized execution under the hood. But the programmer should be mindful of divergent branches etc., as usual in GPU programming (this is more of a performance note than a spec requirement).


In summary, the thread group model in Simple provides a structured parallelism model akin to CUDA/OpenCL. Developers can obtain thread indices, group indices, and use a thread_group object for synchronization and shared data. This design ensures that GPU kernels in Simple can express both high-level parallel logic and low-level synchronization needs, covering use cases from embarrassingly parallel loops to complex algorithms requiring cooperations between threads. The portability is retained: what works in this model will work on both NVIDIA and AMD GPUs, since both support the concept of grouped threads with barrier sync and local memory.

Neighbor Accessors and Getter/Setter Forwarding

Neighbor Accessors: A unique feature in the Simple spec is built-in neighbor access for indexable data structures. If a class or object supports the indexer trait, you can directly access elements relative to a given index via properties like .left_neighbor and .right_neighbor. This is especially handy in data-parallel operations like stencils or smoothing filters, where each element’s computation involves its neighbors.

Usage in @simd kernels: Inside a @simd function, when you index into a collection (say X[i]), you often might want X[i-1] or X[i+1]. Simple’s neighbor syntax provides a readable shortcut. For example:

out[i] = (in.left_neighbor + in[i] + in.right_neighbor) / 3;

Here, in is an indexable (say, an array or image row), and we are in a context where i is the current index (perhaps i = this.index()). The expression in.left_neighbor is understood as “the element of in at index i-1” and in.right_neighbor as “element at i+1”. Under the hood, the compiler will convert these to in[i-1] and in[i+1]. Boundary conditions: if i is 0, in.left_neighbor might be defined to return a default value or the same element (or require the programmer to guard against it). The exact behavior could be defined by the indexer implementation. A class might specify what happens at edges (ignore, clamp, wrap-around, etc.). The simplest default is to treat out-of-range neighbor access as an error or no-op unless handled. In our BlurFilter example earlier, we explicitly did if i < image.length which implicitly also protects the right neighbor (except at the very end). We could also check if (i > 0 && i < image.length-1) to be safe on both ends.

Enums or Symbols for Directions: While .left_neighbor and .right_neighbor are provided as a convenience for one-dimensional sequences, the spec could support a more general neighbor access using an enum of directions. For instance, one could imagine:

enum Neighbor { LEFT, RIGHT, UP, DOWN };
image.getNeighbor(index, Neighbor.LEFT);

for a 2D grid, or using symbols like image[index, <-] hypothetically. However, such syntax might complicate the grammar. Instead, we opt for simple named properties for common cases (left/right, maybe up_neighbor, down_neighbor for 2D if needed). For more complex neighbor patterns, the user can always compute indices manually or write a helper. The provided neighbor accessors are essentially sugar to make code more legible and intention-revealing.

Implementation: If a type implements Indexer for Int, the compiler can automatically provide a property for neighbors. It might do so via default extension: e.g.,

extension<T: Indexer<Int, ElementType>> T {
    var left_neighbor -> ElementType {
        get { this[ current_index() - 1 ] }
    }
}

The tricky part is current_index() – how does the object know which index we mean? This relies on context: typically inside a @simd loop, we have a notion of “current index” (like the thread’s index). We could specify that .left_neighbor is only valid within a @simd function where the current index is well-defined (i.e., it’s this.index() implicitly). Essentially, it’s syntactic sugar that the compiler transforms using the known i (thread index) in the surrounding context. Another way: if you do arr.left_neighbor anywhere, the compiler could treat it as arr[someIndex - 1] if it can find what someIndex should be (for instance, if used in a parallel for loop or kernel where an index var is declared). Admittedly, this is a bit magical; so an alternate, more explicit approach might be to have neighbor properties on an indexed reference. For example, when you write for element in arr, maybe element could be a special proxy that has .left_neighbor etc. But in our design, we assume the use case is primarily within @simd kernels, and in those, you typically use the thread’s global index to refer to array positions. So we allow .left_neighbor on the array assuming it means relative to the current thread’s index. This should be documented clearly for developers.

Multi-dimensional neighbors: If we had a 2D indexer (say a Matrix class with indexer(Int r, Int c)), we might introduce .left_neighbor meaning same row, col-1; .right_neighbor as col+1; maybe .up_neighbor and .down_neighbor for row-1 and row+1. Or we allow something like matrix.left_neighbor(colIndex), passing which dimension’s neighbor you want. That complicates usage, so likely we’d encourage users to explicitly index in multi-d cases (or use an enum as mentioned). For the scope of this update, we focus on 1D neighbor which is most straightforward and already very useful (e.g., for line buffers, 1D arrays, etc., and one can always linearize a 2D index and use left/right in linear memory).


Getter/Setter Forwarding: This concept refers to automatically delegating property access or method calls from a wrapper class to an inner object. We already touched on one aspect: using @indexer on a field forwards obj[index] to obj.field[index]. The same idea can apply to getters and setters of properties:

If a class contains a field and we want the class to expose some properties of that field as if they were its own, we could use an attribute (perhaps something like @forward) on the field or on specific methods. For example:

class Camera {
    private Transform transform;
    @forward(Camera.transform.position) var position: Vec3;
    @forward(Camera.transform.rotate) fun rotate(Float angle);
    ...
}

(This is hypothetical syntax). The effect would be that camera.position actually calls camera.transform.position under the hood, and camera.rotate(x) calls camera.transform.rotate(x). In the context of this GPU spec, the main forwarding we explicitly define is the indexer forwarding as discussed. But similar machinery could be generally available.

Use Case in GPU context: Often, in GPU code, we might have a struct-of-arrays vs array-of-structs situation. Forwarding can help make a wrapper around data act like the data itself. For instance, a class holding multiple arrays (like separate position and velocity arrays) might want to present an indexer that returns a small struct with one element’s position and velocity. That small struct could have neighbor properties forwarded too. While designing such is complex, the language could allow it by composing indexers and forwarding: e.g., class ParticleData might have fields @indexer positions: Float[] and @indexer velocities: Float[], possibly distinguishable by indexer method overloading or by returning a tuple. However, returning multiple values via indexer might not align with one-pass parsing easily, so perhaps the user would define a Particle struct and use two parallel arrays or something.

Summary of Forwarding: The spec provides an automated way to forward common operations:

Indexing of a class forwarded to a field (as covered).

Potentially, neighbor access of a class forwarded to the neighbor of its field. (In the earlier BlurFilter, image.left_neighbor actually went to the Float[] field’s left_neighbor).

In general, method forwarding is possible but not explicitly covered in this GPU-centric update except as it applies to indexers.



These features enhance expressiveness – code can be written at a higher level (using neighbors rather than manual index math, and using wrapper classes without writing boilerplate pass-through methods). They do not add runtime cost: neighbor accessors compile down to simple index arithmetic, and forwarding is resolved at compile time (or inlined) so there’s no indirection penalty. This aligns with the goal of simplicity and performance.

Optional Overflow/Range Check Control (skip_index_range_check)

Array bounds and overflow checking are important for safety, but in tight GPU loops, they incur overhead. Simple’s default stance is to ensure safety, but allow opting-out when needed (much like how high-performance languages allow unsafe blocks or compiler flags for bounds-check elimination).

@skip_index_range_check Attribute: When this attribute is applied to a function or method, all index operations inside skip the runtime bounds verification. This assumes the programmer has ensured indices will be in range (for example, by appropriate if conditions or by sizing the launch to match array sizes exactly). It can also be applied at class level (affecting all methods of that class) or even at a specific indexer definition. For instance:

@simd @skip_index_range_check
fun incrementAll(Int[] data) {
    // We assume the grid covers exactly data.length elements
    let i = this.index();
    data[i] += 1;   // no check of i vs data.length due to the attribute
}

Here, we trust that grid_size == data.length, so every thread index i is valid. This saves the compare and branch that would otherwise be generated for safety.

Relation to Overflow: The term “overflow” could also refer to integer arithmetic overflow (e.g., wrapping vs error on overflow). In a GPU context, typically integer math is two’s complement wrap-around (like C). Simple might have had safer arithmetic by default (throwing on overflow), but for @simd code we likely want to disable such checks too. The skip_index_range_check primarily suggests skipping index bounds checks, but we could generalize an attribute for skipping all runtime checks (including arithmetic overflow). Perhaps an attribute name like @unsafe or @skip_checks could be used for broader effect. For now, we stick to index range because that’s explicitly mentioned.

Safety vs Performance: This attribute should be used with care. If an out-of-range access does occur with checks off, the behavior is undefined (could read/write incorrect memory, possibly causing wrong results or crashes). In CUDA C/C++, there are no automatic bounds checks – kernels rely on the programmer to guard against out-of-bounds. Simple gives you the choice: keep checks on for development and debugging (safer, at cost of a minor performance hit), and turn them off in production kernels where you’ve validated the logic (for maximum speed, akin to C/C++ semantics). This is somewhat like Rust’s opt-in for unchecked indexing (get_unchecked) or array indexing in D with @system code – you get safety by default, with an escape hatch.

Granularity: One can apply @skip_index_range_check to a specific function or to an entire class of methods. It might also be possible to apply it to a particular loop or block via an inner annotation (if the language supports local annotations). But function-level should suffice. If applied to a class, it’s as if all its methods and indexer operations are unchecked by default (suitable for a performance-critical data structure where you always use safe patterns).

Compiler Behavior: The compiler, when seeing this attribute, will omit generation of bounds-check code around index accesses in that scope. This could slightly change how it compiles index expressions. For example, normally arr[i] might compile to:

if (i >= arr.length) then throw IndexError;
else *(arr.base + i)

With skip check, it would compile to just the load/store *(arr.base + i) with no conditional. This is exactly what happens in C/C++ (no checks at all). So in effect, @skip_index_range_check lets Simple code achieve C-like performance for indexing, at the cost of potential safety.

Use with Neighbors: If neighbor accesses are used, skipping range checks means you also won’t get a check on neighbor validity. So if i=0 and you do .left_neighbor with checks off, you’ll likely read from index -1 (which could be invalid memory). Thus, turning off checks should go hand-in-hand with logic to avoid those scenarios (like adjusting your grid launch or adding explicit conditionals that you know will be optimized out if they evaluate to false for invalid cases because you avoided launching those threads, etc.).


In conclusion, @skip_index_range_check gives developers control to eliminate safety overhead when they need every ounce of performance and have other means to guarantee correctness. It keeps with Simple’s philosophy of being safe by default but not getting in the way of expert use-cases.

Async, GC-Free, Strongly-Typed Execution for @simd Code

GPU kernels in Simple are designed to be fast and predictable in execution. To achieve this, the language imposes certain restrictions and default behaviors on @simd code:

No Garbage Collection in Kernels: @simd functions are GC-free. This means that within a GPU kernel, you cannot allocate managed objects that rely on a garbage collector, nor will the GC ever pause a kernel. If the Simple language runtime has a GC (for memory management on the CPU side), that GC is not used for device-side allocations. Device memory should be allocated explicitly (perhaps with library calls or at class instantiation time) and freed explicitly or via deterministic structures. Objects on the GPU are either allocated in bulk before the kernel or are value-types that live in registers or shared memory. This is crucial because a stop-the-world GC cannot interrupt thousands of GPU threads – it would be impractical and GPUs don’t have the OS facilities to pause/resume like a CPU thread. Many modern high-performance systems languages avoid GC for similar reasons (for instance, Rust avoids GC and thus is used in systems programming where predictability is key). In our context, we ensure that any memory management on the GPU is manual or automatic in the sense of stack allocation. For example, local variables in a kernel are allocated in registers or local GPU memory (and freed when the thread ends), and dynamic allocation on device heap is either disallowed or provided via a specialized API (not covered here).

Strongly-Typed by Default: Simple is strongly typed overall, but this especially holds in GPU code. All types of variables, including those across host-device boundary, must be known at compile time. There is no concept of, say, a Python-like dynamic type or a var that changes type at runtime in a kernel. This ensures the compiler can generate efficient GPU code (which doesn’t support heavy runtime type checking). It also prevents errors that might only show up at runtime. By enforcing strong typing, we basically align with the C/C++ and Rust philosophy for kernels – the type of every data element is fixed and operations are checked at compile time. For example, if you have a function expecting a Float array, you cannot accidentally pass an Int array; the compiler will error out, rather than trying to convert or throwing an error mid-execution on GPU.

No Implicit Conversions that Lose Type Safety: In line with strong typing, @simd code should avoid implicit casts that might be allowed on CPU. For instance, if Simple normally allowed passing an integer where a float is expected (auto-casting), in GPU code we might disallow that to avoid subtle performance issues or precision loss. The idea is that kernel code should be as explicit as possible to the compiler.

No Exceptions in Kernels: While not explicitly mentioned in earlier points, it’s generally true that exceptions (stack unwinding, etc.) are not supported on GPUs (CUDA C++ doesn’t support throwing exceptions in device code, for example). Instead, error handling is done via the Result mechanism (discussed later) or by setting error codes. So Simple’s @simd functions do not use exceptions for control flow or error signaling; any such attempt would either be a compile-time error or simply have no effect. This ties into making error handling explicit and non-skippable.

Asynchronous Execution Model: As mentioned, launching a @simd kernel is async with respect to the CPU. Developers must be mindful of data races between host and device. The Simple spec likely would enforce some rules or provide tools: for instance, data passed to a kernel (like arrays) might be logically “owned” by the device until the kernel finishes, preventing the host from modifying it in the meantime (to avoid race conditions). This could be done by the runtime (marking buffers as in-use by device). Alternatively, the language might provide an async/await pattern for kernels: e.g., future = vectorAddAsync(a,b,out); ... future.wait();. In any case, asynchronous by default encourages overlapping of computation and communication, which is how GPUs are best utilized.

Deterministic Behavior: By removing GC and hidden nondeterminism, @simd code behaves in a predictable way (aside from thread scheduling order which is inherently parallel). There are no surprises like a random GC pause or a type check failing mid-kernel. Either the kernel runs to completion or, if an error occurs, it’s handled via the mechanisms we define (or results in termination of the kernel). This determinism is important in debugging parallel programs.


In essence, the environment of a @simd kernel is akin to running in a restricted, high-performance sandbox:

Only certain operations are allowed (compute, explicit memory access, synchronization).

Everything must be resolved at compile time (types, memory layout sizes, etc., except for the data values themselves).

The execution will not be interrupted by external runtime services (no preemption, no GC, etc.), which is consistent with how GPUs work.


This model ensures that Simple’s high-level conveniences do not compromise the low-level efficiency and correctness of GPU programs. If a programmer tries to use a disallowed feature in a kernel (like allocating a new object of a class that is managed by GC, or calling a non-@simd function that isn’t allowed on device), the compiler will flag it. By making these constraints part of the spec, we guide both the implementation and the programmer towards reliable patterns for GPGPU (General-Purpose GPU) programming.

Implicit Defaults and Boilerplate Reduction

One aim of Simple is to spare the programmer from verbosity, especially where the intent is clear. The GPU extensions adhere to this by introducing sensible defaults and contextual inference. A few notable points:

Device/Host function qualifiers: In CUDA C++, you have to mark functions with __device__ if they run on GPU, __host__ if they run on CPU (or both), and __global__ for kernels. In Simple, we infer this from context:

Any function marked @simd is a GPU kernel (executable on device, callable from host as a launch). You do not need separate annotations for host/device.

Functions called within a @simd function are assumed to be device functions. For example, if you have a helper function computeSquare(x:Float) -> Float and you call it inside a @simd kernel, the compiler will compile computeSquare for the device (as long as it doesn’t use illegal operations). You could optionally annotate it @simd as well, but it may not be required. In effect, the compiler might duplicate or inline such helpers for device usage.

Conversely, functions not used in any GPU context can be assumed host-only. If a function is used in both, perhaps the compiler compiles two versions. This is akin to how CUDA has separate host and device code paths. But Simple tries to hide that from the user.

If needed, an annotation like @host could explicitly force a function to be host-only (and error if called from device code), and @device to force device-only (error if called from host). But the default rule “called from a kernel => compile for device” covers many cases without additional syntax.


This approach means you won’t write clutter like:

__device__ float helper(...);
__global__ void kernel(...){ ... helper() ... }

Instead:

fun helper(x:Float) -> Float { ... }   // no annotation, can be inlined/used on device
@simd fun kernel(...) { ... helper(val) ... }

Simpler and the compiler figures it out. This aligns with the idea that “the language should automatically classify functions as device or host based on where they are used” – reducing errors and boilerplate.

Accessibility defaults (public/private): Within classes, we can set defaults to avoid repetitive keywords:

Fields could default to private (encapsulated) unless marked public. This is a common convention in many languages (e.g., C++ defaults class members to private). So if you omit an access modifier, it’s private. Methods might default to public (since methods usually define the interface of the class).

We already have seen that indexer fields and such might be private but with a public interface through the indexer.

These defaults mean the programmer doesn’t have to annotate every single field and method with its visibility unless deviating from the norm. It’s a small convenience but contributes to less clutter.


Implicit memory space specifiers: Traditional GPU programming distinguishes between different memory spaces: e.g., __shared__ (block-local), __constant__, etc. In Simple, we don’t expose these as keywords in the base syntax. Instead, we infer or provide attributes. For instance, as discussed, local variables in a kernel are automatically in private per-thread memory (which is basically registers or local memory). If the user wanted a shared array, maybe an attribute @thread_group on a variable could indicate it’s shared. We haven’t explicitly defined that in the spec, but we can mention:

If a variable is declared inside a @simd function at the top (outside any loop) with something like thread_group storage class, it could be placed in shared memory.

If a function parameter is large or marked in some way, maybe it goes to constant memory.


The key is, if we don’t specify, the system picks reasonable defaults:

Kernel parameters (scalars) might go in constant memory or special registers.

Read-only global data could be automatically treated as constant cache if annotated so.

But these are low-level details. The main spec point is: the developer is not forced to constantly state device/host or storage type; the compiler optimizes that.


Default thread group sizing: If not given by the programmer, the compiler or runtime can choose a default threads-per-group that is known to work well on the target GPU (for example, 128 threads per block is a common good default on NVIDIA). Similarly, if grid size isn’t specified, it could default to the size of an array parameter (if one clearly maps to output). These heuristics avoid requiring every single kernel call to specify launch configuration manually, which is a pain point in CUDA for newcomers. Simple could allow:

@simd fun doSomething(Array<Float> data) { ... }
...
doSomething(myData);  // no explicit grid size given

The compiler might generate code to launch with grid size = data.length, and a default block size. The programmer can override by passing an explicit configuration if needed (maybe via an overloaded call or a separate API, e.g., doSomething.launch(config, myData) if we had such mechanism).

Type inference defaults: While not specifically about GPU, if Simple has type inference, it likely still works in GPU code (e.g., let x = 5 infers Int). If there was any dynamic typing in the language, it’d be disallowed in kernels as mentioned, but local type inference is fine.

Stream default: If using streams (next section), if not specified, every kernel runs on the default stream (stream 0) which is sequential relative to other work on that stream. This matches CUDA’s behavior where by default kernels issue in order on the null stream. It’s a sensible default so that basic code has sequential consistency (one kernel finishes before the next starts on the default stream, unless explicitly opting into concurrency by using multiple streams).


Overall, these defaults aim to make the simplest use-case trivial (e.g., launching a kernel to process an array without worrying about blocks and threads and streams and qualifiers). Yet, when one needs to optimize or tweak, the language provides the knobs (via optional annotation parameters or attributes).

By reducing boilerplate and inferring context, Simple code is more concise. For example, a trivial kernel in Simple might be 5-6 lines of code (just the computation and a @simd tag), whereas in plain CUDA C you’d have to write the kernel, then the host call with launch parameters, etc. In Simple, the host call could look like an ordinary function call. All the “plumbing” is either automated or abstracted.

Stream Support and Thread-Local Functionality

Modern GPU applications often use multiple streams (NVIDIA’s term) or command queues (OpenCL’s term) to overlap computations and data transfers or run independent computations concurrently. This spec introduces basic stream support to schedule @simd kernels in isolation from each other when desired, and a threadlocal attribute to manage data/thread isolation.

Streams Overview: A stream is a sequence of operations (kernels, memcopies) that execute in order on the GPU, but different streams can execute concurrently (subject to hardware resources). By default, all @simd kernels run in the default stream. If the programmer does nothing special, kernels launch sequentially (one after the other) on the GPU, which is a safe default. To leverage concurrency, the programmer can assign kernels to different streams. In Simple, we envision a simple mechanism, for example:

@simd(stream = 1) fun kernelA(...) { ... }
@simd(stream = 2) fun kernelB(...) { ... }

marking at definition that these kernels typically use stream 1 and 2. But that might be too static; more flexibly, perhaps when launching, one can specify a stream:

kernelA.launch(on_stream: myStreamId, args...);

Or Device.async(kernelA(args...), stream_id). The exact API can be decided, but the spec ensures that the concept exists and can be controlled.

Thread-Local Execution Context: The term thread-local in this context can be a bit confusing because on GPU, we have many threads (work-items). However, here it refers to host threads or streams. Suppose you have multiple streams executing the same kernel function concurrently with different data. If that kernel or the functions it calls use any global state, those accesses could conflict. For example, if there is a global accumulator or a static buffer used inside a kernel, two streams might race on it. To avoid this, we can declare certain data or functions as threadlocal, meaning each stream or each host thread gets a separate instance of that data.

Concretely:

A threadlocal function could mean that any static variables inside that function are isolated per stream. Or it could simply be a marker that the function should not be called from multiple streams concurrently (perhaps queued to a single stream at a time) – but that’s less useful. More useful is the data angle:

A threadlocal global variable (or class static) means each thread (or stream) has its own copy of that variable. This is akin to C/C++ thread_local but extended so that GPU streams act like independent threads for this purpose. For example:

threadlocal var tempBuffer: Float[1024];

If tempBuffer is used in a kernel, each stream executing that kernel would refer to a different tempBuffer. On implementation side, this might be done by allocating separate buffers and selecting the right one based on stream ID at kernel launch.

A threadlocal attribute on a function might enforce that the function’s operations do not share state across streams. Possibly it could instruct the compiler to duplicate certain global states for each stream.


Use Cases:

Imagine a library function that uses an internal cache or scratchpad. If two kernels on different streams both call this function, we want them not to interfere via that cache. Marking the cache as threadlocal ensures each has its own.

Another example: random number generator state per stream. If we have a global RNG for GPU, making it threadlocal means each stream (which might correspond conceptually to a separate simulation) has its own sequence of random numbers, rather than all streams sharing and contending on one sequence.


Isolation: By isolating “threadlocal” state, we achieve deterministic and race-free execution when using streams. Each stream can be thought of like an independent worker with its own local memory for certain things. This avoids needing complex locking or atomic operations on those states across streams.

Host Threads vs Streams: It’s possible the user spawns multiple CPU threads that each launch GPU kernels (with each thread perhaps using its own default stream). In that case, marking something threadlocal would also separate those host threads’ accesses. Essentially, whether a kernel is launched on different streams or from different host threads, threadlocal ensures no unintended sharing. We can define that “threadlocal” is with respect to the host thread or explicitly tagged stream that launches the kernel – since typically each host thread has its own sequence of operations, often on its own stream.

Implementation details: The runtime might maintain a mapping from stream (or launching thread) to a context. Threadlocal variables might be transformed under the hood into an array indexed by thread or stream ID. Or each stream could run kernels that use a different symbol for that variable (via some name-mangling scheme). This is internal – the spec just needs to guarantee the semantics.

Default behavior without threadlocal: If the programmer doesn’t mark anything threadlocal and they do have global state, it’s assumed shared. That’s fine if read-only (multiple streams can read a constant table with no issue), but if writable, it’s the programmer’s responsibility to synchronize if needed. By providing threadlocal, we give an easy safety net for the common scenario where no cross-stream sharing was intended in the first place.


In summary, the stream support in Simple allows advanced users to run kernels concurrently for better performance, and the threadlocal attribute helps maintain correctness by isolating state per concurrency domain. These additions ensure that as code scales in complexity, the language can accommodate without introducing data hazards.

Error Handling in GPU Kernels (Rust-Inspired)

Error handling is treated seriously in Simple’s design, even for GPU code, where traditional methods like exceptions are not viable. We adopt a style similar to Rust’s Result type to ensure errors are explicit and cannot be silently ignored.

Result Type: We introduce (if not already present) a generic Result<T,E> type, which has two variants: Ok(T) for successful results and Err(E) for an error with error information. A @simd function that can fail will return a Result instead of, say, throwing an exception. For example:

@simd fun normalize(Float[] data) -> Result<Void, ErrorCode> {
    let i = this.index();
    if (data[i] < 0) {
        return Err(ErrorCode.NegativeValue);
    }
    // do normalization...
    return Ok();
}

In this contrived example, if a negative value is found, the function returns an error code result. There is no stack unwinding; each thread would produce an Err, and perhaps the calling context aggregates these or handles them.

Non-Skippable Checking: The key point is that the caller of a function that returns Result must handle it. The compiler will enforce that the Result is used. If the code simply calls a result-returning function and doesn’t use the outcome, it will be a compile warning or error. For instance:

let res = normalize(data);

If res is a Result, you have to either check if it’s Ok or propagate it. You could do:

match res {
    Ok(_) => { /* continue */ }
    Err(e) => { handleError(e); }
}

or if in a function that itself returns Result, you might do:

normalize(data)?;  // propagate error upward if any (using ? operator)

The ? operator (much like Rust) would immediately return the error to the caller if an Err is encountered. This allows writing concise error-handling code. For example, a multi-step kernel:

@simd fun process(…) -> Result<Void,Error> {
    normalize(data)?;
    computeSomething(data)?;
    // ... other steps
    return Ok();
}

If any step fails, the error propagates and the kernel can signal a failure. In GPU context, handling errors is tricky because thousands of threads might produce errors. Likely, any error would cause the whole kernel to be considered failed (like if one thread returns Err, maybe the kernel would stop or mark that thread’s result as invalid). This is an area where a runtime protocol is needed. Perhaps all threads write to a global status, or the first error is recorded. The specifics can be implementation-defined. The spec can state that if any thread in a kernel returns an error, the result of the entire kernel invocation is an error (with maybe an aggregate or first error code). This is similar to how a SIMT model might handle diverging control: all threads eventually must converge – in case of error, one could imagine a mechanism to safely abort or mark.

No Exceptions: As said, we do not use exceptions. This not only avoids the unwinding problem on GPU but also ensures that you cannot just ignore an error by not catching it (the way exceptions allow if you don’t wrap in try/except). Instead, you must acknowledge the possibility of Err. This explicitness is exactly Rust’s philosophy: *“Returning errors is explicit… If errors are returned, they are pushed onto the developer explicitly. She is aware the function may result in error. ... Rust goes further by enforcing errors are handled, embedding it in the type system, so there is no way to circumvent it.”*. In Simple, we embrace this. If a function can fail, it’s part of its type signature via Result. The compiler will not let you treat a Result<T,E> as a T without dealing with E – you must unwrap (with either a check or a propagate).

Defining Error Types: We might have a built-in Error or allow user-defined error enums. Perhaps a simple built-in error type is an integer code or a small enum ErrorCode. But to mirror Rust, likely an enum per context. For GPU kernels, errors might be limited to specific things (like arithmetic overflow if checked, out-of-bounds if not disabled, etc.). Users can define their own error enums for domain-specific errors as well.

Handling within Kernels: If an error occurs within a single thread of a kernel, ideally we would stop doing work in that thread and mark it as errored. Other threads might continue (since they are independent). But eventually, on the host side, the kernel as a whole might be considered to have an error state. The runtime could gather an error flag (perhaps using an atomic OR to a flag in device memory). Another approach is to require that if an error happens, all threads should eventually notice (like a global barrier, but that’s not feasible). More practically, each thread returns a result, and the reduction of those results (maybe implicit after kernel) determines overall success or failure. For example, if any thread returned Err, the overall kernel call returns an Err to the host. This is an area where the specification can lay out semantics, even if under the hood it’s complex.

No ignoring allowed: If a kernel is launched that returns a Result, the host code must handle that result too. For instance:

let result = process<<<...>>>(args);  // launch kernel that returns Result
if (result.is_err()) { println("Kernel failed: {}", result.unwrap_err()); }

Or if we propagate it further up. Essentially, at some level, it has to be dealt with. This non-skippable chain ensures that errors cannot be accidentally lost, which is crucial in debugging parallel code where silent failures would be nightmare.

Comparison to other languages: In C++, you might ignore an error code (just not check a return value) and the compiler won’t complain, leading to potential issues. In Rust (and Simple with this approach), the compiler forces you to acknowledge it. This leads to more robust code. Even on GPU, where one might be tempted to “just proceed and hope all threads succeed”, it’s better to design for error cases explicitly.

Examples: Suppose we have a @simd fun divideArrays(a: Float[], b: Float[], out: Float[]) -> Result<Void, DivisionByZeroError> that divides elements of a by b. If any element of b is 0, that thread returns Err(DivisionByZeroError). After the kernel, the host sees that result is an error. We can propagate that to the user or handle it (like log an error that the arrays had a zero, and perhaps partial results are in out for other elements). The language ensures we didn't just ignore that and use an out array full of infinities without warning.


In summary, Simple’s error handling model for GPU code follows Rust’s proven pattern: use a Result type to make the possibility of failure part of the function’s signature, and enforce that the programmer handles it. This leads to clearer and more reliable GPU programs, where errors are not swallowed or missed. It brings the confidence of systems programming to parallel code, which is a big win for development in this often tricky domain.

Stride-Based Indexing

Stride-based indexing is supported as a first-class feature via the indexer trait to facilitate stepping through data with constant gaps – a common scenario in numerical and graphics computations.

Default Stride = 1: By default, an indexer in Simple accesses consecutive elements (stride of one). For example, array[5] gives the 6th element, then array[6] the 7th, and so on, with no gaps.

Custom Strides: Simple allows you to define an indexer with a stride parameter. Consider a memory layout where data for one entity is interleaved with others (Structure of Arrays vs Array of Structures). For instance, you have an array of structs but stored as separate parallel arrays: maybe an RGBA image stored as a flat float array, where R, G, B, A values alternate. You could wrap that in a class and provide:

class ChannelView {
    Float[] data;
    Int stride;
    indexer(Int i) -> Float {
        get { data[i * stride] }
        set(value) { data[i * stride] = value }
    }
}

If you set stride = 4 and data points to the start of, say, the Red channel within an RGBA sequence, then channelView[0] accesses data[0], channelView[1] accesses data[4], etc. This is a manual approach. But the language can make it easier by handling stride if declared in the indexer spec:

class MyArray indexer(Int index, Int stride = 1) -> T { ... }

Then the compiler knows any access is base[index * stride] behind the scenes. If the stride is fixed for that class, it could be provided at construction or as a template parameter. Alternatively, if stride is a property of the object, the indexer implementation can use it (as in ChannelView above).

Slicing vs Striding: We are not introducing full Python-like slicing syntax (e.g. arr[ start: end: step ]), but stride covers the constant step size case. A slice with step is essentially a view with a stride. We allow the user to get such a view easily by perhaps providing a method to get a strided view. For example, we could have:

fun Array.strideView(step: Int, offset: Int = 0) -> Self {
    return Self(data_ptr + offset, stride = step);  // pseudo-code: creating a new view
}

Then myArray.strideView(2) would give an indexable that steps by 2. Combined with indexer forwarding, this could be neat.

Stride in Multi-dim: In multi-dimensional arrays, “stride” often refers to the memory stride for rows or planes. Those can be handled by having multiple parameters in the indexer (like indexer(Int row, Int col) where the implementation uses row * rowStride + col * colStride). The Simple spec at least covers the 1D stride explicitly but lays groundwork for multi-dim by allowing indexer with multiple params (where one or more could be conceptually “stride”).

No overhead at runtime: The stride multiplication is done as part of the index calculation – which modern CPUs/GPUs do efficiently. There is no additional loop or anything. In fact, if stride is a compile-time constant (e.g., fixed in class definition or templated), the compiler will optimize the multiplication (maybe even unroll things accordingly). If stride is a runtime variable, it’s one multiply per access, which is negligible in GPU computing context (often memory access latency overshadows an extra multiply). Mojo’s design (as referenced earlier) incorporated the indexer trait partly to handle negative indices and to optimize away certain checks for UInt; similarly, stride can be handled with no extra overhead except the intended offset calculation.

Example usage: Suppose you have a large array of structures, each structure has 4 floats. You want to process just the first float of each structure across the array (this is a stride-4 pattern). You could do:

@simd fun processFirstComponent(StridedArray<Float> arr) { 
    let i = this.index();
    // assume arr was created with stride=4 and offset=0 for the first component
    arr[i] = f(arr[i]);
}

If StridedArray is a class with an indexer(Int, stride=...), then arr[i] is actually accessing the correct memory with step 4. Alternatively, one might just pass the base array and do the calc in kernel: base[ i * 4 ] = f(base[ i * 4 ]). But having a strided view makes the kernel code cleaner and the intent clearer (no magic number 4 in the kernel, it’s encapsulated in the type).

Integration with neighbors: If you use neighbor on a strided indexer, presumably it will apply the same stride logic to neighbor access. For example, if arr.stride = 2, and you do arr.left_neighbor at index i (implicitly), it should fetch element at index (i-1) with that stride, which is underlying data[(i-1)*2]. So it naturally extends.


In conclusion, stride-based indexing in Simple is not an afterthought but built into the indexing abstraction. It allows users to elegantly handle non-contiguous data without manual pointer arithmetic in their kernel code, thereby reducing errors and improving code clarity.


---

Below, we provide example code snippets demonstrating these features in action, and a table comparing Simple’s constructs to CUDA/C++ equivalents.

Example Code Snippets

Example 1: Simple Vector Addition (CUDA-style usage)

In this example, we write a kernel to add two vectors, illustrating method-level @simd usage, implicit indexing, and error checking. This mimics a typical CUDA kernel but in Simple’s syntax.

// Simple vector addition kernel 
@simd(grid_size = N) 
fun addVectors(Float[] a, Float[] b, Float[] out) -> Result<Void, Error> {
    // Each GPU thread handles one index
    let i = this.index();                     // global index for this thread
    if i >= N {
        // If grid_size > N, threads beyond N do nothing (shouldn't happen if grid_size=N)
        return Ok();
    }
    out[i] = a[i] + b[i];
    // Simple error check: just an example (perhaps if result overflows certain bound)
    if out[i] == Float.INF {
        return Err(Error.Overflow);
    }
    return Ok();
}

// Host code calling the kernel (synchronous for illustration)
let result = addVectors(xArray, yArray, outArray);
match result {
    Ok(_)    => print("Addition completed successfully"),
    Err(err) => print("Kernel failed with error: {}", err)
}

Explanation: We mark addVectors with @simd – this compiles it for GPU. We specify grid_size = N (number of elements) and omit group_size, so the compiler will choose a default (e.g., 128). Inside, we get i = this.index(), analogous to int i = blockIdx.x*blockDim.x + threadIdx.x in CUDA. We then perform the addition for that index. We include a bounds check (if i >= N) out of caution (though if grid_size = N exactly, this is unnecessary – this is similar to the if (idx < n) checks seen in CUDA kernels). We also demonstrate returning a Result. In this case, if any output is infinite (say our data had extremely large values), we consider it an overflow error and return Err. Otherwise, return Ok(). This error checking is illustrative; in a real scenario, you might not check for floating infinity in a vector add, but it shows the pattern. On the host side, calling addVectors returns a Result. We use a match expression to handle it – printing a message depending on success or failure. This ensures we didn’t ignore the error.

CUDA Equivalent (for comparison): In CUDA C++, one would write:

__global__ void addVectorsKernel(const float* a, const float* b, float* out, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if(i < N) {
        out[i] = a[i] + b[i];
        // No built-in error handling; if we wanted, we could write to some error flag.
    }
}
// ... launch with <<< (N+blocks-1)/blocks, blocks >>> and check error on host ...

Notice how in CUDA there’s no direct error return from device code – we’d have to use other means. Simple’s approach wraps that up in a Result.

Example 2: Class-based Blur Filter (AMD-style usage)

This example uses a class annotated with @simd to perform a blur operation on an image array. It shows class-level @simd, field indexer, neighbor usage, and thread_group synchronization (to illustrate AMD’s style of local memory usage, though this example doesn’t strictly need sync).

@simd
class Blur1D {
    @indexer Float[] data;         // indexer field for pixel data
    Int length;

    // Each thread will compute the blurred value of one pixel and store it back
    @simd
    fun blur() {
        let idx = this.index();
        if idx >= length { 
            return;    // beyond end, do nothing
        }
        // Use neighbors (with bounds checking implicitly, or we ensure idx not at ends for simplicity)
        var leftVal  = (idx > 0) ? data.left_neighbor : data[idx];
        var rightVal = (idx < length-1) ? data.right_neighbor : data[idx];
        var center   = data[idx];
        // Compute average
        var result = (leftVal + center + rightVal) / 3.0;
        // Optionally, use thread_group barrier if we first read all and then write (not strictly needed here)
        thread_group.barrier();
        data[idx] = result;
    }
}

// Host side usage:
let img = Blur1D(data: pixelArray, length: pixelArray.length);
// Launch the blur kernel on the GPU (implicitly using default stream, default group size):
img.blur();
Device.wait();  // wait for all threads (i.e., kernel completion) before using img further

Explanation: We define a Blur1D class for a 1D image. It has a Float array data with an @indexer attribute, so the class itself can be indexed (though here we index via the data field inside the blur method). The class is marked @simd, meaning its methods are device-capable. The blur() method is marked @simd as well, making it a kernel. When img.blur() is called on the host, it launches the kernel. Inside blur(), each thread gets an idx. We then retrieve neighbor values: if idx is 0 or length-1, we handle boundaries by using the pixel itself as its neighbor (clamp at edges). Otherwise, we use data.left_neighbor and data.right_neighbor. Because @indexer on data allowed neighbor syntax, this is valid. We compute the average and then optionally use thread_group.barrier(). In this 1D blur, threads don’t actually need to sync if each thread writes its own data[idx] (no dependency between threads – they all read original data and write output). However, if we were writing the result back into the same array (which we are) and reusing updated values within the same kernel, that could be an issue. In our case, each thread only needs original neighbors (we stored them in local vars before the barrier). The barrier here just emphasizes how one would use it if needed (for example, in a more complex multi-pass blur in one kernel). After the barrier, we write the result back to data[idx]. On the host, we instantiate Blur1D with the image data. We then call img.blur(). Because we didn’t specify grid_size or group_size, the system will infer grid size = length (likely) and pick a reasonable group size. We call Device.wait() to block until the kernel finishes (this is a hypothetical API to illustrate synchronization). After that, img.data contains the blurred image.

OpenCL/HIP Equivalent: In an OpenCL style, one might write:

__kernel void blurKernel(global float* data, int length) {
    int idx = get_global_id(0);
    if(idx >= length) return;
    float leftVal  = (idx > 0) ? data[idx-1] : data[idx];
    float rightVal = (idx < length-1) ? data[idx+1] : data[idx];
    float center   = data[idx];
    float result = (leftVal + center + rightVal) / 3.0f;
    data[idx] = result;
}

and enqueue it with global work size = length. We see the logic is very similar; Simple just packaged it into a class with nicer syntax for neighbor access.

These examples demonstrate both a function-based kernel (like typical CUDA) and a class-based kernel (which could be akin to using functor objects or SYCL style). In both cases, Simple’s features (indexer, neighbor, thread_group, result type) make the code more declarative and safer, while remaining close to underlying performance.

Feature Mapping: Simple vs CUDA/C++

To solidify understanding, the table below maps features of the Simple language GPU extension to analogous concepts in CUDA (NVIDIA’s GPU programming model) and C++ (or other relevant languages). This highlights how Simple retains familiarity while improving on verbosity and safety.

Simple Language Feature	CUDA / C++ Analogous Construct

@simd function (GPU kernel)	CUDA: __global__ function (kernel launch with <<<>>>); OpenCL: __kernel function. C++ has no direct equivalent; in CUDA C++, you mark a function as __global__ to indicate a kernel.
@simd class	No direct equivalent in CUDA C++. (Closest is using a functor or lambda in Thrust or SYCL kernel objects). It groups device code together; conceptually similar to a C++ struct with device functions, but CUDA does not allow methods in __device__ code easily. SYCL/C++17 parallel STL allow functor objects as kernels, which is analogous.
Implicit __device__/__host__ inference	CUDA: Must explicitly mark functions __device__ or __host__ (or both). C++: all functions are host by default. Simple infers based on context, reducing annotations.
Launch configuration defaults	CUDA: Developer must specify grid and block dimensions in the kernel launch syntax (e.g., kernel<<<gridDim, blockDim>>>()). OpenCL: must provide global and local work sizes when enqueuing kernel. Simple can infer grid_size from array lengths and use a default group_size, making trivial cases easier.
Thread index (this.index())	CUDA: blockIdx.x * blockDim.x + threadIdx.x (or higher dimensions if 2D/3D launch). OpenCL: get_global_id(0) (and get_global_id(1) for y, etc.). Simple provides a unified method.
Thread group (thread_group)	CUDA: thread block (with blockIdx, blockDim, __syncthreads() for barrier, and shared memory allocated via __shared__). Also CUDA’s newer cooperative groups API provides a thread_block type similar to our thread_group abstraction. OpenCL: work-group (with functions like get_local_id, get_group_id, and local memory). Simple’s thread_group.barrier() corresponds to CUDA’s __syncthreads() and OpenCL’s barrier() function (in kernels).
indexer trait for classes	C++: can overload operator[] in classes to allow custom indexing. C# and others: indexer property. Simple’s indexer is similar to defining operator[] in C++ classes or providing .at() methods, but with language support for default indexers and traits.
Indexer with stride	C/C++: manually multiply index by stride (no built-in language support for strided slicing). E.g., *(ptr + i*stride) or loop with step. In high-level libs (NumPy, etc.), slicing with step exists, but not in C++ core. Simple allows stride in the indexer signature itself.
Field marked @indexer (default indexer field)	C++: no direct notion; you would implement operator[] to forward to a member. In code you’d end up writing a getter that returns member[index]. Simple automates this pattern.
Neighbor access (.left_neighbor etc.)	C++: no direct equivalent; you manually compute neighbor indices (e.g., arr[i-1]) and ensure index is valid. No language-provided property. This feature is more akin to domain-specific languages or frameworks (e.g., Halide or image processing libraries) that make neighborhood access succinct.
@skip_index_range_check attribute	C++: By default, C/C++ do no bounds checking on array accesses (so equivalent to having this off always). High-level languages (Java, etc.) always check and usually can’t turn it off in normal code. D and other systems languages allow unsafe blocks to skip checks. Simple gives fine control per function or class.
GC-free memory management (in @simd)	C++: no garbage collector, manual memory management (new/delete or RAII). CUDA: same, plus some restrictions on new in device code (only supported in newer C++ CUDA with certain flags). Java/C#: they have GC but typical GPU compute is not done in those without special frameworks. Simple ensures no GC during kernel much like C++ environment.
Strong typing enforced (in @simd)	C++: is statically typed, but allows implicit conversions (e.g., int to float). Also C++ allows unsafe casts. Rust: static and strict about casts. Simple aligns more with Rust safety (perhaps disallowing some implicit casts in kernel).
Async kernel launch (default)	CUDA: Kernel launches are asynchronous with respect to host by default; you explicitly synchronize (cudaDeviceSynchronize or events). OpenCL: also async, you wait on events or clFinish. C++ threads: not applicable directly, but std::async or threads run concurrently until joined. Simple adopts the GPU model of async launch.
Streams (concurrent execution queues)	CUDA: streams (cudaStreamCreate, and launching kernels on a stream). OpenCL: command queues. C++: no concept of GPU streams, but one might compare to multiple std::threads or tasks running in parallel. Simple stream support parallels CUDA streams but with language-level integration (e.g., annotation or parameter instead of needing separate API calls).
threadlocal attribute (for streams)	C++: thread_local storage specifier for variables (for CPU threads). Not directly used for GPU, but one could simulate per-stream storage by creating separate variables per stream. Simple’s threadlocal extends the idea to GPU execution contexts, ensuring isolation like how thread_local works for CPU threads.
Error handling via Result and ?	C++: uses exceptions or error codes. Exceptions cannot be used in GPU kernels (unsupported), and error codes can be ignored by programmer (no enforcement). CUDA errors (like from API calls) must be manually checked, often omitted leading to bugs. Rust: Result type with compiler enforcing handling. Simple follows Rust-like pattern, making error propagation explicit and compulsory.
Returning error from device code	CUDA: no built-in way for a device function to signal error to host (aside from writing to memory or using asserts which abort). Simple’s model allows a graceful error to propagate as a Result. The runtime would consolidate thread errors into one. This is a new capability in a high-level sense.
Table-driven default behaviors	(No direct analog) – This refers to Simple’s implicit defaults (device inferral, block sizes, etc.). In CUDA/C++, the programmer must specify many of these (so the analog is lots of boilerplate code or using higher-level frameworks that guess for you). Simple bakes reasonable defaults in the language.


Sources: Many of these correspondences are discussed in NVIDIA’s CUDA documentation and various GPU programming guides, which describe the thread/block model, kernel launch semantics, and memory model, all of which Simple mirrors through a more abstract syntax. The Rust error handling approach is referenced from the Rust language book to emphasize the enforced handling of errors.


---

Conclusion: The Simple language’s GPU extensions bring the expressiveness of modern high-level languages to GPU programming, while preserving the performance and model of CUDA/OpenCL. By incorporating features like @simd kernels, indexer traits with neighbor access, explicit thread grouping, and robust error handling, developers can write clear and correct parallel code. Importantly, the syntax and semantics are chosen to be portable across NVIDIA and AMD GPUs – abstracting away the differences where possible – and to keep the language “Simple” as its name suggests, avoiding unnecessary ceremony. This update thus empowers developers to utilize GPU computing in a simpler, safer manner without sacrificing control or speed. 
