# Optimization Algorithms
#
# Provides optimization algorithms and learning rate schedulers for training neural networks.
#
# ## Optimizers
# - `Optimizer`: Base class for all optimizers
# - `SGD`: Stochastic Gradient Descent with momentum
# - `Adam`: Adaptive Moment Estimation
# - `AdamW`: Adam with decoupled weight decay
# - `RMSprop`: Root Mean Square Propagation
#
# ## Learning Rate Schedulers
# - `LRScheduler`: Base class for learning rate schedulers
# - `StepLR`: Decays learning rate by gamma every step_size epochs
# - `ExponentialLR`: Decays learning rate by gamma every epoch
# - `CosineAnnealingLR`: Cosine annealing schedule
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.nn as nn
# import ml.torch.optim as optim
#
# # Create model and optimizer
# val model = MyModel()
# val optimizer = optim.Adam(model.parameters(), lr=0.001)
# val scheduler = optim.StepLR(optimizer, step_size=30, gamma=0.1)
#
# # Training loop
# for epoch in range(100):
#     val output = model(inputs)
#     val loss = compute_loss(output, labels)
#
#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()
#     scheduler.step()
# ```

export Optimizer, SGD, Adam, AdamW, RMSprop
export LRScheduler, StepLR, ExponentialLR, CosineAnnealingLR
export ReduceLROnPlateau, OneCycleLR, WarmupCosineAnnealingLR, LinearWarmupLR

use .. as torch
use optim_schedulers.{StepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau, LinearWarmupLR, WarmupCosineAnnealingLR, OneCycleLR}


# ============================================================================
# Optimizer Base Class
# ============================================================================

class Optimizer:
    """Base class for all optimizers.

    All optimizer classes should inherit from this.
    """
    optimizer_handle: u64
    lr: f64

    fn __init__(lr: f64):
        """Initialize optimizer.

        Args:
            lr: Learning rate
        """
        self.lr = lr
        self.optimizer_handle = 0

    fn __del__():
        """Free optimizer resources."""
        if self.optimizer_handle != 0:
            @rt_torch_optimizer_free(self.optimizer_handle)

    fn zero_grad():
        """Zero all parameter gradients.

        Call this before backward pass to clear old gradients.
        """
        if self.optimizer_handle != 0:
            @rt_torch_optimizer_zero_grad(self.optimizer_handle)

    fn step():
        """Update parameters using computed gradients.

        Call this after backward pass to apply gradients.
        """
        if self.optimizer_handle != 0:
            @rt_torch_optimizer_step(self.optimizer_handle)


# ============================================================================
# SGD Optimizer
# ============================================================================

class SGD(Optimizer):
    """Stochastic Gradient Descent optimizer.

    Implements SGD with optional momentum and weight decay.

    Update rule:
        v = momentum * v + (1 - dampening) * grad
        param = param - lr * (v + weight_decay * param)

    Example:
        ```simple
        val optimizer = optim.SGD(
            model.parameters(),
            lr=0.1,
            momentum=0.9,
            weight_decay=0.0001
        )
        ```
    """
    momentum: f64
    weight_decay: f64

    fn __init__(params: [u64],
        lr: f64,
        momentum: f64 = 0.0,
        weight_decay: f64 = 0.0
    ):
        """Initialize SGD optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate
            momentum: Momentum factor (default: 0.0)
            weight_decay: Weight decay (L2 penalty) (default: 0.0)
        """
        super().__init__(lr)
        self.momentum = momentum
        self.weight_decay = weight_decay

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_sgd_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            momentum,
            weight_decay
        )
        if self.optimizer_handle == 0:
            panic("Failed to create SGD optimizer")


# ============================================================================
# Adam Optimizer
# ============================================================================

class Adam(Optimizer):
    """Adam optimizer (Adaptive Moment Estimation).

    Maintains per-parameter adaptive learning rates using first and second
    moment estimates of gradients.

    Example:
        ```simple
        val optimizer = optim.Adam(
            model.parameters(),
            lr=0.001,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        ```
    """
    beta1: f64
    beta2: f64
    eps: f64
    weight_decay: f64

    fn __init__(params: [u64],
        lr: f64 = 0.001,
        betas: (f64, f64) = (0.9, 0.999),
        eps: f64 = 1e-8,
        weight_decay: f64 = 0.0
    ):
        """Initialize Adam optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate (default: 0.001)
            betas: Coefficients for first and second moment (default: (0.9, 0.999))
            eps: Term for numerical stability (default: 1e-8)
            weight_decay: Weight decay (L2 penalty) (default: 0.0)
        """
        super().__init__(lr)
        self.beta1 = betas.0
        self.beta2 = betas.1
        self.eps = eps
        self.weight_decay = weight_decay

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_adam_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            self.beta1,
            self.beta2,
            eps,
            weight_decay
        )
        if self.optimizer_handle == 0:
            panic("Failed to create Adam optimizer")


# ============================================================================
# AdamW Optimizer
# ============================================================================

class AdamW(Optimizer):
    """AdamW optimizer (Adam with decoupled weight decay).

    Decouples weight decay from gradient-based updates, often leading to
    better generalization than standard Adam with L2 regularization.

    Example:
        ```simple
        val optimizer = optim.AdamW(
            model.parameters(),
            lr=0.001,
            weight_decay=0.01
        )
        ```
    """
    beta1: f64
    beta2: f64
    eps: f64
    weight_decay: f64

    fn __init__(params: [u64],
        lr: f64 = 0.001,
        betas: (f64, f64) = (0.9, 0.999),
        eps: f64 = 1e-8,
        weight_decay: f64 = 0.01
    ):
        """Initialize AdamW optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate (default: 0.001)
            betas: Coefficients for first and second moment (default: (0.9, 0.999))
            eps: Term for numerical stability (default: 1e-8)
            weight_decay: Weight decay (default: 0.01)
        """
        super().__init__(lr)
        self.beta1 = betas.0
        self.beta2 = betas.1
        self.eps = eps
        self.weight_decay = weight_decay

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_adamw_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            self.beta1,
            self.beta2,
            eps,
            weight_decay
        )
        if self.optimizer_handle == 0:
            panic("Failed to create AdamW optimizer")


# ============================================================================
# RMSprop Optimizer
# ============================================================================

class RMSprop(Optimizer):
    """RMSprop optimizer (Root Mean Square Propagation).

    Maintains a moving average of squared gradients to normalize the gradient.
    Particularly effective for recurrent neural networks.

    Update rule:
        E[g²]_t = α * E[g²]_(t-1) + (1 - α) * g²_t
        param = param - lr * g / (sqrt(E[g²]) + ε)

    Example:
        ```simple
        val optimizer = optim.RMSprop(
            model.parameters(),
            lr=0.01,
            alpha=0.99,
            eps=1e-8
        )
        ```
    """
    alpha: f64
    eps: f64
    weight_decay: f64
    momentum: f64

    fn __init__(params: [u64],
        lr: f64 = 0.01,
        alpha: f64 = 0.99,
        eps: f64 = 1e-8,
        weight_decay: f64 = 0.0,
        momentum: f64 = 0.0
    ):
        """Initialize RMSprop optimizer.

        Args:
            params: List of parameter handles to optimize
            lr: Learning rate (default: 0.01)
            alpha: Smoothing constant for moving average (default: 0.99)
            eps: Term for numerical stability (default: 1e-8)
            weight_decay: Weight decay (L2 penalty) (default: 0.0)
            momentum: Momentum factor (default: 0.0)
        """
        super().__init__(lr)
        self.alpha = alpha
        self.eps = eps
        self.weight_decay = weight_decay
        self.momentum = momentum

        # Create optimizer via FFI
        self.optimizer_handle = @rt_torch_rmsprop_new(
            params.data_ptr(),
            params.len() as i32,
            lr,
            alpha,
            eps,
            weight_decay,
            momentum
        )
        if self.optimizer_handle == 0:
            panic("Failed to create RMSprop optimizer")


# ============================================================================
# Learning Rate Scheduler Base Class
# ============================================================================

class LRScheduler:
    """Base class for learning rate schedulers.

    Adjusts the learning rate during training.
    """
    optimizer: Optimizer
    last_epoch: i32

    fn __init__(optimizer: Optimizer, last_epoch: i32 = -1):
        """Initialize scheduler.

        Args:
            optimizer: Wrapped optimizer
            last_epoch: The index of last epoch (default: -1)
        """
        self.optimizer = optimizer
        self.last_epoch = last_epoch

    fn step():
        """Update learning rate."""
        self.last_epoch += 1
        val new_lr = self.get_lr()
        self.optimizer.lr = new_lr
        # Update optimizer handle's LR via FFI
        if self.optimizer.optimizer_handle != 0:
            @rt_torch_optimizer_set_lr(self.optimizer.optimizer_handle, new_lr)

    fn get_lr() -> f64:
        """Calculate current learning rate.

        Override in subclass.
        """
        return self.optimizer.lr


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_optimizer_free(optimizer: u64) -> i32
extern fn rt_torch_optimizer_zero_grad(optimizer: u64) -> i32
extern fn rt_torch_optimizer_step(optimizer: u64) -> i32
extern fn rt_torch_optimizer_set_lr(optimizer: u64, lr: f64) -> i32

extern fn rt_torch_sgd_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    momentum: f64,
    weight_decay: f64
) -> u64

extern fn rt_torch_adam_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    beta1: f64,
    beta2: f64,
    eps: f64,
    weight_decay: f64
) -> u64

extern fn rt_torch_adamw_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    beta1: f64,
    beta2: f64,
    eps: f64,
    weight_decay: f64
) -> u64

extern fn rt_torch_rmsprop_new(
    params_ptr: *u64,
    num_params: i32,
    lr: f64,
    alpha: f64,
    eps: f64,
    weight_decay: f64,
    momentum: f64
) -> u64
