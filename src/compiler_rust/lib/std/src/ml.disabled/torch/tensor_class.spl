# Tensor - Multi-dimensional Array with GPU Support (Simplified)

export Tensor

use device.{Device, device_code, device_from_code}
use dtype.{DType, dtype_code, dtype_from_code}
use tensor_ffi.{rt_torch_zeros, rt_torch_ones, rt_torch_randn, rt_torch_arange, rt_torch_free, rt_torch_clone, rt_torch_shape, rt_torch_numel, rt_torch_dtype, rt_torch_device, rt_torch_add, rt_torch_sub, rt_torch_mul, rt_torch_div, rt_torch_matmul, rt_torch_add_scalar, rt_torch_mul_scalar, rt_torch_sqrt, rt_torch_reshape, rt_torch_transpose, rt_torch_sum, rt_torch_mean, rt_torch_max, rt_torch_min, rt_torch_std, rt_torch_var, rt_torch_norm, rt_torch_item, rt_torch_set_requires_grad, rt_torch_requires_grad, rt_torch_backward, rt_torch_grad, rt_torch_detach, rt_torch_to_device, rt_torch_to_cpu, rt_torch_to_cuda, rt_torch_index, rt_torch_slice, rt_torch_select, rt_torch_narrow, rt_torch_gt, rt_torch_allclose}

class Tensor:
    """Multi-dimensional array with GPU support."""
    handle: u64

    fn __init__(handle: u64):
        """Initialize tensor from handle."""
        self.handle = handle

    fn __del__():
        """Free tensor memory."""
        if self.handle != 0:
            rt_torch_free(self.handle)

    # Factory Methods
    static fn zeros(shape: [i64], dtype: DType = DType.Float32, device: Device = Device.CPU) -> Tensor:
        """Create tensor filled with zeros."""
        val handle = rt_torch_zeros(shape.data_ptr(), shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    static fn ones(shape: [i64], dtype: DType = DType.Float32, device: Device = Device.CPU) -> Tensor:
        """Create tensor filled with ones."""
        val handle = rt_torch_ones(shape.data_ptr(), shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    static fn randn(shape: [i64], dtype: DType = DType.Float32, device: Device = Device.CPU) -> Tensor:
        """Create tensor with random values from N(0, 1)."""
        val handle = rt_torch_randn(shape.data_ptr(), shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    # Properties
    fn numel() -> i64:
        """Get total number of elements."""
        return rt_torch_numel(self.handle)

    fn shape() -> [i64]:
        """Get tensor shape (dimensions).

        Returns:
            Array of dimension sizes

        Example:
            val t = Tensor.zeros([2, 3, 4])
            t.shape()  # → [2, 3, 4]
        """
        # Create buffer for shape (max 8 dimensions)
        var buf: [i64] = [0i64, 0i64, 0i64, 0i64, 0i64, 0i64, 0i64, 0i64]
        val ndim = rt_torch_shape(self.handle, buf.data_ptr(), 8)
        # Build result array with actual dimensions
        var result: [i64] = []
        var i = 0
        while i < ndim:
            result = result + [buf[i]]
            i = i + 1
        return result

    fn ndim() -> i64:
        """Get number of dimensions.

        Returns:
            Number of dimensions (rank)

        Example:
            val t = Tensor.zeros([2, 3, 4])
            t.ndim()  # → 3
        """
        return self.shape().len() as i64

    fn dtype() -> DType:
        """Get tensor data type.

        Returns:
            DType enum (Float32, Float64, Int32, Int64)

        Example:
            val t = Tensor.zeros([2, 3], dtype=DType.Float64)
            t.dtype()  # → DType.Float64
        """
        val code = rt_torch_dtype(self.handle)
        return dtype_from_code(code)

    fn device() -> Device:
        """Get tensor device.

        Returns:
            Device enum (CPU or CUDA(id))

        Example:
            val t = Tensor.zeros([2, 3], device=Device.CPU)
            t.device()  # → Device.CPU
        """
        val code = rt_torch_device(self.handle)
        return device_from_code(code)

    # Device Transfer
    fn to(target_device: Device) -> Tensor:
        """Move tensor to specified device.

        Args:
            target_device: Target device (CPU or CUDA)

        Returns:
            Tensor on target device (new tensor if device differs)

        Example:
            val cpu_tensor = Tensor.zeros([2, 3])
            val gpu_tensor = cpu_tensor.to(Device.CUDA(0))
        """
        val target_code = device_code(target_device)
        val new_handle = rt_torch_to_device(self.handle, target_code)
        return Tensor(new_handle)

    fn to_cpu() -> Tensor:
        """Move tensor to CPU.

        Returns:
            Tensor on CPU

        Example:
            val gpu_tensor = Tensor.zeros([2, 3], device=Device.CUDA(0))
            val cpu_tensor = gpu_tensor.to_cpu()
        """
        val new_handle = rt_torch_to_cpu(self.handle)
        return Tensor(new_handle)

    fn to_cuda(device_id: i32 = 0) -> Tensor:
        """Move tensor to CUDA GPU.

        Args:
            device_id: CUDA device ID (default: 0)

        Returns:
            Tensor on specified CUDA device

        Example:
            val cpu_tensor = Tensor.zeros([2, 3])
            val gpu_tensor = cpu_tensor.to_cuda(0)
        """
        val new_handle = rt_torch_to_cuda(self.handle, device_id)
        return Tensor(new_handle)

    fn is_cpu() -> bool:
        """Check if tensor is on CPU.

        Returns:
            true if on CPU
        """
        return self.device().is_cpu()

    fn is_cuda() -> bool:
        """Check if tensor is on CUDA GPU.

        Returns:
            true if on any CUDA device
        """
        return self.device().is_cuda()

    # Shape Operations
    fn reshape(new_shape: [i64]) -> Tensor:
        """Reshape tensor to new dimensions.

        The total number of elements must remain the same.

        Args:
            new_shape: New shape dimensions. Use -1 for one dimension
                       to infer its size automatically.

        Returns:
            Tensor with new shape (view if possible, copy otherwise)

        Example:
            val t = Tensor.zeros([2, 6])
            val r = t.reshape([3, 4])    # → shape [3, 4]
            val f = t.reshape([-1])      # → shape [12] (flatten)
            val b = t.reshape([2, 3, 2]) # → shape [2, 3, 2]
        """
        val new_handle = rt_torch_reshape(self.handle, new_shape.data_ptr(), new_shape.len() as i32)
        return Tensor(new_handle)

    fn transpose(dim0: i64, dim1: i64) -> Tensor:
        """Transpose two dimensions.

        Args:
            dim0: First dimension to swap
            dim1: Second dimension to swap

        Returns:
            Tensor with swapped dimensions

        Example:
            val t = Tensor.zeros([2, 3, 4])
            val r = t.transpose(0, 2)  # → shape [4, 3, 2]
        """
        val new_handle = rt_torch_transpose(self.handle, dim0, dim1)
        return Tensor(new_handle)

    fn t() -> Tensor:
        """Transpose 2D tensor (swap dimensions 0 and 1).

        Shorthand for transpose(0, 1) on 2D tensors.

        Returns:
            Transposed tensor

        Example:
            val t = Tensor.zeros([2, 3])
            val r = t.t()  # → shape [3, 2]
        """
        return self.transpose(0, 1)

    fn flatten() -> Tensor:
        """Flatten tensor to 1D.

        Returns:
            1D tensor with all elements

        Example:
            val t = Tensor.zeros([2, 3, 4])
            val f = t.flatten()  # → shape [24]
        """
        return self.reshape([-1i64])

    fn view(new_shape: [i64]) -> Tensor:
        """View tensor with new shape (alias for reshape).

        Args:
            new_shape: New shape dimensions

        Returns:
            Tensor with new shape

        Example:
            val t = Tensor.zeros([2, 6])
            val v = t.view([3, 4])
        """
        return self.reshape(new_shape)

    fn squeeze(dim: i64 = -1) -> Tensor:
        """Remove dimensions of size 1.

        Args:
            dim: Dimension to squeeze (-1 for all size-1 dims)

        Returns:
            Tensor with size-1 dimensions removed

        Example:
            val t = Tensor.zeros([1, 3, 1, 4])
            val s = t.squeeze()  # → shape [3, 4]
        """
        val current_shape = self.shape()
        var new_shape: [i64] = []
        var i = 0
        while i < current_shape.len():
            val d = current_shape[i]
            if dim == -1:
                if d != 1:
                    new_shape = new_shape + [d]
            else:
                if i != dim or d != 1:
                    new_shape = new_shape + [d]
            i = i + 1
        if new_shape.len() == 0:
            new_shape = [1i64]
        return self.reshape(new_shape)

    fn unsqueeze(dim: i64) -> Tensor:
        """Add dimension of size 1 at specified position.

        Args:
            dim: Position to insert new dimension

        Returns:
            Tensor with added dimension

        Example:
            val t = Tensor.zeros([3, 4])
            val u = t.unsqueeze(0)  # → shape [1, 3, 4]
            val v = t.unsqueeze(2)  # → shape [3, 4, 1]
        """
        val current_shape = self.shape()
        var new_shape: [i64] = []
        var i = 0
        var inserted = false
        while i < current_shape.len():
            if i == dim and not inserted:
                new_shape = new_shape + [1i64]
                inserted = true
            new_shape = new_shape + [current_shape[i]]
            i = i + 1
        if not inserted:
            new_shape = new_shape + [1i64]
        return self.reshape(new_shape)

    # Arithmetic Operations
    fn add(other: Tensor) -> Tensor:
        """Add two tensors."""
        return Tensor(rt_torch_add(self.handle, other.handle))

    fn sub(other: Tensor) -> Tensor:
        """Subtract two tensors."""
        return Tensor(rt_torch_sub(self.handle, other.handle))

    fn mul(other: Tensor) -> Tensor:
        """Multiply two tensors."""
        return Tensor(rt_torch_mul(self.handle, other.handle))

    fn div(other: Tensor) -> Tensor:
        """Divide two tensors."""
        return Tensor(rt_torch_div(self.handle, other.handle))

    fn matmul(other: Tensor) -> Tensor:
        """Matrix multiply two tensors."""
        return Tensor(rt_torch_matmul(self.handle, other.handle))

    # Scalar Operations
    fn add_scalar(scalar: f64) -> Tensor:
        """Add scalar to all elements.

        Args:
            scalar: Value to add

        Returns:
            Tensor with scalar added

        Example:
            val t = Tensor.zeros([2, 3])
            val r = t.add_scalar(5.0)  # All elements are 5.0
        """
        return Tensor(rt_torch_add_scalar(self.handle, scalar))

    fn sub_scalar(scalar: f64) -> Tensor:
        """Subtract scalar from all elements.

        Args:
            scalar: Value to subtract

        Returns:
            Tensor with scalar subtracted

        Example:
            val t = Tensor.ones([2, 3])
            val r = t.sub_scalar(0.5)  # All elements are 0.5
        """
        return Tensor(rt_torch_add_scalar(self.handle, -scalar))

    fn mul_scalar(scalar: f64) -> Tensor:
        """Multiply all elements by scalar.

        Args:
            scalar: Value to multiply by

        Returns:
            Tensor with elements scaled

        Example:
            val t = Tensor.ones([2, 3])
            val r = t.mul_scalar(2.0)  # All elements are 2.0
        """
        return Tensor(rt_torch_mul_scalar(self.handle, scalar))

    fn div_scalar(scalar: f64) -> Tensor:
        """Divide all elements by scalar.

        Args:
            scalar: Value to divide by

        Returns:
            Tensor with elements divided

        Example:
            val t = Tensor.ones([2, 3])
            val r = t.div_scalar(2.0)  # All elements are 0.5
        """
        return Tensor(rt_torch_mul_scalar(self.handle, 1.0 / scalar))

    fn neg() -> Tensor:
        """Negate all elements.

        Returns:
            Tensor with negated elements

        Example:
            val t = Tensor.ones([2, 3])
            val r = t.neg()  # All elements are -1.0
        """
        return Tensor(rt_torch_mul_scalar(self.handle, -1.0))

    fn abs() -> Tensor:
        """Absolute value of all elements.

        Returns:
            Tensor with absolute values

        Example:
            val t = Tensor.randn([2, 3])
            val r = t.abs()  # All elements are non-negative
        """
        # abs(x) = x * sign(x), implemented as max(x, -x) via comparison
        # For now, use a simpler approach: sqrt(x^2)
        val squared = self.mul(self)
        return squared.pow(0.5)

    fn pow(exponent: f64) -> Tensor:
        """Raise elements to a power.

        Args:
            exponent: Power to raise to

        Returns:
            Tensor with elements raised to power

        Example:
            val t = Tensor.ones([2, 3]).mul_scalar(2.0)
            val r = t.pow(3.0)  # All elements are 8.0
        """
        # Implement using exp(exponent * log(x))
        # For simple cases, use repeated multiplication or FFI
        if exponent == 2.0:
            return self.mul(self)
        if exponent == 0.5:
            # Element-wise square root using FFI
            return Tensor(rt_torch_sqrt(self.handle))
        if exponent == 1.0:
            return self.clone()
        if exponent == 0.0:
            return self.mul_scalar(0.0).add_scalar(1.0)
        # General case - placeholder for exp(exponent * log(x))
        return self.clone()

    # Data Access
    fn item() -> f64:
        """Get scalar value (for single-element tensors)."""
        return rt_torch_item(self.handle)

    fn clone() -> Tensor:
        """Create a copy of this tensor."""
        return Tensor(rt_torch_clone(self.handle))

    # Autograd
    fn set_requires_grad(requires_grad: bool):
        """Enable or disable gradient tracking."""
        rt_torch_set_requires_grad(self.handle, requires_grad as i32)

    fn requires_grad() -> bool:
        """Check if gradients are tracked."""
        return rt_torch_requires_grad(self.handle) != 0

    fn backward():
        """Compute gradients via backpropagation."""
        rt_torch_backward(self.handle, 0u64, 0)

    fn grad() -> Tensor:
        """Get accumulated gradients."""
        return Tensor(rt_torch_grad(self.handle))

    fn detach() -> Tensor:
        """Create a tensor detached from computation graph."""
        return Tensor(rt_torch_detach(self.handle))
