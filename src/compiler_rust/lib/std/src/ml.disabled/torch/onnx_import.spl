# TorchScript Compilation and Import
#
# Compile PyTorch models to TorchScript for production deployment.
# Load and run TorchScript and ONNX models.
#
# ## Functions
# - `script()`: Compile model via scripting (supports control flow)
# - `trace()`: Compile model via tracing (faster, fixed control flow)
# - `save_torchscript()`: Save TorchScript module to file
# - `load_torchscript()`: Load TorchScript module from file
#
# ## Classes
# - `ScriptModule`: JIT-compiled TorchScript module

export ScriptModule, script, trace, save_torchscript, load_torchscript

use ml.torch.tensor_class.{Tensor}

use .. as torch


# ============================================================================
# TorchScript Compilation
# ============================================================================

class ScriptModule:
    """TorchScript compiled module.

    A JIT-compiled version of a PyTorch model for production deployment.
    Supports serialization and can run without Python interpreter.

    Attributes:
        handle: Internal TorchScript module handle
        original_model: Original PyTorch model (if available)
    """
    handle: u64
    original_model: any

    fn __init__(handle: u64, original_model: any = None):
        self.handle = handle
        self.original_model = original_model

    fn __del__():
        if self.handle != 0:
            @rt_torch_jit_free(self.handle)

    fn __call__(*args, **kwargs) -> any:
        """Run inference.

        Args:
            *args: Input tensors

        Returns:
            Model output
        """
        return self.forward(*args, **kwargs)

    fn forward(*args) -> any:
        """Forward pass.

        Args:
            *args: Input tensors

        Returns:
            Model output
        """
        # Get input handles
        var input_handles = []
        for arg in args:
            if hasattr(arg, "handle"):
                input_handles.append(arg.handle)

        var output_handle = 0u64

        @rt_torch_jit_forward(
            self.handle,
            input_handles.data_ptr(),
            input_handles.len() as i32,
            &output_handle
        )

        if output_handle == 0:
            panic("TorchScript forward failed")

        return Tensor(output_handle)

    fn save(filepath: str):
        """Save TorchScript module to file.

        Args:
            filepath: Output file path
        """
        save_torchscript(self, filepath)

    fn eval():
        """Set to evaluation mode."""
        @rt_torch_jit_eval(self.handle)

    fn train(mode: bool = true):
        """Set training mode.

        Args:
            mode: Training mode (default: true)
        """
        @rt_torch_jit_train(self.handle, mode as i32)


fn script(model: any, optimize: bool = true) -> ScriptModule:
    """Compile model to TorchScript via scripting.

    Scripting analyzes the model's source code and compiles it to TorchScript.
    Supports control flow (if, for, while) but requires type annotations.

    Args:
        model: PyTorch model (nn.Module)
        optimize: Apply optimizations (default: true)

    Returns:
        Compiled TorchScript module

    Example:
        ```simple
        import ml.torch.onnx as onnx

        # Compile model
        val scripted = onnx.script(model)

        # Save
        scripted.save("model_scripted.pt")

        # Run inference
        val output = scripted(input_tensor)
        ```

    Note:
        Model code must be compatible with TorchScript (limited Python features).
        Use torch.jit.script decorators for functions.
    """
    val model_handle = model.handle if hasattr(model, "handle") else 0u64

    var script_handle = 0u64

    @rt_torch_jit_script(
        model_handle,
        optimize as i32,
        &script_handle
    )

    if script_handle == 0:
        panic("TorchScript scripting failed")

    return ScriptModule(script_handle, model)


fn trace(
    model: any,
    example_inputs: any,
    check_trace: bool = true,
    check_tolerance: f64 = 1e-5,
    strict: bool = true,
    optimize: bool = true
) -> ScriptModule:
    """Compile model to TorchScript via tracing.

    Tracing records operations during a forward pass with example inputs.
    Faster than scripting but doesn't capture control flow (if/for/while).

    Args:
        model: PyTorch model (nn.Module)
        example_inputs: Example inputs for tracing
        check_trace: Verify trace correctness (default: true)
        check_tolerance: Tolerance for verification (default: 1e-5)
        strict: Strict tracing (default: true)
        optimize: Apply optimizations (default: true)

    Returns:
        Compiled TorchScript module

    Example:
        ```simple
        # Trace model
        val dummy_input = torch.randn([1, 3, 224, 224])
        val traced = onnx.trace(model, example_inputs=dummy_input)

        # Save
        traced.save("model_traced.pt")

        # Run inference
        val output = traced(test_input)
        ```

    Note:
        Control flow is not captured. The traced graph follows the path
        taken by the example inputs. Use scripting if you need control flow.
    """
    val model_handle = model.handle if hasattr(model, "handle") else 0u64

    # Get example input handles
    var input_handles = []
    if isinstance(example_inputs, tuple) or isinstance(example_inputs, list):
        for inp in example_inputs:
            if hasattr(inp, "handle"):
                input_handles.append(inp.handle)
    else:
        if hasattr(example_inputs, "handle"):
            input_handles.append(example_inputs.handle)

    var trace_handle = 0u64

    @rt_torch_jit_trace(
        model_handle,
        input_handles.data_ptr(),
        input_handles.len() as i32,
        check_trace as i32,
        check_tolerance,
        strict as i32,
        optimize as i32,
        &trace_handle
    )

    if trace_handle == 0:
        panic("TorchScript tracing failed")

    return ScriptModule(trace_handle, model)


fn save_torchscript(module: ScriptModule, filepath: str):
    """Save TorchScript module to file.

    Args:
        module: TorchScript module
        filepath: Output file path

    Example:
        ```simple
        val scripted = onnx.script(model)
        onnx.save_torchscript(scripted, "model.pt")
        ```
    """
    val filepath_ptr = filepath.as_ptr()
    val filepath_len = filepath.len() as i32

    val result = @rt_torch_jit_save(
        module.handle,
        filepath_ptr,
        filepath_len
    )

    if result != 0:
        panic("Failed to save TorchScript module to {filepath}")


fn load_torchscript(filepath: str, device: torch.Device = torch.Device.CPU) -> ScriptModule:
    """Load TorchScript module from file.

    Args:
        filepath: TorchScript file path
        device: Device to load on (default: CPU)

    Returns:
        Loaded TorchScript module

    Example:
        ```simple
        # Load model
        val model = onnx.load_torchscript("model.pt")

        # Move to GPU
        model = model.to(torch.Device.CUDA(0))

        # Run inference
        val output = model(input_tensor)
        ```
    """
    val filepath_ptr = filepath.as_ptr()
    val filepath_len = filepath.len() as i32

    var handle = 0u64

    @rt_torch_jit_load(
        filepath_ptr,
        filepath_len,
        device_code(device),
        &handle
    )

    if handle == 0:
        panic("Failed to load TorchScript module from {filepath}")

    return ScriptModule(handle)


# ============================================================================
# External FFI Functions
# ============================================================================

# These are implemented in src/runtime/src/value/torch.rs

# TorchScript JIT
extern fn rt_torch_jit_script(
    model_handle: u64,
    optimize: i32,
    script_handle_out: *u64
) -> i32

extern fn rt_torch_jit_trace(
    model_handle: u64,
    input_handles: *u64,
    num_inputs: i32,
    check_trace: i32,
    check_tolerance: f64,
    strict: i32,
    optimize: i32,
    trace_handle_out: *u64
) -> i32

extern fn rt_torch_jit_save(
    module_handle: u64,
    filepath_ptr: *u8,
    filepath_len: i32
) -> i32

extern fn rt_torch_jit_load(
    filepath_ptr: *u8,
    filepath_len: i32,
    device: i32,
    handle_out: *u64
) -> i32

extern fn rt_torch_jit_forward(
    module_handle: u64,
    input_handles: *u64,
    num_inputs: i32,
    output_handle_out: *u64
) -> i32

extern fn rt_torch_jit_free(handle: u64) -> i32

extern fn rt_torch_jit_eval(handle: u64) -> i32

extern fn rt_torch_jit_train(handle: u64, mode: i32) -> i32
