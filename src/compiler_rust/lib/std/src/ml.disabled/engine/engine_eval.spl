# Engine Evaluation Tools
#
# Training handlers and diagnostic tools:
# - EarlyStopping: Stop training when metric plateaus
# - ModelCheckpoint: Save model on metric improvement
# - GradientClipper: Prevent exploding gradients
# - ProgressBar: Visualize training progress
# - LRFinder: Find optimal learning rate
# - ModelSummary: Summarize model architecture

export EarlyStopping, ModelCheckpoint, GradientClipper, ProgressBar
export LRFinder, ModelSummary

use engine.__init__.{Engine}


# ============================================================================
# Training Handlers
# ============================================================================

class EarlyStopping:
    """Early stopping handler to terminate training when metric stops improving.

    Monitors a metric and stops training if no improvement is seen
    for a specified number of epochs (patience).

    Attributes:
        patience: Number of epochs with no improvement to wait
        min_delta: Minimum change to qualify as improvement
        mode: 'min' or 'max' - whether lower or higher is better

    Example:
        ```simple
        val early_stop = EarlyStopping(
            patience=10,
            min_delta=0.001,
            mode="min"
        )

        trainer.attach_handler(EPOCH_COMPLETED, early_stop.check, priority=0)
        trainer.run(data, max_epochs=100)
        ```
    """
    patience: i64
    min_delta: f64
    mode: str
    metric_name: str
    _best: f64
    _counter: i64
    _stopped_epoch: i64

    fn __init__(patience: i64 = 10, min_delta: f64 = 0.0, mode: str = "min", metric_name: str = "loss"):
        """Initialize early stopping.

        Args:
            patience: Epochs to wait before stopping
            min_delta: Minimum improvement threshold
            mode: 'min' or 'max'
            metric_name: Name of metric to monitor
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.metric_name = metric_name
        self._counter = 0
        self._stopped_epoch = 0

        if mode == "min":
            self._best = 1e30
        else:
            self._best = -1e30

    fn check(engine: Engine):
        """Check if training should stop.

        Called as handler on EPOCH_COMPLETED.

        Args:
            engine: Training engine
        """
        if not self.metric_name in engine.state.metrics:
            return

        val current = engine.state.metrics[self.metric_name]
        val improved = self._is_improvement(current)

        if improved:
            self._best = current
            self._counter = 0
        else:
            self._counter += 1

        if self._counter >= self.patience:
            self._stopped_epoch = engine.state.epoch
            engine.terminate()

    fn _is_improvement(current: f64) -> bool:
        """Check if current value is an improvement."""
        if self.mode == "min":
            return current < self._best - self.min_delta
        else:
            return current > self._best + self.min_delta

    fn stopped_epoch() -> i64:
        """Get epoch when training stopped (-1 if not stopped)."""
        return self._stopped_epoch

    fn best_value() -> f64:
        """Get best metric value seen."""
        return self._best


class ModelCheckpoint:
    """Handler to save model checkpoints during training.

    Saves model state when monitored metric improves, or at regular intervals.

    Attributes:
        filepath: Path template for saved checkpoints
        monitor: Metric name to monitor
        mode: 'min' or 'max'
        save_best_only: Only save when metric improves
        period: Save every N epochs (if not save_best_only)

    Example:
        ```simple
        val checkpoint = ModelCheckpoint(
            filepath="checkpoints/model_{epoch}.pt",
            monitor="val_loss",
            mode="min",
            save_best_only=true
        )

        trainer.attach_handler(EPOCH_COMPLETED, checkpoint.save, priority=0)
        ```
    """
    filepath: str
    monitor: str
    mode: str
    save_best_only: bool
    period: i64
    _best: f64
    _last_saved_epoch: i64
    save_fn: any  # Function to save model

    fn __init__(filepath: str, monitor: str = "loss", mode: str = "min",
                save_best_only: bool = true, period: i64 = 1, save_fn: any = nil):
        """Initialize checkpoint handler.

        Args:
            filepath: Path template (use {epoch}, {metric} for substitution)
            monitor: Metric to monitor
            mode: 'min' or 'max'
            save_best_only: Only save on improvement
            period: Save frequency if not save_best_only
            save_fn: Custom save function(engine, path)
        """
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.save_best_only = save_best_only
        self.period = period
        self.save_fn = save_fn
        self._last_saved_epoch = -1

        if mode == "min":
            self._best = 1e30
        else:
            self._best = -1e30

    fn save(engine: Engine):
        """Save checkpoint if conditions are met.

        Args:
            engine: Training engine
        """
        val epoch = engine.state.epoch

        if self.save_best_only:
            if self.monitor not in engine.state.metrics:
                return

            val current = engine.state.metrics[self.monitor]
            val improved = self._is_improvement(current)

            if improved:
                self._best = current
                self._save_checkpoint(engine, epoch, current)
        else:
            if (epoch + 1) % self.period == 0:
                val current = 0.0
                if self.monitor in engine.state.metrics:
                    current = engine.state.metrics[self.monitor]
                self._save_checkpoint(engine, epoch, current)

    fn _is_improvement(current: f64) -> bool:
        """Check if metric improved."""
        if self.mode == "min":
            return current < self._best
        else:
            return current > self._best

    fn _save_checkpoint(engine: Engine, epoch: i64, metric: f64):
        """Actually save the checkpoint."""
        var path = self.filepath
        path = path.replace("{epoch}", str(epoch))
        path = path.replace("{metric}", str(metric))

        if self.save_fn is not nil:
            self.save_fn(engine, path)
        else:
            # Default: just log that we would save
            print("Saving checkpoint to {path}")

        self._last_saved_epoch = epoch

    fn best_value() -> f64:
        """Get best metric value."""
        return self._best


class GradientClipper:
    """Gradient clipping handler to prevent exploding gradients.

    Clips gradients by value or by norm after backward pass.

    Attributes:
        max_norm: Maximum gradient norm (for norm clipping)
        max_value: Maximum gradient value (for value clipping)
        clip_type: 'norm' or 'value'

    Example:
        ```simple
        val clipper = GradientClipper(max_norm=1.0)

        # In training step, after loss.backward():
        clipper.clip(model.parameters())
        optimizer.step()
        ```
    """
    max_norm: f64
    max_value: f64
    clip_type: str
    _total_norm: f64

    fn __init__(max_norm: f64 = 0.0, max_value: f64 = 0.0, clip_type: str = "norm"):
        """Initialize gradient clipper.

        Args:
            max_norm: Maximum gradient norm (for norm clipping)
            max_value: Maximum gradient value (for value clipping)
            clip_type: 'norm' or 'value'
        """
        self.max_norm = max_norm
        self.max_value = max_value
        self.clip_type = clip_type
        self._total_norm = 0.0

        if max_norm > 0.0:
            self.clip_type = "norm"
        elif max_value > 0.0:
            self.clip_type = "value"

    fn clip(parameters: any) -> f64:
        """Clip gradients and return total norm.

        Args:
            parameters: Model parameters (list of tensors)

        Returns:
            Total gradient norm before clipping
        """
        if self.clip_type == "norm":
            return self._clip_by_norm(parameters)
        else:
            return self._clip_by_value(parameters)

    fn _clip_by_norm(parameters: any) -> f64:
        """Clip gradients by global norm.

        Scales all gradients so total norm <= max_norm.
        """
        # Compute total norm
        var total_norm = 0.0
        for param in parameters:
            if param.grad is not nil:
                val grad_norm = param.grad.norm()
                total_norm += grad_norm * grad_norm

        total_norm = sqrt(total_norm)
        self._total_norm = total_norm

        # Scale if needed
        if total_norm > self.max_norm:
            val scale = self.max_norm / (total_norm + 1e-6)
            for param in parameters:
                if param.grad is not nil:
                    param.grad = param.grad.mul_scalar(scale)

        return total_norm

    fn _clip_by_value(parameters: any) -> f64:
        """Clip gradients by value.

        Clamps each gradient element to [-max_value, max_value].
        """
        var total_norm = 0.0
        for param in parameters:
            if param.grad is not nil:
                val grad_norm = param.grad.norm()
                total_norm += grad_norm * grad_norm
                param.grad = param.grad.clamp(-self.max_value, self.max_value)

        self._total_norm = sqrt(total_norm)
        return self._total_norm

    fn total_norm() -> f64:
        """Get total gradient norm from last clip call."""
        return self._total_norm


class ProgressBar:
    """Progress bar handler for training visualization.

    Shows epoch and iteration progress with metrics.

    Example:
        ```simple
        val pbar = ProgressBar()
        trainer.attach_handler(ITERATION_COMPLETED, pbar.update, priority=-1)
        trainer.attach_handler(EPOCH_COMPLETED, pbar.epoch_complete, priority=-1)
        ```
    """
    _width: i64
    _show_metrics: bool

    fn __init__(width: i64 = 50, show_metrics: bool = true):
        """Initialize progress bar.

        Args:
            width: Bar width in characters
            show_metrics: Whether to show metrics
        """
        self._width = width
        self._show_metrics = show_metrics

    fn update(engine: Engine):
        """Update progress bar.

        Args:
            engine: Training engine
        """
        val epoch = engine.state.epoch + 1
        val iteration = engine.state.epoch_iteration

        # Get total iterations if possible
        var total = 0
        if engine.state.dataloader is not nil:
            total = len(engine.state.dataloader)

        if total > 0:
            val progress = iteration.to_float() / total.to_float()
            val filled = (progress * self._width.to_float()) as i64
            val bar = "=" * filled + ">" + " " * (self._width - filled - 1)
            print("\rEpoch {epoch} [{bar}] {iteration}/{total}", end="")
        else:
            print("\rEpoch {epoch} - Iteration {iteration}", end="")

    fn epoch_complete(engine: Engine):
        """Called when epoch completes.

        Args:
            engine: Training engine
        """
        print("")  # New line after progress bar

        if self._show_metrics:
            var metrics_str = ""
            for (name, value) in engine.state.metrics.items():
                metrics_str += "{name}: {value:.4f}  "
            if metrics_str != "":
                print("  Metrics: {metrics_str}")


# ============================================================================
# Learning Rate Finder
# ============================================================================

class LRFinder:
    """Learning rate range test to find optimal LR.

    Runs training with exponentially increasing LR and plots loss.
    Optimal LR is typically 1/10 of the LR where loss starts increasing.

    Based on "Cyclical Learning Rates for Training Neural Networks" paper.

    Example:
        ```simple
        val lr_finder = LRFinder(min_lr=1e-7, max_lr=10.0, num_iter=100)

        fn train_step(engine, batch):
            # Set LR from finder
            optimizer.lr = lr_finder.get_lr(engine.state.iteration)
            # Training step...
            return {"loss": loss}

        trainer = Engine(train_step)
        trainer.attach_handler(ITERATION_COMPLETED, lr_finder.record, priority=0)
        trainer.run(data, max_epochs=1)

        val suggested_lr = lr_finder.suggestion()
        ```
    """
    min_lr: f64
    max_lr: f64
    num_iter: i64
    _lrs: any
    _losses: any
    _best_loss: f64
    _diverge_threshold: f64

    fn __init__(min_lr: f64 = 1e-7, max_lr: f64 = 10.0, num_iter: i64 = 100,
                diverge_threshold: f64 = 5.0):
        """Initialize LR finder.

        Args:
            min_lr: Starting learning rate
            max_lr: Maximum learning rate
            num_iter: Number of iterations for the test
            diverge_threshold: Stop if loss > best_loss * threshold
        """
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.num_iter = num_iter
        self._diverge_threshold = diverge_threshold
        self._lrs = []
        self._losses = []
        self._best_loss = 1e30

    fn get_lr(iteration: i64) -> f64:
        """Get learning rate for current iteration.

        Args:
            iteration: Current iteration number

        Returns:
            Learning rate (exponentially increasing)
        """
        val progress = iteration.to_float() / self.num_iter.to_float()
        val log_min = _log10(self.min_lr)
        val log_max = _log10(self.max_lr)
        val log_lr = log_min + progress * (log_max - log_min)
        return _pow10(log_lr)

    fn record(engine: Engine):
        """Record loss at current LR.

        Args:
            engine: Training engine
        """
        if "loss" not in engine.state.output:
            return

        val lr = self.get_lr(engine.state.iteration)
        val loss = engine.state.output["loss"]

        self._lrs = self._lrs + [lr]
        self._losses = self._losses + [loss]

        if loss < self._best_loss:
            self._best_loss = loss

        # Check for divergence
        if loss > self._best_loss * self._diverge_threshold:
            engine.terminate()

    fn suggestion() -> f64:
        """Get suggested learning rate.

        Returns:
            Suggested LR (steepest descent point / 10)
        """
        if len(self._losses) < 3:
            return self.min_lr

        # Find steepest negative gradient
        var best_idx = 0
        var best_grad = 0.0

        for i in range(1, len(self._losses) - 1):
            val grad = self._losses[i - 1] - self._losses[i + 1]
            if grad > best_grad:
                best_grad = grad
                best_idx = i

        # Return LR at steepest point / 10
        if best_idx > 0 and best_idx < len(self._lrs):
            return self._lrs[best_idx] / 10.0
        return self.min_lr

    fn lrs() -> any:
        """Get recorded learning rates."""
        return self._lrs

    fn losses() -> any:
        """Get recorded losses."""
        return self._losses


# ============================================================================
# Model Summary
# ============================================================================

class ModelSummary:
    """Generate summary of model architecture.

    Shows layer names, output shapes, and parameter counts.

    Example:
        ```simple
        val summary = ModelSummary(model, input_size=(1, 3, 224, 224))
        print(summary.to_string())
        ```
    """
    _layers: any
    _total_params: i64
    _trainable_params: i64
    _input_size: any

    fn __init__(model: any = nil, input_size: any = nil):
        """Initialize model summary.

        Args:
            model: Model to summarize
            input_size: Input tensor shape (batch, channels, height, width)
        """
        self._layers = []
        self._total_params = 0
        self._trainable_params = 0
        self._input_size = input_size

        if model is not nil:
            self._analyze(model)

    fn _analyze(model: any):
        """Analyze model structure."""
        # Walk through model layers
        if hasattr(model, "modules"):
            for (name, module) in model.named_modules():
                val layer_info = self._analyze_layer(name, module)
                self._layers = self._layers + [layer_info]

    fn _analyze_layer(name: str, module: any) -> any:
        """Analyze single layer."""
        var num_params: i64 = 0
        var trainable: i64 = 0

        if hasattr(module, "parameters"):
            for param in module.parameters():
                val count = param.numel()
                num_params += count
                if param.requires_grad:
                    trainable += count

        self._total_params += num_params
        self._trainable_params += trainable

        return {
            "name": name,
            "type": type(module).__name__,
            "params": num_params,
            "trainable": trainable
        }

    fn total_params() -> i64:
        """Get total parameter count."""
        return self._total_params

    fn trainable_params() -> i64:
        """Get trainable parameter count."""
        return self._trainable_params

    fn non_trainable_params() -> i64:
        """Get non-trainable parameter count."""
        return self._total_params - self._trainable_params

    fn to_string() -> str:
        """Generate summary string."""
        var result = "=" * 60 + "\n"
        result += "Model Summary\n"
        result += "=" * 60 + "\n"
        result += "Layer (type)                 Params\n"
        result += "-" * 60 + "\n"

        for layer in self._layers:
            val name = layer["name"]
            val ltype = layer["type"]
            val params = layer["params"]
            result += "{name} ({ltype})".ljust(30) + "{params}\n"

        result += "=" * 60 + "\n"
        result += "Total params: {self._total_params}\n"
        result += "Trainable params: {self._trainable_params}\n"
        result += "Non-trainable params: {self._total_params - self._trainable_params}\n"
        result += "=" * 60 + "\n"

        return result


# ============================================================================
# Additional Helper Functions
# ============================================================================

fn _log10(x: f64) -> f64:
    """Compute log base 10."""
    if x <= 0.0:
        return -30.0  # Very negative
    # Use natural log and convert: log10(x) = ln(x) / ln(10)
    val ln10 = 2.302585093
    return _ln(x) / ln10


fn _ln(x: f64) -> f64:
    """Compute natural logarithm using series expansion."""
    if x <= 0.0:
        return -1e30

    # Normalize to [0.5, 1.5] range for better convergence
    var normalized = x
    var exponent: i64 = 0

    while normalized > 2.0:
        normalized = normalized / 2.0
        exponent += 1
    while normalized < 0.5:
        normalized = normalized * 2.0
        exponent -= 1

    # Use Taylor series around 1: ln(1+y) = y - y^2/2 + y^3/3 - ...
    val y = normalized - 1.0
    var result = 0.0
    var term = y
    for n in range(1, 30):
        if n % 2 == 1:
            result += term / n.to_float()
        else:
            result -= term / n.to_float()
        term = term * y

    # Add back the exponent: ln(x) = ln(normalized) + exponent * ln(2)
    val ln2 = 0.693147181
    return result + exponent.to_float() * ln2


fn _pow10(x: f64) -> f64:
    """Compute 10^x."""
    # 10^x = e^(x * ln(10))
    val ln10 = 2.302585093
    return _exp(x * ln10)


fn _exp(x: f64) -> f64:
    """Compute e^x using Taylor series."""
    # Clamp to prevent overflow
    if x > 30.0:
        return 1e30
    if x < -30.0:
        return 0.0

    var result = 1.0
    var term = 1.0
    for n in range(1, 30):
        term = term * x / n.to_float()
        result += term

    return result


fn sqrt(x: f64) -> f64:
    """Square root using Newton's method."""
    if x <= 0.0:
        return 0.0
    var guess = x / 2.0
    for _ in range(20):  # Newton iterations
        guess = (guess + x / guess) / 2.0
    return guess
