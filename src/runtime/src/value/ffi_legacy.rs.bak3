//! Core FFI functions (legacy - being gradually extracted to submodules).
//!
//! This file contains FFI functions that haven't yet been extracted to dedicated modules.
//! Functions are being moved to:
//! - ffi/value_ops.rs - Value creation/extraction/checking (DONE)
//! - ffi/memory.rs - Memory allocation (DONE)
//! - ffi/equality.rs - Equality comparison (DONE)
//! - ffi/interpreter_bridge.rs - Interpreter bridge (TODO)
//! - ffi/error_handling.rs - Error handling (TODO)
//! - And more...

use crate::value::collections::{self, RuntimeString};
use crate::value::core::RuntimeValue;
use crate::value::heap::{HeapHeader, HeapObjectType};
use std::cell::RefCell;
use std::collections::{HashMap, HashSet};
use std::path::Path;
use std::sync::atomic::Ordering;
use std::sync::Mutex;

// Import FFI functions that are used within this module
use crate::value::{rt_array_new, rt_array_push};

// Hash function imports
use sha1::{Digest, Sha1};
use sha2::Sha256;
use xxhash_rust::xxh3::Xxh3;

// PyTorch imports (conditional)
#[cfg(feature = "pytorch")]
use tch::{Device, Kind, Tensor};

// ============================================================================
// EXTRACTED MODULES (Phase 1-3)
// ============================================================================
// - Phase 1: ffi/value_ops.rs, ffi/memory.rs, ffi/equality.rs
// - Phase 2: ffi/hash/*.rs, ffi/concurrent/*.rs
// - Phase 3: ffi/interpreter_bridge.rs, ffi/error_handling.rs, ffi/contracts.rs,
//            ffi/io_capture.rs, ffi/io_print.rs
// ============================================================================

// ============================================================================
// NOTE: Code coverage & instrumentation probes (rt_decision_probe,
//       rt_condition_probe, rt_path_probe) have been extracted to
//       ffi/env_process.rs (Phase 7)
// ============================================================================


// ============================================================================
// NOTE: Process control function (rt_exit) has been extracted to
//       ffi/env_process.rs (Phase 7)
// ============================================================================


// ============================================================================
// NOTE: Math functions (rt_math_*) have been extracted to ffi/math.rs (Phase 4)
// ============================================================================

// ============================================================================
// NOTE: File system functions (rt_file_*, rt_dir_*) have been extracted to
//       ffi/file_io.rs (Phase 6)
// ============================================================================


// ============================================================================
// NOTE: Environment variable functions (rt_env_get, rt_env_set, rt_env_cwd)
//       have been extracted to ffi/env_process.rs (Phase 7)
// ============================================================================


// ============================================================================
// Base64 encoding/decoding FFI functions
// ============================================================================

/// Decode base64 string to bytes
#[no_mangle]
pub unsafe extern "C" fn rt_base64_decode(input_ptr: *const u8, input_len: u64) -> RuntimeValue {
    if input_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let input_bytes = std::slice::from_raw_parts(input_ptr, input_len as usize);
    let input_str = match std::str::from_utf8(input_bytes) {
        Ok(s) => s,
        Err(_) => return RuntimeValue::NIL,
    };

    // Simple base64 decoding implementation
    const DECODE_TABLE: [u8; 128] = [
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 62,
        255, 255, 255, 63, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 255, 255, 255, 0, 255, 255, 255, 0, 1, 2, 3, 4, 5,
        6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 255, 255, 255, 255, 255, 255, 26,
        27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 255, 255,
        255, 255, 255,
    ];

    let mut result = Vec::new();
    let input_chars: Vec<u8> = input_str.bytes().filter(|&b| !b.is_ascii_whitespace()).collect();

    let mut i = 0;
    while i < input_chars.len() {
        // Get 4 characters (or remaining)
        let mut chunk = [0u8; 4];
        let mut chunk_len = 0;

        while chunk_len < 4 && i < input_chars.len() {
            let c = input_chars[i];
            i += 1;

            if c == b'=' {
                chunk[chunk_len] = 0;
                chunk_len += 1;
            } else if (c as usize) < 128 && DECODE_TABLE[c as usize] != 255 {
                chunk[chunk_len] = DECODE_TABLE[c as usize];
                chunk_len += 1;
            }
        }

        if chunk_len >= 2 {
            result.push((chunk[0] << 2) | (chunk[1] >> 4));
        }
        if chunk_len >= 3 {
            result.push((chunk[1] << 4) | (chunk[2] >> 2));
        }
        if chunk_len >= 4 {
            result.push((chunk[2] << 6) | chunk[3]);
        }
    }

    // Remove padding bytes
    while result.last() == Some(&0) && input_str.ends_with('=') {
        result.pop();
    }

    // Convert to string (assuming UTF-8)
    match String::from_utf8(result) {
        Ok(decoded) => {
            let bytes = decoded.as_bytes();
            crate::value::collections::rt_string_new(bytes.as_ptr(), bytes.len() as u64)
        }
        Err(_) => RuntimeValue::NIL,
    }
}

/// Encode bytes to base64 string
#[no_mangle]
pub unsafe extern "C" fn rt_base64_encode(input_ptr: *const u8, input_len: u64) -> RuntimeValue {
    if input_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let input_bytes = std::slice::from_raw_parts(input_ptr, input_len as usize);

    const ENCODE_TABLE: &[u8; 64] = b"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/";

    let mut result = Vec::new();
    let mut i = 0;

    while i < input_bytes.len() {
        let b1 = input_bytes[i];
        let b2 = if i + 1 < input_bytes.len() {
            input_bytes[i + 1]
        } else {
            0
        };
        let b3 = if i + 2 < input_bytes.len() {
            input_bytes[i + 2]
        } else {
            0
        };

        result.push(ENCODE_TABLE[(b1 >> 2) as usize]);
        result.push(ENCODE_TABLE[(((b1 & 0x03) << 4) | (b2 >> 4)) as usize]);

        if i + 1 < input_bytes.len() {
            result.push(ENCODE_TABLE[(((b2 & 0x0F) << 2) | (b3 >> 6)) as usize]);
        } else {
            result.push(b'=');
        }

        if i + 2 < input_bytes.len() {
            result.push(ENCODE_TABLE[(b3 & 0x3F) as usize]);
        } else {
            result.push(b'=');
        }

        i += 3;
    }

    match String::from_utf8(result) {
        Ok(encoded) => {
            let bytes = encoded.as_bytes();
            crate::value::collections::rt_string_new(bytes.as_ptr(), bytes.len() as u64)
        }
        Err(_) => RuntimeValue::NIL,
    }
}

// ============================================================================
// NOTE: Time & timestamp functions (rt_time_*, rt_timestamp_*) have been
//       extracted to ffi/time.rs (Phase 5)
// ============================================================================

// ============================================================================
// NOTE: File I/O functions (rt_file_open, rt_file_close, rt_file_mmap, etc.)
//       have been extracted to ffi/file_io.rs (Phase 6)
// ============================================================================

// ============================================================================
// ============================================================================
// NOTE: Process execution functions (rt_process_run, rt_process_spawn,
//       rt_process_execute) have been extracted to ffi/env_process.rs (Phase 7)
// ============================================================================


// ============================================================================
// File/Directory Search FFI Functions
// ============================================================================

/// Find files matching pattern (simplified glob matching)
/// Returns List[String] of matching file paths
#[no_mangle]
// ============================================================================
// NOTE: File find/glob functions (rt_file_find, rt_dir_glob) have been
//       extracted to ffi/file_io.rs (Phase 6)
// ============================================================================


// ============================================================================
// Path Manipulation FFI Functions
// ============================================================================

/// Get basename (filename) from path
/// Returns the final component of the path
#[no_mangle]
// ============================================================================
// NOTE: Path manipulation functions (rt_path_*) have been extracted to
//       ffi/file_io.rs (Phase 6)
// ============================================================================


// ============================================================================
// NOTE: Platform detection function (rt_platform_name) has been extracted to
//       ffi/env_process.rs (Phase 7)
// ============================================================================


// ============================================================================
// Atomic Operations FFI
// ============================================================================

use std::sync::atomic::{AtomicBool, AtomicI64};

lazy_static::lazy_static! {
    static ref ATOMIC_BOOL_MAP: Mutex<HashMap<i64, Box<AtomicBool>>> = Mutex::new(HashMap::new());
    static ref ATOMIC_INT_MAP: Mutex<HashMap<i64, Box<AtomicI64>>> = Mutex::new(HashMap::new());
}

static mut ATOMIC_BOOL_COUNTER: i64 = 1;
static mut ATOMIC_INT_COUNTER: i64 = 1;

// ----------------------------------------------------------------------------
// AtomicBool Operations
// ----------------------------------------------------------------------------

/// Create a new atomic boolean
#[no_mangle]
pub extern "C" fn rt_atomic_bool_new(initial: bool) -> i64 {
    let atomic = Box::new(AtomicBool::new(initial));
    unsafe {
        let handle = ATOMIC_BOOL_COUNTER;
        ATOMIC_BOOL_COUNTER += 1;
        ATOMIC_BOOL_MAP.lock().unwrap().insert(handle, atomic);
        handle
    }
}

/// Load value from atomic boolean
#[no_mangle]
pub extern "C" fn rt_atomic_bool_load(handle: i64) -> bool {
    ATOMIC_BOOL_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.load(Ordering::SeqCst))
        .unwrap_or(false)
}

/// Store value to atomic boolean
#[no_mangle]
pub extern "C" fn rt_atomic_bool_store(handle: i64, value: bool) {
    if let Some(atomic) = ATOMIC_BOOL_MAP.lock().unwrap().get(&handle) {
        atomic.store(value, Ordering::SeqCst);
    }
}

/// Swap value of atomic boolean
#[no_mangle]
pub extern "C" fn rt_atomic_bool_swap(handle: i64, value: bool) -> bool {
    ATOMIC_BOOL_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.swap(value, Ordering::SeqCst))
        .unwrap_or(false)
}

/// Free atomic boolean
#[no_mangle]
pub extern "C" fn rt_atomic_bool_free(handle: i64) {
    ATOMIC_BOOL_MAP.lock().unwrap().remove(&handle);
}

// ----------------------------------------------------------------------------
// AtomicInt Operations
// ----------------------------------------------------------------------------

/// Create a new atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_new(initial: i64) -> i64 {
    let atomic = Box::new(AtomicI64::new(initial));
    unsafe {
        let handle = ATOMIC_INT_COUNTER;
        ATOMIC_INT_COUNTER += 1;
        ATOMIC_INT_MAP.lock().unwrap().insert(handle, atomic);
        handle
    }
}

/// Load value from atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_load(handle: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.load(Ordering::SeqCst))
        .unwrap_or(0)
}

/// Store value to atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_store(handle: i64, value: i64) {
    if let Some(atomic) = ATOMIC_INT_MAP.lock().unwrap().get(&handle) {
        atomic.store(value, Ordering::SeqCst);
    }
}

/// Swap value of atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_swap(handle: i64, value: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.swap(value, Ordering::SeqCst))
        .unwrap_or(0)
}

/// Compare and exchange atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_compare_exchange(handle: i64, current: i64, new: i64) -> bool {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| {
            atomic
                .compare_exchange(current, new, Ordering::SeqCst, Ordering::SeqCst)
                .is_ok()
        })
        .unwrap_or(false)
}

/// Fetch and add to atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_fetch_add(handle: i64, value: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.fetch_add(value, Ordering::SeqCst))
        .unwrap_or(0)
}

/// Fetch and subtract from atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_fetch_sub(handle: i64, value: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.fetch_sub(value, Ordering::SeqCst))
        .unwrap_or(0)
}

/// Fetch and bitwise AND with atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_fetch_and(handle: i64, value: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.fetch_and(value, Ordering::SeqCst))
        .unwrap_or(0)
}

/// Fetch and bitwise OR with atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_fetch_or(handle: i64, value: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.fetch_or(value, Ordering::SeqCst))
        .unwrap_or(0)
}

/// Fetch and bitwise XOR with atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_fetch_xor(handle: i64, value: i64) -> i64 {
    ATOMIC_INT_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|atomic| atomic.fetch_xor(value, Ordering::SeqCst))
        .unwrap_or(0)
}

/// Free atomic integer
#[no_mangle]
pub extern "C" fn rt_atomic_int_free(handle: i64) {
    ATOMIC_INT_MAP.lock().unwrap().remove(&handle);
}

// ============================================================================
// AtomicFlag Operations (simpler bool without load)
// ============================================================================

lazy_static::lazy_static! {
    static ref ATOMIC_FLAG_MAP: Mutex<HashMap<i64, Box<AtomicBool>>> = Mutex::new(HashMap::new());
}

static mut ATOMIC_FLAG_COUNTER: i64 = 1;

/// Create a new AtomicFlag (initially false)
#[no_mangle]
pub extern "C" fn rt_atomic_flag_new() -> i64 {
    let flag = Box::new(AtomicBool::new(false));
    unsafe {
        let handle = ATOMIC_FLAG_COUNTER;
        ATOMIC_FLAG_COUNTER += 1;
        ATOMIC_FLAG_MAP.lock().unwrap().insert(handle, flag);
        handle
    }
}

/// Test and set atomic flag
/// Returns true if flag was already set, false if it was clear
#[no_mangle]
pub extern "C" fn rt_atomic_flag_test_and_set(handle: i64) -> bool {
    ATOMIC_FLAG_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|flag| flag.swap(true, Ordering::SeqCst))
        .unwrap_or(false)
}

/// Clear atomic flag
#[no_mangle]
pub extern "C" fn rt_atomic_flag_clear(handle: i64) {
    if let Some(flag) = ATOMIC_FLAG_MAP.lock().unwrap().get(&handle) {
        flag.store(false, Ordering::SeqCst);
    }
}

/// Free atomic flag
#[no_mangle]
pub extern "C" fn rt_atomic_flag_free(handle: i64) {
    ATOMIC_FLAG_MAP.lock().unwrap().remove(&handle);
}

// ============================================================================
// Once Operations (one-time initialization)
// ============================================================================

use std::sync::Once as StdOnce;

lazy_static::lazy_static! {
    static ref ONCE_MAP: Mutex<HashMap<i64, Box<StdOnce>>> = Mutex::new(HashMap::new());
}

static mut ONCE_COUNTER: i64 = 1;

/// Create a new Once
#[no_mangle]
pub extern "C" fn rt_once_new() -> i64 {
    let once = Box::new(StdOnce::new());
    unsafe {
        let handle = ONCE_COUNTER;
        ONCE_COUNTER += 1;
        ONCE_MAP.lock().unwrap().insert(handle, once);
        handle
    }
}

/// Call function once (takes function pointer)
/// Note: In real implementation, this would need FFI callback support
#[no_mangle]
pub extern "C" fn rt_once_call(handle: i64, _func_ptr: i64) {
    // For now, just mark as called
    // Full implementation needs callback infrastructure
    if let Some(_once) = ONCE_MAP.lock().unwrap().get(&handle) {
        // once.call_once(|| { /* call func_ptr */ });
    }
}

/// Check if Once has been called
#[no_mangle]
pub extern "C" fn rt_once_is_completed(handle: i64) -> bool {
    ONCE_MAP
        .lock()
        .unwrap()
        .get(&handle)
        .map(|once| once.is_completed())
        .unwrap_or(false)
}

/// Free Once
#[no_mangle]
pub extern "C" fn rt_once_free(handle: i64) {
    ONCE_MAP.lock().unwrap().remove(&handle);
}

// ============================================================================
// Condvar Operations (condition variables)
// ============================================================================

use std::sync::Condvar as StdCondvar;

lazy_static::lazy_static! {
    static ref CONDVAR_MAP: Mutex<HashMap<i64, Box<StdCondvar>>> = Mutex::new(HashMap::new());
}

static mut CONDVAR_COUNTER: i64 = 1;

/// Create a new Condvar
#[no_mangle]
pub extern "C" fn rt_condvar_new() -> i64 {
    let condvar = Box::new(StdCondvar::new());
    unsafe {
        let handle = CONDVAR_COUNTER;
        CONDVAR_COUNTER += 1;
        CONDVAR_MAP.lock().unwrap().insert(handle, condvar);
        handle
    }
}

/// Wait on condvar (note: simplified, real impl needs mutex integration)
#[no_mangle]
pub extern "C" fn rt_condvar_wait(handle: i64, _mutex_handle: i64) {
    // Simplified stub - full implementation needs proper mutex integration
    if let Some(_condvar) = CONDVAR_MAP.lock().unwrap().get(&handle) {
        // condvar.wait(mutex_guard);
    }
}

/// Notify one waiting thread
#[no_mangle]
pub extern "C" fn rt_condvar_notify_one(handle: i64) {
    if let Some(condvar) = CONDVAR_MAP.lock().unwrap().get(&handle) {
        condvar.notify_one();
    }
}

/// Notify all waiting threads
#[no_mangle]
pub extern "C" fn rt_condvar_notify_all(handle: i64) {
    if let Some(condvar) = CONDVAR_MAP.lock().unwrap().get(&handle) {
        condvar.notify_all();
    }
}

/// Free condvar
#[no_mangle]
pub extern "C" fn rt_condvar_free(handle: i64) {
    CONDVAR_MAP.lock().unwrap().remove(&handle);
}

// ============================================================================
// NOTE: Arena allocator and concurrent data structures have been extracted to:
// - ffi/concurrent/arena.rs
// - ffi/concurrent/map.rs
// - ffi/concurrent/queue.rs
// - ffi/concurrent/stack.rs
// - ffi/concurrent/pool.rs
// - ffi/concurrent/tls.rs
// ============================================================================


// ============================================================================
// Hash Functions
// ============================================================================

/// FNV-1a hash function
#[no_mangle]
pub unsafe extern "C" fn rt_fnv_hash(data_ptr: *const u8, data_len: u64) -> u64 {
    if data_ptr.is_null() {
        return 0;
    }

    let data = std::slice::from_raw_parts(data_ptr, data_len as usize);
    let mut hash: u64 = 0xcbf29ce484222325; // FNV offset basis

    for &byte in data {
        hash ^= byte as u64;
        hash = hash.wrapping_mul(0x100000001b3); // FNV prime
    }

    hash
}

// ============================================================================
// Threading Utilities (Extended)
// ============================================================================
// NOTE: Standard input operations (rt_read_stdin_line_ffi) extracted to ffi/io_print.rs

/// Hint to CPU for spin loops
#[no_mangle]
pub extern "C" fn rt_spin_loop_hint() {
    std::hint::spin_loop();
}

// ============================================================================
// Condvar Extended
// ============================================================================

/// Wait on condvar with timeout (milliseconds)
/// Returns 1 if notified, 0 if timeout
#[no_mangle]
pub extern "C" fn rt_condvar_wait_timeout(condvar_handle: i64, _mutex_handle: i64, timeout_ms: u64) -> i64 {
    // Simplified implementation - real version needs proper mutex integration
    if CONDVAR_MAP.lock().unwrap().get(&condvar_handle).is_some() {
        // In a real implementation, we'd wait with timeout on the condvar
        // For now, just sleep for the timeout period and return timeout
        std::thread::sleep(std::time::Duration::from_millis(timeout_ms));
        0 // timeout
    } else {
        -1 // error: invalid handle
    }
}

// ============================================================================
// RwLock Extended (No-op unlock functions)
// ============================================================================

/// Unlock read lock (no-op in Rust, handled by RAII)
#[no_mangle]
pub extern "C" fn rt_rwlock_unlock_read(_handle: i64) {
    // No-op: locks are released automatically when guard is dropped
}

/// Unlock write lock (no-op in Rust, handled by RAII)
#[no_mangle]
pub extern "C" fn rt_rwlock_unlock_write(_handle: i64) {
    // No-op: locks are released automatically when guard is dropped
}

// ============================================================================
// NOTE: Hash functions (SHA1, SHA256, XXHash) have been extracted to:
// - ffi/hash/sha1.rs
// - ffi/hash/sha256.rs
// - ffi/hash/xxhash.rs
// ============================================================================


// ============================================================================
// Thread-Local Storage (Alternative API)
// ============================================================================

thread_local! {
    static THREAD_LOCAL_STORAGE: RefCell<HashMap<i64, RuntimeValue>> = RefCell::new(HashMap::new());
}

lazy_static::lazy_static! {
    static ref THREAD_LOCAL_HANDLES: Mutex<HashSet<i64>> = Mutex::new(HashSet::new());
}

static mut THREAD_LOCAL_COUNTER: i64 = 1;

/// Create new thread-local storage slot
#[no_mangle]
pub extern "C" fn rt_thread_local_new() -> i64 {
    unsafe {
        let handle = THREAD_LOCAL_COUNTER;
        THREAD_LOCAL_COUNTER += 1;
        THREAD_LOCAL_HANDLES.lock().unwrap().insert(handle);
        handle
    }
}

/// Get value from thread-local storage
#[no_mangle]
pub extern "C" fn rt_thread_local_get(handle: i64) -> RuntimeValue {
    // Check if handle is valid
    if !THREAD_LOCAL_HANDLES.lock().unwrap().contains(&handle) {
        return RuntimeValue::NIL;
    }

    THREAD_LOCAL_STORAGE.with(|storage| storage.borrow().get(&handle).copied().unwrap_or(RuntimeValue::NIL))
}

/// Set value in thread-local storage
#[no_mangle]
pub extern "C" fn rt_thread_local_set(handle: i64, value: RuntimeValue) {
    // Check if handle is valid
    if !THREAD_LOCAL_HANDLES.lock().unwrap().contains(&handle) {
        return;
    }

    THREAD_LOCAL_STORAGE.with(|storage| {
        storage.borrow_mut().insert(handle, value);
    });
}

/// Free thread-local storage slot
#[no_mangle]
pub extern "C" fn rt_thread_local_free(handle: i64) {
    THREAD_LOCAL_HANDLES.lock().unwrap().remove(&handle);
    // Note: Each thread's local copy will be cleaned up when thread exits
}

// PyTorch/ML Operations
// ============================================================================

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref TENSOR_MAP: Mutex<HashMap<i64, Tensor>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut TENSOR_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
fn store_tensor(tensor: Tensor) -> i64 {
    unsafe {
        let handle = TENSOR_COUNTER;
        TENSOR_COUNTER += 1;
        TENSOR_MAP.lock().unwrap().insert(handle, tensor);
        handle
    }
}

#[cfg(feature = "pytorch")]
fn get_tensor(handle: i64) -> Option<Tensor> {
    TENSOR_MAP.lock().unwrap().get(&handle).cloned()
}

#[cfg(feature = "pytorch")]
fn remove_tensor(handle: i64) -> Option<Tensor> {
    TENSOR_MAP.lock().unwrap().remove(&handle)
}

// Autograd Context Storage
// An autograd context stores tensors saved during forward pass for use in backward pass

#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct AutogradContext {
    saved_tensors: Vec<i64>, // Tensor handles
    saved_values: HashMap<String, RuntimeValue>,
}

#[cfg(feature = "pytorch")]
impl AutogradContext {
    fn new() -> Self {
        AutogradContext {
            saved_tensors: Vec::new(),
            saved_values: HashMap::new(),
        }
    }
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref AUTOGRAD_CTX_MAP: Mutex<HashMap<i64, AutogradContext>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut AUTOGRAD_CTX_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
fn store_autograd_ctx(ctx: AutogradContext) -> i64 {
    unsafe {
        let handle = AUTOGRAD_CTX_COUNTER;
        AUTOGRAD_CTX_COUNTER += 1;
        AUTOGRAD_CTX_MAP.lock().unwrap().insert(handle, ctx);
        handle
    }
}

#[cfg(feature = "pytorch")]
fn get_autograd_ctx(handle: i64) -> Option<AutogradContext> {
    AUTOGRAD_CTX_MAP.lock().unwrap().get(&handle).cloned()
}

#[cfg(feature = "pytorch")]
fn get_autograd_ctx_mut<F, R>(handle: i64, f: F) -> Option<R>
where
    F: FnOnce(&mut AutogradContext) -> R,
{
    AUTOGRAD_CTX_MAP.lock().unwrap().get_mut(&handle).map(f)
}

// Tensor List Storage (for operations like stack that take multiple tensors)

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref TENSOR_LIST_MAP: Mutex<HashMap<i64, Vec<i64>>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut TENSOR_LIST_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
fn store_tensor_list(handles: Vec<i64>) -> i64 {
    unsafe {
        let handle = TENSOR_LIST_COUNTER;
        TENSOR_LIST_COUNTER += 1;
        TENSOR_LIST_MAP.lock().unwrap().insert(handle, handles);
        handle
    }
}

#[cfg(feature = "pytorch")]
fn get_tensor_list(handle: i64) -> Option<Vec<i64>> {
    TENSOR_LIST_MAP.lock().unwrap().get(&handle).cloned()
}

// Helper to convert RuntimeValue to tensor handle
#[cfg(feature = "pytorch")]
fn value_to_tensor_handle(value: RuntimeValue) -> Option<i64> {
    if value.is_int() {
        Some(value.as_int())
    } else {
        None
    }
}

// Basic Tensor Operations

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_add(a: RuntimeValue, b: RuntimeValue) -> RuntimeValue {
    let a_handle = match value_to_tensor_handle(a) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let b_handle = match value_to_tensor_handle(b) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let a_tensor = match get_tensor(a_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let b_tensor = match get_tensor(b_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = a_tensor + b_tensor;
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_add(_a: RuntimeValue, _b: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_add_scalar(tensor: RuntimeValue, scalar: f64) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val + scalar;
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_add_scalar(_tensor: RuntimeValue, _scalar: f64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_sub(a: RuntimeValue, b: RuntimeValue) -> RuntimeValue {
    let a_handle = match value_to_tensor_handle(a) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let b_handle = match value_to_tensor_handle(b) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let a_tensor = match get_tensor(a_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let b_tensor = match get_tensor(b_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = a_tensor - b_tensor;
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_sub(_a: RuntimeValue, _b: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_mul(a: RuntimeValue, b: RuntimeValue) -> RuntimeValue {
    let a_handle = match value_to_tensor_handle(a) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let b_handle = match value_to_tensor_handle(b) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let a_tensor = match get_tensor(a_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let b_tensor = match get_tensor(b_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = a_tensor * b_tensor;
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_mul(_a: RuntimeValue, _b: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_mul_scalar(tensor: RuntimeValue, scalar: f64) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val * scalar;
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_mul_scalar(_tensor: RuntimeValue, _scalar: f64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_div(a: RuntimeValue, b: RuntimeValue) -> RuntimeValue {
    let a_handle = match value_to_tensor_handle(a) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let b_handle = match value_to_tensor_handle(b) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let a_tensor = match get_tensor(a_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let b_tensor = match get_tensor(b_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = a_tensor / b_tensor;
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_div(_a: RuntimeValue, _b: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_matmul(a: RuntimeValue, b: RuntimeValue) -> RuntimeValue {
    let a_handle = match value_to_tensor_handle(a) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let b_handle = match value_to_tensor_handle(b) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let a_tensor = match get_tensor(a_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let b_tensor = match get_tensor(b_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = a_tensor.matmul(&b_tensor);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_matmul(_a: RuntimeValue, _b: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_cos(tensor: RuntimeValue) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.cos();
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_cos(_tensor: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_max(tensor: RuntimeValue) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.max();
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_max(_tensor: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_min(tensor: RuntimeValue) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.min();
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_min(_tensor: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_std(tensor: RuntimeValue) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.std(true);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_std(_tensor: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_var(tensor: RuntimeValue) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.var(true);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_var(_tensor: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_norm(tensor: RuntimeValue) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.norm();
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_norm(_tensor: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_gt(a: RuntimeValue, b: RuntimeValue) -> RuntimeValue {
    let a_handle = match value_to_tensor_handle(a) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let b_handle = match value_to_tensor_handle(b) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let a_tensor = match get_tensor(a_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let b_tensor = match get_tensor(b_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = a_tensor.gt(&b_tensor);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_gt(_a: RuntimeValue, _b: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_index(tensor: RuntimeValue, indices: RuntimeValue) -> RuntimeValue {
    // Tensor indexing: tensor[indices]
    // indices can be a tensor of indices or a scalar index
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // If indices is a tensor handle, use index_select on dim 0
    if let Some(indices_handle) = value_to_tensor_handle(indices) {
        if let Some(indices_tensor) = get_tensor(indices_handle) {
            let result = tensor_val.index_select(0, &indices_tensor);
            let handle = store_tensor(result);
            return RuntimeValue::from_int(handle);
        }
    }

    // If indices is a scalar integer, use select on dim 0
    if indices.is_int() {
        let idx = indices.as_int();
        let result = tensor_val.select(0, idx);
        let handle = store_tensor(result);
        return RuntimeValue::from_int(handle);
    }

    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_index(_tensor: RuntimeValue, _indices: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_select(tensor: RuntimeValue, dim: i64, index: i64) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.select(dim, index);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_select(_tensor: RuntimeValue, _dim: i64, _index: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_narrow(tensor: RuntimeValue, dim: i64, start: i64, length: i64) -> RuntimeValue {
    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = tensor_val.narrow(dim, start, length);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_narrow(_tensor: RuntimeValue, _dim: i64, _start: i64, _length: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_stack(tensors: RuntimeValue, dim: i64) -> RuntimeValue {
    // Stack tensors along a new dimension
    // tensors is a handle to a tensor list (stored via store_tensor_list)
    if !tensors.is_int() {
        return RuntimeValue::NIL;
    }

    let list_handle = tensors.as_int();
    let tensor_handles = match get_tensor_list(list_handle) {
        Some(handles) => handles,
        None => return RuntimeValue::NIL,
    };

    // Collect actual tensors from handles
    let tensors_vec: Vec<Tensor> = tensor_handles.iter().filter_map(|&h| get_tensor(h)).collect();

    if tensors_vec.is_empty() {
        return RuntimeValue::NIL;
    }

    // Stack tensors
    let result = Tensor::stack(&tensors_vec, dim);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_stack(_tensors: RuntimeValue, _dim: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

// Autograd Operations

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_new() -> RuntimeValue {
    // Create a new autograd context for storing tensors during forward pass
    let ctx = AutogradContext::new();
    let handle = store_autograd_ctx(ctx);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_new() -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_save_tensor(ctx: RuntimeValue, tensor: RuntimeValue) {
    // Save a tensor to the autograd context for use in backward pass
    if !ctx.is_int() || !tensor.is_int() {
        return;
    }

    let ctx_handle = ctx.as_int();
    let tensor_handle = tensor.as_int();

    get_autograd_ctx_mut(ctx_handle, |context| {
        context.saved_tensors.push(tensor_handle);
    });
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_save_tensor(_ctx: RuntimeValue, _tensor: RuntimeValue) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_save_value(ctx: RuntimeValue, key: RuntimeValue, value: RuntimeValue) {
    // Save an arbitrary value to the autograd context with a string key
    if !ctx.is_int() {
        return;
    }

    let ctx_handle = ctx.as_int();
    // For simplicity, use the key's integer representation as a string key
    let key_str = format!("{}", key.as_int());

    get_autograd_ctx_mut(ctx_handle, |context| {
        context.saved_values.insert(key_str, value);
    });
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_save_value(_ctx: RuntimeValue, _key: RuntimeValue, _value: RuntimeValue) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_get_saved_tensors(ctx: RuntimeValue) -> RuntimeValue {
    // Get all saved tensors from the autograd context as a tensor list handle
    if !ctx.is_int() {
        return RuntimeValue::NIL;
    }

    let ctx_handle = ctx.as_int();
    if let Some(context) = get_autograd_ctx(ctx_handle) {
        // Return a tensor list handle containing all saved tensor handles
        let list_handle = store_tensor_list(context.saved_tensors.clone());
        return RuntimeValue::from_int(list_handle);
    }

    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_get_saved_tensors(_ctx: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_get_value(ctx: RuntimeValue, key: RuntimeValue) -> RuntimeValue {
    // Get a saved value from the autograd context by key
    if !ctx.is_int() {
        return RuntimeValue::NIL;
    }

    let ctx_handle = ctx.as_int();
    let key_str = format!("{}", key.as_int());

    if let Some(context) = get_autograd_ctx(ctx_handle) {
        if let Some(&value) = context.saved_values.get(&key_str) {
            return value;
        }
    }

    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_context_get_value(_ctx: RuntimeValue, _key: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

// Custom Autograd Function Storage
// Stores forward/backward function pairs for custom autograd operations

#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct AutogradFunction {
    // Handle to the forward function (stored externally)
    forward_fn: i64,
    // Handle to the backward function (stored externally)
    backward_fn: i64,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref AUTOGRAD_FN_MAP: Mutex<HashMap<i64, AutogradFunction>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut AUTOGRAD_FN_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
fn store_autograd_fn(func: AutogradFunction) -> i64 {
    unsafe {
        let handle = AUTOGRAD_FN_COUNTER;
        AUTOGRAD_FN_COUNTER += 1;
        AUTOGRAD_FN_MAP.lock().unwrap().insert(handle, func);
        handle
    }
}

#[cfg(feature = "pytorch")]
fn get_autograd_fn(handle: i64) -> Option<AutogradFunction> {
    AUTOGRAD_FN_MAP.lock().unwrap().get(&handle).cloned()
}

/// Create a custom autograd function from forward and backward function handles
#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_function_new(forward_fn: i64, backward_fn: i64) -> RuntimeValue {
    let func = AutogradFunction {
        forward_fn,
        backward_fn,
    };
    let handle = store_autograd_fn(func);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_function_new(_forward_fn: i64, _backward_fn: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

/// Apply a custom autograd function to input tensors
/// This executes the forward pass and sets up for backward pass
#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_function_apply(func: RuntimeValue, inputs: RuntimeValue) -> RuntimeValue {
    // Get the autograd function
    if !func.is_int() {
        return RuntimeValue::NIL;
    }
    let func_handle = func.as_int();

    let _autograd_fn = match get_autograd_fn(func_handle) {
        Some(f) => f,
        None => return RuntimeValue::NIL,
    };

    // Get input tensors from tensor list
    if !inputs.is_int() {
        return RuntimeValue::NIL;
    }
    let inputs_handle = inputs.as_int();

    let input_handles = match get_tensor_list(inputs_handle) {
        Some(handles) => handles,
        None => {
            // If not a tensor list, treat as single tensor
            vec![inputs_handle]
        }
    };

    // Collect input tensors
    let input_tensors: Vec<Tensor> = input_handles.iter().filter_map(|&h| get_tensor(h)).collect();

    if input_tensors.is_empty() {
        return RuntimeValue::NIL;
    }

    // For custom autograd functions, we need to:
    // 1. Create an autograd context
    // 2. Call the forward function with context and inputs
    // 3. Return the outputs
    //
    // Since we can't directly call Simple functions from here,
    // we return the input tensors with requires_grad set for now.
    // The actual forward/backward logic is handled at the Simple language level.

    // Set requires_grad on inputs and return them
    let output_handles: Vec<i64> = input_tensors
        .into_iter()
        .map(|t| {
            let t_with_grad = t.set_requires_grad(true);
            store_tensor(t_with_grad)
        })
        .collect();

    // Return as tensor list
    let list_handle = store_tensor_list(output_handles);
    RuntimeValue::from_int(list_handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_autograd_function_apply(_func: RuntimeValue, _inputs: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

/// Gradient checkpointing: recompute activations during backward pass
/// to save memory at the cost of extra computation
#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_checkpoint(func: RuntimeValue, inputs: RuntimeValue) -> RuntimeValue {
    // Gradient checkpointing trades memory for compute by not storing
    // intermediate activations during forward pass.
    //
    // In tch-rs, we can simulate this by:
    // 1. Running forward with no_grad to avoid storing activations
    // 2. Storing only the inputs
    // 3. During backward, recompute the forward pass
    //
    // For now, we implement a simplified version that just runs the
    // forward pass with gradient computation enabled.

    if !inputs.is_int() {
        return RuntimeValue::NIL;
    }

    let inputs_handle = inputs.as_int();

    // Get input tensors
    let input_handles = match get_tensor_list(inputs_handle) {
        Some(handles) => handles,
        None => vec![inputs_handle],
    };

    let input_tensors: Vec<Tensor> = input_handles.iter().filter_map(|&h| get_tensor(h)).collect();

    if input_tensors.is_empty() {
        return RuntimeValue::NIL;
    }

    // For checkpointing, we detach inputs and set requires_grad
    // This ensures gradients will flow back through the checkpoint boundary
    let checkpointed: Vec<i64> = input_tensors
        .into_iter()
        .map(|t| {
            // Detach and re-enable gradients for checkpointing
            let detached = t.detach().set_requires_grad(true);
            store_tensor(detached)
        })
        .collect();

    // If func is provided as a valid autograd function, we could apply it here
    // For now, return the checkpointed tensors
    if func.is_int() {
        let func_handle = func.as_int();
        if let Some(_autograd_fn) = get_autograd_fn(func_handle) {
            // Would apply the function here in a full implementation
            // For now, just return checkpointed inputs
        }
    }

    let list_handle = store_tensor_list(checkpointed);
    RuntimeValue::from_int(list_handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_checkpoint(_func: RuntimeValue, _inputs: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

// Loss Functions

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_bce_loss(pred: RuntimeValue, target: RuntimeValue) -> RuntimeValue {
    let pred_handle = match value_to_tensor_handle(pred) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let target_handle = match value_to_tensor_handle(target) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let pred_tensor = match get_tensor(pred_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let target_tensor = match get_tensor(target_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = pred_tensor.binary_cross_entropy::<tch::Tensor>(&target_tensor, None, tch::Reduction::Mean);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_bce_loss(_pred: RuntimeValue, _target: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_cross_entropy_loss(pred: RuntimeValue, target: RuntimeValue) -> RuntimeValue {
    let pred_handle = match value_to_tensor_handle(pred) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };
    let target_handle = match value_to_tensor_handle(target) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let pred_tensor = match get_tensor(pred_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let target_tensor = match get_tensor(target_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let result = pred_tensor.cross_entropy_loss::<tch::Tensor>(&target_tensor, None, tch::Reduction::Mean, -100, 0.0);
    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_cross_entropy_loss(_pred: RuntimeValue, _target: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

// Neural Network Layer Storage
// ============================================================================

// Conv3d layer: stores weight, bias, and config
#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct Conv3dLayer {
    weight: i64,       // Tensor handle for weight [out_channels, in_channels, kD, kH, kW]
    bias: Option<i64>, // Tensor handle for bias [out_channels]
    stride: i64,
    padding: i64,
    dilation: i64,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref CONV3D_MAP: Mutex<HashMap<i64, Conv3dLayer>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut CONV3D_COUNTER: i64 = 1;

// RNN layer: stores weights for input-hidden and hidden-hidden
#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct RnnLayer {
    weight_ih: i64, // Tensor handle [hidden_size, input_size]
    weight_hh: i64, // Tensor handle [hidden_size, hidden_size]
    bias_ih: Option<i64>,
    bias_hh: Option<i64>,
    hidden_size: i64,
    num_layers: i64,
    batch_first: bool,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref RNN_MAP: Mutex<HashMap<i64, RnnLayer>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut RNN_COUNTER: i64 = 1;

// MultiheadAttention layer
#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct MultiheadAttentionLayer {
    in_proj_weight: i64, // [3 * embed_dim, embed_dim]
    in_proj_bias: Option<i64>,
    out_proj_weight: i64, // [embed_dim, embed_dim]
    out_proj_bias: Option<i64>,
    embed_dim: i64,
    num_heads: i64,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref MULTIHEAD_ATTENTION_MAP: Mutex<HashMap<i64, MultiheadAttentionLayer>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut MULTIHEAD_ATTENTION_COUNTER: i64 = 1;

// Positional encoding: precomputed encoding tensor
#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref POSITIONAL_ENCODING_MAP: Mutex<HashMap<i64, i64>> = Mutex::new(HashMap::new()); // Maps handle to tensor handle
}

#[cfg(feature = "pytorch")]
static mut POSITIONAL_ENCODING_COUNTER: i64 = 1;

// TransformerEncoderLayer
#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct TransformerEncoderLayer {
    self_attn: i64, // MultiheadAttention handle
    linear1_weight: i64,
    linear1_bias: i64,
    linear2_weight: i64,
    linear2_bias: i64,
    norm1_weight: i64,
    norm1_bias: i64,
    norm2_weight: i64,
    norm2_bias: i64,
    d_model: i64,
    nhead: i64,
    dim_feedforward: i64,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref TRANSFORMER_ENCODER_LAYER_MAP: Mutex<HashMap<i64, TransformerEncoderLayer>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut TRANSFORMER_ENCODER_LAYER_COUNTER: i64 = 1;

// TransformerDecoderLayer
#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct TransformerDecoderLayer {
    self_attn: i64,  // MultiheadAttention handle
    cross_attn: i64, // MultiheadAttention handle
    linear1_weight: i64,
    linear1_bias: i64,
    linear2_weight: i64,
    linear2_bias: i64,
    norm1_weight: i64,
    norm1_bias: i64,
    norm2_weight: i64,
    norm2_bias: i64,
    norm3_weight: i64,
    norm3_bias: i64,
    d_model: i64,
    nhead: i64,
    dim_feedforward: i64,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref TRANSFORMER_DECODER_LAYER_MAP: Mutex<HashMap<i64, TransformerDecoderLayer>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut TRANSFORMER_DECODER_LAYER_COUNTER: i64 = 1;

// ============================================================================
// Conv3d Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_conv3d_new(in_channels: i64, out_channels: i64, kernel_size: i64) -> RuntimeValue {
    // Create Conv3d layer with Xavier-initialized weights
    // Weight shape: [out_channels, in_channels, kD, kH, kW]
    let weight = Tensor::randn(
        &[out_channels, in_channels, kernel_size, kernel_size, kernel_size],
        (Kind::Float, Device::Cpu),
    );
    // Xavier initialization
    let fan_in = in_channels * kernel_size * kernel_size * kernel_size;
    let std_dev = (2.0 / fan_in as f64).sqrt();
    let weight = weight * std_dev;
    let weight_handle = store_tensor(weight);

    // Bias shape: [out_channels]
    let bias = Tensor::zeros(&[out_channels], (Kind::Float, Device::Cpu));
    let bias_handle = store_tensor(bias);

    let layer = Conv3dLayer {
        weight: weight_handle,
        bias: Some(bias_handle),
        stride: 1,
        padding: 0,
        dilation: 1,
    };

    unsafe {
        let handle = CONV3D_COUNTER;
        CONV3D_COUNTER += 1;
        CONV3D_MAP.lock().unwrap().insert(handle, layer);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_conv3d_new(_in_channels: i64, _out_channels: i64, _kernel_size: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_conv3d_forward(layer: RuntimeValue, input: RuntimeValue) -> RuntimeValue {
    let layer_handle = match layer.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let conv_layer = match CONV3D_MAP.lock().unwrap().get(&layer_handle).cloned() {
        Some(l) => l,
        None => return RuntimeValue::NIL,
    };

    let input_handle = match value_to_tensor_handle(input) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let input_tensor = match get_tensor(input_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let weight = match get_tensor(conv_layer.weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let bias = conv_layer.bias.and_then(get_tensor);

    let result = input_tensor.conv3d(
        &weight,
        bias.as_ref(),
        &[conv_layer.stride, conv_layer.stride, conv_layer.stride],
        &[conv_layer.padding, conv_layer.padding, conv_layer.padding],
        &[conv_layer.dilation, conv_layer.dilation, conv_layer.dilation],
        1, // groups
    );

    let handle = store_tensor(result);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_conv3d_forward(_layer: RuntimeValue, _input: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// RNN Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_rnn_new(input_size: i64, hidden_size: i64) -> RuntimeValue {
    // Create RNN layer with Xavier-initialized weights
    let weight_ih = Tensor::randn(&[hidden_size, input_size], (Kind::Float, Device::Cpu));
    let std_ih = (2.0 / input_size as f64).sqrt();
    let weight_ih = weight_ih * std_ih;
    let weight_ih_handle = store_tensor(weight_ih);

    let weight_hh = Tensor::randn(&[hidden_size, hidden_size], (Kind::Float, Device::Cpu));
    let std_hh = (2.0 / hidden_size as f64).sqrt();
    let weight_hh = weight_hh * std_hh;
    let weight_hh_handle = store_tensor(weight_hh);

    let bias_ih = Tensor::zeros(&[hidden_size], (Kind::Float, Device::Cpu));
    let bias_ih_handle = store_tensor(bias_ih);

    let bias_hh = Tensor::zeros(&[hidden_size], (Kind::Float, Device::Cpu));
    let bias_hh_handle = store_tensor(bias_hh);

    let layer = RnnLayer {
        weight_ih: weight_ih_handle,
        weight_hh: weight_hh_handle,
        bias_ih: Some(bias_ih_handle),
        bias_hh: Some(bias_hh_handle),
        hidden_size,
        num_layers: 1,
        batch_first: true,
    };

    unsafe {
        let handle = RNN_COUNTER;
        RNN_COUNTER += 1;
        RNN_MAP.lock().unwrap().insert(handle, layer);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_rnn_new(_input_size: i64, _hidden_size: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_rnn_forward(layer: RuntimeValue, input: RuntimeValue, hidden: RuntimeValue) -> RuntimeValue {
    let layer_handle = match layer.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let rnn_layer = match RNN_MAP.lock().unwrap().get(&layer_handle).cloned() {
        Some(l) => l,
        None => return RuntimeValue::NIL,
    };

    let input_handle = match value_to_tensor_handle(input) {
        Some(h) => h,
        None => return RuntimeValue::NIL,
    };

    let input_tensor = match get_tensor(input_handle) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Get or create hidden state
    let hidden_tensor = if let Some(h) = value_to_tensor_handle(hidden) {
        get_tensor(h)
    } else {
        None
    };

    let weight_ih = match get_tensor(rnn_layer.weight_ih) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let weight_hh = match get_tensor(rnn_layer.weight_hh) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Collect weights into a flat list as expected by rnn_tanh
    let mut params: Vec<Tensor> = vec![weight_ih, weight_hh];
    if let Some(handle) = rnn_layer.bias_ih {
        if let Some(t) = get_tensor(handle) {
            params.push(t);
        }
    }
    if let Some(handle) = rnn_layer.bias_hh {
        if let Some(t) = get_tensor(handle) {
            params.push(t);
        }
    }

    // Create initial hidden if none provided: [num_layers, batch, hidden_size]
    let batch_size = if rnn_layer.batch_first {
        input_tensor.size()[0]
    } else {
        input_tensor.size()[1]
    };
    let hx = hidden_tensor.unwrap_or_else(|| {
        Tensor::zeros(
            &[rnn_layer.num_layers, batch_size, rnn_layer.hidden_size],
            (Kind::Float, Device::Cpu),
        )
    });

    // Use rnn_tanh functional API
    let (output, h_n) = input_tensor.rnn_tanh(
        &hx,
        &params,
        rnn_layer.bias_ih.is_some(),
        rnn_layer.num_layers,
        0.0,   // dropout
        false, // training
        false, // bidirectional
        rnn_layer.batch_first,
    );

    // Return tuple of (output, hidden_n) as a tensor list
    let output_handle = store_tensor(output);
    let hidden_handle = store_tensor(h_n);
    let list_handle = store_tensor_list(vec![output_handle, hidden_handle]);
    RuntimeValue::from_int(list_handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_rnn_forward(
    _layer: RuntimeValue,
    _input: RuntimeValue,
    _hidden: RuntimeValue,
) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// MultiheadAttention Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_multihead_attention_new(embed_dim: i64, num_heads: i64) -> RuntimeValue {
    // in_proj_weight: [3 * embed_dim, embed_dim] for Q, K, V projections
    let in_proj_weight = Tensor::randn(&[3 * embed_dim, embed_dim], (Kind::Float, Device::Cpu));
    let std_in = (2.0 / embed_dim as f64).sqrt();
    let in_proj_weight = in_proj_weight * std_in;
    let in_proj_weight_handle = store_tensor(in_proj_weight);

    let in_proj_bias = Tensor::zeros(&[3 * embed_dim], (Kind::Float, Device::Cpu));
    let in_proj_bias_handle = store_tensor(in_proj_bias);

    // out_proj_weight: [embed_dim, embed_dim]
    let out_proj_weight = Tensor::randn(&[embed_dim, embed_dim], (Kind::Float, Device::Cpu));
    let out_proj_weight = out_proj_weight * std_in;
    let out_proj_weight_handle = store_tensor(out_proj_weight);

    let out_proj_bias = Tensor::zeros(&[embed_dim], (Kind::Float, Device::Cpu));
    let out_proj_bias_handle = store_tensor(out_proj_bias);

    let layer = MultiheadAttentionLayer {
        in_proj_weight: in_proj_weight_handle,
        in_proj_bias: Some(in_proj_bias_handle),
        out_proj_weight: out_proj_weight_handle,
        out_proj_bias: Some(out_proj_bias_handle),
        embed_dim,
        num_heads,
    };

    unsafe {
        let handle = MULTIHEAD_ATTENTION_COUNTER;
        MULTIHEAD_ATTENTION_COUNTER += 1;
        MULTIHEAD_ATTENTION_MAP.lock().unwrap().insert(handle, layer);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_multihead_attention_new(_embed_dim: i64, _num_heads: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_multihead_attention_forward(
    layer: RuntimeValue,
    query: RuntimeValue,
    key: RuntimeValue,
    value: RuntimeValue,
) -> RuntimeValue {
    let layer_handle = match layer.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let mha_layer = match MULTIHEAD_ATTENTION_MAP.lock().unwrap().get(&layer_handle).cloned() {
        Some(l) => l,
        None => return RuntimeValue::NIL,
    };

    let query_tensor = match value_to_tensor_handle(query).and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let key_tensor = match value_to_tensor_handle(key).and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let value_tensor = match value_to_tensor_handle(value).and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let in_proj_weight = match get_tensor(mha_layer.in_proj_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let out_proj_weight = match get_tensor(mha_layer.out_proj_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let in_proj_bias = mha_layer.in_proj_bias.and_then(get_tensor);
    let out_proj_bias = mha_layer.out_proj_bias.and_then(get_tensor);

    // Use scaled_dot_product_attention for the core attention computation
    // First, compute Q, K, V projections
    let qkv = query_tensor.matmul(&in_proj_weight.transpose(-2, -1));
    let qkv = if let Some(ref bias) = in_proj_bias {
        qkv + bias
    } else {
        qkv
    };

    // Split into Q, K, V
    let chunks = qkv.chunk(3, -1);
    let q = &chunks[0];
    let k = &chunks[1];
    let v = &chunks[2];

    // Reshape for multi-head attention: [batch, seq, heads, head_dim]
    let head_dim = mha_layer.embed_dim / mha_layer.num_heads;
    let batch_size = q.size()[0];
    let seq_len = q.size()[1];

    let q = q
        .view([batch_size, seq_len, mha_layer.num_heads, head_dim])
        .transpose(1, 2);
    let k = key_tensor.matmul(
        &in_proj_weight
            .narrow(0, mha_layer.embed_dim, mha_layer.embed_dim)
            .transpose(-2, -1),
    );
    let k = k.view([batch_size, -1, mha_layer.num_heads, head_dim]).transpose(1, 2);
    let v = value_tensor.matmul(
        &in_proj_weight
            .narrow(0, 2 * mha_layer.embed_dim, mha_layer.embed_dim)
            .transpose(-2, -1),
    );
    let v = v.view([batch_size, -1, mha_layer.num_heads, head_dim]).transpose(1, 2);

    // Scaled dot-product attention
    let scale = (head_dim as f64).sqrt();
    let scores = q.matmul(&k.transpose(-2, -1)) / scale;
    let attn_weights = scores.softmax(-1, Kind::Float);
    let attn_output = attn_weights.matmul(&v);

    // Reshape back: [batch, seq, embed_dim]
    let attn_output = attn_output
        .transpose(1, 2)
        .contiguous()
        .view([batch_size, seq_len, mha_layer.embed_dim]);

    // Output projection
    let output = attn_output.matmul(&out_proj_weight.transpose(-2, -1));
    let output = if let Some(ref bias) = out_proj_bias {
        output + bias
    } else {
        output
    };

    let handle = store_tensor(output);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_multihead_attention_forward(
    _layer: RuntimeValue,
    _query: RuntimeValue,
    _key: RuntimeValue,
    _value: RuntimeValue,
) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// Positional Encoding Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_positional_encoding_new(d_model: i64, max_len: i64) -> RuntimeValue {
    // Create sinusoidal positional encoding
    // PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    // PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    let pe = Tensor::zeros(&[max_len, d_model], (Kind::Float, Device::Cpu));
    let position = Tensor::arange(max_len, (Kind::Float, Device::Cpu)).unsqueeze(1);
    let div_term = (Tensor::arange_start_step(0, d_model, 2, (Kind::Float, Device::Cpu))
        * (-((10000.0_f64).ln()) / d_model as f64))
        .exp();

    // Fill even indices with sin
    let sin_vals = (&position * &div_term).sin();
    let cos_vals = (&position * &div_term).cos();

    // Interleave sin and cos values
    let mut pe_vec = Vec::new();
    for i in 0..max_len {
        let mut row = Vec::new();
        for j in 0..d_model / 2 {
            row.push(sin_vals.double_value(&[i, j]));
            row.push(cos_vals.double_value(&[i, j]));
        }
        // Handle odd d_model
        if d_model % 2 == 1 {
            row.push(sin_vals.double_value(&[i, d_model / 2]));
        }
        pe_vec.extend(row);
    }

    let pe_tensor = Tensor::of_slice(&pe_vec).view([max_len, d_model]);
    let pe_handle = store_tensor(pe_tensor);

    unsafe {
        let handle = POSITIONAL_ENCODING_COUNTER;
        POSITIONAL_ENCODING_COUNTER += 1;
        POSITIONAL_ENCODING_MAP.lock().unwrap().insert(handle, pe_handle);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_positional_encoding_new(_d_model: i64, _max_len: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// TransformerEncoderLayer Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_encoder_layer_new(d_model: i64, nhead: i64) -> RuntimeValue {
    let dim_feedforward = 4 * d_model; // Standard transformer uses 4x

    // Create self-attention
    let self_attn_handle = match rt_torch_multihead_attention_new(d_model, nhead).as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    // Feedforward layers
    let linear1_weight = Tensor::randn(&[dim_feedforward, d_model], (Kind::Float, Device::Cpu));
    let std1 = (2.0 / d_model as f64).sqrt();
    let linear1_weight = linear1_weight * std1;
    let linear1_weight_handle = store_tensor(linear1_weight);

    let linear1_bias = Tensor::zeros(&[dim_feedforward], (Kind::Float, Device::Cpu));
    let linear1_bias_handle = store_tensor(linear1_bias);

    let linear2_weight = Tensor::randn(&[d_model, dim_feedforward], (Kind::Float, Device::Cpu));
    let std2 = (2.0 / dim_feedforward as f64).sqrt();
    let linear2_weight = linear2_weight * std2;
    let linear2_weight_handle = store_tensor(linear2_weight);

    let linear2_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let linear2_bias_handle = store_tensor(linear2_bias);

    // Layer normalization parameters
    let norm1_weight = Tensor::ones(&[d_model], (Kind::Float, Device::Cpu));
    let norm1_weight_handle = store_tensor(norm1_weight);
    let norm1_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let norm1_bias_handle = store_tensor(norm1_bias);

    let norm2_weight = Tensor::ones(&[d_model], (Kind::Float, Device::Cpu));
    let norm2_weight_handle = store_tensor(norm2_weight);
    let norm2_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let norm2_bias_handle = store_tensor(norm2_bias);

    let layer = TransformerEncoderLayer {
        self_attn: self_attn_handle,
        linear1_weight: linear1_weight_handle,
        linear1_bias: linear1_bias_handle,
        linear2_weight: linear2_weight_handle,
        linear2_bias: linear2_bias_handle,
        norm1_weight: norm1_weight_handle,
        norm1_bias: norm1_bias_handle,
        norm2_weight: norm2_weight_handle,
        norm2_bias: norm2_bias_handle,
        d_model,
        nhead,
        dim_feedforward,
    };

    unsafe {
        let handle = TRANSFORMER_ENCODER_LAYER_COUNTER;
        TRANSFORMER_ENCODER_LAYER_COUNTER += 1;
        TRANSFORMER_ENCODER_LAYER_MAP.lock().unwrap().insert(handle, layer);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_encoder_layer_new(_d_model: i64, _nhead: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_encoder_layer_forward(layer: RuntimeValue, src: RuntimeValue) -> RuntimeValue {
    let layer_handle = match layer.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let enc_layer = match TRANSFORMER_ENCODER_LAYER_MAP
        .lock()
        .unwrap()
        .get(&layer_handle)
        .cloned()
    {
        Some(l) => l,
        None => return RuntimeValue::NIL,
    };

    let src_tensor = match value_to_tensor_handle(src).and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Self-attention
    let attn_output = rt_torch_multihead_attention_forward(
        RuntimeValue::from_int(enc_layer.self_attn),
        RuntimeValue::from_int(store_tensor(src_tensor.shallow_clone())),
        RuntimeValue::from_int(store_tensor(src_tensor.shallow_clone())),
        RuntimeValue::from_int(store_tensor(src_tensor.shallow_clone())),
    );

    let attn_output = match attn_output.as_int().ok().and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Residual + LayerNorm
    let norm1_weight = match get_tensor(enc_layer.norm1_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let norm1_bias = match get_tensor(enc_layer.norm1_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let x = &src_tensor + &attn_output;
    let x = x.layer_norm(&[enc_layer.d_model], Some(&norm1_weight), Some(&norm1_bias), 1e-5, true);

    // Feedforward
    let linear1_weight = match get_tensor(enc_layer.linear1_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let linear1_bias = match get_tensor(enc_layer.linear1_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let linear2_weight = match get_tensor(enc_layer.linear2_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let linear2_bias = match get_tensor(enc_layer.linear2_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let ff = x.matmul(&linear1_weight.transpose(-2, -1)) + &linear1_bias;
    let ff = ff.relu();
    let ff = ff.matmul(&linear2_weight.transpose(-2, -1)) + &linear2_bias;

    // Residual + LayerNorm
    let norm2_weight = match get_tensor(enc_layer.norm2_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let norm2_bias = match get_tensor(enc_layer.norm2_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let output = &x + &ff;
    let output = output.layer_norm(&[enc_layer.d_model], Some(&norm2_weight), Some(&norm2_bias), 1e-5, true);

    let handle = store_tensor(output);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_encoder_layer_forward(_layer: RuntimeValue, _src: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// TransformerDecoderLayer Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_decoder_layer_new(d_model: i64, nhead: i64) -> RuntimeValue {
    let dim_feedforward = 4 * d_model;

    // Self-attention
    let self_attn_handle = match rt_torch_multihead_attention_new(d_model, nhead).as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    // Cross-attention
    let cross_attn_handle = match rt_torch_multihead_attention_new(d_model, nhead).as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    // Feedforward layers
    let linear1_weight = Tensor::randn(&[dim_feedforward, d_model], (Kind::Float, Device::Cpu));
    let std1 = (2.0 / d_model as f64).sqrt();
    let linear1_weight = linear1_weight * std1;
    let linear1_weight_handle = store_tensor(linear1_weight);

    let linear1_bias = Tensor::zeros(&[dim_feedforward], (Kind::Float, Device::Cpu));
    let linear1_bias_handle = store_tensor(linear1_bias);

    let linear2_weight = Tensor::randn(&[d_model, dim_feedforward], (Kind::Float, Device::Cpu));
    let std2 = (2.0 / dim_feedforward as f64).sqrt();
    let linear2_weight = linear2_weight * std2;
    let linear2_weight_handle = store_tensor(linear2_weight);

    let linear2_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let linear2_bias_handle = store_tensor(linear2_bias);

    // Layer normalization parameters (3 norms for decoder)
    let norm1_weight = Tensor::ones(&[d_model], (Kind::Float, Device::Cpu));
    let norm1_weight_handle = store_tensor(norm1_weight);
    let norm1_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let norm1_bias_handle = store_tensor(norm1_bias);

    let norm2_weight = Tensor::ones(&[d_model], (Kind::Float, Device::Cpu));
    let norm2_weight_handle = store_tensor(norm2_weight);
    let norm2_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let norm2_bias_handle = store_tensor(norm2_bias);

    let norm3_weight = Tensor::ones(&[d_model], (Kind::Float, Device::Cpu));
    let norm3_weight_handle = store_tensor(norm3_weight);
    let norm3_bias = Tensor::zeros(&[d_model], (Kind::Float, Device::Cpu));
    let norm3_bias_handle = store_tensor(norm3_bias);

    let layer = TransformerDecoderLayer {
        self_attn: self_attn_handle,
        cross_attn: cross_attn_handle,
        linear1_weight: linear1_weight_handle,
        linear1_bias: linear1_bias_handle,
        linear2_weight: linear2_weight_handle,
        linear2_bias: linear2_bias_handle,
        norm1_weight: norm1_weight_handle,
        norm1_bias: norm1_bias_handle,
        norm2_weight: norm2_weight_handle,
        norm2_bias: norm2_bias_handle,
        norm3_weight: norm3_weight_handle,
        norm3_bias: norm3_bias_handle,
        d_model,
        nhead,
        dim_feedforward,
    };

    unsafe {
        let handle = TRANSFORMER_DECODER_LAYER_COUNTER;
        TRANSFORMER_DECODER_LAYER_COUNTER += 1;
        TRANSFORMER_DECODER_LAYER_MAP.lock().unwrap().insert(handle, layer);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_decoder_layer_new(_d_model: i64, _nhead: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_decoder_layer_forward(
    layer: RuntimeValue,
    tgt: RuntimeValue,
    memory: RuntimeValue,
) -> RuntimeValue {
    let layer_handle = match layer.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let dec_layer = match TRANSFORMER_DECODER_LAYER_MAP
        .lock()
        .unwrap()
        .get(&layer_handle)
        .cloned()
    {
        Some(l) => l,
        None => return RuntimeValue::NIL,
    };

    let tgt_tensor = match value_to_tensor_handle(tgt).and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let memory_tensor = match value_to_tensor_handle(memory).and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Self-attention on target
    let self_attn_output = rt_torch_multihead_attention_forward(
        RuntimeValue::from_int(dec_layer.self_attn),
        RuntimeValue::from_int(store_tensor(tgt_tensor.shallow_clone())),
        RuntimeValue::from_int(store_tensor(tgt_tensor.shallow_clone())),
        RuntimeValue::from_int(store_tensor(tgt_tensor.shallow_clone())),
    );

    let self_attn_output = match self_attn_output.as_int().ok().and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Residual + LayerNorm 1
    let norm1_weight = match get_tensor(dec_layer.norm1_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let norm1_bias = match get_tensor(dec_layer.norm1_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let x = &tgt_tensor + &self_attn_output;
    let x = x.layer_norm(&[dec_layer.d_model], Some(&norm1_weight), Some(&norm1_bias), 1e-5, true);

    // Cross-attention (query=x, key=memory, value=memory)
    let cross_attn_output = rt_torch_multihead_attention_forward(
        RuntimeValue::from_int(dec_layer.cross_attn),
        RuntimeValue::from_int(store_tensor(x.shallow_clone())),
        RuntimeValue::from_int(store_tensor(memory_tensor.shallow_clone())),
        RuntimeValue::from_int(store_tensor(memory_tensor.shallow_clone())),
    );

    let cross_attn_output = match cross_attn_output.as_int().ok().and_then(get_tensor) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    // Residual + LayerNorm 2
    let norm2_weight = match get_tensor(dec_layer.norm2_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let norm2_bias = match get_tensor(dec_layer.norm2_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let x = &x + &cross_attn_output;
    let x = x.layer_norm(&[dec_layer.d_model], Some(&norm2_weight), Some(&norm2_bias), 1e-5, true);

    // Feedforward
    let linear1_weight = match get_tensor(dec_layer.linear1_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let linear1_bias = match get_tensor(dec_layer.linear1_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let linear2_weight = match get_tensor(dec_layer.linear2_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let linear2_bias = match get_tensor(dec_layer.linear2_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let ff = x.matmul(&linear1_weight.transpose(-2, -1)) + &linear1_bias;
    let ff = ff.relu();
    let ff = ff.matmul(&linear2_weight.transpose(-2, -1)) + &linear2_bias;

    // Residual + LayerNorm 3
    let norm3_weight = match get_tensor(dec_layer.norm3_weight) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };
    let norm3_bias = match get_tensor(dec_layer.norm3_bias) {
        Some(t) => t,
        None => return RuntimeValue::NIL,
    };

    let output = &x + &ff;
    let output = output.layer_norm(&[dec_layer.d_model], Some(&norm3_weight), Some(&norm3_bias), 1e-5, true);

    let handle = store_tensor(output);
    RuntimeValue::from_int(handle)
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_transformer_decoder_layer_forward(
    _layer: RuntimeValue,
    _tgt: RuntimeValue,
    _memory: RuntimeValue,
) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// Optimizers Storage
// ============================================================================

#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct RmsPropOptimizer {
    params: Vec<i64>,     // Tensor handles for parameters
    square_avg: Vec<i64>, // Running average of squared gradients
    lr: f64,
    alpha: f64, // Smoothing constant (default 0.99)
    eps: f64,   // Term added for numerical stability
    momentum: f64,
    momentum_buffer: Vec<i64>, // Momentum buffer tensors
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref RMSPROP_MAP: Mutex<HashMap<i64, RmsPropOptimizer>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut RMSPROP_COUNTER: i64 = 1;

// ============================================================================
// RMSprop Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_rmsprop_new(params: RuntimeValue, lr: f64) -> RuntimeValue {
    // params is a tensor list handle
    let params_handle = match params.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let param_handles = match get_tensor_list(params_handle) {
        Some(h) => h,
        None => {
            // If not a list, treat as single tensor
            vec![params_handle]
        }
    };

    // Initialize square_avg to zeros for each parameter
    let mut square_avg = Vec::new();
    let mut momentum_buffer = Vec::new();

    for &handle in &param_handles {
        if let Some(tensor) = get_tensor(handle) {
            let zeros = Tensor::zeros_like(&tensor);
            square_avg.push(store_tensor(zeros.shallow_clone()));
            momentum_buffer.push(store_tensor(zeros));
        }
    }

    let optimizer = RmsPropOptimizer {
        params: param_handles,
        square_avg,
        lr,
        alpha: 0.99,
        eps: 1e-8,
        momentum: 0.0,
        momentum_buffer,
    };

    unsafe {
        let handle = RMSPROP_COUNTER;
        RMSPROP_COUNTER += 1;
        RMSPROP_MAP.lock().unwrap().insert(handle, optimizer);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_rmsprop_new(_params: RuntimeValue, _lr: f64) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// JIT/TorchScript Storage and Implementation
// ============================================================================

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref JIT_MODULE_MAP: Mutex<HashMap<i64, tch::CModule>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut JIT_MODULE_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_script(_module: RuntimeValue) -> RuntimeValue {
    // TorchScript compilation from Simple code isn't directly supported
    // This would require a complete code generation pipeline
    // Return NIL to indicate not implemented for in-memory compilation
    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_script(_module: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_trace(_func: RuntimeValue, _example_inputs: RuntimeValue) -> RuntimeValue {
    // JIT tracing requires executing a function with example inputs
    // This would need integration with the interpreter
    // Return NIL for now - full implementation requires more infrastructure
    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_trace(_func: RuntimeValue, _example_inputs: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_load(path_ptr: *const u8, path_len: u64) -> RuntimeValue {
    if path_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return RuntimeValue::NIL,
    };

    match tch::CModule::load(path_str) {
        Ok(module) => unsafe {
            let handle = JIT_MODULE_COUNTER;
            JIT_MODULE_COUNTER += 1;
            JIT_MODULE_MAP.lock().unwrap().insert(handle, module);
            RuntimeValue::from_int(handle)
        },
        Err(_) => RuntimeValue::NIL,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_load(_path_ptr: *const u8, _path_len: u64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_save(module: RuntimeValue, path_ptr: *const u8, path_len: u64) {
    if path_ptr.is_null() {
        return;
    }

    let module_handle = match module.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return,
    };

    if let Some(m) = JIT_MODULE_MAP.lock().unwrap().get(&module_handle) {
        let _ = m.save(path_str);
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_save(_module: RuntimeValue, _path_ptr: *const u8, _path_len: u64) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_forward(module: RuntimeValue, inputs: RuntimeValue) -> RuntimeValue {
    let module_handle = match module.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let inputs_handle = match inputs.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    // inputs should be a tensor list
    let input_handles = match get_tensor_list(inputs_handle) {
        Some(h) => h,
        None => {
            // Single tensor input
            if let Some(t) = get_tensor(inputs_handle) {
                vec![store_tensor(t)]
            } else {
                return RuntimeValue::NIL;
            }
        }
    };

    let input_tensors: Vec<tch::IValue> = input_handles
        .iter()
        .filter_map(|&h| get_tensor(h).map(tch::IValue::Tensor))
        .collect();

    if let Some(m) = JIT_MODULE_MAP.lock().unwrap().get(&module_handle) {
        match m.forward_is(&input_tensors) {
            Ok(output) => {
                if let Some(tensor) = output.into_tensor().ok() {
                    let handle = store_tensor(tensor);
                    return RuntimeValue::from_int(handle);
                }
            }
            Err(_) => {}
        }
    }

    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_forward(_module: RuntimeValue, _inputs: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_eval(module: RuntimeValue) {
    let module_handle = match module.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    if let Some(m) = JIT_MODULE_MAP.lock().unwrap().get_mut(&module_handle) {
        m.set_eval();
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_eval(_module: RuntimeValue) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_train(module: RuntimeValue) {
    let module_handle = match module.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    if let Some(m) = JIT_MODULE_MAP.lock().unwrap().get_mut(&module_handle) {
        m.set_train();
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_train(_module: RuntimeValue) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_jit_free(module: RuntimeValue) {
    let module_handle = match module.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    JIT_MODULE_MAP.lock().unwrap().remove(&module_handle);
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_jit_free(_module: RuntimeValue) {}

// ============================================================================
// Model Serialization (Tensor Save/Load)
// ============================================================================

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_load(path_ptr: *const u8, path_len: u64) -> RuntimeValue {
    if path_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return RuntimeValue::NIL,
    };

    match Tensor::load(path_str) {
        Ok(tensor) => {
            let handle = store_tensor(tensor);
            RuntimeValue::from_int(handle)
        }
        Err(_) => RuntimeValue::NIL,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_load(_path_ptr: *const u8, _path_len: u64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_save(tensor: RuntimeValue, path_ptr: *const u8, path_len: u64) {
    if path_ptr.is_null() {
        return;
    }

    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return,
    };

    let tensor_val = match get_tensor(tensor_handle) {
        Some(t) => t,
        None => return,
    };

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return,
    };

    let _ = tensor_val.save(path_str);
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_save(_tensor: RuntimeValue, _path_ptr: *const u8, _path_len: u64) {}

// ============================================================================
// ONNX Operations
// Note: Full ONNX support requires the ort (ONNX Runtime) crate
// These are stub implementations that work with PyTorch's ONNX export
// ============================================================================

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref ONNX_SESSION_MAP: Mutex<HashMap<i64, Vec<u8>>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut ONNX_SESSION_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_export(
    _module: RuntimeValue,
    _dummy_input: RuntimeValue,
    path_ptr: *const u8,
    path_len: u64,
) {
    // ONNX export from tch-rs requires going through PyTorch's export mechanism
    // This is a placeholder - full implementation needs torch.onnx.export binding
    if path_ptr.is_null() || path_len == 0 {
        return;
    }
    // For now, this is a no-op as direct ONNX export isn't supported in tch-rs
    // Users should export from Python or use a pre-exported ONNX model
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_export(
    _module: RuntimeValue,
    _dummy_input: RuntimeValue,
    _path_ptr: *const u8,
    _path_len: u64,
) {
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_load(path_ptr: *const u8, path_len: u64) -> RuntimeValue {
    if path_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return RuntimeValue::NIL,
    };

    // Read ONNX file into memory
    match std::fs::read(path_str) {
        Ok(data) => unsafe {
            let handle = ONNX_SESSION_COUNTER;
            ONNX_SESSION_COUNTER += 1;
            ONNX_SESSION_MAP.lock().unwrap().insert(handle, data);
            RuntimeValue::from_int(handle)
        },
        Err(_) => RuntimeValue::NIL,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_load(_path_ptr: *const u8, _path_len: u64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_run(_session: RuntimeValue, _inputs: RuntimeValue) -> RuntimeValue {
    // ONNX inference requires ONNX Runtime (ort crate)
    // This is a placeholder - returns NIL until ort integration is added
    RuntimeValue::NIL
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_run(_session: RuntimeValue, _inputs: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_check(path_ptr: *const u8, path_len: u64) -> i64 {
    if path_ptr.is_null() {
        return 0;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return 0,
    };

    // Basic validation: check if file exists and has ONNX magic bytes
    if let Ok(data) = std::fs::read(path_str) {
        // ONNX files start with protobuf header
        // Basic check: file is not empty and can be read
        if data.len() > 8 {
            return 1; // Valid (exists and has content)
        }
    }
    0
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_check(_path_ptr: *const u8, _path_len: u64) -> i64 {
    0
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_free(session: RuntimeValue) {
    let session_handle = match session.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    ONNX_SESSION_MAP.lock().unwrap().remove(&session_handle);
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_onnx_free(_session: RuntimeValue) {}

// ============================================================================
// Dataset Operations
// ============================================================================

#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct DatasetHandle {
    images: i64, // Tensor handle for images
    labels: i64, // Tensor handle for labels
    size: i64,   // Number of samples
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref DATASET_MAP: Mutex<HashMap<i64, DatasetHandle>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut DATASET_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_mnist_download(path_ptr: *const u8, path_len: u64) -> i64 {
    if path_ptr.is_null() {
        return 0;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return 0,
    };

    // Use tch-rs vision module to download MNIST
    match tch::vision::mnist::download(path_str) {
        Ok(_) => 1,
        Err(_) => 0,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_mnist_download(_path_ptr: *const u8, _path_len: u64) -> i64 {
    0
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_mnist_load(path_ptr: *const u8, path_len: u64, train: i64) -> RuntimeValue {
    if path_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return RuntimeValue::NIL,
    };

    match tch::vision::mnist::load_dir(path_str) {
        Ok(dataset) => {
            let (images, labels) = if train != 0 {
                (dataset.train_images, dataset.train_labels)
            } else {
                (dataset.test_images, dataset.test_labels)
            };

            let size = images.size()[0];
            let images_handle = store_tensor(images);
            let labels_handle = store_tensor(labels);

            let ds = DatasetHandle {
                images: images_handle,
                labels: labels_handle,
                size,
            };

            unsafe {
                let handle = DATASET_COUNTER;
                DATASET_COUNTER += 1;
                DATASET_MAP.lock().unwrap().insert(handle, ds);
                RuntimeValue::from_int(handle)
            }
        }
        Err(_) => RuntimeValue::NIL,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_mnist_load(_path_ptr: *const u8, _path_len: u64, _train: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_cifar10_download(path_ptr: *const u8, path_len: u64) -> i64 {
    if path_ptr.is_null() {
        return 0;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return 0,
    };

    // Use tch-rs vision module to download CIFAR10
    match tch::vision::cifar::download(path_str) {
        Ok(_) => 1,
        Err(_) => 0,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_cifar10_download(_path_ptr: *const u8, _path_len: u64) -> i64 {
    0
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_cifar10_load(path_ptr: *const u8, path_len: u64, train: i64) -> RuntimeValue {
    if path_ptr.is_null() {
        return RuntimeValue::NIL;
    }

    let path_slice = unsafe { std::slice::from_raw_parts(path_ptr, path_len as usize) };
    let path_str = match std::str::from_utf8(path_slice) {
        Ok(s) => s,
        Err(_) => return RuntimeValue::NIL,
    };

    match tch::vision::cifar::load_dir(path_str) {
        Ok(dataset) => {
            let (images, labels) = if train != 0 {
                (dataset.train_images, dataset.train_labels)
            } else {
                (dataset.test_images, dataset.test_labels)
            };

            let size = images.size()[0];
            let images_handle = store_tensor(images);
            let labels_handle = store_tensor(labels);

            let ds = DatasetHandle {
                images: images_handle,
                labels: labels_handle,
                size,
            };

            unsafe {
                let handle = DATASET_COUNTER;
                DATASET_COUNTER += 1;
                DATASET_MAP.lock().unwrap().insert(handle, ds);
                RuntimeValue::from_int(handle)
            }
        }
        Err(_) => RuntimeValue::NIL,
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_cifar10_load(_path_ptr: *const u8, _path_len: u64, _train: i64) -> RuntimeValue {
    RuntimeValue::NIL
}

// ============================================================================
// Distributed Training
// Note: Distributed training requires special PyTorch builds and MPI/NCCL
// These are placeholder implementations for API compatibility
// ============================================================================

#[cfg(feature = "pytorch")]
static DIST_INITIALIZED: std::sync::atomic::AtomicBool = std::sync::atomic::AtomicBool::new(false);

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_is_available() -> i64 {
    // Check if distributed training is available
    // tch-rs doesn't expose distributed APIs directly
    // Return 0 for now as it requires special PyTorch builds
    0
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_is_available() -> i64 {
    0
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_init_process_group(backend_ptr: *const u8, backend_len: u64) -> i64 {
    if backend_ptr.is_null() {
        return 0;
    }

    let backend_slice = unsafe { std::slice::from_raw_parts(backend_ptr, backend_len as usize) };
    let _backend_str = match std::str::from_utf8(backend_slice) {
        Ok(s) => s,
        Err(_) => return 0,
    };

    // Distributed training initialization is not directly available in tch-rs
    // This would require direct FFI to libtorch distributed APIs
    // For now, mark as "initialized" for testing purposes
    DIST_INITIALIZED.store(true, std::sync::atomic::Ordering::SeqCst);
    1
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_init_process_group(_backend_ptr: *const u8, _backend_len: u64) -> i64 {
    0
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_destroy_process_group() {
    DIST_INITIALIZED.store(false, std::sync::atomic::Ordering::SeqCst);
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_destroy_process_group() {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_barrier() {
    // In a real distributed setup, this would synchronize all processes
    // For single-process, this is a no-op
    if !DIST_INITIALIZED.load(std::sync::atomic::Ordering::SeqCst) {
        return;
    }
    // Barrier is a no-op in single-process mode
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_barrier() {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_all_reduce(tensor: RuntimeValue, op: i64) {
    // All-reduce: combine tensors from all processes
    // In single-process mode, this is a no-op (tensor stays the same)
    if !DIST_INITIALIZED.load(std::sync::atomic::Ordering::SeqCst) {
        return;
    }

    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return,
    };

    // In actual distributed mode, this would use NCCL/Gloo
    // op: 0 = SUM, 1 = PRODUCT, 2 = MIN, 3 = MAX
    let _ = (tensor_handle, op); // Silence unused warning
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_all_reduce(_tensor: RuntimeValue, _op: i64) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_all_gather(tensor_list: RuntimeValue, tensor: RuntimeValue) {
    // All-gather: gather tensors from all processes
    // In single-process mode, just copy tensor to the list
    if !DIST_INITIALIZED.load(std::sync::atomic::Ordering::SeqCst) {
        return;
    }

    let list_handle = match tensor_list.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    let tensor_handle = match value_to_tensor_handle(tensor) {
        Some(h) => h,
        None => return,
    };

    if let Some(t) = get_tensor(tensor_handle) {
        // In single-process, gather just adds the tensor to the list
        if let Some(mut list) = get_tensor_list(list_handle) {
            list.push(store_tensor(t));
            // Update the list in storage
            TENSOR_LIST_MAP.lock().unwrap().insert(list_handle, list);
        }
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_all_gather(_tensor_list: RuntimeValue, _tensor: RuntimeValue) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_broadcast(tensor: RuntimeValue, src: i64) {
    // Broadcast: send tensor from src to all processes
    // In single-process mode, this is a no-op
    if !DIST_INITIALIZED.load(std::sync::atomic::Ordering::SeqCst) {
        return;
    }

    let _ = (tensor, src); // Silence unused warning
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_broadcast(_tensor: RuntimeValue, _src: i64) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_reduce_scatter(output: RuntimeValue, input_list: RuntimeValue) {
    // Reduce-scatter: reduce and scatter to all processes
    // In single-process mode, just sum the inputs
    if !DIST_INITIALIZED.load(std::sync::atomic::Ordering::SeqCst) {
        return;
    }

    let output_handle = match value_to_tensor_handle(output) {
        Some(h) => h,
        None => return,
    };

    let list_handle = match input_list.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    if let Some(handles) = get_tensor_list(list_handle) {
        let tensors: Vec<Tensor> = handles.iter().filter_map(|&h| get_tensor(h)).collect();
        if !tensors.is_empty() {
            // Sum all tensors
            let mut result = tensors[0].shallow_clone();
            for t in tensors.iter().skip(1) {
                result = &result + t;
            }
            // Update output tensor
            if let Some(out) = get_tensor(output_handle) {
                let _ = out.copy_(&result);
            }
        }
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_reduce_scatter(_output: RuntimeValue, _input_list: RuntimeValue) {}

// DDP (DistributedDataParallel) wrapper storage
#[cfg(feature = "pytorch")]
#[derive(Clone)]
struct DDPWrapper {
    module_handle: i64, // Underlying module/tensor list
    sync_gradients: bool,
}

#[cfg(feature = "pytorch")]
lazy_static::lazy_static! {
    static ref DDP_MAP: Mutex<HashMap<i64, DDPWrapper>> = Mutex::new(HashMap::new());
}

#[cfg(feature = "pytorch")]
static mut DDP_COUNTER: i64 = 1;

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_ddp_new(module: RuntimeValue) -> RuntimeValue {
    if !DIST_INITIALIZED.load(std::sync::atomic::Ordering::SeqCst) {
        return RuntimeValue::NIL;
    }

    let module_handle = match module.as_int() {
        Ok(h) => h,
        Err(_) => return RuntimeValue::NIL,
    };

    let ddp = DDPWrapper {
        module_handle,
        sync_gradients: true,
    };

    unsafe {
        let handle = DDP_COUNTER;
        DDP_COUNTER += 1;
        DDP_MAP.lock().unwrap().insert(handle, ddp);
        RuntimeValue::from_int(handle)
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_ddp_new(_module: RuntimeValue) -> RuntimeValue {
    RuntimeValue::NIL
}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_ddp_free(ddp: RuntimeValue) {
    let ddp_handle = match ddp.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    DDP_MAP.lock().unwrap().remove(&ddp_handle);
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_ddp_free(_ddp: RuntimeValue) {}

#[cfg(feature = "pytorch")]
#[no_mangle]
pub extern "C" fn rt_torch_dist_ddp_set_sync(ddp: RuntimeValue, sync: i64) {
    let ddp_handle = match ddp.as_int() {
        Ok(h) => h,
        Err(_) => return,
    };

    if let Some(wrapper) = DDP_MAP.lock().unwrap().get_mut(&ddp_handle) {
        wrapper.sync_gradients = sync != 0;
    }
}

#[cfg(not(feature = "pytorch"))]
#[no_mangle]
pub extern "C" fn rt_torch_dist_ddp_set_sync(_ddp: RuntimeValue, _sync: i64) {}
