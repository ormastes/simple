# Recurrent Neural Network Layers
#
# Provides recurrent layer implementations for sequence modeling:
# - RNN: Vanilla recurrent layer with tanh or ReLU activation
# - LSTM: Long Short-Term Memory with gating mechanisms
# - GRU: Gated Recurrent Unit (efficient alternative to LSTM)
#
# ## Classes
# - `RNN`: Vanilla RNN layer
# - `LSTM`: Long Short-Term Memory layer
# - `GRU`: Gated Recurrent Unit layer
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.nn as nn
#
# # LSTM for sequence classification
# let lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)
# let input = torch.randn([seq_len, batch, 10])
# let output, (hn, cn) = lstm(input)
# ```

export RNN, LSTM, GRU

import base.{Module}


class RNN(Module):
    """Vanilla Recurrent Neural Network layer.

    Applies RNN with tanh or ReLU activation over input sequence.

    Example:
        ```simple
        # Input: [seq_len, batch, input_size]
        # Output: [seq_len, batch, hidden_size]
        let rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2)
        let output, hidden = rnn(input)
        ```
    """
    module_handle: u64
    input_size: i32
    hidden_size: i32
    num_layers: i32

    fn __init__(input_size: i32, hidden_size: i32, num_layers: i32 = 1, nonlinearity: str = "tanh", dropout: f64 = 0.0):
        """Initialize RNN layer.

        Args:
            input_size: Size of input features
            hidden_size: Size of hidden state
            num_layers: Number of stacked RNN layers (default: 1)
            nonlinearity: "tanh" or "relu" (default: "tanh")
            dropout: Dropout probability between layers (default: 0.0)
        """
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Convert nonlinearity to int: 0=tanh, 1=relu
        let nonlin_code = 0
        if nonlinearity == "relu":
            nonlin_code = 1

        # Create module via FFI
        self.module_handle = @rt_torch_rnn_new(
            input_size,
            hidden_size,
            num_layers,
            nonlin_code,
            dropout
        )
        if self.module_handle == 0:
            panic("Failed to create RNN layer")

    fn __del__():
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(x: Tensor, h0: Tensor = None) -> (Tensor, Tensor):
        """Apply RNN forward pass.

        Args:
            x: Input tensor [seq_len, batch, input_size]
            h0: Initial hidden state [num_layers, batch, hidden_size] (default: zeros)

        Returns:
            (output, hn): Output tensor and final hidden state
                output: [seq_len, batch, hidden_size]
                hn: [num_layers, batch, hidden_size]
        """
        let h0_handle = 0u64
        if h0 is not None:
            h0_handle = h0.handle

        let mut out_handle = 0u64
        let mut hn_handle = 0u64

        @rt_torch_rnn_forward(
            self.module_handle,
            x.handle,
            h0_handle,
            &out_handle,
            &hn_handle
        )

        if out_handle == 0 or hn_handle == 0:
            panic("RNN forward pass failed")

        return (Tensor(out_handle), Tensor(hn_handle))


class LSTM(Module):
    """Long Short-Term Memory recurrent layer.

    Applies LSTM with forget gate, input gate, and output gate over input sequence.

    Example:
        ```simple
        # Input: [seq_len, batch, input_size]
        # Output: [seq_len, batch, hidden_size]
        let lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)
        let output, (hn, cn) = lstm(input)
        ```
    """
    module_handle: u64
    input_size: i32
    hidden_size: i32
    num_layers: i32

    fn __init__(input_size: i32, hidden_size: i32, num_layers: i32 = 1, dropout: f64 = 0.0):
        """Initialize LSTM layer.

        Args:
            input_size: Size of input features
            hidden_size: Size of hidden state
            num_layers: Number of stacked LSTM layers (default: 1)
            dropout: Dropout probability between layers (default: 0.0)
        """
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Create module via FFI
        self.module_handle = @rt_torch_lstm_new(
            input_size,
            hidden_size,
            num_layers,
            dropout
        )
        if self.module_handle == 0:
            panic("Failed to create LSTM layer")

    fn __del__():
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(x: Tensor, h0: Tensor = None, c0: Tensor = None) -> (Tensor, (Tensor, Tensor)):
        """Apply LSTM forward pass.

        Args:
            x: Input tensor [seq_len, batch, input_size]
            h0: Initial hidden state [num_layers, batch, hidden_size] (default: zeros)
            c0: Initial cell state [num_layers, batch, hidden_size] (default: zeros)

        Returns:
            (output, (hn, cn)): Output tensor, final hidden state, and final cell state
                output: [seq_len, batch, hidden_size]
                hn: [num_layers, batch, hidden_size]
                cn: [num_layers, batch, hidden_size]
        """
        let h0_handle = 0u64
        let c0_handle = 0u64
        if h0 is not None:
            h0_handle = h0.handle
        if c0 is not None:
            c0_handle = c0.handle

        let mut out_handle = 0u64
        let mut hn_handle = 0u64
        let mut cn_handle = 0u64

        @rt_torch_lstm_forward(
            self.module_handle,
            x.handle,
            h0_handle,
            c0_handle,
            &out_handle,
            &hn_handle,
            &cn_handle
        )

        if out_handle == 0 or hn_handle == 0 or cn_handle == 0:
            panic("LSTM forward pass failed")

        return (Tensor(out_handle), (Tensor(hn_handle), Tensor(cn_handle)))


class GRU(Module):
    """Gated Recurrent Unit layer.

    Applies GRU with update and reset gates over input sequence.
    More efficient than LSTM with similar performance.

    Example:
        ```simple
        # Input: [seq_len, batch, input_size]
        # Output: [seq_len, batch, hidden_size]
        let gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2)
        let output, hidden = gru(input)
        ```
    """
    module_handle: u64
    input_size: i32
    hidden_size: i32
    num_layers: i32

    fn __init__(input_size: i32, hidden_size: i32, num_layers: i32 = 1, dropout: f64 = 0.0):
        """Initialize GRU layer.

        Args:
            input_size: Size of input features
            hidden_size: Size of hidden state
            num_layers: Number of stacked GRU layers (default: 1)
            dropout: Dropout probability between layers (default: 0.0)
        """
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Create module via FFI
        self.module_handle = @rt_torch_gru_new(
            input_size,
            hidden_size,
            num_layers,
            dropout
        )
        if self.module_handle == 0:
            panic("Failed to create GRU layer")

    fn __del__():
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(x: Tensor, h0: Tensor = None) -> (Tensor, Tensor):
        """Apply GRU forward pass.

        Args:
            x: Input tensor [seq_len, batch, input_size]
            h0: Initial hidden state [num_layers, batch, hidden_size] (default: zeros)

        Returns:
            (output, hn): Output tensor and final hidden state
                output: [seq_len, batch, hidden_size]
                hn: [num_layers, batch, hidden_size]
        """
        let h0_handle = 0u64
        if h0 is not None:
            h0_handle = h0.handle

        let mut out_handle = 0u64
        let mut hn_handle = 0u64

        @rt_torch_gru_forward(
            self.module_handle,
            x.handle,
            h0_handle,
            &out_handle,
            &hn_handle
        )

        if out_handle == 0 or hn_handle == 0:
            panic("GRU forward pass failed")

        return (Tensor(out_handle), Tensor(hn_handle))


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_rnn_new(input_size: i32, hidden_size: i32, num_layers: i32, nonlinearity: i32, dropout: f64) -> u64
extern fn rt_torch_rnn_forward(module: u64, input: u64, h0: u64, out_ptr: *u64, hn_ptr: *u64) -> i32

extern fn rt_torch_lstm_new(input_size: i32, hidden_size: i32, num_layers: i32, dropout: f64) -> u64
extern fn rt_torch_lstm_forward(module: u64, input: u64, h0: u64, c0: u64, out_ptr: *u64, hn_ptr: *u64, cn_ptr: *u64) -> i32

extern fn rt_torch_gru_new(input_size: i32, hidden_size: i32, num_layers: i32, dropout: f64) -> u64
extern fn rt_torch_gru_forward(module: u64, input: u64, h0: u64, out_ptr: *u64, hn_ptr: *u64) -> i32

extern fn rt_torch_module_free(module: u64) -> i32
