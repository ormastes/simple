# Tree-sitter Parser for Simple Language
# Recursive descent parser using grammar rules

import core.{Option, Result}
import parser.treesitter.tree.{Tree, Node, NodeId, Span, NodeArena}
import parser.treesitter.grammar.{Grammar, GrammarRule, Token, TokenKind, build_simple_grammar}
import parser.treesitter.lexer.{Lexer}
import parser.treesitter.edits.{InputEdit, Point, compute_edits, find_affected_nodes, find_reparse_boundary}

# Parser state
struct TreeSitterParser:
    grammar: Grammar
    tokens: [Token]
    pos: i64
    arena: NodeArena

    # Create new parser for Simple language
    fn new(language: str) -> Result[TreeSitterParser, str]:
        if language != "simple":
            return Err("Unsupported language: " + language)

        return Ok(TreeSitterParser(
            grammar: build_simple_grammar(),
            tokens: [],
            pos: 0,
            arena: NodeArena.new()
        ))

    # Parse source code into tree
    me parse(self, source: str) -> Result[Tree, str]:
        # Tokenize source
        let mut lexer = Lexer.new(source)
        self.tokens = lexer.tokenize()?
        self.pos = 0

        # Parse module (entry point)
        let root_rule = self.grammar.rules.get("module").ok_or("Missing module rule")?
        let root_id = self.parse_rule(root_rule)?

        # Create tree
        return Ok(Tree(
            root_node: root_id,
            arena: self.arena,
            source: source,
            version: 0
        ))

    # Parse source code incrementally (reuse unchanged subtrees)
    me parse_incremental(self, source: str,
        old_tree: Tree,
        edits: [InputEdit]
    ) -> Result[Tree, str]:
        # Phase 2.1: Simple full reparse (same as parse())
        # Phase 2.2: Efficient incremental with structural sharing (TODO)

        if edits.len() == 0:
            # No changes: return old tree with updated source
            return Ok(Tree(
                root_node: old_tree.root_node,
                arena: old_tree.arena,
                source: source,
                version: old_tree.version + 1
            ))

        # For Phase 2.1: Full reparse
        # Phase 2.2 will:
        # 1. Find affected nodes
        # 2. Find reparse boundary (minimal stable parent)
        # 3. Reparse only affected subtree
        # 4. Splice into old tree with structural sharing

        # Tokenize new source
        let mut lexer = Lexer.new(source)
        self.tokens = lexer.tokenize()?
        self.pos = 0

        # Full reparse from root
        let root_rule = self.grammar.rules.get("module").ok_or("Missing module rule")?

        # Clone old arena for structural sharing (Phase 2.2)
        # For Phase 2.1: Create fresh arena
        self.arena = NodeArena.new()

        let root_id = self.parse_rule(root_rule)?

        # Create new tree
        return Ok(Tree(
            root_node: root_id,
            arena: self.arena,
            source: source,
            version: old_tree.version + 1
        ))

    # Get current token
    fn current(self) -> Option[Token]:
        if self.pos >= 0 and self.pos < self.tokens.len():
            return Some(self.tokens[self.pos])
        else:
            return None

    # Advance to next token
    me advance(self):
        self.pos = self.pos + 1

    # Peek ahead N tokens
    fn peek(self, n: i64) -> Option[Token]:
        let idx = self.pos + n
        if idx >= 0 and idx < self.tokens.len():
            return Some(self.tokens[idx])
        else:
            return None

    # Check if at end of input
    fn is_at_end(self) -> bool:
        return self.pos >= self.tokens.len()

    # Expect specific token kind
    me expect(self, expected: TokenKind) -> Result[Token, str]:
        match self.current():
            case Some(token):
                if self.matches_token(token.kind, expected):
                    self.advance()
                    return Ok(token)
                else:
                    return Err("Expected " + self.token_kind_name(expected) +
                              ", found " + self.token_kind_name(token.kind))
            case None:
                return Err("Unexpected end of input")

    # Check if token kinds match (handling parameterized variants)
    fn matches_token(self, actual: TokenKind, expected: TokenKind) -> bool:
        # Simple name-based matching for Phase 1
        # TODO: [parser][P3] Proper variant matching
        return true  # Placeholder

    fn token_kind_name(self, kind: TokenKind) -> str:
        # TODO: [parser][P1] Implement proper token kind name extraction
        return "TOKEN"

    # Parse using grammar rule
    me parse_rule(self, rule: GrammarRule) -> Result[NodeId, str]:
        match rule:
            case GrammarRule.TokenRule(kind):
                return self.parse_token(kind)

            case GrammarRule.Seq(rules):
                return self.parse_sequence(rules)

            case GrammarRule.Choice(rules):
                return self.parse_choice(rules)

            case GrammarRule.Optional(r):
                return self.parse_optional(r)

            case GrammarRule.ZeroOrMore(r):
                return self.parse_zero_or_more(r)

            case GrammarRule.OneOrMore(r):
                return self.parse_one_or_more(r)

            case GrammarRule.Named(name, r):
                return self.parse_named(name, r)

            case GrammarRule.Field(field_name, r):
                return self.parse_field(field_name, r)

            case _:
                return Err("Unknown grammar rule")

    # Parse terminal token
    me parse_token(self, kind: TokenKind) -> Result[NodeId, str]:
        let token = self.expect(kind)?

        # Create leaf node
        let node = Node(
            id: NodeId(index: 0, generation: 0),  # Will be set by arena
            kind: self.token_kind_name(kind),
            span: token.span,
            children: [],
            fields: {},
            has_error: false,
            text: token.text
        )

        return Ok(self.arena.alloc(node))

    # Parse sequence of rules
    me parse_sequence(self, rules: [GrammarRule]) -> Result[NodeId, str]:
        let mut children: [NodeId] = []

        for rule in rules:
            let child = self.parse_rule(rule)?
            children.push(child)

        # Create sequence node
        let node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "sequence",
            span: self.compute_span(children),
            children: children,
            fields: {},
            has_error: false,
            text: ""
        )

        return Ok(self.arena.alloc(node))

    # Parse ordered choice (try each alternative)
    me parse_choice(self, rules: [GrammarRule]) -> Result[NodeId, str]:
        let start_pos = self.pos

        for rule in rules:
            # Try this alternative
            match self.parse_rule(rule):
                case Ok(node_id):
                    return Ok(node_id)
                case Err(_):
                    # Backtrack and try next alternative
                    self.pos = start_pos

        return Err("No alternative matched")

    # Parse optional rule
    me parse_optional(self, rule: GrammarRule) -> Result[NodeId, str]:
        let start_pos = self.pos

        match self.parse_rule(rule):
            case Ok(node_id):
                return Ok(node_id)
            case Err(_):
                # Optional failed, backtrack and return empty
                self.pos = start_pos
                # Return empty node
                let node = Node(
                    id: NodeId(index: 0, generation: 0),
                    kind: "empty",
                    span: Span(
                        start_byte: 0, end_byte: 0,
                        start_line: 0, end_line: 0,
                        start_column: 0, end_column: 0
                    ),
                    children: [],
                    fields: {},
                    has_error: false,
                    text: ""
                )
                return Ok(self.arena.alloc(node))

    # Parse zero or more repetitions
    me parse_zero_or_more(self, rule: GrammarRule) -> Result[NodeId, str]:
        let mut children: [NodeId] = []

        loop:
            let start_pos = self.pos
            match self.parse_rule(rule):
                case Ok(node_id):
                    children.push(node_id)
                case Err(_):
                    self.pos = start_pos
                    break

        # Create repetition node
        let node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "repetition",
            span: self.compute_span(children),
            children: children,
            fields: {},
            has_error: false,
            text: ""
        )

        return Ok(self.arena.alloc(node))

    # Parse one or more repetitions
    me parse_one_or_more(self, rule: GrammarRule) -> Result[NodeId, str]:
        # Parse first (required)
        let first = self.parse_rule(rule)?
        let mut children = [first]

        # Parse remaining (optional)
        loop:
            let start_pos = self.pos
            match self.parse_rule(rule):
                case Ok(node_id):
                    children.push(node_id)
                case Err(_):
                    self.pos = start_pos
                    break

        # Create repetition node
        let node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "repetition",
            span: self.compute_span(children),
            children: children,
            fields: {},
            has_error: false,
            text: ""
        )

        return Ok(self.arena.alloc(node))

    # Parse named rule (creates node with specific kind)
    me parse_named(self, name: str, rule: GrammarRule) -> Result[NodeId, str]:
        let child = self.parse_rule(rule)?

        # Get child node to extract span
        match self.arena.get(child):
            case Some(child_node):
                # Create named node wrapping child
                let node = Node(
                    id: NodeId(index: 0, generation: 0),
                    kind: name,
                    span: child_node.span,
                    children: [child],
                    fields: {},
                    has_error: false,
                    text: ""
                )
                return Ok(self.arena.alloc(node))
            case None:
                return Err("Failed to get child node")

    # Parse field (stores in parent's fields dict)
    me parse_field(self, field_name: str, rule: GrammarRule) -> Result[NodeId, str]:
        # For Phase 1: just parse the rule
        # Field storage will be handled by parent node
        return self.parse_rule(rule)

    # Compute span covering all children
    fn compute_span(self, children: [NodeId]) -> Span:
        if children.len() == 0:
            return Span(
                start_byte: 0, end_byte: 0,
                start_line: 0, end_line: 0,
                start_column: 0, end_column: 0
            )

        # Get first and last child spans
        let first_span = match self.arena.get(children[0]):
            case Some(node): node.span
            case None: Span(
                start_byte: 0, end_byte: 0,
                start_line: 0, end_line: 0,
                start_column: 0, end_column: 0
            )

        let last_span = match self.arena.get(children[children.len() - 1]):
            case Some(node): node.span
            case None: first_span

        return Span(
            start_byte: first_span.start_byte,
            end_byte: last_span.end_byte,
            start_line: first_span.start_line,
            end_line: last_span.end_line,
            start_column: first_span.start_column,
            end_column: last_span.end_column
        )

    # Error recovery: Create ERROR node
    me create_error_node(self, message: str, start_pos: i64) -> NodeId:
        let end_pos = self.pos

        # Compute span from start to current position
        let start_token = if start_pos >= 0 and start_pos < self.tokens.len():
            Some(self.tokens[start_pos])
        else:
            None

        let end_token = self.current()

        let span = match (start_token, end_token):
            case (Some(st), Some(et)):
                Span(
                    start_byte: st.span.start_byte,
                    end_byte: et.span.end_byte,
                    start_line: st.span.start_line,
                    end_line: et.span.end_line,
                    start_column: st.span.start_column,
                    end_column: et.span.end_column
                )
            case (Some(st), None):
                st.span
            case (None, Some(et)):
                et.span
            case (None, None):
                Span(
                    start_byte: 0, end_byte: 0,
                    start_line: 0, end_line: 0,
                    start_column: 0, end_column: 0
                )

        let error_node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "ERROR",
            span: span,
            children: [],
            fields: {},
            has_error: true,
            text: message
        )

        return self.arena.alloc(error_node)

    # Error recovery: Skip to synchronization point
    me skip_to_sync_point(self) -> bool:
        # Sync points: keywords that start new top-level constructs
        while not self.is_at_end():
            match self.current():
                case Some(token):
                    # Found sync point (statement/declaration keyword)
                    match token.kind:
                        case TokenKind.Fn:
                            return true
                        case TokenKind.Struct:
                            return true
                        case TokenKind.Class:
                            return true
                        case TokenKind.Enum:
                            return true
                        case TokenKind.Let:
                            return true
                        case TokenKind.Return:
                            return true
                        case TokenKind.Newline:
                            # Skip newline and continue
                            self.advance()
                        case TokenKind.Dedent:
                            return true
                        case _:
                            # Skip token
                            self.advance()
                case None:
                    return false

        return false

    # Error recovery: Try to insert missing token (for expect failures)
    me try_recover_missing_token(self, expected: TokenKind, rule_name: str) -> Result[NodeId, str]:
        # Create ERROR node for missing token
        let error_msg = "Expected " + self.token_kind_name(expected) + " in " + rule_name
        let error_id = self.create_error_node(error_msg, self.pos)

        # Don't advance - next parse might succeed
        return Ok(error_id)

    # Error recovery: Balance braces/brackets/parens
    me balance_delimiters(self, open: TokenKind, close: TokenKind) -> bool:
        let mut depth = 1  # We've already consumed the opening delimiter

        while not self.is_at_end() and depth > 0:
            match self.current():
                case Some(token):
                    if self.matches_token(token.kind, open):
                        depth = depth + 1
                    elif self.matches_token(token.kind, close):
                        depth = depth - 1
                    self.advance()
                case None:
                    return false

        return depth == 0

    # Recover from parse error in sequence
    me recover_sequence(self, failed_at: i64) -> NodeId:
        # Skip to next sync point
        self.skip_to_sync_point()

        # Create ERROR node
        let error_msg = "Failed to parse sequence"
        return self.create_error_node(error_msg, failed_at)

    # Tolerant expect: Try to match token, create ERROR node if not found
    me expect_tolerant(self, expected: TokenKind, rule_name: str) -> Result[NodeId, str]:
        match self.expect(expected):
            case Ok(token):
                # Success: create token node
                let node = Node(
                    id: NodeId(index: 0, generation: 0),
                    kind: self.token_kind_name(expected),
                    span: token.span,
                    children: [],
                    fields: {},
                    has_error: false,
                    text: token.text
                )
                return Ok(self.arena.alloc(node))
            case Err(msg):
                # Failed: create ERROR node and try to recover
                return self.try_recover_missing_token(expected, rule_name)
