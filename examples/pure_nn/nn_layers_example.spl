# Neural Network Layers Example - Pure Simple Deep Learning
# Demonstrates building neural network architectures

use lib.pure.autograd (Tensor, backward, tensor_mean)
use lib.pure.nn (
    Linear, ReLU, Sigmoid, Tanh, Softmax,
    Sequential, count_parameters, zero_grad
)

print "=== Pure Simple Neural Network Layers Demo ==="
print ""

# Example 1: Simple 2-layer MLP
print "Example 1: Building a 2-layer MLP"
val mlp = Sequential.create([
    Linear.create(784, 256),    # Input layer: 784 -> 256
    ReLU.create(),               # Activation
    Linear.create(256, 10),      # Output layer: 256 -> 10
    Softmax.create()             # Softmax for classification
])

print "  Model architecture:"
print mlp.to_string()
print "  Total parameters: {count_parameters(mlp)}"
print ""

# Example 2: XOR Network
print "Example 2: XOR Problem Network"
val xor_net = Sequential.create([
    Linear.create(2, 4),         # 2 inputs -> 4 hidden
    ReLU.create(),
    Linear.create(4, 1),         # 4 hidden -> 1 output
    Sigmoid.create()             # Binary classification
])

print "  Architecture for XOR problem:"
print xor_net.to_string()
print "  Total parameters: {count_parameters(xor_net)}"
print ""

# Example 3: Forward Pass
print "Example 3: Forward Pass"
val input = Tensor.from_data([0.0, 1.0], [1, 2], requires_grad: false)
print "  Input: [0.0, 1.0]"

val output = xor_net.forward(input)
print "  Output shape: {output.shape()}"
print "  Output value: {output.value.data[0]}"
print "  (Random initialization, not trained yet)"
print ""

# Example 4: Parameter Inspection
print "Example 4: Inspecting Parameters"
val params = xor_net.parameters()
print "  Number of parameter tensors: {params.len()}"
for (i, param) in params.enumerate():
    print "  Param {i}: shape={param.shape()}, numel={param.numel()}"
print ""

# Example 5: Deep Network
print "Example 5: Deeper Network"
val deep_net = Sequential.create([
    Linear.create(100, 64),
    ReLU.create(),
    Linear.create(64, 32),
    ReLU.create(),
    Linear.create(32, 16),
    ReLU.create(),
    Linear.create(16, 4),
    Softmax.create()
])

print "  Deep network with 4 layers"
print "  Total parameters: {count_parameters(deep_net)}"
print "  Layers: {deep_net.layers.len()}"
print ""

# Example 6: Gradient Flow (with backward pass)
print "Example 6: Gradient Flow Through Network"
val simple_net = Sequential.create([
    Linear.create(3, 2, bias: true)
])

val x = Tensor.from_data([1.0, 2.0, 3.0], [1, 3], requires_grad: false)
val out = simple_net.forward(x)
val loss = tensor_mean(out)

print "  Before backward:"
print "  Loss: {loss.value.data[0]}"

backward(loss)

print "  After backward:"
var has_grads = true
for param in simple_net.parameters():
    if not param.grad.?:
        has_grads = false

if has_grads:
    print "  ✓ Gradients computed for all parameters"
    print "  Weight gradient shape: {simple_net.parameters()[0].grad.?.shape}"
else:
    print "  ⚠ Some parameters missing gradients"

print ""

# Example 7: Train/Eval Mode
print "Example 7: Train vs Eval Mode"
val model = Sequential.create([
    Linear.create(10, 5),
    ReLU.create()
])

print "  Initial mode: {if model.layers[0].training: 'train' else: 'eval'}"

model.eval()
print "  After .eval(): {if model.layers[0].training: 'train' else: 'eval'}"

model.train()
print "  After .train(): {if model.layers[0].training: 'train' else: 'eval'}"
print ""

# Example 8: Zero Gradients
print "Example 8: Zeroing Gradients"
val net = Sequential.create([
    Linear.create(2, 2, bias: true)
])

# Create gradients
val inp = Tensor.from_data([1.0, 1.0], [1, 2], requires_grad: false)
val outp = net.forward(inp)
val l = tensor_mean(outp)
backward(l)

print "  Gradients before zero_grad:"
var grad_count = 0
for param in net.parameters():
    if param.grad.?:
        grad_count = grad_count + 1
print "  Parameters with gradients: {grad_count}/{net.parameters().len()}"

zero_grad(net)

print "  Gradients after zero_grad:"
grad_count = 0
for param in net.parameters():
    if param.grad.?:
        grad_count = grad_count + 1
print "  Parameters with gradients: {grad_count}/{net.parameters().len()}"
print ""

# Summary
print "✓ Neural network layers working!"
print "✓ Can build arbitrary architectures"
print "✓ Forward pass computes outputs"
print "✓ Backward pass computes gradients"
print "✓ Ready for training loop (Phase 4)"
print ""
print "Next: Implement optimizers (SGD, Adam) and training loop!"
