# Simple Regression - Learn y = 2x + 1
# Demonstrates neural network learning a linear function

use lib.pure.autograd (Tensor)
use lib.pure.nn (Linear, Sequential, count_parameters)
use lib.pure.training (SGD, Trainer, mse_loss)
use lib.pure.data (create_linear_dataset)

print "=== Simple Linear Regression ==="
print ""

# Generate dataset: y = 2x + 1
print "Generating dataset: y = 2x + 1"
val train_data = create_linear_dataset(
    num_samples: 20,
    slope: 2.0,
    intercept: 1.0,
    noise: 0.0
)

print "  Samples: {train_data.len()}"
print "  Function: y = 2x + 1"
print ""

# Show some samples
print "Sample data points:"
for i in 0..3:
    val (x, y) = train_data[i]
    print "  x={x.value.data[0]:.2f} -> y={y.value.data[0]:.2f}"
print ""

# Build simple linear model
print "Building linear model (no hidden layers)..."
val model = Sequential.create([
    Linear.create(1, 1, bias: true)
])

print "Architecture: Linear(1 -> 1) with bias"
print "Parameters: {count_parameters(model)}"
print "  Weight: 1 parameter (should learn ~2.0)"
print "  Bias: 1 parameter (should learn ~1.0)"
print ""

# Create optimizer
print "Creating SGD optimizer (lr=0.01)..."
val optimizer = SGD.create(model.parameters(), lr: 0.01, momentum: 0.9)
print ""

# Create trainer
val trainer = Trainer.create(model, optimizer, mse_loss)

# Train
print "Training for 100 epochs..."
trainer.fit(train_data, epochs: 100, verbose: false)

# Show training progress
val history = trainer.get_history()
print "Training complete!"
print ""

print "Loss progression:"
print "  Epoch 0:   {history.losses[0]:.4f}"
print "  Epoch 25:  {history.losses[25]:.4f}"
print "  Epoch 50:  {history.losses[50]:.4f}"
print "  Epoch 99:  {history.losses[99]:.4f}"
print ""

# Test the model
print "=== Testing Learned Function ==="
print ""

print "Predictions vs True values:"
val test_points = [0.0, 0.25, 0.5, 0.75, 1.0]

for x_val in test_points:
    val x_tensor = Tensor.from_data([x_val], [1, 1], requires_grad: false)
    val y_pred = model.forward(x_tensor)
    val y_true = 2.0 * x_val + 1.0

    val error = (y_pred.value.data[0] - y_true).abs()

    print "  x={x_val:.2f}"
    print "    Predicted: {y_pred.value.data[0]:.4f}"
    print "    True:      {y_true:.4f}"
    print "    Error:     {error:.4f}"
    print ""

# Inspect learned parameters
print "=== Learned Parameters ==="
print ""

val params = model.parameters()
val weight = params[0]
val bias = params[1]

print "Weight (should be ~2.0):"
print "  Learned: {weight.value.data[0]:.4f}"
print ""

print "Bias (should be ~1.0):"
print "  Learned: {bias.value.data[0]:.4f}"
print ""

# Summary
print "=== Summary ==="
print "✓ Dataset generated: y = 2x + 1"
print "✓ Model trained: Linear(1 -> 1)"
print "✓ Loss decreased from {history.losses[0]:.4f} to {history.get_final_loss():.4f}"
print "✓ Model learned the linear function!"
print ""
print "This demonstrates the network can:"
print "  - Learn linear relationships"
print "  - Converge to correct parameters"
print "  - Generalize to new inputs"
