# Autograd Example - Pure Simple Deep Learning
# Demonstrates automatic differentiation (backpropagation)

use lib.pure.autograd (
    Tensor, backward,
    tensor_add, tensor_mul, tensor_matmul, tensor_relu,
    tensor_sum, tensor_mean, tensor_mul_scalar
)

print "=== Pure Simple Autograd Demo ==="
print ""

# Example 1: Simple gradient computation
print "Example 1: Basic gradient (y = x * 2)"
val x1 = Tensor.from_data([3.0], [1], requires_grad: true)
val y1 = tensor_mul_scalar(x1, 2.0)
print "  x = {x1.value.data[0]}"
print "  y = x * 2 = {y1.value.data[0]}"

backward(y1)
print "  dy/dx = {x1.grad.?.data[0]}"
print "  Expected: 2.0"
print ""

# Example 2: Chain rule
print "Example 2: Chain rule (z = (x + y) * x)"
val x2 = Tensor.from_data([2.0], [1], requires_grad: true)
val y2 = Tensor.from_data([3.0], [1], requires_grad: true)
val sum2 = tensor_add(x2, y2)
val z2 = tensor_mul(sum2, x2)
print "  x = {x2.value.data[0]}, y = {y2.value.data[0]}"
print "  z = (x + y) * x = {z2.value.data[0]}"

backward(z2)
print "  dz/dx = {x2.grad.?.data[0]} (expected: 7.0)"
print "  dz/dy = {y2.grad.?.data[0]} (expected: 2.0)"
print ""

# Example 3: Matrix multiplication gradients
print "Example 3: Matrix multiplication"
val a = Tensor.from_data([1.0, 2.0, 3.0, 4.0], [2, 2], requires_grad: true)
val b = Tensor.from_data([5.0, 6.0, 7.0, 8.0], [2, 2], requires_grad: true)
val c = tensor_matmul(a, b)
print "  A = [[1, 2], [3, 4]]"
print "  B = [[5, 6], [7, 8]]"
print "  C = A @ B = {c.to_string()}"

backward(c)
print "  Gradients computed via backprop"
print "  dC/dA shape: {a.grad.?.shape}"
print "  dC/dB shape: {b.grad.?.shape}"
print ""

# Example 4: ReLU activation gradients
print "Example 4: ReLU activation"
val x4 = Tensor.from_data([-2.0, -1.0, 0.0, 1.0, 2.0], [5], requires_grad: true)
val y4 = tensor_relu(x4)
print "  Input: [-2, -1, 0, 1, 2]"
print "  ReLU output: {y4.value.to_string()}"

backward(y4)
print "  Gradients: {x4.grad.?.to_string()}"
print "  Expected: [0, 0, 0, 1, 1] (gradient only where x > 0)"
print ""

# Example 5: Mean loss (common in neural networks)
print "Example 5: Mean loss"
val predictions = Tensor.from_data([1.0, 2.0, 3.0, 4.0], [4], requires_grad: true)
val loss = tensor_mean(predictions)
print "  Predictions: [1, 2, 3, 4]"
print "  Mean loss: {loss.value.data[0]}"

backward(loss)
print "  Gradients: {predictions.grad.?.to_string()}"
print "  Expected: [0.25, 0.25, 0.25, 0.25] (1/N for each element)"
print ""

# Example 6: Simple neural network forward and backward
print "Example 6: Simple NN-like computation"
print "  Forward: h = ReLU(X @ W), loss = mean(h)"

val X = Tensor.from_data([1.0, 2.0], [1, 2], requires_grad: false)
val W = Tensor.from_data([0.5, 0.3], [2, 1], requires_grad: true)

# Forward pass
val z = tensor_matmul(X, W)
val h = tensor_relu(z)
val loss6 = tensor_mean(h)

print "  X = [1, 2]"
print "  W = [0.5, 0.3]"
print "  z = X @ W = {z.value.data[0]}"
print "  h = ReLU(z) = {h.value.data[0]}"
print "  loss = mean(h) = {loss6.value.data[0]}"

# Backward pass
backward(loss6)
print "  dLoss/dW = {W.grad.?.to_string()}"
print "  (This is how we'd update weights in training!)"
print ""

print "✓ Autograd system working!"
print "✓ Gradients computed via backpropagation"
print "✓ Ready for neural network training (Phase 3)"
