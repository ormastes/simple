#!/usr/bin/env simple
# PyTorch Tensor Demo
# Demonstrates pure Simple tensor operations
# Self-contained (no module imports due to runtime limitation)

# ============================================================================
# Tensor Infrastructure (inline)
# ============================================================================

class PureTensor:
    data: [f64]
    shape: [i64]
    strides: [i64]

    fn numel() -> i64:
        var total = 1
        for dim in self.shape:
            total = total * dim
        total

fn compute_strides(shape: [i64]) -> [i64]:
    var strides: [i64] = []
    var stride = 1
    var i = shape.len() - 1
    while i >= 0:
        strides.insert(0, stride)
        stride = stride * shape[i]
        i = i - 1
    strides

fn tensor_from_data(data: [f64], shape: [i64]) -> PureTensor:
    PureTensor(data: data, shape: shape, strides: compute_strides(shape))

fn tensor_zeros(shape: [i64]) -> PureTensor:
    var numel = 1
    for dim in shape:
        numel = numel * dim
    var data: [f64] = []
    var i = 0
    while i < numel:
        data.push(0.0)
        i = i + 1
    PureTensor(data: data, shape: shape, strides: compute_strides(shape))

fn tensor_ones(shape: [i64]) -> PureTensor:
    var numel = 1
    for dim in shape:
        numel = numel * dim
    var data: [f64] = []
    var i = 0
    while i < numel:
        data.push(1.0)
        i = i + 1
    PureTensor(data: data, shape: shape, strides: compute_strides(shape))

fn exp_approx(x: f64) -> f64:
    var result = 1.0
    var term = 1.0
    var i = 1
    while i < 20:
        term = term * x / i
        result = result + term
        i = i + 1
    result

# ============================================================================
# Tensor Class
# ============================================================================

class Tensor:
    pure_tensor: PureTensor

    fn ndim() -> i64:
        self.pure_tensor.shape.len()

    fn numel() -> i64:
        self.pure_tensor.numel()

    fn shape() -> [i64]:
        self.pure_tensor.shape

    fn add(other: Tensor) -> Tensor:
        var result_data: [f64] = []
        var i = 0
        while i < self.pure_tensor.data.len():
            result_data.push(self.pure_tensor.data[i] + other.pure_tensor.data[i])
            i = i + 1
        val pt = tensor_from_data(result_data, self.pure_tensor.shape)
        Tensor(pure_tensor: pt)

    fn mul(other: Tensor) -> Tensor:
        var result_data: [f64] = []
        var i = 0
        while i < self.pure_tensor.data.len():
            result_data.push(self.pure_tensor.data[i] * other.pure_tensor.data[i])
            i = i + 1
        val pt = tensor_from_data(result_data, self.pure_tensor.shape)
        Tensor(pure_tensor: pt)

    fn matmul(other: Tensor) -> Tensor:
        val M = self.pure_tensor.shape[0]
        val K = self.pure_tensor.shape[1]
        val N = other.pure_tensor.shape[1]
        var result_data: [f64] = []
        var i = 0
        while i < M:
            var j = 0
            while j < N:
                var sum = 0.0
                var k = 0
                while k < K:
                    val a_idx = i * K + k
                    val b_idx = k * N + j
                    sum = sum + self.pure_tensor.data[a_idx] * other.pure_tensor.data[b_idx]
                    k = k + 1
                result_data.push(sum)
                j = j + 1
            i = i + 1
        val pt = tensor_from_data(result_data, [M, N])
        Tensor(pure_tensor: pt)

    fn relu() -> Tensor:
        var result_data: [f64] = []
        for v in self.pure_tensor.data:
            result_data.push(if v > 0.0: v else: 0.0)
        val pt = tensor_from_data(result_data, self.pure_tensor.shape)
        Tensor(pure_tensor: pt)

    fn sigmoid() -> Tensor:
        var result_data: [f64] = []
        for v in self.pure_tensor.data:
            val exp_neg = exp_approx(-v)
            val sig = 1.0 / (1.0 + exp_neg)
            result_data.push(sig)
        val pt = tensor_from_data(result_data, self.pure_tensor.shape)
        Tensor(pure_tensor: pt)

# ============================================================================
# Factory Functions
# ============================================================================

fn torch_zeros(shape: [i64]) -> Tensor:
    val pt = tensor_zeros(shape)
    Tensor(pure_tensor: pt)

fn torch_ones(shape: [i64]) -> Tensor:
    val pt = tensor_ones(shape)
    Tensor(pure_tensor: pt)

fn torch_from_array(data: [f64], shape: [i64]) -> Tensor:
    val pt = tensor_from_data(data, shape)
    Tensor(pure_tensor: pt)

# ============================================================================
# Loss Functions
# ============================================================================

fn mse_loss(pred: PureTensor, target: PureTensor) -> f64:
    var diff_sq_sum = 0.0
    var i = 0
    while i < pred.data.len():
        val diff = pred.data[i] - target.data[i]
        diff_sq_sum = diff_sq_sum + diff * diff
        i = i + 1
    diff_sq_sum / (pred.data.len() * 1.0)

fn mae_loss(pred: PureTensor, target: PureTensor) -> f64:
    var abs_sum = 0.0
    var i = 0
    while i < pred.data.len():
        val diff = pred.data[i] - target.data[i]
        abs_sum = abs_sum + (if diff < 0.0: -diff else: diff)
        i = i + 1
    abs_sum / (pred.data.len() * 1.0)

# ============================================================================
# Linear Model
# ============================================================================

class LinearModel:
    w: f64
    b: f64

fn model_forward(model: LinearModel, x: PureTensor) -> PureTensor:
    var result: [f64] = []
    var i = 0
    while i < x.data.len():
        result.push(model.w * x.data[i] + model.b)
        i = i + 1
    tensor_from_data(result, x.shape)

fn compute_gradients(model: LinearModel, x: PureTensor, y: PureTensor) -> [f64]:
    val pred = model_forward(model, x)
    var grad_w = 0.0
    var grad_b = 0.0
    var i = 0
    while i < x.data.len():
        val error = pred.data[i] - y.data[i]
        grad_w = grad_w + 2.0 * error * x.data[i]
        grad_b = grad_b + 2.0 * error
        i = i + 1
    val n = x.data.len() * 1.0
    [grad_w / n, grad_b / n]

# ============================================================================
# Demo
# ============================================================================

print "=== PyTorch Demo ==="
print ""

print "Backend: pure"
print "Tensor library available: true"
print "Version: Pure Simple DL v1.0 (100% Simple, zero dependencies)"
print "CUDA available: false"
print ""

# Tensor creation
print "=== Tensor Creation ==="
val zeros = torch_zeros([2, 3])
print "Zeros [2, 3]: ndim={zeros.ndim()}, numel={zeros.numel()}"

val ones = torch_ones([3, 2])
print "Ones [3, 2]: ndim={ones.ndim()}, numel={ones.numel()}"

val from_data = torch_from_array([1.0, 2.0, 3.0, 4.0], [2, 2])
print "From array [2, 2]: ndim={from_data.ndim()}, numel={from_data.numel()}"
print ""

# Element-wise operations
print "=== Element-wise Operations ==="
val a = torch_ones([2, 2])
val b = torch_ones([2, 2])
val sum_t = a.add(b)
print "a + b: numel={sum_t.numel()}, data={sum_t.pure_tensor.data}"

val product = a.mul(b)
print "a * b: numel={product.numel()}, data={product.pure_tensor.data}"
print ""

# Matrix multiplication
print "=== Matrix Multiplication ==="
val mat_a = torch_ones([2, 3])
val mat_b = torch_ones([3, 2])
val mat_c = mat_a.matmul(mat_b)
print "A([2, 3]) @ B([3, 2]) = C: ndim={mat_c.ndim()}, data={mat_c.pure_tensor.data}"
print ""

# Activations
print "=== Activations ==="
val x = torch_from_array([2.0, -1.0, 0.5, -0.5], [4])
val relu_out = x.relu()
print "ReLU([2.0, -1.0, 0.5, -0.5]): {relu_out.pure_tensor.data}"

val sig_out = x.sigmoid()
print "Sigmoid: {sig_out.pure_tensor.data}"
print ""

# Chained operations
print "=== Chained Operations ==="
val x1 = torch_ones([3, 3])
val x2 = torch_ones([3, 3])
val added = x1.add(x2)
val after_relu = added.relu()
val result = after_relu.sigmoid()
print "Chained (add -> relu -> sigmoid): numel={result.numel()}"
print ""

# Loss functions
print "=== Loss Functions ==="
val pred_data = tensor_from_data([2.5, 0.0, 2.0, 8.0], [4])
val target_data = tensor_from_data([3.0, -0.5, 2.0, 7.0], [4])
val mse = mse_loss(pred_data, target_data)
val mae = mae_loss(pred_data, target_data)
print "Predictions: [2.5, 0.0, 2.0, 8.0]"
print "Targets:     [3.0, -0.5, 2.0, 7.0]"
print "MSE Loss: {mse}"
print "MAE Loss: {mae}"
print ""

# Training loop
print "=== Simple Linear Regression ==="
val train_x = tensor_from_data([1.0, 2.0, 3.0, 4.0], [4])
val train_y = tensor_from_data([2.0, 4.0, 6.0, 8.0], [4])
var model = LinearModel(w: 0.5, b: 0.1)
val lr = 0.01

var epoch = 0
while epoch < 5:
    val pred = model_forward(model, train_x)
    val loss = mse_loss(pred, train_y)
    val grads = compute_gradients(model, train_x, train_y)
    model = LinearModel(w: model.w - lr * grads[0], b: model.b - lr * grads[1])
    print "Epoch {epoch}: loss={loss}, w={model.w}, b={model.b}"
    epoch = epoch + 1
print ""

print "=== Demo Complete ==="
