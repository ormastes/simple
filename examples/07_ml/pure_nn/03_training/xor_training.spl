# XOR Problem - Complete Training Example
# Demonstrates the full Pure Simple Deep Learning pipeline

use std.pure.autograd.{Tensor}
use std.pure.nn.{Linear, ReLU, Sigmoid, Sequential, count_parameters}
use std.pure.training.{SGD, Adam, Trainer, mse_loss, accuracy}

print "=== XOR Problem - Pure Simple Deep Learning ==="
print ""

# The XOR Problem
# Classic machine learning problem: learn XOR function
# XOR truth table:
#   Input: (0,0) -> Output: 0
#   Input: (0,1) -> Output: 1
#   Input: (1,0) -> Output: 1
#   Input: (1,1) -> Output: 0

print "Building Neural Network..."
val model = Sequential.create([
    Linear.create(2, 4),      # 2 inputs -> 4 hidden units
    ReLU.create(),             # Non-linear activation
    Linear.create(4, 1),      # 4 hidden -> 1 output
    Sigmoid.create()          # Output in [0, 1]
])

print "Architecture:"
print model.to_string()
print "Total parameters: {count_parameters(model)}"
print ""

# Prepare training data
print "Preparing XOR training data..."
val train_data = [
    # (input, target)
    (Tensor.from_data([0.0, 0.0], [1, 2], requires_grad: false),
     Tensor.from_data([0.0], [1, 1], requires_grad: false)),

    (Tensor.from_data([0.0, 1.0], [1, 2], requires_grad: false),
     Tensor.from_data([1.0], [1, 1], requires_grad: false)),

    (Tensor.from_data([1.0, 0.0], [1, 2], requires_grad: false),
     Tensor.from_data([1.0], [1, 1], requires_grad: false)),

    (Tensor.from_data([1.0, 1.0], [1, 2], requires_grad: false),
     Tensor.from_data([0.0], [1, 1], requires_grad: false))
]

print "Training data: {train_data.len()} examples"
print "  [0, 0] -> 0"
print "  [0, 1] -> 1"
print "  [1, 0] -> 1"
print "  [1, 1] -> 0"
print ""

# Create optimizer
print "Creating SGD optimizer (lr=0.5)..."
val optimizer = SGD.create(model.parameters(), lr: 0.5, momentum: 0.9)
print optimizer.to_string()
print ""

# Create trainer
print "Creating trainer with MSE loss..."
val trainer = Trainer.create(model, optimizer, mse_loss)
print ""

# Train the model
print "Training for 100 epochs..."
print "(Note: With random initialization, results vary)"
print ""

trainer.fit(train_data, epochs: 100, verbose: true)

print ""
print "Training complete!"
print ""

# Evaluate the model
print "=== Evaluation ==="
print ""

print "Testing predictions:"
for (input, target) in train_data:
    val output = model.forward(input)
    val pred_val = output.value.data[0]
    val target_val = target.value.data[0]
    val input_str = "[{input.value.data[0]}, {input.value.data[1]}]"

    print "  Input: {input_str}"
    print "    Prediction: {pred_val:.4f}"
    print "    Target:     {target_val}"
    print "    Correct:    {if (pred_val > 0.5 and target_val == 1.0) or (pred_val <= 0.5 and target_val == 0.0): 'Yes' else: 'No'}"
    print ""

# Compute accuracy
val test_acc = trainer.evaluate(train_data)
print "Final accuracy: {test_acc:.2%}"
print ""

# Show training history
val history = trainer.get_history()
print "Training history:"
print "  Epochs trained: {history.epochs.len()}"
print "  Initial loss: {history.losses[0]:.4f}"
print "  Final loss: {history.get_final_loss():.4f}"
print "  Loss reduction: {(1.0 - history.get_final_loss() / history.losses[0]) * 100:.1f}%"
print ""

# Summary
print "=== Summary ==="
print "✓ XOR network built (2-4-1 architecture)"
print "✓ Trained with SGD optimizer"
print "✓ Loss decreased from {history.losses[0]:.4f} to {history.get_final_loss():.4f}"
print "✓ Final accuracy: {test_acc:.2%}"
print ""
print "Note: XOR is a non-linearly separable problem"
print "      Requires hidden layer with non-linear activation"
print "      This demonstrates the network can learn non-linear functions!"
print ""
print "✓ Complete training pipeline working!"
