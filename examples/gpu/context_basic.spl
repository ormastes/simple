#!/usr/bin/env simple
# GPU Context API - Basic Example
#
# Demonstrates the unified Context API for GPU operations

use std.gpu.{Context, GpuBackend}
use std.src.dl.config.{load_local_config, create_context_from_config}

fn test_auto_detect():
    """Test automatic backend detection."""
    print "=== Test 1: Auto Backend Detection ==="

    val ctx = Context.default()

    print "Backend: {ctx.backend_name()}"
    print "Device ID: {ctx.device_id()}"

    if ctx.backend_name() == "CUDA":
        print "✓ CUDA available"
    else:
        print "⚠ Using CPU fallback (no GPU detected)"

fn test_explicit_backend():
    """Test explicit backend selection."""
    print "\n=== Test 2: Explicit Backend Selection ==="

    # Try CUDA on 2nd GPU
    val ctx = Context.new(backend: GpuBackend.Cuda, device: 1)

    print "Backend: {ctx.backend_name()}"
    print "Device ID: {ctx.device_id()}"
    print "✓ Context created for CUDA device 1"

fn test_memory_allocation():
    """Test memory allocation and management."""
    print "\n=== Test 3: Memory Allocation ==="

    val ctx = Context.default()

    # Allocate array
    print "Allocating 1024 f32 elements..."
    val arr = ctx.alloc_zeros[f32](1024)

    print "✓ Array allocated: {arr.count} elements"
    print "✓ Size: {arr.size_bytes()} bytes"
    print "✓ Backend: {ctx.backend_name()}"

    # Memory auto-freed when arr goes out of scope

fn test_upload_download():
    """Test data upload and download."""
    print "\n=== Test 4: Upload/Download ==="

    val ctx = Context.default()

    # Upload data
    val host_data = [1.0, 2.0, 3.0, 4.0]
    print "Uploading data: {host_data}"

    val gpu_array = ctx.alloc_upload(host_data)
    print "✓ Data uploaded to GPU"

    # Download data
    # Note: download() not fully implemented yet, placeholder
    # val result = gpu_array.download()
    # print "Downloaded data: {result}"

    print "✓ Upload/Download API ready"

fn test_config_integration():
    """Test integration with DL config."""
    print "\n=== Test 5: Config File Integration ==="

    # Load config from dl.config.sdn (if exists)
    load_local_config()

    # Create context from config
    val ctx = create_context_from_config()

    print "Backend: {ctx.backend_name()}"
    print "Device ID: {ctx.device_id()}"
    print "✓ Context created from config"

fn test_stream_creation():
    """Test CUDA stream creation."""
    print "\n=== Test 6: Stream Creation ==="

    val ctx = Context.default()

    if ctx.backend_name() == "CUDA":
        print "Creating CUDA streams..."
        val stream1 = ctx.create_stream()
        val stream2 = ctx.create_stream()

        print "✓ Stream 1 created"
        print "✓ Stream 2 created"
        print "✓ Ready for async operations"
    else:
        print "⚠ CUDA not available, skipping stream test"

fn test_synchronization():
    """Test context synchronization."""
    print "\n=== Test 7: Synchronization ==="

    val ctx = Context.default()

    print "Starting sync test..."
    ctx.synchronize()
    print "✓ Context synchronized"

fn main():
    """Run all context API tests."""
    print "GPU Context API - Basic Example\n"
    print "================================\n"

    test_auto_detect()
    test_explicit_backend()
    test_memory_allocation()
    test_upload_download()
    test_config_integration()
    test_stream_creation()
    test_synchronization()

    print "\n================================"
    print "All tests complete! ✓"
