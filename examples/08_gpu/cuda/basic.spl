#!/usr/bin/env simple
# CUDA Basic Example - Native CUDA without PyTorch
#
# Demonstrates direct CUDA C API bindings via SFFI

use std.cuda.{cuda_available, cuda_device_count, cuda_get_device_name, cuda_get_device_memory, CudaStream, CudaEvent, CudaDeviceMem, cuda_set_device}

fn main():
    print "CUDA Direct FFI - Basic Example\n"
    print "================================\n"

    # Check CUDA availability
    if not cuda_available():
        print "❌ CUDA not available on this system"
        print "   (Make sure NVIDIA drivers and CUDA runtime are installed)"
        return

    print "✓ CUDA is available!\n"

    # Get device count
    val device_count = cuda_device_count()
    print "Devices found: {device_count}"

    # List all devices
    for device_id in 0..device_count:
        val name = cuda_get_device_name(device_id)
        val memory_bytes = cuda_get_device_memory(device_id)
        val memory_gb = (memory_bytes as f32) / (1024.0 * 1024.0 * 1024.0)
        print "  Device {device_id}: {name}"
        print "    Memory: {memory_gb} GB\n"

    if device_count == 0:
        print "No CUDA devices available"
        return

    # Select first device
    print "=== Using Device 0 ===\n"
    cuda_set_device(0)

    # Create stream
    print "Creating CUDA stream..."
    val stream = CudaStream.create()
    print "✓ Stream created\n"

    # Create events for timing
    print "Creating CUDA events for timing..."
    val start_event = CudaEvent.create()
    val end_event = CudaEvent.create()
    print "✓ Events created\n"

    # Allocate device memory
    print "Allocating device memory (1 MB)..."
    val mem_size = 1024 * 1024
    val device_mem = CudaDeviceMem.alloc(mem_size)

    if device_mem.is_null():
        print "❌ Memory allocation failed"
        return

    print "✓ Memory allocated\n"

    # Record start event
    print "Recording start event..."
    start_event.record(stream)

    # Fill memory with zeros
    print "Filling memory with zeros..."
    device_mem.memset(0, mem_size)

    # Record end event
    print "Recording end event..."
    end_event.record(stream)

    # Synchronize stream
    print "Synchronizing stream..."
    stream.synchronize()
    print "✓ Stream synchronized\n"

    # Calculate elapsed time
    val elapsed_ms = start_event.elapsed_time(end_event)
    print "Memset operation took: {elapsed_ms} ms\n"

    # Allocate second memory block
    print "Allocating second memory block (1 MB)..."
    val device_mem2 = CudaDeviceMem.alloc(mem_size)

    if device_mem2.is_null():
        print "❌ Second allocation failed"
        return

    print "✓ Second block allocated\n"

    # Copy device-to-device
    print "Copying from block 1 to block 2 (device-to-device)..."
    start_event.record(stream)
    device_mem.copy_to(device_mem2, mem_size)
    end_event.record(stream)
    stream.synchronize()

    val copy_elapsed_ms = start_event.elapsed_time(end_event)
    print "✓ Copy complete: {copy_elapsed_ms} ms\n"

    # Query stream status (non-blocking)
    print "Querying stream status (non-blocking)..."
    val is_complete = stream.query()
    if is_complete:
        print "✓ Stream is idle\n"
    else:
        print "⚠ Stream is still busy\n"

    # Memory automatically freed when device_mem and device_mem2 go out of scope
    # Stream and events automatically destroyed

    print "================================\n"
    print "All operations complete! ✓\n"
    print "Memory, streams, and events automatically cleaned up (RAII)"
