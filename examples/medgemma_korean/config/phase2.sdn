# Phase 2: MCQ Training (Medical Reasoning)
# Progressive LoRA: Merge LoRA_0 + LoRA_1, add LoRA_2

extends: "base.sdn"

training:
  epochs: 5
  max_samples: 2000
  learning_rate: 0.0001
  max_length: 1024  # Longer for MCQ with reasoning

data:
  type: "mcq"
  path: "example/medgemma_korean/data/mcq"
  format: "jsonl"

# Previous LoRAs to merge before training
previous_loras:
  - "example/medgemma_korean/models/phase0/lora_0"
  - "example/medgemma_korean/models/phase1/lora_1"

output:
  lora_path: "example/medgemma_korean/models/phase2/lora_2"
  merged_path: "example/medgemma_korean/models/phase2/merged"
  final_path: "example/medgemma_korean/models/final"

validation:
  test_plain_text: true  # Verify Phase 0 knowledge retained
  test_medical_dict: true  # Verify Phase 1 knowledge retained
  test_mcq: true
  target_accuracy: 0.90  # 90% target for KorMedMCQA

tags:
  - "phase2"
  - "mcq"
  - "medical_reasoning"
  - "progressive_lora"
