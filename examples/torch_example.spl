# PyTorch-like Neural Network Example in Simple
# Demonstrates the high-level torch API for building and training models

use lib.torch.mod.{
    Tensor,
    Linear,
    Conv2d,
    Sequential,
    SGD,
    Adam,
    MSELoss,
    CrossEntropyLoss,
    torch_available,
    torch_version,
    cuda_available
}

fn main():
    print "=== PyTorch-like API Example ==="
    print ""

    # Check backend availability
    if torch_available():
        print "PyTorch backend available: {torch_version()}"
        print "CUDA available: {cuda_available()}"
    else:
        print "PyTorch backend not available (using stubs)"
        return ()

    print ""

    # ========================================================================
    # Example 1: Basic Tensor Operations
    # ========================================================================
    print "Example 1: Basic Tensor Operations"
    print "-----------------------------------"

    val t1 = Tensor.zeros([3, 3])
    print "Created 3x3 zero tensor"
    print "  Shape: {t1.shape()}"
    print "  Dims: {t1.ndim()}"
    print "  Elements: {t1.numel()}"

    val t2 = Tensor.ones([3, 3])
    print "Created 3x3 ones tensor"

    val t3 = t1.add(t2)
    print "Added tensors (0 + 1 = 1)"

    val t4 = t3.mul(t2)
    print "Multiplied tensors (1 * 1 = 1)"

    val t5 = Tensor.randn([3, 3])
    print "Created random normal tensor"

    print ""

    # ========================================================================
    # Example 2: Activation Functions
    # ========================================================================
    print "Example 2: Activation Functions"
    print "--------------------------------"

    val x = Tensor.randn([2, 4])
    print "Input tensor: {x.shape()}"

    val relu_out = x.relu()
    print "After ReLU: {relu_out.shape()}"

    val sigmoid_out = x.sigmoid()
    print "After Sigmoid: {sigmoid_out.shape()}"

    val tanh_out = x.tanh()
    print "After Tanh: {tanh_out.shape()}"

    print ""

    # ========================================================================
    # Example 3: Linear Layer (Fully Connected)
    # ========================================================================
    print "Example 3: Linear Layer"
    print "-----------------------"

    val fc1 = Linear.create(784, 128)
    print "Created Linear layer: 784 -> 128"

    val input_batch = Tensor.randn([32, 784])
    print "Input batch: {input_batch.shape()}"

    val fc1_out = fc1.forward(input_batch)
    print "After fc1: {fc1_out.shape()}"

    val params = fc1.parameters()
    print "Layer has {params.len()} trainable parameters"

    print ""

    # ========================================================================
    # Example 4: Sequential Model (MLP)
    # ========================================================================
    print "Example 4: Multi-Layer Perceptron"
    print "----------------------------------"

    val model = Sequential.create()
    model.add_layer_linear(Linear.create(784, 256))
    model.add_layer_linear(Linear.create(256, 128))
    model.add_layer_linear(Linear.create(128, 10))
    print "Created 3-layer MLP: 784 -> 256 -> 128 -> 10"

    val mlp_input = Tensor.randn([32, 784])
    print "Input: {mlp_input.shape()}"

    val mlp_output = model.forward(mlp_input)
    print "Output: {mlp_output.shape()}"

    val all_params = model.parameters()
    print "Total parameters: {all_params.len()}"

    print ""

    # ========================================================================
    # Example 5: Loss Functions
    # ========================================================================
    print "Example 5: Loss Functions"
    print "-------------------------"

    val mse_criterion = MSELoss.create()
    val pred = Tensor.randn([32, 10])
    val target = Tensor.randn([32, 10])
    val mse_loss = mse_criterion.forward(pred, target)
    print "MSE Loss computed for {pred.shape()}"

    val ce_criterion = CrossEntropyLoss.create()
    val logits = Tensor.randn([32, 10])
    val labels = Tensor.zeros([32])
    val ce_loss = ce_criterion.forward(logits, labels)
    print "Cross-Entropy Loss computed for {logits.shape()}"

    print ""

    # ========================================================================
    # Example 6: Optimizers
    # ========================================================================
    print "Example 6: Optimizers"
    print "---------------------"

    val simple_model = Sequential.create()
    simple_model.add_layer_linear(Linear.create(10, 5))
    simple_model.add_layer_linear(Linear.create(5, 2))

    val model_params = simple_model.parameters()

    val sgd = SGD.create(model_params, 0, 0)
    print "Created SGD optimizer with lr=0.01, momentum=0.9"

    val adam = Adam.create(model_params, 0, 0, 0)
    print "Created Adam optimizer with lr=0.001"

    print "Optimizer step() and zero_grad() methods available"

    print ""

    # ========================================================================
    # Example 7: CUDA Operations
    # ========================================================================
    print "Example 7: Device Management"
    print "----------------------------"

    val cpu_tensor = Tensor.randn([100, 100])
    print "Created CPU tensor: {cpu_tensor.shape()}"
    print "Is CUDA: {cpu_tensor.is_cuda()}"

    if cuda_available():
        val gpu_tensor = cpu_tensor.cuda(0)
        print "Moved to CUDA device 0"
        print "Is CUDA: {gpu_tensor.is_cuda()}"

        val back_to_cpu = gpu_tensor.cpu()
        print "Moved back to CPU"
        print "Is CUDA: {back_to_cpu.is_cuda()}"
    else:
        print "CUDA not available, skipping GPU operations"

    print ""

    # ========================================================================
    # Example 8: Convolutional Layer
    # ========================================================================
    print "Example 8: Convolutional Neural Network"
    print "----------------------------------------"

    val conv1 = Conv2d.create(3, 64, 3, 1, 1)
    print "Created Conv2d: in=3, out=64, kernel=3x3, stride=1, padding=1"

    val image_batch = Tensor.randn([8, 3, 224, 224])
    print "Input images: {image_batch.shape()} (batch=8, channels=3, 224x224)"

    val conv_out = conv1.forward(image_batch)
    print "After Conv2d: {conv_out.shape()}"

    val conv_params = conv1.parameters()
    print "Conv layer parameters: {conv_params.len()}"

    print ""

    # ========================================================================
    # Example 9: Training Loop Structure (Placeholder)
    # ========================================================================
    print "Example 9: Training Loop Structure"
    print "-----------------------------------"

    val train_model = Sequential.create()
    train_model.add_layer_linear(Linear.create(10, 20))
    train_model.add_layer_linear(Linear.create(20, 2))

    val optimizer_train = SGD.create(train_model.parameters(), 0, 0)
    val criterion_train = MSELoss.create()

    print "Training setup:"
    print "  Model: 10 -> 20 -> 2"
    print "  Optimizer: SGD"
    print "  Loss: MSE"

    print ""
    print "Typical training loop:"
    print "  1. optimizer.zero_grad()  # Clear gradients"
    print "  2. output = model.forward(input)  # Forward pass"
    print "  3. loss = criterion.forward(output, target)  # Compute loss"
    print "  4. loss.backward()  # Backward pass (requires autograd FFI)"
    print "  5. optimizer.step()  # Update parameters"

    print ""
    print "=== All examples completed ==="

main()
