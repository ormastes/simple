# Run All Training Phases (Real CUDA)
#
# Complete progressive LoRA training pipeline on GPU:
#   Phase 0: Plain text (Korean fluency)
#   Phase 1: Medical dictionary (terminology)
#   Phase 2: MCQ (medical reasoning)
#
# Each phase adds a new LoRA adapter, trains it, then merges into
# base weights before the next phase. This prevents catastrophic forgetting.
#
# Usage:
#   LD_PRELOAD=build/libspl_torch.so bin/simple examples/projects/medgemma_korean/run_all.spl

use src.model.{TextModel, check_gpu}
use src.train_phase0.{run_phase0}
use src.train_phase1.{run_phase1}
use src.train_phase2.{run_phase2}
use src.validation.{validate_all_phases}

use std.gc_async_mut.torch.mod.{torch_available, cuda_available}
use std.torch.ffi.{
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_max_memory_allocated,
    rt_torch_cuda_empty_cache
}


fn print_header(title: text):
    print("")
    print("=" * 70)
    print("  {title}")
    print("=" * 70)
    print("")


fn main():
    print_header("MEDGEMMA KOREAN - PROGRESSIVE LORA TRAINING (REAL CUDA)")

    # Check prerequisites
    if not torch_available():
        print("ERROR: libtorch not available.")
        print("Build: bash scripts/build/build-torch-ffi.sh")
        print("Run:   LD_PRELOAD=build/libspl_torch.so bin/simple run_all.spl")
        return

    if not cuda_available():
        print("ERROR: CUDA not available. GPU required.")
        return

    check_gpu()
    print("")

    print("Pipeline: 3-phase progressive LoRA training")
    print("  Phase 0: Korean fluency (plain text features)")
    print("  Phase 1: Medical terminology (dictionary features)")
    print("  Phase 2: Medical reasoning (MCQ features)")
    print("")
    print("Each phase: freeze previous -> add LoRA -> train -> merge")
    print("This prevents catastrophic forgetting.")
    print("")

    # ========================================================================
    # Phase 0: Plain Text (Korean fluency)
    # ========================================================================
    var model = run_phase0()
    validate_all_phases(model, 0)

    # ========================================================================
    # Phase 1: Medical Dictionary
    # ========================================================================
    model = run_phase1(model)
    validate_all_phases(model, 1)

    # ========================================================================
    # Phase 2: MCQ (Medical Reasoning)
    # ========================================================================
    model = run_phase2(model)
    validate_all_phases(model, 2)

    # ========================================================================
    # Pipeline Complete
    # ========================================================================
    print_header("PIPELINE COMPLETE - ALL PHASES TRAINED ON CUDA")

    print("Final model capabilities (all merged into base weights):")
    print("  Phase 0: Korean language fluency")
    print("  Phase 1: Medical terminology")
    print("  Phase 2: Medical reasoning (MCQ)")
    print("")
    print("Progressive LoRA prevented catastrophic forgetting:")
    print("  - Each phase froze previous knowledge")
    print("  - Only new LoRA adapters were trained")
    print("  - LoRA merged into base weights after each phase")
    print("")

    val peak_mem = rt_torch_cuda_max_memory_allocated(0)
    val peak_mb = peak_mem / (1024 * 1024)
    print("Peak GPU memory: {peak_mb}MB")

    rt_torch_cuda_empty_cache()
    print("")
    print("Training complete.")


main()
