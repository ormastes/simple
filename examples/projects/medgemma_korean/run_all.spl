# Run All Training Phases (Real CUDA)
#
# MedGemma Korean progressive training pipeline.
# Matches Note.md phases 0-1 (foundation scope).
#
# Phase 0: Prepare English medical training data (baseline)
# Phase 1: Evaluate score, train with LoRA if below 97%
# Phase 2: MCQ training (medical reasoning)
#
# Each phase: freeze previous -> add LoRA -> train -> merge -> validate.
# 97% English retention threshold checked after each phase.
#
# Usage:
#   LD_PRELOAD=build/libspl_torch.so bin/simple examples/projects/medgemma_korean/run_all.spl

use src.model.{TextModel, check_gpu}
use src.shared_logic.{TrainConfig, report_gpu_memory}
use src.train_phase0.{run_phase0}
use src.train_phase1.{run_phase1}
use src.train_phase2.{run_phase2}
use src.validation.{validate_all_phases, check_english_retention}

use std.gc_async_mut.torch.mod.{torch_available, cuda_available}
use std.torch.ffi.{
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_max_memory_allocated,
    rt_torch_cuda_empty_cache
}


fn print_header(title: text):
    print("")
    print("=" * 70)
    print("  {title}")
    print("=" * 70)
    print("")


fn main():
    print_header("MEDGEMMA KOREAN - PROGRESSIVE LORA TRAINING (REAL CUDA)")

    # Check prerequisites
    if not torch_available():
        print("ERROR: libtorch not available.")
        print("Build: bash scripts/build/build-torch-ffi.sh")
        print("Run:   LD_PRELOAD=build/libspl_torch.so bin/simple run_all.spl")
        return

    if not cuda_available():
        print("ERROR: CUDA not available. GPU required.")
        return

    check_gpu()
    print("")

    print("Pipeline: Progressive LoRA training (Note.md Phases 0-1)")
    print("  Phase 0: English medical baseline (prevent forgetting)")
    print("  Phase 1: Evaluate + train English medical knowledge")
    print("  Phase 2: MCQ medical reasoning")
    print("")
    print("Each phase: freeze -> LoRA -> train -> merge -> validate")
    print("97% English retention threshold checked after each phase.")
    print("")

    # ========================================================================
    # Phase 0: English Medical Baseline
    # ========================================================================
    var model = run_phase0()
    validate_all_phases(model, 0)

    # ========================================================================
    # Phase 1: Evaluate + Train (97% threshold check)
    # ========================================================================
    model = run_phase1(model)
    validate_all_phases(model, 1)

    # ========================================================================
    # Phase 2: MCQ (Medical Reasoning)
    # ========================================================================
    model = run_phase2(model)
    validate_all_phases(model, 2)

    # ========================================================================
    # Pipeline Complete
    # ========================================================================
    print_header("PIPELINE COMPLETE - ALL PHASES TRAINED ON CUDA")

    print("Final model capabilities (all merged into base weights):")
    print("  Phase 0: English medical baseline")
    print("  Phase 1: English medical LoRA (if needed)")
    print("  Phase 2: Medical reasoning (MCQ)")
    print("")

    # Final English retention check
    val config = TrainConfig.default_config()
    val retained = check_english_retention(model, config)
    if retained:
        print("FINAL: English knowledge retained (>= 95%)")
    else:
        print("WARNING: English knowledge degraded below 95%!")
    print("")

    model.print_summary()
    report_gpu_memory()

    val peak_mem = rt_torch_cuda_max_memory_allocated(0)
    val peak_mb = peak_mem / (1024 * 1024)
    print("Peak GPU memory: {peak_mb}MB")

    rt_torch_cuda_empty_cache()
    print("")
    print("Training complete.")


main()
