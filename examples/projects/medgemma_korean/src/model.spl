# TextModel - Real Neural Network for MedGemma Korean Training
#
# A feed-forward language model that runs on CUDA via libtorch FFI.
# Used across all three progressive LoRA training phases.
#
# Architecture:
#   Input [batch, input_dim]
#   -> Linear(input_dim, hidden_dim) -> GELU
#   -> Linear(hidden_dim, hidden_dim) -> GELU
#   -> Linear(hidden_dim, output_dim)
#   -> Output [batch, output_dim]
#
# Requirements:
#   - libspl_torch.so must be built and loaded
#   - CUDA GPU required

use std.gc_async_mut.torch.mod.{
    Tensor,
    torch_available,
    cuda_available
}

use std.torch.ffi.{
    rt_torch_tensor_zeros,
    rt_torch_tensor_ones,
    rt_torch_tensor_randn,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_neg,
    rt_torch_torchtensor_matmul,
    rt_torch_torchtensor_relu,
    rt_torch_torchtensor_gelu,
    rt_torch_torchtensor_softmax,
    rt_torch_torchtensor_log_softmax,
    rt_torch_torchtensor_sum,
    rt_torch_torchtensor_sum_dim,
    rt_torch_torchtensor_mean,
    rt_torch_torchtensor_mean_dim,
    rt_torch_torchtensor_transpose,
    rt_torch_torchtensor_reshape,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_cpu,
    rt_torch_torchtensor_is_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_ndim,
    rt_torch_torchtensor_numel,
    rt_torch_torchtensor_shape,
    rt_torch_autograd_set_requires_grad,
    rt_torch_autograd_requires_grad,
    rt_torch_autograd_grad,
    rt_torch_autograd_backward,
    rt_torch_autograd_zero_grad,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end,
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_max_memory_allocated,
    rt_torch_cuda_empty_cache
}

export TextModel, compute_mse_loss, sgd_update_tensor, check_gpu

# ============================================================================
# GPU Check
# ============================================================================

fn check_gpu():
    if not torch_available():
        print("ERROR: libtorch not available. Build with:")
        print("  bash scripts/build/build-torch-ffi.sh")
        print("Run with:")
        print("  LD_PRELOAD=build/libspl_torch.so bin/simple <script>")
        return
    if not cuda_available():
        print("ERROR: CUDA not available. GPU required for training.")
        return
    print("GPU: CUDA available")
    val mem = rt_torch_cuda_memory_allocated(0)
    print("  Allocated: {mem} bytes")

# ============================================================================
# Utility: move tensor handle to CUDA
# ============================================================================

fn to_cuda(h: i64) -> i64:
    rt_torch_torchtensor_cuda(h, 0)

fn make_param(dims: [i64]) -> i64:
    # Create random weight on CUDA with requires_grad
    val h = rt_torch_tensor_randn(dims)
    val h_gpu = to_cuda(h)
    rt_torch_torchtensor_free(h)
    # Scale down for better initialization (Xavier-like)
    val scale = 0.1
    val h_scaled = rt_torch_torchtensor_mul_scalar(h_gpu, scale)
    rt_torch_torchtensor_free(h_gpu)
    rt_torch_autograd_set_requires_grad(h_scaled, true)
    h_scaled

fn make_zeros_param(dims: [i64]) -> i64:
    val h = rt_torch_tensor_zeros(dims)
    val h_gpu = to_cuda(h)
    rt_torch_torchtensor_free(h)
    rt_torch_autograd_set_requires_grad(h_gpu, true)
    h_gpu

# ============================================================================
# TextModel - 3-layer feed-forward network
# ============================================================================

class TextModel:
    # Layer 1 weights
    w1: i64
    b1: i64
    # Layer 2 weights
    w2: i64
    b2: i64
    # Output layer weights
    w3: i64
    b3: i64
    # Dimensions
    in_dim: i64
    hid_dim: i64
    out_dim: i64

    static fn create(in_dim: i64, hid_dim: i64, out_dim: i64) -> TextModel:
        print("Creating TextModel({in_dim} -> {hid_dim} -> {hid_dim} -> {out_dim}) on CUDA")
        val w1 = make_param([hid_dim, in_dim])
        val b1 = make_zeros_param([hid_dim])
        val w2 = make_param([hid_dim, hid_dim])
        val b2 = make_zeros_param([hid_dim])
        val w3 = make_param([out_dim, hid_dim])
        val b3 = make_zeros_param([out_dim])
        val total_params = (in_dim * hid_dim + hid_dim) + (hid_dim * hid_dim + hid_dim) + (hid_dim * out_dim + out_dim)
        print("  Total parameters: {total_params}")
        TextModel(
            w1: w1, b1: b1,
            w2: w2, b2: b2,
            w3: w3, b3: b3,
            in_dim: in_dim, hid_dim: hid_dim, out_dim: out_dim
        )

    fn forward(x: i64) -> i64:
        # Layer 1: x @ W1^T + b1 -> GELU
        val w1t = rt_torch_torchtensor_transpose(self.w1, 0, 1)
        val xw1 = rt_torch_torchtensor_matmul(x, w1t)
        rt_torch_torchtensor_free(w1t)
        val h1_pre = rt_torch_torchtensor_add(xw1, self.b1)
        rt_torch_torchtensor_free(xw1)
        val h1 = rt_torch_torchtensor_gelu(h1_pre)
        rt_torch_torchtensor_free(h1_pre)

        # Layer 2: h1 @ W2^T + b2 -> GELU
        val w2t = rt_torch_torchtensor_transpose(self.w2, 0, 1)
        val hw2 = rt_torch_torchtensor_matmul(h1, w2t)
        rt_torch_torchtensor_free(w2t)
        rt_torch_torchtensor_free(h1)
        val h2_pre = rt_torch_torchtensor_add(hw2, self.b2)
        rt_torch_torchtensor_free(hw2)
        val h2 = rt_torch_torchtensor_gelu(h2_pre)
        rt_torch_torchtensor_free(h2_pre)

        # Output layer: h2 @ W3^T + b3
        val w3t = rt_torch_torchtensor_transpose(self.w3, 0, 1)
        val hw3 = rt_torch_torchtensor_matmul(h2, w3t)
        rt_torch_torchtensor_free(w3t)
        rt_torch_torchtensor_free(h2)
        val out = rt_torch_torchtensor_add(hw3, self.b3)
        rt_torch_torchtensor_free(hw3)
        out

    fn param_handles() -> [i64]:
        [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]

    me sgd_step(lr: f64):
        # Update all parameters with SGD
        rt_torch_autograd_no_grad_begin()
        self.w1 = sgd_update_tensor(self.w1, lr)
        self.b1 = sgd_update_tensor(self.b1, lr)
        self.w2 = sgd_update_tensor(self.w2, lr)
        self.b2 = sgd_update_tensor(self.b2, lr)
        self.w3 = sgd_update_tensor(self.w3, lr)
        self.b3 = sgd_update_tensor(self.b3, lr)
        rt_torch_autograd_no_grad_end()
        # Re-enable grad tracking
        rt_torch_autograd_set_requires_grad(self.w1, true)
        rt_torch_autograd_set_requires_grad(self.b1, true)
        rt_torch_autograd_set_requires_grad(self.w2, true)
        rt_torch_autograd_set_requires_grad(self.b2, true)
        rt_torch_autograd_set_requires_grad(self.w3, true)
        rt_torch_autograd_set_requires_grad(self.b3, true)

    fn zero_grads():
        rt_torch_autograd_zero_grad(self.w1)
        rt_torch_autograd_zero_grad(self.b1)
        rt_torch_autograd_zero_grad(self.w2)
        rt_torch_autograd_zero_grad(self.b2)
        rt_torch_autograd_zero_grad(self.w3)
        rt_torch_autograd_zero_grad(self.b3)

    me freeze():
        # Freeze all parameters (no gradient)
        rt_torch_autograd_set_requires_grad(self.w1, false)
        rt_torch_autograd_set_requires_grad(self.b1, false)
        rt_torch_autograd_set_requires_grad(self.w2, false)
        rt_torch_autograd_set_requires_grad(self.b2, false)
        rt_torch_autograd_set_requires_grad(self.w3, false)
        rt_torch_autograd_set_requires_grad(self.b3, false)

    me unfreeze():
        rt_torch_autograd_set_requires_grad(self.w1, true)
        rt_torch_autograd_set_requires_grad(self.b1, true)
        rt_torch_autograd_set_requires_grad(self.w2, true)
        rt_torch_autograd_set_requires_grad(self.b2, true)
        rt_torch_autograd_set_requires_grad(self.w3, true)
        rt_torch_autograd_set_requires_grad(self.b3, true)


# ============================================================================
# Loss: Differentiable MSE (returns tensor handle for backward)
# ============================================================================

fn compute_mse_loss(pred: i64, target: i64) -> i64:
    # MSE = mean((pred - target)^2)
    # All ops create tensors in the computation graph
    val diff = rt_torch_torchtensor_sub(pred, target)
    val sq = rt_torch_torchtensor_mul(diff, diff)
    rt_torch_torchtensor_free(diff)
    # Sum over feature dim -> [batch]
    val per_sample = rt_torch_torchtensor_sum_dim(sq, 1, false)
    rt_torch_torchtensor_free(sq)
    # Mean over batch -> scalar tensor
    val loss = rt_torch_torchtensor_mean_dim(per_sample, 0, false)
    rt_torch_torchtensor_free(per_sample)
    loss


# ============================================================================
# SGD: Update a single tensor parameter
# ============================================================================

fn sgd_update_tensor(param_h: i64, lr: f64) -> i64:
    # param = param - lr * grad
    val grad_h = rt_torch_autograd_grad(param_h)
    if grad_h == 0:
        # No gradient, return unchanged (clone to get new handle)
        return rt_torch_torchtensor_clone(param_h)
    val scaled = rt_torch_torchtensor_mul_scalar(grad_h, lr)
    val updated = rt_torch_torchtensor_sub(param_h, scaled)
    rt_torch_torchtensor_free(scaled)
    rt_torch_torchtensor_free(param_h)
    updated
