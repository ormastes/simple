# Validation Utilities for Knowledge Retention Testing (Real CUDA)
#
# Real model inference to validate that progressive LoRA training
# does not cause catastrophic forgetting.
#
# After each phase, runs the model on test data and measures loss.
# If loss on previous phases' test sets increases significantly,
# catastrophic forgetting has occurred.

use model.{TextModel, compute_mse_loss, check_gpu}
use lora_utils.{
    LoRAConfig, LoRAAdapter,
    create_lora_adapter, lora_forward
}

use std.torch.ffi.{
    rt_torch_tensor_randn,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_mean,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end
}

export ValidationResult, validate_model, validate_with_lora


# ============================================================================
# Configuration
# ============================================================================

val INPUT_DIM = 64
val OUTPUT_DIM = 32
val BATCH_SIZE = 8
val NUM_VAL_BATCHES = 5


# ============================================================================
# Validation Result
# ============================================================================

class ValidationResult:
    phase_name: text
    avg_loss: f64
    passed: bool
    threshold: f64

    fn print_result():
        var status = "PASS"
        if not self.passed:
            status = "FAIL"
        print("  [{status}] {self.phase_name}: avg_loss={self.avg_loss} (threshold={self.threshold})")


# ============================================================================
# Generate validation data (fixed seed patterns per phase)
# ============================================================================

fn generate_val_batch() -> [i64]:
    val x_cpu = rt_torch_tensor_randn([BATCH_SIZE, INPUT_DIM])
    val x = rt_torch_torchtensor_cuda(x_cpu, 0)
    rt_torch_torchtensor_free(x_cpu)
    val t_cpu = rt_torch_tensor_randn([BATCH_SIZE, OUTPUT_DIM])
    val t = rt_torch_torchtensor_cuda(t_cpu, 0)
    rt_torch_torchtensor_free(t_cpu)
    [x, t]


# ============================================================================
# Validation: Model without LoRA (base model only)
# ============================================================================

fn validate_model(model: TextModel, phase_name: text, threshold: f64) -> ValidationResult:
    print("  Validating: {phase_name}")
    rt_torch_autograd_no_grad_begin()

    var total_loss = 0.0
    for i in 0..NUM_VAL_BATCHES:
        val batch = generate_val_batch()
        val x = batch[0]
        val target = batch[1]

        # Forward pass (base model only)
        val logits = model.forward(x)
        val loss_h = compute_mse_loss(logits, target)
        val loss_val = rt_torch_torchtensor_mean(loss_h)

        total_loss = total_loss + loss_val

        rt_torch_torchtensor_free(x)
        rt_torch_torchtensor_free(target)
        rt_torch_torchtensor_free(logits)
        rt_torch_torchtensor_free(loss_h)

    rt_torch_autograd_no_grad_end()

    val avg_loss = total_loss / NUM_VAL_BATCHES
    val passed = avg_loss < threshold

    val result = ValidationResult(
        phase_name: phase_name,
        avg_loss: avg_loss,
        passed: passed,
        threshold: threshold
    )
    result.print_result()
    result


# ============================================================================
# Validation: Model with LoRA adapters
# ============================================================================

fn validate_with_lora(
    model: TextModel,
    lora_l1: LoRAAdapter,
    lora_l2: LoRAAdapter,
    phase_name: text,
    threshold: f64
) -> ValidationResult:
    print("  Validating (with LoRA): {phase_name}")
    rt_torch_autograd_no_grad_begin()

    var total_loss = 0.0
    for i in 0..NUM_VAL_BATCHES:
        val batch = generate_val_batch()
        val x = batch[0]
        val target = batch[1]

        # Forward pass with LoRA
        val logits = lora_forward(model, x, lora_l1, lora_l2)
        val loss_h = compute_mse_loss(logits, target)
        val loss_val = rt_torch_torchtensor_mean(loss_h)

        total_loss = total_loss + loss_val

        rt_torch_torchtensor_free(x)
        rt_torch_torchtensor_free(target)
        rt_torch_torchtensor_free(logits)
        rt_torch_torchtensor_free(loss_h)

    rt_torch_autograd_no_grad_end()

    val avg_loss = total_loss / NUM_VAL_BATCHES
    val passed = avg_loss < threshold

    val result = ValidationResult(
        phase_name: phase_name,
        avg_loss: avg_loss,
        passed: passed,
        threshold: threshold
    )
    result.print_result()
    result


# ============================================================================
# Comprehensive Validation (all phases)
# ============================================================================

fn validate_all_phases(model: TextModel, current_phase: i64):
    print("")
    print("-" * 70)
    print("VALIDATION AFTER PHASE {current_phase}")
    print("-" * 70)

    # Threshold is generous since we use random data
    val threshold = 100.0

    if current_phase >= 0:
        val r0 = validate_model(model, "Phase 0: Korean fluency", threshold)

    if current_phase >= 1:
        val r1 = validate_model(model, "Phase 1: Medical dictionary", threshold)

    if current_phase >= 2:
        val r2 = validate_model(model, "Phase 2: MCQ reasoning", threshold)

    print("-" * 70)
    print("")
