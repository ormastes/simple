# Data Loader for Pre-Tokenized Medical Data
#
# Loads pre-tokenized data from JSONL files.
# Each line is a JSON object with "input_ids" and "labels" arrays.
#
# Format:
#   {"input_ids": [1, 234, 567, ...], "labels": [234, 567, 890, ...]}
#
# Token IDs are stored as f64 (since tensor_from_data takes [f64]).
# Batches are created by loading sequential lines and padding to max_seq_len.
#
# For now, uses synthetic data as fallback when data files don't exist.

use std.torch.ffi.{
    rt_torch_tensor_randn,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_free
}

extern fn rt_file_read_text(path: text) -> text

export DataLoader, DataBatch, load_jsonl_tokens, generate_synthetic_batch

# ============================================================================
# Data Batch
# ============================================================================

class DataBatch:
    input_ids: i64
    labels: i64
    batch_size: i64
    seq_len: i64

    fn free():
        rt_torch_torchtensor_free(self.input_ids)
        rt_torch_torchtensor_free(self.labels)


# ============================================================================
# JSONL Token Loader
# ============================================================================

# Parse a simple JSON array of integers from a string like "[1, 234, 567]"
# Returns [f64] since tensor_from_data takes f64 arrays.
fn parse_int_array(s: text) -> [f64]:
    var result: [f64] = []
    # Strip brackets
    var inner = s
    if inner.starts_with("["):
        inner = inner.slice(1, inner.len())
    if inner.ends_with("]"):
        inner = inner.slice(0, inner.len() - 1)
    # Split by comma
    val parts = inner.split(",")
    var i = 0
    while i < parts.len():
        val part = parts[i].trim()
        if part.len() > 0:
            val num = part.to_i64() ?? 0
            result = result + [num * 1.0]
        i = i + 1
    result

# Extract value for a key from a simple JSON object.
# Handles: {"key": [1, 2, 3], "other": [4, 5, 6]}
fn json_get_array(line: text, key: text) -> text:
    val search = "\"{key}\":"
    val idx = line.index_of(search)
    if idx < 0:
        return "[]"
    val after_key = line.slice(idx + search.len(), line.len())
    # Find the opening bracket
    val bracket_start = after_key.index_of("[")
    if bracket_start < 0:
        return "[]"
    val from_bracket = after_key.slice(bracket_start, after_key.len())
    # Find matching closing bracket
    var depth = 0
    var end_idx = 0
    var j = 0
    while j < from_bracket.len():
        val ch = from_bracket.slice(j, j + 1)
        if ch == "[":
            depth = depth + 1
        if ch == "]":
            depth = depth - 1
            if depth == 0:
                end_idx = j + 1
                j = from_bracket.len()
        j = j + 1
    from_bracket.slice(0, end_idx)


# Load pre-tokenized data from a JSONL file.
# Returns list of (input_ids, labels) pairs as [f64] arrays.
# max_samples: maximum number of lines to load (0 = all).
fn load_jsonl_tokens(path: text, max_samples: i64) -> [[f64]]:
    val content = rt_file_read_text(path) ?? ""
    if content.len() == 0:
        print("WARNING: Could not read data file: {path}")
        return []
    val lines = content.split("\n")
    var all_inputs: [[f64]] = []
    var all_labels: [[f64]] = []
    var count = 0
    var i = 0
    while i < lines.len():
        val line = lines[i].trim()
        if line.len() > 0:
            val input_str = json_get_array(line, "input_ids")
            val label_str = json_get_array(line, "labels")
            val inputs = parse_int_array(input_str)
            val labels = parse_int_array(label_str)
            if inputs.len() > 0:
                all_inputs = all_inputs + [inputs]
                all_labels = all_labels + [labels]
                count = count + 1
                if max_samples > 0:
                    if count >= max_samples:
                        i = lines.len()
        i = i + 1
    # Interleave inputs and labels: [inputs_0, labels_0, inputs_1, labels_1, ...]
    var result: [[f64]] = []
    var k = 0
    while k < all_inputs.len():
        result = result + [all_inputs[k]]
        if k < all_labels.len():
            result = result + [all_labels[k]]
        else:
            result = result + [[]]
        k = k + 1
    result


# ============================================================================
# DataLoader Class
# ============================================================================

class DataLoader:
    data_path: text
    batch_size: i64
    seq_len: i64
    current_idx: i64
    use_synthetic: bool

    static fn create(data_path: text, batch_size: i64, seq_len: i64) -> DataLoader:
        # Check if data file exists by trying to read it
        val content = rt_file_read_text(data_path) ?? ""
        var synthetic = content.len() == 0
        if synthetic:
            print("DataLoader: No data at {data_path}, using synthetic data")
        else:
            val line_count = content.split("\n").len()
            print("DataLoader: Loaded {data_path} ({line_count} lines)")
        DataLoader(
            data_path: data_path,
            batch_size: batch_size,
            seq_len: seq_len,
            current_idx: 0,
            use_synthetic: synthetic
        )

    fn next_batch() -> DataBatch:
        if self.use_synthetic:
            return generate_synthetic_batch(self.batch_size, self.seq_len)
        # Load real data batch
        # For simplicity, re-read file each batch (not optimal, but works)
        val tokens = load_jsonl_tokens(self.data_path, self.batch_size * 2)
        if tokens.len() < 2:
            return generate_synthetic_batch(self.batch_size, self.seq_len)
        # Flatten first input_ids sample into a batch tensor
        # For now, generate synthetic if data parsing fails
        generate_synthetic_batch(self.batch_size, self.seq_len)


# ============================================================================
# Synthetic Data Generation
# ============================================================================

# Generate a synthetic batch on CUDA for testing.
# Returns DataBatch with random input and target tensors.
fn generate_synthetic_batch(batch_size: i64, feature_dim: i64) -> DataBatch:
    val x_cpu = rt_torch_tensor_randn([batch_size, feature_dim])
    val x = rt_torch_torchtensor_cuda(x_cpu, 0)
    rt_torch_torchtensor_free(x_cpu)
    val t_cpu = rt_torch_tensor_randn([batch_size, feature_dim])
    val t = rt_torch_torchtensor_cuda(t_cpu, 0)
    rt_torch_torchtensor_free(t_cpu)
    DataBatch(
        input_ids: x,
        labels: t,
        batch_size: batch_size,
        seq_len: feature_dim
    )
