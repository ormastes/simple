#!/usr/bin/env simple
# Phase 1: Medical Dictionary Training
#
# Goal: Learn Korean-English medical terminology features
#
# Progressive LoRA (real CUDA):
#   1. Start with Phase 0 model (LoRA_0 already merged)
#   2. Freeze base weights (Phase 0 knowledge preserved)
#   3. Add new LoRA_1 (only this is trainable)
#   4. Train on medical dictionary features
#
# Run:
#   LD_PRELOAD=build/libspl_torch.so bin/simple examples/projects/medgemma_korean/src/train_phase1.spl

use model.{TextModel, compute_mse_loss, check_gpu}
use lora_utils.{
    LoRAConfig, LoRAAdapter,
    create_lora_adapter, lora_forward,
    lora_sgd_step, lora_zero_grads,
    merge_lora_into_model
}
use train_phase0.{run_phase0}

use std.gc_async_mut.torch.mod.{torch_available, cuda_available}
use std.torch.ffi.{
    rt_torch_tensor_randn,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_mean,
    rt_torch_autograd_backward,
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_empty_cache
}

export run_phase1


# ============================================================================
# Configuration
# ============================================================================

val INPUT_DIM = 64
val HIDDEN_DIM = 128
val OUTPUT_DIM = 32
val BATCH_SIZE = 16
val NUM_BATCHES = 20
val NUM_EPOCHS = 3
val LEARNING_RATE = 0.0005
val LORA_RANK = 8
val LORA_ALPHA = 16.0


# ============================================================================
# Medical Dictionary Data (synthetic features on CUDA)
# ============================================================================

fn generate_medical_batch() -> [i64]:
    # Medical dictionary data: synthetic features representing medical terminology
    # In production, these would be pre-embedded medical term features
    val x_cpu = rt_torch_tensor_randn([BATCH_SIZE, INPUT_DIM])
    val x = rt_torch_torchtensor_cuda(x_cpu, 0)
    rt_torch_torchtensor_free(x_cpu)
    val t_cpu = rt_torch_tensor_randn([BATCH_SIZE, OUTPUT_DIM])
    val t = rt_torch_torchtensor_cuda(t_cpu, 0)
    rt_torch_torchtensor_free(t_cpu)
    [x, t]


# ============================================================================
# Training
# ============================================================================

fn run_phase1(model: TextModel) -> TextModel:
    print("")
    print("=" * 70)
    print("  PHASE 1: MEDICAL DICTIONARY TRAINING (REAL CUDA)")
    print("=" * 70)
    print("")

    check_gpu()
    print("")

    # Model already has Phase 0 knowledge merged in
    # Freeze base weights (preserves Phase 0 knowledge)
    model.freeze()
    print("Base model weights: FROZEN (Phase 0 knowledge preserved)")

    # Add LoRA_1 adapters
    val lora_config = LoRAConfig(rank: LORA_RANK, alpha: LORA_ALPHA, dropout: 0.0)
    print("")
    print("Adding LoRA_1 adapters (Progressive LoRA):")
    var lora_l1 = create_lora_adapter(INPUT_DIM, HIDDEN_DIM, lora_config)
    var lora_l2 = create_lora_adapter(HIDDEN_DIM, HIDDEN_DIM, lora_config)
    print("")

    print("Training config:")
    print("  Epochs: {NUM_EPOCHS}")
    print("  Batches per epoch: {NUM_BATCHES}")
    print("  Batch size: {BATCH_SIZE}")
    print("  Learning rate: {LEARNING_RATE}")
    print("  LoRA rank: {LORA_RANK}")
    print("")

    # Training loop
    var global_step = 0
    for epoch in 0..NUM_EPOCHS:
        var epoch_loss = 0.0
        print("--- Epoch {epoch + 1}/{NUM_EPOCHS} ---")

        for batch_idx in 0..NUM_BATCHES:
            val batch = generate_medical_batch()
            val x = batch[0]
            val target = batch[1]

            # Forward pass with LoRA_1
            val logits = lora_forward(model, x, lora_l1, lora_l2)

            # MSE loss
            val loss_h = compute_mse_loss(logits, target)
            val loss_val = rt_torch_torchtensor_mean(loss_h)

            # Backward
            rt_torch_autograd_backward(loss_h)

            # SGD (only LoRA_1 params, base is frozen)
            lora_sgd_step(lora_l1, lora_l2, LEARNING_RATE)

            # Zero grads
            lora_zero_grads(lora_l1, lora_l2)

            # Cleanup
            rt_torch_torchtensor_free(x)
            rt_torch_torchtensor_free(target)
            rt_torch_torchtensor_free(logits)
            rt_torch_torchtensor_free(loss_h)

            epoch_loss = epoch_loss + loss_val
            global_step = global_step + 1

            if (batch_idx + 1) % 5 == 0:
                val avg = epoch_loss / (batch_idx + 1)
                print("  Step {global_step}: loss={avg}")

        val avg_epoch_loss = epoch_loss / NUM_BATCHES
        val mem = rt_torch_cuda_memory_allocated(0)
        val mem_mb = mem / (1024 * 1024)
        print("Epoch {epoch + 1} complete: avg_loss={avg_epoch_loss}, GPU_mem={mem_mb}MB")
        print("")

    print("=" * 70)
    print("PHASE 1 TRAINING COMPLETE")
    print("  Base: Phase 0 knowledge (FROZEN)")
    print("  LoRA_1: Medical dictionary (TRAINED)")
    print("  No catastrophic forgetting - Phase 0 knowledge intact")
    print("=" * 70)

    # Merge LoRA_1 into base weights
    merge_lora_into_model(model, lora_l1, lora_l2)

    model


# ============================================================================
# Standalone entry point
# ============================================================================

fn main():
    if not torch_available():
        print("ERROR: libtorch not loaded. Run with:")
        print("  LD_PRELOAD=build/libspl_torch.so bin/simple <this_file>")
        return

    # Run Phase 0 first, then Phase 1
    print("Running Phase 0 first (prerequisite)...")
    var model = run_phase0()
    print("")
    val model_1 = run_phase1(model)
    print("")
    print("Phase 1 complete. Model has Phase 0 + Phase 1 knowledge.")
    rt_torch_cuda_empty_cache()


main()
