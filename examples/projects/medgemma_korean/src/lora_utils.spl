# LoRA Utilities for Progressive Training (Real Implementation)
#
# Real LoRA (Low-Rank Adaptation) using libtorch tensors on CUDA.
#
# LoRA adds trainable low-rank matrices A and B to frozen base weights:
#   output = x @ W^T + (x @ A^T) @ B^T * (alpha / rank)
#
# Progressive LoRA prevents catastrophic forgetting by:
# 1. Merging previous LoRA (W = W + B @ A * scale) to freeze knowledge
# 2. Adding new LoRA (only new A, B are trainable)

use model.{TextModel, sgd_update_tensor}

use std.torch.ffi.{
    rt_torch_tensor_zeros,
    rt_torch_tensor_randn,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_matmul,
    rt_torch_torchtensor_transpose,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_numel,
    rt_torch_torchtensor_shape,
    rt_torch_autograd_set_requires_grad,
    rt_torch_autograd_grad,
    rt_torch_autograd_zero_grad,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end
}

export LoRAConfig, LoRAAdapter, create_lora_adapter, merge_lora_into_model
export lora_forward, lora_sgd_step, lora_zero_grads


# ============================================================================
# LoRA Configuration
# ============================================================================

class LoRAConfig:
    rank: i64
    alpha: f64
    dropout: f64

    static fn default_config() -> LoRAConfig:
        LoRAConfig(rank: 8, alpha: 16.0, dropout: 0.0)

    fn scale() -> f64:
        self.alpha / self.rank


# ============================================================================
# LoRA Adapter - Trainable A, B matrices for one weight
# ============================================================================

class LoRAAdapter:
    # A: [rank, in_features] - initialized with small random values
    lora_a: i64
    # B: [out_features, rank] - initialized to zeros (so LoRA starts as identity)
    lora_b: i64
    # Scaling factor: alpha / rank
    lora_scale: f64
    rank: i64
    in_features: i64
    out_features: i64

    fn forward(x: i64) -> i64:
        # LoRA output: (x @ A^T) @ B^T * scale
        # x: [batch, in_features]
        # A^T: [in_features, rank]
        # B^T: [rank, out_features]
        val at = rt_torch_torchtensor_transpose(self.lora_a, 0, 1)
        val xa = rt_torch_torchtensor_matmul(x, at)
        rt_torch_torchtensor_free(at)
        val bt = rt_torch_torchtensor_transpose(self.lora_b, 0, 1)
        val xab = rt_torch_torchtensor_matmul(xa, bt)
        rt_torch_torchtensor_free(bt)
        rt_torch_torchtensor_free(xa)
        val scaled = rt_torch_torchtensor_mul_scalar(xab, self.lora_scale)
        rt_torch_torchtensor_free(xab)
        scaled

    fn param_handles() -> [i64]:
        [self.lora_a, self.lora_b]

    me sgd_step(lr: f64):
        rt_torch_autograd_no_grad_begin()
        self.lora_a = sgd_update_tensor(self.lora_a, lr)
        self.lora_b = sgd_update_tensor(self.lora_b, lr)
        rt_torch_autograd_no_grad_end()
        rt_torch_autograd_set_requires_grad(self.lora_a, true)
        rt_torch_autograd_set_requires_grad(self.lora_b, true)

    fn zero_grads():
        rt_torch_autograd_zero_grad(self.lora_a)
        rt_torch_autograd_zero_grad(self.lora_b)


# ============================================================================
# LoRA Creation
# ============================================================================

fn create_lora_adapter(in_features: i64, out_features: i64, config: LoRAConfig) -> LoRAAdapter:
    print("  Creating LoRA adapter: [{config.rank}, {in_features}] + [{out_features}, {config.rank}]")
    print("    Scale: {config.scale()}, Rank: {config.rank}")

    # A: small random init (Kaiming-like)
    val a_cpu = rt_torch_tensor_randn([config.rank, in_features])
    val a_gpu = rt_torch_torchtensor_cuda(a_cpu, 0)
    rt_torch_torchtensor_free(a_cpu)
    val a_init_scale = 0.01
    val a_scaled = rt_torch_torchtensor_mul_scalar(a_gpu, a_init_scale)
    rt_torch_torchtensor_free(a_gpu)
    rt_torch_autograd_set_requires_grad(a_scaled, true)

    # B: zero init (LoRA starts as no change)
    val b_cpu = rt_torch_tensor_zeros([out_features, config.rank])
    val b_gpu = rt_torch_torchtensor_cuda(b_cpu, 0)
    rt_torch_torchtensor_free(b_cpu)
    rt_torch_autograd_set_requires_grad(b_gpu, true)

    val trainable_params = config.rank * in_features + out_features * config.rank
    print("    Trainable params: {trainable_params}")

    LoRAAdapter(
        lora_a: a_scaled,
        lora_b: b_gpu,
        lora_scale: config.scale(),
        rank: config.rank,
        in_features: in_features,
        out_features: out_features
    )


# ============================================================================
# Full LoRA-augmented forward pass
# ============================================================================

fn lora_forward(model: TextModel, x: i64, lora_l1: LoRAAdapter, lora_l2: LoRAAdapter) -> i64:
    # Layer 1 with LoRA: (x @ W1^T + b1) + lora_l1(x)  -> GELU
    val w1t = rt_torch_torchtensor_transpose(model.w1, 0, 1)
    val xw1 = rt_torch_torchtensor_matmul(x, w1t)
    rt_torch_torchtensor_free(w1t)
    val h1_base = rt_torch_torchtensor_add(xw1, model.b1)
    rt_torch_torchtensor_free(xw1)
    val lora1_out = lora_l1.forward(x)
    val h1_with_lora = rt_torch_torchtensor_add(h1_base, lora1_out)
    rt_torch_torchtensor_free(h1_base)
    rt_torch_torchtensor_free(lora1_out)
    val h1 = rt_torch_torchtensor_gelu(h1_with_lora)
    rt_torch_torchtensor_free(h1_with_lora)

    # Layer 2 with LoRA: (h1 @ W2^T + b2) + lora_l2(h1) -> GELU
    val w2t = rt_torch_torchtensor_transpose(model.w2, 0, 1)
    val hw2 = rt_torch_torchtensor_matmul(h1, w2t)
    rt_torch_torchtensor_free(w2t)
    val h2_base = rt_torch_torchtensor_add(hw2, model.b2)
    rt_torch_torchtensor_free(hw2)
    val lora2_out = lora_l2.forward(h1)
    val h2_with_lora = rt_torch_torchtensor_add(h2_base, lora2_out)
    rt_torch_torchtensor_free(h2_base)
    rt_torch_torchtensor_free(lora2_out)
    val h2 = rt_torch_torchtensor_gelu(h2_with_lora)
    rt_torch_torchtensor_free(h1)
    rt_torch_torchtensor_free(h2_with_lora)

    # Output layer (no LoRA on output): h2 @ W3^T + b3
    val w3t = rt_torch_torchtensor_transpose(model.w3, 0, 1)
    val hw3 = rt_torch_torchtensor_matmul(h2, w3t)
    rt_torch_torchtensor_free(w3t)
    rt_torch_torchtensor_free(h2)
    val out = rt_torch_torchtensor_add(hw3, model.b3)
    rt_torch_torchtensor_free(hw3)
    out


fn lora_sgd_step(lora_l1: LoRAAdapter, lora_l2: LoRAAdapter, lr: f64):
    lora_l1.sgd_step(lr)
    lora_l2.sgd_step(lr)


fn lora_zero_grads(lora_l1: LoRAAdapter, lora_l2: LoRAAdapter):
    lora_l1.zero_grads()
    lora_l2.zero_grads()


# ============================================================================
# Merge LoRA into base model weights
# ============================================================================

fn merge_lora_weight(base_w: i64, lora: LoRAAdapter) -> i64:
    # W_merged = W + B @ A * scale
    # B: [out, rank], A: [rank, in]
    # B @ A: [out, in]
    rt_torch_autograd_no_grad_begin()
    val ba = rt_torch_torchtensor_matmul(lora.lora_b, lora.lora_a)
    val ba_scaled = rt_torch_torchtensor_mul_scalar(ba, lora.lora_scale)
    rt_torch_torchtensor_free(ba)
    val merged = rt_torch_torchtensor_add(base_w, ba_scaled)
    rt_torch_torchtensor_free(ba_scaled)
    rt_torch_torchtensor_free(base_w)
    rt_torch_autograd_no_grad_end()
    merged


fn merge_lora_into_model(model: TextModel, lora_l1: LoRAAdapter, lora_l2: LoRAAdapter):
    # Merge LoRA weights into base model
    # After merge, LoRA knowledge is "baked in" and frozen
    print("Merging LoRA into base model weights...")
    print("  Merging layer 1 LoRA: B[{lora_l1.out_features},{lora_l1.rank}] @ A[{lora_l1.rank},{lora_l1.in_features}]")
    model.w1 = merge_lora_weight(model.w1, lora_l1)
    print("  Merging layer 2 LoRA: B[{lora_l2.out_features},{lora_l2.rank}] @ A[{lora_l2.rank},{lora_l2.in_features}]")
    model.w2 = merge_lora_weight(model.w2, lora_l2)
    print("  LoRA merged into base weights (knowledge frozen)")
