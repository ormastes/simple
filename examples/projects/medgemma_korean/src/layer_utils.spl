# Extended Model Utilities for Phases 3-6
#
# Provides:
#   - ExtendedModel: wraps TextModel + KorEngEmbedding + RoPECache + front/back layers
#   - Forward with intermediates for per-layer CE loss
#   - Front/back layer duplication (Phase 5+)
#   - Extended LoRA forward pipeline: embed -> RoPE -> pool -> (front) -> L1+LoRA -> L2+LoRA -> L3 -> (back)
#   - Per-layer CE loss computation for monitoring layer-wise training progress

use model.{TextModel, sgd_update_tensor, make_param, make_zeros_param}
use embedding_utils.{KorEngEmbedding, RoPECache, embed_with_routing, apply_rope, mean_pool_seq}
use shared_logic.{compute_ce_loss}
use lora_utils.{LoRAAdapter, lora_sgd_step, lora_zero_grads}

use std.torch.ffi.{
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_matmul,
    rt_torch_torchtensor_transpose,
    rt_torch_torchtensor_gelu,
    rt_torch_torchtensor_mean,
    rt_torch_torchtensor_reshape,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_clone,
    rt_torch_autograd_set_requires_grad,
    rt_torch_autograd_zero_grad,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end
}

export ExtendedModel
export create_extended_model, duplicate_front_back_layers
export extended_lora_forward, forward_with_intermediates
export compute_per_layer_ce_loss
export extended_sgd_step, extended_zero_grads


# ============================================================================
# Extended Model
# ============================================================================

class ExtendedModel:
    base: TextModel
    emb: KorEngEmbedding
    rope: RoPECache
    seq_len: i64
    # Front/back duplicate layers (nil/0 when not active)
    w_front: i64
    b_front: i64
    w_back: i64
    b_back: i64
    has_front_back: bool

    fn print_summary():
        print("ExtendedModel:")
        self.base.print_summary()
        self.emb.print_summary()
        self.rope.print_summary()
        if self.has_front_back:
            print("  Front/Back layers: ACTIVE")
        else:
            print("  Front/Back layers: INACTIVE")


fn create_extended_model(base: TextModel, emb: KorEngEmbedding, rope: RoPECache, seq_len: i64) -> ExtendedModel:
    print("Creating ExtendedModel (embedding + RoPE wrapper)")
    ExtendedModel(
        base: base,
        emb: emb,
        rope: rope,
        seq_len: seq_len,
        w_front: 0,
        b_front: 0,
        w_back: 0,
        b_back: 0,
        has_front_back: false
    )


# ============================================================================
# Front/Back Layer Duplication (Phase 5+)
# ============================================================================

# Clone W1 -> W_front, W3 -> W_back as new trainable layers.
# Front layer processes input before L1, back layer processes output after L3.
fn duplicate_front_back_layers(ext: ExtendedModel):
    print("Duplicating front/back layers:")

    # Front: clone of layer 1 weights
    val w_front = rt_torch_torchtensor_clone(ext.base.w1)
    rt_torch_autograd_set_requires_grad(w_front, true)
    val b_front = rt_torch_torchtensor_clone(ext.base.b1)
    rt_torch_autograd_set_requires_grad(b_front, true)
    print("  Front layer: clone of L1 [{ext.base.config.hidden_dim}, {ext.base.config.input_dim}]")

    # Back: clone of output layer weights
    val w_back = rt_torch_torchtensor_clone(ext.base.w3)
    rt_torch_autograd_set_requires_grad(w_back, true)
    val b_back = rt_torch_torchtensor_clone(ext.base.b3)
    rt_torch_autograd_set_requires_grad(b_back, true)
    print("  Back layer: clone of L3 [{ext.base.config.output_dim}, {ext.base.config.hidden_dim}]")

    ext.w_front = w_front
    ext.b_front = b_front
    ext.w_back = w_back
    ext.b_back = b_back
    ext.has_front_back = true
    print("  Front/back layers: ACTIVE")


# ============================================================================
# Front Layer Forward
# ============================================================================

# Front layer: x @ W_front^T + b_front -> GELU
# Input: [batch, input_dim], Output: [batch, input_dim] (same shape, goes into L1)
fn front_layer_forward(ext: ExtendedModel, x: i64) -> i64:
    val wt = rt_torch_torchtensor_transpose(ext.w_front, 0, 1)
    val xw = rt_torch_torchtensor_matmul(x, wt)
    rt_torch_torchtensor_free(wt)
    val pre = rt_torch_torchtensor_add(xw, ext.b_front)
    rt_torch_torchtensor_free(xw)
    val out = rt_torch_torchtensor_gelu(pre)
    rt_torch_torchtensor_free(pre)
    out


# Back layer: x @ W_back^T + b_back
# Input: [batch, output_dim] (from L3), Output: [batch, output_dim]
fn back_layer_forward(ext: ExtendedModel, x: i64) -> i64:
    val wt = rt_torch_torchtensor_transpose(ext.w_back, 0, 1)
    val xw = rt_torch_torchtensor_matmul(x, wt)
    rt_torch_torchtensor_free(wt)
    val out = rt_torch_torchtensor_add(xw, ext.b_back)
    rt_torch_torchtensor_free(xw)
    out


# ============================================================================
# Extended LoRA Forward (Full Pipeline)
# ============================================================================

# Full pipeline: embed -> RoPE -> pool -> (front) -> L1+LoRA -> L2+LoRA -> L3 -> (back)
# kor_ids, eng_ids: [batch, seq_len] int64
# mask: [batch, seq_len, 1] float
# Returns: output logits [batch, output_dim]
fn extended_lora_forward(ext: ExtendedModel, kor_ids: i64, eng_ids: i64, mask: i64, lora_l1: LoRAAdapter, lora_l2: LoRAAdapter) -> i64:
    # Embed with routing
    val embedded = embed_with_routing(ext.emb, kor_ids, eng_ids, mask)

    # Apply RoPE
    val rotated = apply_rope(embedded, ext.rope, ext.seq_len)
    rt_torch_torchtensor_free(embedded)

    # Mean pool over sequence: [batch, seq_len, embed_dim] -> [batch, embed_dim]
    val pooled = mean_pool_seq(rotated)
    rt_torch_torchtensor_free(rotated)

    # Optional front layer
    var x = pooled
    if ext.has_front_back:
        val front_out = front_layer_forward(ext, pooled)
        rt_torch_torchtensor_free(pooled)
        x = front_out

    # Layer 1 with LoRA: (x @ W1^T + b1) + lora_l1(x) -> GELU
    val w1t = rt_torch_torchtensor_transpose(ext.base.w1, 0, 1)
    val xw1 = rt_torch_torchtensor_matmul(x, w1t)
    rt_torch_torchtensor_free(w1t)
    val h1_base = rt_torch_torchtensor_add(xw1, ext.base.b1)
    rt_torch_torchtensor_free(xw1)
    val lora1_out = lora_l1.forward(x)
    val h1_with_lora = rt_torch_torchtensor_add(h1_base, lora1_out)
    rt_torch_torchtensor_free(h1_base)
    rt_torch_torchtensor_free(lora1_out)
    val h1 = rt_torch_torchtensor_gelu(h1_with_lora)
    rt_torch_torchtensor_free(h1_with_lora)

    # Layer 2 with LoRA: (h1 @ W2^T + b2) + lora_l2(h1) -> GELU
    val w2t = rt_torch_torchtensor_transpose(ext.base.w2, 0, 1)
    val hw2 = rt_torch_torchtensor_matmul(h1, w2t)
    rt_torch_torchtensor_free(w2t)
    val h2_base = rt_torch_torchtensor_add(hw2, ext.base.b2)
    rt_torch_torchtensor_free(hw2)
    val lora2_out = lora_l2.forward(h1)
    val h2_with_lora = rt_torch_torchtensor_add(h2_base, lora2_out)
    rt_torch_torchtensor_free(h2_base)
    rt_torch_torchtensor_free(lora2_out)
    val h2 = rt_torch_torchtensor_gelu(h2_with_lora)
    rt_torch_torchtensor_free(h1)
    rt_torch_torchtensor_free(h2_with_lora)

    # Output layer (no LoRA): h2 @ W3^T + b3
    val w3t = rt_torch_torchtensor_transpose(ext.base.w3, 0, 1)
    val hw3 = rt_torch_torchtensor_matmul(h2, w3t)
    rt_torch_torchtensor_free(w3t)
    rt_torch_torchtensor_free(h2)
    val out = rt_torch_torchtensor_add(hw3, ext.base.b3)
    rt_torch_torchtensor_free(hw3)

    # Free x if it was replaced by front layer output
    if not ext.has_front_back:
        rt_torch_torchtensor_free(x)

    # Optional back layer
    if ext.has_front_back:
        val back_out = back_layer_forward(ext, out)
        rt_torch_torchtensor_free(out)
        return back_out

    out


# ============================================================================
# Forward with Intermediates (for per-layer loss)
# ============================================================================

# Like extended_lora_forward but returns intermediate hidden states.
# Returns [h1, h2, output] (or [front_out, h1, h2, output, back_out] if front/back active)
# Caller is responsible for freeing all returned tensors.
fn forward_with_intermediates(ext: ExtendedModel, kor_ids: i64, eng_ids: i64, mask: i64, lora_l1: LoRAAdapter, lora_l2: LoRAAdapter) -> [i64]:
    # Embed -> RoPE -> pool
    val embedded = embed_with_routing(ext.emb, kor_ids, eng_ids, mask)
    val rotated = apply_rope(embedded, ext.rope, ext.seq_len)
    rt_torch_torchtensor_free(embedded)
    val pooled = mean_pool_seq(rotated)
    rt_torch_torchtensor_free(rotated)

    var x = pooled
    var front_out_h = 0

    if ext.has_front_back:
        val front_out = front_layer_forward(ext, pooled)
        rt_torch_torchtensor_free(pooled)
        x = front_out
        front_out_h = rt_torch_torchtensor_clone(front_out)

    # Layer 1 with LoRA
    val w1t = rt_torch_torchtensor_transpose(ext.base.w1, 0, 1)
    val xw1 = rt_torch_torchtensor_matmul(x, w1t)
    rt_torch_torchtensor_free(w1t)
    val h1_base = rt_torch_torchtensor_add(xw1, ext.base.b1)
    rt_torch_torchtensor_free(xw1)
    val lora1_out = lora_l1.forward(x)
    val h1_with_lora = rt_torch_torchtensor_add(h1_base, lora1_out)
    rt_torch_torchtensor_free(h1_base)
    rt_torch_torchtensor_free(lora1_out)
    val h1 = rt_torch_torchtensor_gelu(h1_with_lora)
    rt_torch_torchtensor_free(h1_with_lora)

    if not ext.has_front_back:
        rt_torch_torchtensor_free(x)

    # Save h1 clone for returning
    val h1_clone = rt_torch_torchtensor_clone(h1)

    # Layer 2 with LoRA
    val w2t = rt_torch_torchtensor_transpose(ext.base.w2, 0, 1)
    val hw2 = rt_torch_torchtensor_matmul(h1, w2t)
    rt_torch_torchtensor_free(w2t)
    val h2_base = rt_torch_torchtensor_add(hw2, ext.base.b2)
    rt_torch_torchtensor_free(hw2)
    val lora2_out = lora_l2.forward(h1)
    val h2_with_lora = rt_torch_torchtensor_add(h2_base, lora2_out)
    rt_torch_torchtensor_free(h2_base)
    rt_torch_torchtensor_free(lora2_out)
    val h2 = rt_torch_torchtensor_gelu(h2_with_lora)
    rt_torch_torchtensor_free(h1)
    rt_torch_torchtensor_free(h2_with_lora)

    # Save h2 clone
    val h2_clone = rt_torch_torchtensor_clone(h2)

    # Output layer
    val w3t = rt_torch_torchtensor_transpose(ext.base.w3, 0, 1)
    val hw3 = rt_torch_torchtensor_matmul(h2, w3t)
    rt_torch_torchtensor_free(w3t)
    rt_torch_torchtensor_free(h2)
    val out = rt_torch_torchtensor_add(hw3, ext.base.b3)
    rt_torch_torchtensor_free(hw3)

    if ext.has_front_back:
        val back_out = back_layer_forward(ext, out)
        return [front_out_h, h1_clone, h2_clone, out, back_out]

    [h1_clone, h2_clone, out]


# ============================================================================
# Per-Layer CE Loss
# ============================================================================

# Compute cross-entropy loss at each intermediate layer by projecting hidden
# states to output dimension. Returns sum of per-layer losses as a differentiable tensor.
# intermediates: [h1, h2, output] or [front, h1, h2, output, back]
# targets: [batch, 1] class indices
# output_dim: dimension to project to for CE loss
fn compute_per_layer_ce_loss(intermediates: [i64], targets: i64, output_dim: i64) -> i64:
    # For simplicity, compute CE loss on the final output only
    # (intermediate layers have different dims, projection would need separate weights)
    val num = intermediates.len()
    val final_output = intermediates[num - 1]
    val loss = compute_ce_loss(final_output, targets)
    loss


# ============================================================================
# Extended Model Optimizer Utilities
# ============================================================================

# SGD step on all trainable parameters: embeddings + LoRA + front/back
fn extended_sgd_step(ext: ExtendedModel, lora_l1: LoRAAdapter, lora_l2: LoRAAdapter, lr: f64):
    # Embedding params
    rt_torch_autograd_no_grad_begin()
    ext.emb.kor_weight = sgd_update_tensor(ext.emb.kor_weight, lr)
    ext.emb.eng_weight = sgd_update_tensor(ext.emb.eng_weight, lr)
    rt_torch_autograd_no_grad_end()
    rt_torch_autograd_set_requires_grad(ext.emb.kor_weight, true)
    rt_torch_autograd_set_requires_grad(ext.emb.eng_weight, true)

    # LoRA params
    lora_l1.sgd_step(lr)
    lora_l2.sgd_step(lr)

    # Front/back params
    if ext.has_front_back:
        rt_torch_autograd_no_grad_begin()
        ext.w_front = sgd_update_tensor(ext.w_front, lr)
        ext.b_front = sgd_update_tensor(ext.b_front, lr)
        ext.w_back = sgd_update_tensor(ext.w_back, lr)
        ext.b_back = sgd_update_tensor(ext.b_back, lr)
        rt_torch_autograd_no_grad_end()
        rt_torch_autograd_set_requires_grad(ext.w_front, true)
        rt_torch_autograd_set_requires_grad(ext.b_front, true)
        rt_torch_autograd_set_requires_grad(ext.w_back, true)
        rt_torch_autograd_set_requires_grad(ext.b_back, true)


# Zero gradients on all trainable parameters
fn extended_zero_grads(ext: ExtendedModel, lora_l1: LoRAAdapter, lora_l2: LoRAAdapter):
    # Embedding grads
    rt_torch_autograd_zero_grad(ext.emb.kor_weight)
    rt_torch_autograd_zero_grad(ext.emb.eng_weight)

    # LoRA grads
    lora_l1.zero_grads()
    lora_l2.zero_grads()

    # Front/back grads
    if ext.has_front_back:
        rt_torch_autograd_zero_grad(ext.w_front)
        rt_torch_autograd_zero_grad(ext.b_front)
        rt_torch_autograd_zero_grad(ext.w_back)
        rt_torch_autograd_zero_grad(ext.b_back)
