# Shared Logic for MedGemma Korean Training Pipeline
#
# Common utilities used across all training phases:
#   - Training configuration
#   - Batch creation
#   - Loss computation wrappers
#   - Validation scoring
#   - GPU memory reporting
#   - Learning rate scheduling
#
# Import this module from all phase scripts to avoid duplication.

use std.torch.ffi.{
    rt_torch_tensor_randn,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_neg,
    rt_torch_torchtensor_log_softmax,
    rt_torch_torchtensor_gather,
    rt_torch_torchtensor_sum_dim,
    rt_torch_torchtensor_mean,
    rt_torch_torchtensor_mean_dim,
    rt_torch_torchtensor_argmax,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_clone,
    rt_torch_autograd_backward,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end,
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_max_memory_allocated,
    rt_torch_cuda_empty_cache
}

export TrainConfig, TrainState
export make_batch_on_cuda, make_randn_batch_on_cuda
export compute_ce_loss, compute_mse_loss
export report_gpu_memory, report_training_step, report_epoch_end
export lr_linear_warmup

# ============================================================================
# Training Configuration
# ============================================================================

class TrainConfig:
    input_dim: i64
    hidden_dim: i64
    output_dim: i64
    batch_size: i64
    num_batches: i64
    num_epochs: i64
    learning_rate: f64
    lora_rank: i64
    lora_alpha: f64
    max_grad_norm: f64
    warmup_steps: i64
    log_every: i64

    static fn default_config() -> TrainConfig:
        TrainConfig(
            input_dim: 64,
            hidden_dim: 128,
            output_dim: 32,
            batch_size: 16,
            num_batches: 20,
            num_epochs: 3,
            learning_rate: 0.001,
            lora_rank: 8,
            lora_alpha: 16.0,
            max_grad_norm: 1.0,
            warmup_steps: 10,
            log_every: 5
        )

    static fn phase0() -> TrainConfig:
        TrainConfig(
            input_dim: 64,
            hidden_dim: 128,
            output_dim: 32,
            batch_size: 16,
            num_batches: 20,
            num_epochs: 3,
            learning_rate: 0.001,
            lora_rank: 8,
            lora_alpha: 16.0,
            max_grad_norm: 1.0,
            warmup_steps: 10,
            log_every: 5
        )

    static fn phase1() -> TrainConfig:
        TrainConfig(
            input_dim: 64,
            hidden_dim: 128,
            output_dim: 32,
            batch_size: 16,
            num_batches: 20,
            num_epochs: 3,
            learning_rate: 0.0005,
            lora_rank: 8,
            lora_alpha: 16.0,
            max_grad_norm: 1.0,
            warmup_steps: 10,
            log_every: 5
        )

    fn total_steps() -> i64:
        self.num_batches * self.num_epochs

    fn print_config(phase_name: text):
        print("Training config [{phase_name}]:")
        print("  Epochs: {self.num_epochs}")
        print("  Batches/epoch: {self.num_batches}")
        print("  Batch size: {self.batch_size}")
        print("  Learning rate: {self.learning_rate}")
        print("  LoRA rank: {self.lora_rank}")
        print("  Max grad norm: {self.max_grad_norm}")
        print("  Warmup steps: {self.warmup_steps}")
        print("")


# ============================================================================
# Training State (tracks step count, best loss, etc.)
# ============================================================================

class TrainState:
    global_step: i64
    best_loss: f64
    epoch_loss: f64
    epoch_steps: i64

    static fn create() -> TrainState:
        TrainState(
            global_step: 0,
            best_loss: 999999.0,
            epoch_loss: 0.0,
            epoch_steps: 0
        )

    me reset_epoch():
        self.epoch_loss = 0.0
        self.epoch_steps = 0

    me add_loss(loss_val: f64):
        self.epoch_loss = self.epoch_loss + loss_val
        self.epoch_steps = self.epoch_steps + 1
        self.global_step = self.global_step + 1

    fn avg_epoch_loss() -> f64:
        if self.epoch_steps == 0:
            return 0.0
        self.epoch_loss / self.epoch_steps

    me update_best():
        val avg = self.avg_epoch_loss()
        if avg < self.best_loss:
            self.best_loss = avg


# ============================================================================
# Batch Creation
# ============================================================================

# Create a pair of random tensors on CUDA: [input, target]
fn make_randn_batch_on_cuda(batch_size: i64, input_dim: i64, target_dim: i64) -> [i64]:
    val x_cpu = rt_torch_tensor_randn([batch_size, input_dim])
    val x = rt_torch_torchtensor_cuda(x_cpu, 0)
    rt_torch_torchtensor_free(x_cpu)
    val t_cpu = rt_torch_tensor_randn([batch_size, target_dim])
    val t = rt_torch_torchtensor_cuda(t_cpu, 0)
    rt_torch_torchtensor_free(t_cpu)
    [x, t]

# Create tensors from f64 arrays on CUDA
fn make_batch_on_cuda(input_data: [f64], input_dims: [i64], target_data: [f64], target_dims: [i64]) -> [i64]:
    val x_cpu = rt_torch_tensor_from_data(input_data, input_dims)
    val x = rt_torch_torchtensor_cuda(x_cpu, 0)
    rt_torch_torchtensor_free(x_cpu)
    val t_cpu = rt_torch_tensor_from_data(target_data, target_dims)
    val t = rt_torch_torchtensor_cuda(t_cpu, 0)
    rt_torch_torchtensor_free(t_cpu)
    [x, t]


# ============================================================================
# Loss Computation
# ============================================================================

# Cross-entropy loss (differentiable tensor for backward).
# logits: [batch, num_classes], targets: [batch, 1] (class indices as f64)
fn compute_ce_loss(logits: i64, targets: i64) -> i64:
    val log_probs = rt_torch_torchtensor_log_softmax(logits, 1)
    val neg_log_probs = rt_torch_torchtensor_neg(log_probs)
    rt_torch_torchtensor_free(log_probs)
    val gathered = rt_torch_torchtensor_gather(neg_log_probs, 1, targets)
    rt_torch_torchtensor_free(neg_log_probs)
    val loss = rt_torch_torchtensor_mean_dim(gathered, 1, false)
    rt_torch_torchtensor_free(gathered)
    val scalar_loss = rt_torch_torchtensor_mean_dim(loss, 0, false)
    rt_torch_torchtensor_free(loss)
    scalar_loss

# MSE loss (differentiable tensor for backward).
# pred, target: same shape tensors
fn compute_mse_loss(pred: i64, target: i64) -> i64:
    val diff = rt_torch_torchtensor_sub(pred, target)
    val sq = rt_torch_torchtensor_mul(diff, diff)
    rt_torch_torchtensor_free(diff)
    val per_sample = rt_torch_torchtensor_sum_dim(sq, 1, false)
    rt_torch_torchtensor_free(sq)
    val loss = rt_torch_torchtensor_mean_dim(per_sample, 0, false)
    rt_torch_torchtensor_free(per_sample)
    loss


# ============================================================================
# GPU Memory Reporting
# ============================================================================

fn report_gpu_memory():
    val mem = rt_torch_cuda_memory_allocated(0)
    val mem_mb = mem / (1024 * 1024)
    val peak = rt_torch_cuda_max_memory_allocated(0)
    val peak_mb = peak / (1024 * 1024)
    print("GPU Memory: {mem_mb}MB allocated, {peak_mb}MB peak")

fn report_training_step(state: TrainState, config: TrainConfig):
    if state.epoch_steps > 0:
        if state.epoch_steps % config.log_every == 0:
            val avg = state.epoch_loss / state.epoch_steps
            print("  Step {state.global_step}: loss={avg}")

fn report_epoch_end(epoch: i64, total_epochs: i64, state: TrainState):
    val avg = state.avg_epoch_loss()
    val mem = rt_torch_cuda_memory_allocated(0)
    val mem_mb = mem / (1024 * 1024)
    print("Epoch {epoch + 1}/{total_epochs}: avg_loss={avg}, GPU={mem_mb}MB")


# ============================================================================
# Learning Rate Scheduling
# ============================================================================

# Linear warmup from 0 to base_lr over warmup_steps, then constant.
fn lr_linear_warmup(step: i64, warmup_steps: i64, base_lr: f64) -> f64:
    if step < warmup_steps:
        return base_lr * step / warmup_steps
    base_lr
