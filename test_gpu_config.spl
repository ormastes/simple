#!/usr/bin/env simple
# Test GPU Configuration - Verify 2nd GPU setup

use std.src.dl.config.{dl, Device, cuda, use_gpu}
use std.src.dl.config_loader.{load_local_config, load_and_apply}
use lib.pure.nn.{Linear, ReLU, Sequential}
use lib.pure.tensor.{tensor_from_data}

fn main():
    print "=== Simple Deep Learning GPU Configuration Test ==="
    print ""

    # Method 1: Load from local config file
    print "Method 1: Loading config from dl.config.sdn..."
    if load_and_apply("dl.config.sdn"):
        print "  ✓ Config loaded successfully"
    else:
        print "  ✗ Config loading failed, using defaults"

    print ""
    print "Current DL Configuration:"
    print "  Device: {dl.default_device.to_string()}"
    print "  DType: {dl.default_dtype.to_string()}"
    print "  Backend: {dl.default_backend.to_string()}"
    print "  Autograd: {dl.autograd_enabled}"
    print "  AMP: {dl.amp_enabled}"

    if dl.seed.?:
        print "  Seed: {dl.seed.unwrap()}"

    print ""

    # Method 2: Programmatic configuration
    print "Method 2: Programmatic GPU selection..."

    # Set 2nd GPU (CUDA:1)
    dl.device(Device.CUDA(1))
    print "  ✓ Set device to: {dl.default_device.to_string()}"

    # Alternative: Use helper functions
    # dl.device(cuda(1))  # Same as above
    # use_gpu()           # Sets CUDA:0 (default GPU)

    print ""

    # Method 3: Check GPU availability (when FFI is implemented)
    print "Method 3: GPU Availability Check..."
    print "  NOTE: PyTorch FFI not yet fully implemented"
    print "  Current backend: Pure Simple (CPU-only)"

    # When PyTorch FFI is available:
    # use lib.pure.torch_ffi.{torch_cuda_available, torch_version}
    # print "  CUDA available: {torch_cuda_available()}"
    # print "  PyTorch version: {torch_version()}"

    print ""

    # Test with simple neural network
    print "=== Testing Neural Network on Configured Device ==="
    print ""

    print "Creating model: Sequential(Linear(10, 5), ReLU, Linear(5, 2))"
    val model = Sequential.create([
        Linear.create(10, 5, bias: true),
        ReLU.create(),
        Linear.create(5, 2, bias: true)
    ])

    print "  ✓ Model created"
    print "  Total parameters: {count_parameters(model)}"
    print ""

    print "Creating input tensor: [10] random values"
    var input_data: [f64] = []
    var i = 0
    while i < 10:
        input_data.push(0.1 * (i + 1))
        i = i + 1

    val input = tensor_from_data(input_data, [10])
    print "  ✓ Input tensor created"
    print ""

    print "Running forward pass..."
    val output = model.forward(input)
    print "  ✓ Forward pass complete"
    print "  Output shape: [{output.shape[0]}]"
    print "  Output values: {output.data[0]}, {output.data[1]}"
    print ""

    print "=== GPU Configuration Summary ==="
    print ""
    print "To use 2nd GPU in your code:"
    print ""
    print "  Option 1 - Config file (recommended):"
    print "    1. Create dl.config.sdn with: device: \"cuda:1\""
    print "    2. Load with: use std.src.dl.config_loader.{load_local_config}"
    print "    3. Call: load_local_config()"
    print ""
    print "  Option 2 - Programmatic:"
    print "    use std.src.dl.config.{dl, Device}"
    print "    dl.device(Device.CUDA(1))  # 2nd GPU"
    print ""
    print "  Option 3 - Preset functions:"
    print "    use std.src.dl.config.{cuda}"
    print "    dl.device(cuda(1))  # 2nd GPU"
    print ""

    print "NOTE: Full GPU acceleration requires PyTorch FFI implementation."
    print "      Current version uses Pure Simple (CPU-only)."
    print "      GPU support is part of the planned PyTorch integration."

fn count_parameters(model: any) -> i64:
    """Count total parameters in model."""
    val params = model.parameters()
    var total = 0
    for param in params:
        var size = 1
        for dim in param.shape:
            size = size * dim
        total = total + size
    total
