# Validation Utilities for Knowledge Retention Testing
#
# This module provides utilities to validate that progressive LoRA training
# does not cause catastrophic forgetting. Each phase's knowledge should be
# retained after training subsequent phases.
#
# Tests:
#   - test_plain_text: Validate Phase 0 knowledge (Korean fluency)
#   - test_medical_dict: Validate Phase 1 knowledge (terminology)
#   - test_mcq: Validate Phase 2 knowledge (medical reasoning)
#   - test_all_phases: Run all validation tests

export test_plain_text, test_medical_dict, test_mcq, test_all_phases
export ValidationResult, ValidationReport


# ============================================================================
# Validation Result Classes
# ============================================================================

class ValidationResult:
    """Result of a single validation test.

    Attributes:
        phase: Phase number (0, 1, 2)
        test_name: Name of test
        passed: Whether test passed
        score: Score (0.0 to 1.0)
        details: Additional details
    """
    phase: i32
    test_name: str
    passed: bool
    score: f64
    details: str

    fn __init__(phase: i32, test_name: str, passed: bool, score: f64, details: str):
        self.phase = phase
        self.test_name = test_name
        self.passed = passed
        self.score = score
        self.details = details

    fn print():
        """Print validation result."""
        val status = "✓ PASS" if self.passed else "✗ FAIL"
        print(f"  [{status}] Phase {self.phase}: {self.test_name}")
        print(f"          Score: {self.score:.2%}")
        if self.details:
            print(f"          Details: {self.details}")


class ValidationReport:
    """Complete validation report for all phases.

    Attributes:
        results: List of ValidationResult
        total_tests: Total number of tests
        passed_tests: Number of passed tests
        overall_score: Overall score
    """
    results: [ValidationResult]
    total_tests: i32
    passed_tests: i32
    overall_score: f64

    fn __init__():
        self.results = []
        self.total_tests = 0
        self.passed_tests = 0
        self.overall_score = 0.0

    fn add_result(result: ValidationResult):
        """Add a validation result."""
        self.results.append(result)
        self.total_tests += 1
        if result.passed:
            self.passed_tests += 1

    fn compute_overall_score():
        """Compute overall score from all results."""
        if self.total_tests == 0:
            self.overall_score = 0.0
            return

        var total_score = 0.0
        for result in self.results:
            total_score += result.score

        self.overall_score = total_score / self.total_tests.to_f64()

    fn print():
        """Print validation report."""
        print("\n" + "=" * 70)
        print("VALIDATION REPORT")
        print("=" * 70)
        print()

        # Print individual results
        for result in self.results:
            result.print()

        print()
        print("=" * 70)
        print(f"Overall: {self.passed_tests}/{self.total_tests} tests passed")
        print(f"Overall Score: {self.overall_score:.2%}")

        if self.passed_tests == self.total_tests:
            print("Status: ✓ ALL TESTS PASSED - No catastrophic forgetting!")
        else:
            print("Status: ✗ SOME TESTS FAILED - Catastrophic forgetting detected!")

        print("=" * 70)

    fn has_catastrophic_forgetting() -> bool:
        """Check if catastrophic forgetting occurred.

        Returns:
            True if any test failed
        """
        return self.passed_tests < self.total_tests


# ============================================================================
# Phase 0 Validation: Korean Fluency
# ============================================================================

fn test_plain_text(model: any, tokenizer: any, device: str) -> ValidationResult:
    """Test Phase 0 knowledge: Korean language fluency.

    Tests that the model can generate fluent Korean text.

    Args:
        model: Model to test
        tokenizer: Tokenizer
        device: Device to run on

    Returns:
        ValidationResult for plain text generation

    Example:
        val result = test_plain_text(model, tokenizer, "cuda:0")
        result.print()
    """
    print("\n<Phase 0 Test> Plain Text Generation")
    print("  Testing Korean fluency...")

    # Test prompts
    val test_prompts = [
        "한국어로 설명하면",
        "의학적으로",
        "환자의 증상은"
    ]

    var total_score = 0.0
    var passed_count = 0

    for prompt in test_prompts:
        print(f"  Prompt: '{prompt}'")

        # TODO: Implement actual generation
        # val inputs = tokenizer(prompt, return_tensors="pt").to(device)
        # val outputs = model.generate(
        #     **inputs,
        #     max_new_tokens=50,
        #     temperature=0.7,
        #     do_sample=true
        # )
        # val response = tokenizer.decode(outputs[0], skip_special_tokens=true)

        # Mock response for example
        val response = "한국어로 설명하면 다음과 같습니다. 의학적인 관점에서..."

        # Score: Check if response is in Korean
        val has_korean = response.contains("한") or response.contains("의") or response.contains("는")
        val score = 1.0 if has_korean else 0.0

        total_score += score
        if score >= 0.7:
            passed_count += 1

        print(f"    Response: {response[:50]}...")
        print(f"    Score: {score:.2%}")

    val avg_score = total_score / test_prompts.len().to_f64()
    val passed = avg_score >= 0.7

    return ValidationResult(
        phase=0,
        test_name="Korean Fluency",
        passed=passed,
        score=avg_score,
        details=f"{passed_count}/{test_prompts.len()} prompts passed"
    )


# ============================================================================
# Phase 1 Validation: Medical Dictionary
# ============================================================================

fn test_medical_dict(model: any, tokenizer: any, device: str) -> ValidationResult:
    """Test Phase 1 knowledge: Medical terminology.

    Tests that the model can define medical terms correctly.

    Args:
        model: Model to test
        tokenizer: Tokenizer
        device: Device to run on

    Returns:
        ValidationResult for medical dictionary

    Example:
        val result = test_medical_dict(model, tokenizer, "cuda:0")
        result.print()
    """
    print("\n<Phase 1 Test> Medical Dictionary")
    print("  Testing medical terminology...")

    # Test cases: term -> expected keywords
    val test_cases = {
        "고혈압": ["Hypertension", "혈압", "blood pressure"],
        "당뇨병": ["Diabetes", "인슐린", "insulin"],
        "폐렴": ["Pneumonia", "폐", "lung"]
    }

    var total_score = 0.0
    var passed_count = 0

    for (term, expected_keywords) in test_cases.items():
        val prompt = f"<start_of_turn>user\nMeaning of word {term}:<end_of_turn>\n<start_of_turn>model\n"

        print(f"  Term: {term}")

        # TODO: Implement actual generation
        # val response = model.generate(prompt, max_tokens=100)

        # Mock response
        val response = f"Hypertension. 혈압이 정상 범위보다 높은 상태"

        # Score: Check if expected keywords are in response
        var keyword_count = 0
        for keyword in expected_keywords:
            if response.contains(keyword):
                keyword_count += 1

        val score = keyword_count.to_f64() / expected_keywords.len().to_f64()
        total_score += score

        if score >= 0.5:
            passed_count += 1

        print(f"    Response: {response[:60]}...")
        print(f"    Keywords found: {keyword_count}/{expected_keywords.len()}")
        print(f"    Score: {score:.2%}")

    val avg_score = total_score / test_cases.len().to_f64()
    val passed = avg_score >= 0.6

    return ValidationResult(
        phase=1,
        test_name="Medical Dictionary",
        passed=passed,
        score=avg_score,
        details=f"{passed_count}/{test_cases.len()} terms defined correctly"
    )


# ============================================================================
# Phase 2 Validation: MCQ Reasoning
# ============================================================================

fn test_mcq(model: any, tokenizer: any, test_data: [any], device: str) -> ValidationResult:
    """Test Phase 2 knowledge: Medical reasoning with MCQ.

    Tests that the model can answer medical multiple-choice questions
    with reasoning.

    Args:
        model: Model to test
        tokenizer: Tokenizer
        test_data: List of MCQ test samples
        device: Device to run on

    Returns:
        ValidationResult for MCQ

    Example:
        val mcq_data = load_test_mcq_data()
        val result = test_mcq(model, tokenizer, mcq_data, "cuda:0")
        result.print()
    """
    print("\n<Phase 2 Test> MCQ Reasoning")
    print("  Testing medical reasoning...")

    var correct = 0
    var total = 0

    # Use subset of test data
    val max_samples = min(test_data.len(), 10)

    for i in 0..max_samples:
        val sample = test_data[i]

        val prompt = f"""<start_of_turn>user
Reasoning 후 정답 알파벳 하나만 답하세요.

{sample.question}
A) {sample.A}
B) {sample.B}
C) {sample.C}
D) {sample.D}
E) {sample.E}

<end_of_turn>
<start_of_turn>model
"""

        # TODO: Implement actual generation
        # val response = model.generate(prompt, max_tokens=200)

        # Mock response
        val response = f"<reasoning>ST분절 상승은 심근경색의 특징</reasoning>{sample.answer}"

        # Extract answer
        var predicted = ""
        for char in "ABCDE":
            if response.contains(char):
                predicted = char
                break

        val is_correct = predicted == sample.answer
        if is_correct:
            correct += 1
        total += 1

        print(f"  Q{i+1}: {'✓' if is_correct else '✗'} (Answer: {sample.answer}, Predicted: {predicted})")

    val accuracy = correct.to_f64() / total.to_f64()
    val passed = accuracy >= 0.7  # 70% threshold

    return ValidationResult(
        phase=2,
        test_name="MCQ Reasoning",
        passed=passed,
        score=accuracy,
        details=f"{correct}/{total} questions correct"
    )


# ============================================================================
# Comprehensive Validation
# ============================================================================

fn test_all_phases(
    model: any,
    tokenizer: any,
    mcq_test_data: [any],
    device: str
) -> ValidationReport:
    """Run all validation tests for all phases.

    This is the main validation function that should be called after
    each training phase to ensure no catastrophic forgetting.

    Args:
        model: Model to test
        tokenizer: Tokenizer
        mcq_test_data: MCQ test data (for Phase 2)
        device: Device to run on

    Returns:
        ValidationReport with all results

    Example:
        # After Phase 1 training
        val report = test_all_phases(model, tokenizer, [], "cuda:0")
        report.print()

        if report.has_catastrophic_forgetting():
            print("WARNING: Catastrophic forgetting detected!")
            # Take corrective action

        # After Phase 2 training
        val mcq_data = load_mcq_test_data()
        val report = test_all_phases(model, tokenizer, mcq_data, "cuda:0")
        report.print()
    """
    print("\n" + "=" * 70)
    print("COMPREHENSIVE VALIDATION - ALL PHASES")
    print("=" * 70)

    val report = ValidationReport()

    # Test Phase 0: Korean fluency (always test)
    val result0 = test_plain_text(model, tokenizer, device)
    report.add_result(result0)

    # Test Phase 1: Medical dictionary (always test)
    val result1 = test_medical_dict(model, tokenizer, device)
    report.add_result(result1)

    # Test Phase 2: MCQ (only if test data provided)
    if mcq_test_data.len() > 0:
        val result2 = test_mcq(model, tokenizer, mcq_test_data, device)
        report.add_result(result2)

    # Compute overall score
    report.compute_overall_score()

    # Print report
    report.print()

    return report


# ============================================================================
# Quick Validation Tests
# ============================================================================

fn quick_validation_check(model: any, phase: i32) -> bool:
    """Quick validation check for a specific phase.

    This is a lightweight check that can be run frequently during training.

    Args:
        model: Model to test
        phase: Phase number (0, 1, or 2)

    Returns:
        True if quick check passes

    Example:
        # During training
        if engine.state.iteration % 500 == 0:
            if not quick_validation_check(model, phase=1):
                print("WARNING: Model performance degrading!")
    """
    print(f"\n<Quick Check> Phase {phase}")

    # TODO: Implement quick checks
    # For now, just return true
    return true


fn validate_no_forgetting(
    current_phase: i32,
    model: any,
    tokenizer: any,
    device: str
) -> bool:
    """Validate that previous phases' knowledge is retained.

    This checks only the phases that should already be learned.

    Args:
        current_phase: Current training phase (1 or 2)
        model: Model to test
        tokenizer: Tokenizer
        device: Device to run on

    Returns:
        True if no forgetting detected

    Example:
        # During Phase 2 training
        @trainer.on(Events.EPOCH_COMPLETED)
        fn check_retention(engine: Engine):
            if not validate_no_forgetting(2, model, tokenizer, "cuda:0"):
                print("ERROR: Catastrophic forgetting!")
                engine.terminate()
    """
    print(f"\n<Retention Check> Current phase: {current_phase}")
    print("Checking previous phases for catastrophic forgetting...")

    val report = ValidationReport()

    # Always check Phase 0
    if current_phase >= 1:
        val result = test_plain_text(model, tokenizer, device)
        report.add_result(result)

        if not result.passed:
            print("✗ Phase 0 knowledge lost!")
            return false

    # Check Phase 1 if in Phase 2
    if current_phase >= 2:
        val result = test_medical_dict(model, tokenizer, device)
        report.add_result(result)

        if not result.passed:
            print("✗ Phase 1 knowledge lost!")
            return false

    print("✓ No catastrophic forgetting detected")
    return true
