# Advanced Learning Rate Schedulers
#
# Advanced learning rate scheduling strategies for training.
#
# ## Classes
# - `ReduceLROnPlateau`: Reduce LR when metric plateaus
# - `OneCycleLR`: One-cycle learning rate policy
# - `WarmupLR`: Linear warmup scheduler
# - `CyclicLR`: Cyclic learning rate scheduler
# - `PolynomialLR`: Polynomial decay scheduler
#
# ## Example
# ```simple
# import ml.torch.optim as optim
# import ml.torch.optim.schedulers as sched
#
# val optimizer = optim.Adam(model.parameters(), lr=0.001)
#
# # Reduce on plateau
# val scheduler = sched.ReduceLROnPlateau(optimizer, patience=10)
# for epoch in range(100):
#     val loss = train()
#     scheduler.step(loss)  # Pass validation loss
#
# # One-cycle policy
# val scheduler = sched.OneCycleLR(optimizer, max_lr=0.1, total_steps=1000)
# for step in range(1000):
#     train_step()
#     scheduler.step()
# ```

export ReduceLROnPlateau, OneCycleLR, WarmupLR, CyclicLR, PolynomialLR

use ..{Optimizer, LRScheduler}


# ============================================================================
# ReduceLROnPlateau
# ============================================================================

class ReduceLROnPlateau:
    """Reduce learning rate when a metric has stopped improving.

    Monitors a quantity and reduces LR by a factor when no improvement
    is seen for 'patience' number of epochs.

    Attributes:
        optimizer: Wrapped optimizer
        mode: 'min' or 'max' (whether to minimize or maximize metric)
        factor: Factor to reduce LR by
        patience: Number of epochs with no improvement
        threshold: Threshold for measuring improvement
        min_lr: Lower bound on learning rate

    Example:
        ```simple
        val scheduler = ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.1,
            patience=10
        )

        for epoch in range(100):
            val loss = train_epoch()
            val val_loss = validate()
            scheduler.step(val_loss)
        ```
    """
    optimizer: Optimizer
    mode: str
    factor: f64
    patience: i32
    threshold: f64
    min_lr: f64
    best: f64
    num_bad_epochs: i32
    last_epoch: i32

    fn __init__(
        optimizer: Optimizer,
        mode: str = "min",
        factor: f64 = 0.1,
        patience: i32 = 10,
        threshold: f64 = 0.0001,
        min_lr: f64 = 0.0
    ):
        """Initialize ReduceLROnPlateau scheduler.

        Args:
            optimizer: Wrapped optimizer
            mode: 'min' or 'max' (default: 'min')
            factor: Factor to reduce LR by (default: 0.1)
            patience: Epochs with no improvement (default: 10)
            threshold: Minimum improvement threshold (default: 0.0001)
            min_lr: Minimum learning rate (default: 0.0)
        """
        self.optimizer = optimizer
        self.mode = mode
        self.factor = factor
        self.patience = patience
        self.threshold = threshold
        self.min_lr = min_lr
        self.num_bad_epochs = 0
        self.last_epoch = -1

        # Initialize best based on mode
        if mode == "min":
            self.best = 1e10  # Large value
        else:
            self.best = -1e10  # Small value

    fn step(metric: f64):
        """Update learning rate based on metric.

        Args:
            metric: Current metric value to monitor
        """
        self.last_epoch += 1

        val is_better = if self.mode == "min":
            metric < self.best - self.threshold
        else:
            metric > self.best + self.threshold

        if is_better:
            self.best = metric
            self.num_bad_epochs = 0
        else:
            self.num_bad_epochs += 1

        if self.num_bad_epochs > self.patience:
            self._reduce_lr()
            self.num_bad_epochs = 0

    fn _reduce_lr():
        """Reduce learning rate by factor."""
        val old_lr = self.optimizer.lr
        val new_lr = _max(old_lr * self.factor, self.min_lr)

        if new_lr < old_lr:
            self.optimizer.lr = new_lr
            if self.optimizer.optimizer_handle != 0:
                val _ = @rt_torch_optimizer_set_lr(self.optimizer.optimizer_handle, new_lr)


# ============================================================================
# OneCycleLR
# ============================================================================

class OneCycleLR:
    """One-cycle learning rate policy.

    The 1cycle policy anneals the learning rate from an initial learning
    rate to some maximum learning rate and then from that maximum learning
    rate to some minimum learning rate much lower than the initial learning rate.

    Attributes:
        optimizer: Wrapped optimizer
        max_lr: Upper learning rate bound
        total_steps: Total number of training steps
        pct_start: Percentage of cycle spent increasing LR
        div_factor: Initial LR = max_lr / div_factor
        final_div_factor: Final LR = max_lr / final_div_factor

    Example:
        ```simple
        val scheduler = OneCycleLR(
            optimizer,
            max_lr=0.1,
            total_steps=1000,
            pct_start=0.3
        )

        for step in range(1000):
            train_step()
            scheduler.step()
        ```
    """
    optimizer: Optimizer
    max_lr: f64
    total_steps: i32
    pct_start: f64
    div_factor: f64
    final_div_factor: f64
    current_step: i32
    initial_lr: f64
    final_lr: f64

    fn __init__(
        optimizer: Optimizer,
        max_lr: f64,
        total_steps: i32,
        pct_start: f64 = 0.3,
        div_factor: f64 = 25.0,
        final_div_factor: f64 = 10000.0
    ):
        """Initialize OneCycleLR scheduler.

        Args:
            optimizer: Wrapped optimizer
            max_lr: Maximum learning rate
            total_steps: Total number of training steps
            pct_start: Fraction of steps for warmup (default: 0.3)
            div_factor: Initial LR divisor (default: 25.0)
            final_div_factor: Final LR divisor (default: 10000.0)
        """
        self.optimizer = optimizer
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.pct_start = pct_start
        self.div_factor = div_factor
        self.final_div_factor = final_div_factor
        self.current_step = 0
        self.initial_lr = max_lr / div_factor
        self.final_lr = max_lr / final_div_factor

    fn step():
        """Update learning rate for current step."""
        self.current_step += 1

        val pct = self.current_step as f64 / self.total_steps as f64
        val warmup_steps = (self.total_steps as f64 * self.pct_start) as i32

        val new_lr = if self.current_step <= warmup_steps:
            # Warmup phase: linear increase to max_lr
            val warmup_pct = self.current_step as f64 / warmup_steps as f64
            self.initial_lr + (self.max_lr - self.initial_lr) * warmup_pct
        else:
            # Annealing phase: cosine decay to final_lr
            val anneal_pct = (self.current_step - warmup_steps) as f64 /
                            (self.total_steps - warmup_steps) as f64
            val pi = 3.141592653589793
            val cosine = (1.0 + _cos(pi * anneal_pct)) / 2.0
            self.final_lr + (self.max_lr - self.final_lr) * cosine

        self.optimizer.lr = new_lr
        if self.optimizer.optimizer_handle != 0:
            val _ = @rt_torch_optimizer_set_lr(self.optimizer.optimizer_handle, new_lr)


# ============================================================================
# WarmupLR
# ============================================================================

class WarmupLR(LRScheduler):
    """Linear warmup learning rate scheduler.

    Linearly increases learning rate from 0 to base_lr over warmup_steps.

    Attributes:
        warmup_steps: Number of warmup steps
        base_lr: Target learning rate after warmup

    Example:
        ```simple
        val scheduler = WarmupLR(optimizer, warmup_steps=1000)

        for step in range(10000):
            train_step()
            scheduler.step()
        ```
    """
    warmup_steps: i32
    base_lr: f64

    fn __init__(optimizer: Optimizer, warmup_steps: i32):
        """Initialize warmup scheduler.

        Args:
            optimizer: Wrapped optimizer
            warmup_steps: Number of warmup steps
        """
        super().__init__(optimizer)
        self.warmup_steps = warmup_steps
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate."""
        if self.last_epoch < self.warmup_steps:
            return self.base_lr * (self.last_epoch + 1) as f64 / self.warmup_steps as f64
        else:
            return self.base_lr


# ============================================================================
# CyclicLR
# ============================================================================

class CyclicLR:
    """Cyclic learning rate scheduler.

    Sets the learning rate according to cyclical learning rate policy.
    The policy cycles the learning rate between two boundaries.

    Attributes:
        optimizer: Wrapped optimizer
        base_lr: Initial learning rate (lower bound)
        max_lr: Upper learning rate bound
        step_size_up: Steps in increasing half of cycle
        step_size_down: Steps in decreasing half of cycle
        mode: 'triangular', 'triangular2', or 'exp_range'

    Example:
        ```simple
        val scheduler = CyclicLR(
            optimizer,
            base_lr=0.001,
            max_lr=0.1,
            step_size_up=2000
        )
        ```
    """
    optimizer: Optimizer
    base_lr: f64
    max_lr: f64
    step_size_up: i32
    step_size_down: i32
    mode: str
    current_step: i32
    cycle: i32

    fn __init__(
        optimizer: Optimizer,
        base_lr: f64,
        max_lr: f64,
        step_size_up: i32 = 2000,
        step_size_down: i32 = 0,
        mode: str = "triangular"
    ):
        """Initialize cyclic scheduler.

        Args:
            optimizer: Wrapped optimizer
            base_lr: Lower learning rate bound
            max_lr: Upper learning rate bound
            step_size_up: Steps in increasing phase (default: 2000)
            step_size_down: Steps in decreasing phase (default: same as up)
            mode: Cycle mode (default: 'triangular')
        """
        self.optimizer = optimizer
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size_up = step_size_up
        self.step_size_down = if step_size_down == 0: step_size_up else: step_size_down
        self.mode = mode
        self.current_step = 0
        self.cycle = 0

    fn step():
        """Update learning rate for current step."""
        self.current_step += 1

        val cycle_length = self.step_size_up + self.step_size_down
        val cycle_position = self.current_step % cycle_length

        val new_lr = if cycle_position <= self.step_size_up:
            # Increasing phase
            val scale = cycle_position as f64 / self.step_size_up as f64
            self.base_lr + (self.max_lr - self.base_lr) * scale
        else:
            # Decreasing phase
            val down_pos = cycle_position - self.step_size_up
            val scale = 1.0 - (down_pos as f64 / self.step_size_down as f64)
            self.base_lr + (self.max_lr - self.base_lr) * scale

        self.optimizer.lr = new_lr
        if self.optimizer.optimizer_handle != 0:
            val _ = @rt_torch_optimizer_set_lr(self.optimizer.optimizer_handle, new_lr)


# ============================================================================
# PolynomialLR
# ============================================================================

class PolynomialLR(LRScheduler):
    """Polynomial learning rate decay.

    Decays learning rate using polynomial function:
        lr = base_lr * (1 - step/total_steps)^power

    Attributes:
        total_steps: Total number of training steps
        power: Polynomial power
        end_lr: Final learning rate

    Example:
        ```simple
        val scheduler = PolynomialLR(
            optimizer,
            total_steps=10000,
            power=1.0  # Linear decay
        )
        ```
    """
    total_steps: i32
    power: f64
    end_lr: f64
    base_lr: f64

    fn __init__(
        optimizer: Optimizer,
        total_steps: i32,
        power: f64 = 1.0,
        end_lr: f64 = 0.0
    ):
        """Initialize polynomial scheduler.

        Args:
            optimizer: Wrapped optimizer
            total_steps: Total training steps
            power: Polynomial power (default: 1.0 for linear)
            end_lr: Final learning rate (default: 0.0)
        """
        super().__init__(optimizer)
        self.total_steps = total_steps
        self.power = power
        self.end_lr = end_lr
        self.base_lr = optimizer.lr

    fn get_lr() -> f64:
        """Calculate current learning rate."""
        if self.last_epoch >= self.total_steps:
            return self.end_lr

        val decay = (1.0 - self.last_epoch as f64 / self.total_steps as f64) ** self.power
        return (self.base_lr - self.end_lr) * decay + self.end_lr


# ============================================================================
# Helper Functions
# ============================================================================

fn _max(a: f64, b: f64) -> f64:
    """Return maximum of two values."""
    if a > b: a else: b

fn _cos(x: f64) -> f64:
    """Compute cosine."""
    return @rt_torch_cos(x)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_optimizer_set_lr(optimizer: u64, lr: f64) -> i32
extern fn rt_torch_cos(x: f64) -> f64
