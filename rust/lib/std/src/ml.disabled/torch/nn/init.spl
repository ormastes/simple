# Weight Initialization
#
# Provides weight initialization methods for neural network layers.
#
# ## Functions
# - `xavier_uniform`: Xavier/Glorot uniform initialization
# - `xavier_normal`: Xavier/Glorot normal initialization
# - `kaiming_uniform`: Kaiming/He uniform initialization
# - `kaiming_normal`: Kaiming/He normal initialization
# - `uniform`: Uniform distribution initialization
# - `normal`: Normal distribution initialization
# - `zeros`: Initialize with zeros
# - `ones`: Initialize with ones
# - `constant`: Initialize with constant value
#
# ## Example
# ```simple
# import ml.torch.nn.init as init
# import ml.torch as torch
#
# # Xavier initialization (good for sigmoid/tanh activations)
# val weight = torch.empty([256, 512])
# init.xavier_uniform(weight)
#
# # Kaiming initialization (good for ReLU activations)
# val weight = torch.empty([256, 512])
# init.kaiming_normal(weight, mode="fan_in", nonlinearity="relu")
# ```

export xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, uniform, normal, zeros, ones, constant, orthogonal, sparse


use ml.torch.tensor_class.{Tensor}


# ============================================================================
# Xavier/Glorot Initialization
# ============================================================================

fn xavier_uniform(tensor: Tensor, gain: f64 = 1.0) -> Tensor:
    """Fill tensor with Xavier uniform distribution.

    Also known as Glorot initialization. Designed to keep the scale
    of gradients roughly the same across layers.

    Values are drawn from U(-a, a) where:
        a = gain * sqrt(6 / (fan_in + fan_out))

    Args:
        tensor: Tensor to initialize
        gain: Scaling factor (default: 1.0)

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.xavier_uniform(weight)
        ```
    """
    val handle = @rt_torch_init_xavier_uniform(tensor.handle, gain)
    if handle == 0:
        panic("Xavier uniform initialization failed")
    return Tensor(handle)


fn xavier_normal(tensor: Tensor, gain: f64 = 1.0) -> Tensor:
    """Fill tensor with Xavier normal distribution.

    Values are drawn from N(0, std²) where:
        std = gain * sqrt(2 / (fan_in + fan_out))

    Args:
        tensor: Tensor to initialize
        gain: Scaling factor (default: 1.0)

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.xavier_normal(weight)
        ```
    """
    val handle = @rt_torch_init_xavier_normal(tensor.handle, gain)
    if handle == 0:
        panic("Xavier normal initialization failed")
    return Tensor(handle)


# ============================================================================
# Kaiming/He Initialization
# ============================================================================

fn kaiming_uniform(
    tensor: Tensor,
    a: f64 = 0.0,
    mode: str = "fan_in",
    nonlinearity: str = "leaky_relu"
) -> Tensor:
    """Fill tensor with Kaiming uniform distribution.

    Also known as He initialization. Designed for ReLU and variants.

    Values are drawn from U(-bound, bound) where:
        bound = gain * sqrt(3 / fan_mode)

    Args:
        tensor: Tensor to initialize
        a: Negative slope for leaky_relu (default: 0.0)
        mode: "fan_in" or "fan_out" (default: "fan_in")
        nonlinearity: "relu", "leaky_relu", etc. (default: "leaky_relu")

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.kaiming_uniform(weight, mode="fan_in", nonlinearity="relu")
        ```
    """
    val handle = @rt_torch_init_kaiming_uniform(
        tensor.handle,
        a,
        mode.as_ptr(),
        mode.len() as i32,
        nonlinearity.as_ptr(),
        nonlinearity.len() as i32
    )
    if handle == 0:
        panic("Kaiming uniform initialization failed")
    return Tensor(handle)


fn kaiming_normal(
    tensor: Tensor,
    a: f64 = 0.0,
    mode: str = "fan_in",
    nonlinearity: str = "leaky_relu"
) -> Tensor:
    """Fill tensor with Kaiming normal distribution.

    Values are drawn from N(0, std²) where:
        std = gain / sqrt(fan_mode)

    Args:
        tensor: Tensor to initialize
        a: Negative slope for leaky_relu (default: 0.0)
        mode: "fan_in" or "fan_out" (default: "fan_in")
        nonlinearity: "relu", "leaky_relu", etc. (default: "leaky_relu")

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.kaiming_normal(weight, mode="fan_in", nonlinearity="relu")
        ```
    """
    val handle = @rt_torch_init_kaiming_normal(
        tensor.handle,
        a,
        mode.as_ptr(),
        mode.len() as i32,
        nonlinearity.as_ptr(),
        nonlinearity.len() as i32
    )
    if handle == 0:
        panic("Kaiming normal initialization failed")
    return Tensor(handle)


# ============================================================================
# Basic Initialization
# ============================================================================

fn uniform(tensor: Tensor, low: f64 = 0.0, high: f64 = 1.0) -> Tensor:
    """Fill tensor with uniform distribution.

    Values are drawn from U(low, high).

    Args:
        tensor: Tensor to initialize
        low: Lower bound (default: 0.0)
        high: Upper bound (default: 1.0)

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.uniform(weight, low=-0.5, high=0.5)
        ```
    """
    val handle = @rt_torch_init_uniform(tensor.handle, low, high)
    if handle == 0:
        panic("Uniform initialization failed")
    return Tensor(handle)


fn normal(tensor: Tensor, mean: f64 = 0.0, std: f64 = 1.0) -> Tensor:
    """Fill tensor with normal distribution.

    Values are drawn from N(mean, std²).

    Args:
        tensor: Tensor to initialize
        mean: Mean of distribution (default: 0.0)
        std: Standard deviation (default: 1.0)

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.normal(weight, mean=0.0, std=0.02)
        ```
    """
    val handle = @rt_torch_init_normal(tensor.handle, mean, std)
    if handle == 0:
        panic("Normal initialization failed")
    return Tensor(handle)


fn zeros(tensor: Tensor) -> Tensor:
    """Fill tensor with zeros.

    Args:
        tensor: Tensor to initialize

    Returns:
        Initialized tensor

    Example:
        ```simple
        val bias = torch.empty([512])
        init.zeros(bias)
        ```
    """
    val handle = @rt_torch_init_constant(tensor.handle, 0.0)
    if handle == 0:
        panic("Zeros initialization failed")
    return Tensor(handle)


fn ones(tensor: Tensor) -> Tensor:
    """Fill tensor with ones.

    Args:
        tensor: Tensor to initialize

    Returns:
        Initialized tensor

    Example:
        ```simple
        val scale = torch.empty([512])
        init.ones(scale)
        ```
    """
    val handle = @rt_torch_init_constant(tensor.handle, 1.0)
    if handle == 0:
        panic("Ones initialization failed")
    return Tensor(handle)


fn constant(tensor: Tensor, value: f64) -> Tensor:
    """Fill tensor with constant value.

    Args:
        tensor: Tensor to initialize
        value: Constant value to fill

    Returns:
        Initialized tensor

    Example:
        ```simple
        val bias = torch.empty([512])
        init.constant(bias, 0.01)
        ```
    """
    val handle = @rt_torch_init_constant(tensor.handle, value)
    if handle == 0:
        panic("Constant initialization failed")
    return Tensor(handle)


# ============================================================================
# Advanced Initialization
# ============================================================================

fn orthogonal(tensor: Tensor, gain: f64 = 1.0) -> Tensor:
    """Fill tensor with orthogonal matrix.

    Fills the input Tensor with a (semi) orthogonal matrix.
    Good for RNNs to help with gradient flow.

    Args:
        tensor: Tensor to initialize (must be 2D)
        gain: Scaling factor (default: 1.0)

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 256])
        init.orthogonal(weight)
        ```
    """
    val handle = @rt_torch_init_orthogonal(tensor.handle, gain)
    if handle == 0:
        panic("Orthogonal initialization failed")
    return Tensor(handle)


fn sparse(tensor: Tensor, sparsity: f64, std: f64 = 0.01) -> Tensor:
    """Fill tensor as sparse matrix.

    Fills the 2D tensor with values from N(0, std²)
    where a fraction of elements equal to sparsity are set to 0.

    Args:
        tensor: Tensor to initialize (must be 2D)
        sparsity: Fraction of elements to be set to 0
        std: Standard deviation of normal distribution (default: 0.01)

    Returns:
        Initialized tensor

    Example:
        ```simple
        val weight = torch.empty([256, 512])
        init.sparse(weight, sparsity=0.9)  # 90% zeros
        ```
    """
    val handle = @rt_torch_init_sparse(tensor.handle, sparsity, std)
    if handle == 0:
        panic("Sparse initialization failed")
    return Tensor(handle)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_init_xavier_uniform(tensor: u64, gain: f64) -> u64
extern fn rt_torch_init_xavier_normal(tensor: u64, gain: f64) -> u64

extern fn rt_torch_init_kaiming_uniform(
    tensor: u64,
    a: f64,
    mode: *u8,
    mode_len: i32,
    nonlinearity: *u8,
    nonlinearity_len: i32
) -> u64

extern fn rt_torch_init_kaiming_normal(
    tensor: u64,
    a: f64,
    mode: *u8,
    mode_len: i32,
    nonlinearity: *u8,
    nonlinearity_len: i32
) -> u64

extern fn rt_torch_init_uniform(tensor: u64, low: f64, high: f64) -> u64
extern fn rt_torch_init_normal(tensor: u64, mean: f64, std: f64) -> u64
extern fn rt_torch_init_constant(tensor: u64, value: f64) -> u64
extern fn rt_torch_init_orthogonal(tensor: u64, gain: f64) -> u64
extern fn rt_torch_init_sparse(tensor: u64, sparsity: f64, std: f64) -> u64
