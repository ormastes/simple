# Distributed FFI - External Function Declarations
#
# All FFI bindings for PyTorch distributed operations.

# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_dist_is_available() -> i32

extern fn rt_torch_dist_init_process_group(
    backend_ptr: *u8, backend_len: i32,
    rank: i64, world_size: i64,
    init_method_ptr: *u8, init_method_len: i32,
    timeout_seconds: i64,
    handle_out: *u64
) -> i32

extern fn rt_torch_dist_destroy_process_group(handle: u64) -> i32

extern fn rt_torch_dist_barrier(handle: u64, timeout_seconds: i64) -> i32

extern fn rt_torch_dist_all_reduce(
    tensor_handle: u64,
    op: i32,
    pg_handle: u64,
    async: i32,
    result_handle_out: *u64
) -> i32

extern fn rt_torch_dist_all_gather(
    tensor_handle: u64,
    pg_handle: u64,
    async: i32,
    handles_out: *u64,
    max_handles: i32
) -> i32

extern fn rt_torch_dist_broadcast(
    tensor_handle: u64,
    src: i64,
    pg_handle: u64,
    async: i32,
    result_handle_out: *u64
) -> i32

extern fn rt_torch_dist_reduce_scatter(
    handles_ptr: *u64,
    count: i32,
    op: i32,
    pg_handle: u64,
    async: i32,
    result_handle_out: *u64
) -> i32

extern fn rt_torch_dist_ddp_new(
    module_handle: u64,
    device_ids_ptr: *i64,
    device_ids_len: i32,
    output_device: i64,
    broadcast_buffers: i32,
    pg_handle: u64,
    bucket_cap_mb: i64,
    find_unused_parameters: i32,
    check_reduction: i32,
    gradient_as_bucket_view: i32,
    ddp_handle_out: *u64
) -> i32

extern fn rt_torch_dist_ddp_free(handle: u64) -> i32

extern fn rt_torch_dist_ddp_set_sync(handle: u64, enable: i32) -> i32
