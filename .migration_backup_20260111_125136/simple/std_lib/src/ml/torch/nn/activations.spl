# Activation Functions
#
# Neural network activation functions for introducing non-linearity.
#
# ## Functions
# - `relu(x)` - Rectified Linear Unit: max(0, x)
# - `gelu(x)` - Gaussian Error Linear Unit (used in transformers)
# - `silu(x)` - Sigmoid Linear Unit (Swish): x * sigmoid(x)
# - `mish(x)` - Mish activation: x * tanh(softplus(x))
# - `elu(x, alpha)` - Exponential Linear Unit
# - `softplus(x, beta, threshold)` - Smooth approximation of ReLU
# - `leaky_relu(x, negative_slope)` - Leaky ReLU with slope for negatives
# - `tanh(x)` - Hyperbolic tangent
# - `sigmoid(x)` - Sigmoid: 1 / (1 + exp(-x))
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.nn as nn
#
# let x = torch.randn([10, 20])
# let activated = nn.relu(x)
# let gelu_out = nn.gelu(x)  # For transformer models
# ```

export relu, gelu, silu, mish, elu, softplus, leaky_relu, tanh, sigmoid

import ml.torch.tensor_class.{Tensor}


# ============================================================================
# Activation Functions
# ============================================================================

fn relu(x: Tensor) -> Tensor:
    """Rectified Linear Unit: max(0, x).

    Args:
        x: Input tensor

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_relu(x.handle)
    if handle == 0:
        panic("ReLU failed")
    return Tensor(handle)


fn gelu(x: Tensor) -> Tensor:
    """Gaussian Error Linear Unit.

    GELU(x) = x * Φ(x) where Φ is the Gaussian CDF.
    Commonly used in transformers (BERT, GPT).

    Args:
        x: Input tensor

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_gelu(x.handle)
    if handle == 0:
        panic("GELU failed")
    return Tensor(handle)


fn silu(x: Tensor) -> Tensor:
    """Sigmoid Linear Unit (Swish): x * sigmoid(x).

    Args:
        x: Input tensor

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_silu(x.handle)
    if handle == 0:
        panic("SiLU failed")
    return Tensor(handle)


fn mish(x: Tensor) -> Tensor:
    """Mish activation: x * tanh(softplus(x)).

    Args:
        x: Input tensor

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_mish(x.handle)
    if handle == 0:
        panic("Mish failed")
    return Tensor(handle)


fn elu(x: Tensor, alpha: f64 = 1.0) -> Tensor:
    """Exponential Linear Unit.

    ELU(x) = max(0, x) + min(0, alpha * (exp(x) - 1))

    Args:
        x: Input tensor
        alpha: Scale for negative values (default: 1.0)

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_elu(x.handle, alpha)
    if handle == 0:
        panic("ELU failed")
    return Tensor(handle)


fn softplus(x: Tensor, beta: f64 = 1.0, threshold: f64 = 20.0) -> Tensor:
    """Softplus: log(1 + exp(beta * x)) / beta.

    Args:
        x: Input tensor
        beta: Scaling factor (default: 1.0)
        threshold: Threshold for reverting to linear (default: 20.0)

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_softplus(x.handle, beta, threshold)
    if handle == 0:
        panic("Softplus failed")
    return Tensor(handle)


fn leaky_relu(x: Tensor, negative_slope: f64 = 0.01) -> Tensor:
    """Leaky ReLU: max(0, x) + negative_slope * min(0, x).

    Args:
        x: Input tensor
        negative_slope: Slope for negative values (default: 0.01)

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_leaky_relu(x.handle, negative_slope)
    if handle == 0:
        panic("LeakyReLU failed")
    return Tensor(handle)


fn tanh(x: Tensor) -> Tensor:
    """Hyperbolic tangent: tanh(x).

    Args:
        x: Input tensor

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_tanh(x.handle)
    if handle == 0:
        panic("Tanh failed")
    return Tensor(handle)


fn sigmoid(x: Tensor) -> Tensor:
    """Sigmoid: 1 / (1 + exp(-x)).

    Args:
        x: Input tensor

    Returns:
        Output tensor (same shape as input)
    """
    let handle = @rt_torch_sigmoid(x.handle)
    if handle == 0:
        panic("Sigmoid failed")
    return Tensor(handle)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_relu(x: u64) -> u64
extern fn rt_torch_gelu(x: u64) -> u64
extern fn rt_torch_silu(x: u64) -> u64
extern fn rt_torch_mish(x: u64) -> u64
extern fn rt_torch_elu(x: u64, alpha: f64) -> u64
extern fn rt_torch_softplus(x: u64, beta: f64, threshold: f64) -> u64
extern fn rt_torch_leaky_relu(x: u64, negative_slope: f64) -> u64
extern fn rt_torch_tanh(x: u64) -> u64
extern fn rt_torch_sigmoid(x: u64) -> u64
