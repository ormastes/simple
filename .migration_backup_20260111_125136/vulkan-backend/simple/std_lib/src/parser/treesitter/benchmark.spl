# Tree-sitter Performance Benchmarking
# Measure parsing and query performance

import sys.time as time
import parser.treesitter.{TreeSitterParser, Query, QueryCursor}

# Benchmark result
class BenchmarkResult:
    name: String
    iterations: Int
    total_time_ms: Float
    avg_time_ms: Float
    min_time_ms: Float
    max_time_ms: Float
    lines_of_code: Int

    fn new(name: String, iterations: Int, times_ms: List<Float>, lines: Int) -> BenchmarkResult:
        let total = times_ms.sum()
        let avg = total / iterations
        let min_time = times_ms.min().unwrap_or(0.0)
        let max_time = times_ms.max().unwrap_or(0.0)

        BenchmarkResult(
            name: name,
            iterations: iterations,
            total_time_ms: total,
            avg_time_ms: avg,
            min_time_ms: min_time,
            max_time_ms: max_time,
            lines_of_code: lines
        )

    fn print_summary():
        print(f"\n=== {self.name} ===")
        print(f"Iterations: {self.iterations}")
        print(f"Lines of code: {self.lines_of_code}")
        print(f"Average time: {self.avg_time_ms:.2f} ms")
        print(f"Min time: {self.min_time_ms:.2f} ms")
        print(f"Max time: {self.max_time_ms:.2f} ms")
        print(f"Total time: {self.total_time_ms:.2f} ms")

        if self.lines_of_code > 0:
            let lines_per_ms = self.lines_of_code / self.avg_time_ms
            print(f"Throughput: {lines_per_ms:.0f} lines/ms")

# Benchmark parsing performance
fn benchmark_parse(source: String, iterations: Int) -> BenchmarkResult:
    let parser = TreeSitterParser.new("simple").unwrap()
    let mut times_ms: List<Float> = []
    let line_count = source.split("\n").len()

    for i in 0..iterations:
        let start = time.now_ms()
        let tree = parser.parse(source).unwrap()
        let end = time.now_ms()

        let elapsed = end - start
        times_ms.push(elapsed)

    BenchmarkResult.new("Parse", iterations, times_ms, line_count)

# Benchmark incremental parsing performance
fn benchmark_incremental_parse(
    original: String,
    modified: String,
    iterations: Int
) -> BenchmarkResult:
    let parser = TreeSitterParser.new("simple").unwrap()
    let line_count = modified.split("\n").len()

    # Pre-parse original
    let old_tree = parser.parse(original).unwrap()

    let mut times_ms: List<Float> = []

    for i in 0..iterations:
        # Compute edits
        let edits = parser.treesitter.edits.compute_edits(original, modified)

        let start = time.now_ms()
        let new_tree = parser.parse_incremental(modified, old_tree, edits).unwrap()
        let end = time.now_ms()

        let elapsed = end - start
        times_ms.push(elapsed)

    BenchmarkResult.new("Incremental Parse", iterations, times_ms, line_count)

# Benchmark query execution performance
fn benchmark_query(source: String, iterations: Int) -> BenchmarkResult:
    let parser = TreeSitterParser.new("simple").unwrap()
    let tree = parser.parse(source).unwrap()
    let line_count = source.split("\n").len()

    # Create highlighting query
    let query = Query.new("simple", "").unwrap()

    let mut times_ms: List<Float> = []

    for i in 0..iterations:
        let start = time.now_ms()

        let cursor = QueryCursor.new(query, tree)
        let matches = cursor.all_matches()

        # Force evaluation
        let match_count = matches.len()

        let end = time.now_ms()

        let elapsed = end - start
        times_ms.push(elapsed)

    BenchmarkResult.new("Query Execution", iterations, times_ms, line_count)

# Generate test source code of specific size
fn generate_test_source(num_lines: Int) -> String:
    let mut source = ""

    # Generate functions
    for i in 0..(num_lines / 10):
        source = source + f"fn func_{i}(x: i32, y: i32):\n"
        source = source + f"    let result = x + y\n"
        source = source + f"    if result > 0:\n"
        source = source + f"        return result\n"
        source = source + f"    else:\n"
        source = source + f"        return 0\n"
        source = source + f"\n"

    # Generate variables
    for i in 0..(num_lines / 20):
        source = source + f"let var_{i} = {i}\n"

    # Generate match statements
    for i in 0..(num_lines / 30):
        source = source + f"match var_{i}:\n"
        source = source + f"    case 0: return 'zero'\n"
        source = source + f"    case 1: return 'one'\n"
        source = source + f"    case _: return 'other'\n"
        source = source + f"\n"

    source

# Run comprehensive benchmark suite
fn run_benchmarks():
    print("=== Tree-sitter Performance Benchmarks ===\n")

    # Small file (100 lines)
    print("\n--- Small File (100 lines) ---")
    let small_source = generate_test_source(100)
    let small_parse = benchmark_parse(small_source, 100)
    small_parse.print_summary()

    let small_query = benchmark_query(small_source, 100)
    small_query.print_summary()

    # Medium file (1000 lines)
    print("\n--- Medium File (1000 lines) ---")
    let medium_source = generate_test_source(1000)
    let medium_parse = benchmark_parse(medium_source, 50)
    medium_parse.print_summary()

    let medium_query = benchmark_query(medium_source, 50)
    medium_query.print_summary()

    # Large file (10000 lines)
    print("\n--- Large File (10000 lines) ---")
    let large_source = generate_test_source(10000)
    let large_parse = benchmark_parse(large_source, 10)
    large_parse.print_summary()

    let large_query = benchmark_query(large_source, 10)
    large_query.print_summary()

    # Incremental parsing benchmark
    print("\n--- Incremental Parsing (1-line edit in 1000-line file) ---")
    let original = generate_test_source(1000)
    let modified = original.replace("let result = x + y", "let result = x * y")
    let incremental = benchmark_incremental_parse(original, modified, 100)
    incremental.print_summary()

    # Performance targets
    print("\n=== Performance Targets ===")
    print("Target for 1000 lines: < 20ms")
    if medium_parse.avg_time_ms < 20.0:
        print(f"✅ PASS: {medium_parse.avg_time_ms:.2f} ms < 20 ms")
    else:
        print(f"❌ FAIL: {medium_parse.avg_time_ms:.2f} ms >= 20 ms")

    print("\nTarget for incremental parse: < 5ms")
    if incremental.avg_time_ms < 5.0:
        print(f"✅ PASS: {incremental.avg_time_ms:.2f} ms < 5 ms")
    else:
        print(f"❌ FAIL: {incremental.avg_time_ms:.2f} ms >= 5 ms")

# Hot spot profiling
fn profile_hot_spots(source: String):
    print("\n=== Profiling Hot Spots ===")

    let parser = TreeSitterParser.new("simple").unwrap()

    # Profile parsing phases
    let start_total = time.now_ms()

    let start_lex = time.now_ms()
    # Lexing happens inside parse
    let end_lex = time.now_ms()

    let start_parse = time.now_ms()
    let tree = parser.parse(source).unwrap()
    let end_parse = time.now_ms()

    let start_query = time.now_ms()
    let query = Query.new("simple", "").unwrap()
    let cursor = QueryCursor.new(query, tree)
    let matches = cursor.all_matches()
    let end_query = time.now_ms()

    let end_total = time.now_ms()

    print(f"Total time: {end_total - start_total:.2f} ms")
    print(f"Parsing: {end_parse - start_parse:.2f} ms")
    print(f"Query execution: {end_query - start_query:.2f} ms")

    # Breakdown
    let parse_percent = ((end_parse - start_parse) / (end_total - start_total)) * 100.0
    let query_percent = ((end_query - start_query) / (end_total - start_total)) * 100.0

    print(f"\nBreakdown:")
    print(f"Parsing: {parse_percent:.1f}%")
    print(f"Query: {query_percent:.1f}%")
