# Run All Training Phases (Real CUDA)
#
# MedGemma Korean progressive training pipeline.
# Full 7-phase pipeline matching Note.md phases 0-5.
#
# Phase 0: Prepare English medical training data (baseline)
# Phase 1: Evaluate score, train with LoRA if below 97%
# Phase 2: MCQ training (medical reasoning)
# Phase 3: Create Korean/English embeddings + RoPE (Note.md Phase 2)
# Phase 4: Korean text training with per-layer CE (Note.md Phase 3)
# Phase 5: Korean translation with front/back layers (Note.md Phase 4)
# Phase 6: Korean medical exam reasoning (Note.md Phase 5)
#
# Each phase: freeze previous -> add LoRA -> train -> merge -> validate.
# 97% English retention threshold checked after each phase.
#
# Usage:
#   LD_PRELOAD=build/libspl_torch.so bin/simple examples/projects/medgemma_korean/run_all.spl

use src.model.{TextModel, check_gpu}
use src.shared_logic.{TrainConfig, report_gpu_memory}
use src.train_phase0.{run_phase0}
use src.train_phase1.{run_phase1}
use src.train_phase2.{run_phase2}
use src.train_phase3.{run_phase3}
use src.train_phase4.{run_phase4}
use src.train_phase5.{run_phase5}
use src.train_phase6.{run_phase6}
use src.validation.{validate_all_phases, check_english_retention, validate_all_phases_extended}
use src.layer_utils.{ExtendedModel}
use src.lora_utils.{LoRAConfig, LoRAAdapter, create_lora_adapter}

use std.gc_async_mut.torch.mod.{torch_available, cuda_available}
use std.torch.ffi.{
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_max_memory_allocated,
    rt_torch_cuda_empty_cache
}


fn print_header(title: text):
    print("")
    print("=" * 70)
    print("  {title}")
    print("=" * 70)
    print("")


fn main():
    print_header("MEDGEMMA KOREAN - FULL 7-PHASE PIPELINE (REAL CUDA)")

    # Check prerequisites
    if not torch_available():
        print("ERROR: libtorch not available.")
        print("Build: bash scripts/build/build-torch-ffi.sh")
        print("Run:   LD_PRELOAD=build/libspl_torch.so bin/simple run_all.spl")
        return

    if not cuda_available():
        print("ERROR: CUDA not available. GPU required.")
        return

    check_gpu()
    print("")

    print("Pipeline: Progressive LoRA training (Note.md Phases 0-5)")
    print("  Phase 0: English medical baseline (prevent forgetting)")
    print("  Phase 1: Evaluate + train English medical knowledge")
    print("  Phase 2: MCQ medical reasoning")
    print("  Phase 3: Korean/English embeddings + RoPE")
    print("  Phase 4: Korean text + per-layer CE loss")
    print("  Phase 5: Korean translation + front/back layers")
    print("  Phase 6: Korean medical exam reasoning")
    print("")
    print("Each phase: freeze -> LoRA -> train -> merge -> validate")
    print("97% English retention threshold checked after each phase.")
    print("")

    # ========================================================================
    # Phase 0: English Medical Baseline
    # ========================================================================
    var model = run_phase0()
    validate_all_phases(model, 0)

    # ========================================================================
    # Phase 1: Evaluate + Train (97% threshold check)
    # ========================================================================
    model = run_phase1(model)
    validate_all_phases(model, 1)

    # ========================================================================
    # Phase 2: MCQ (Medical Reasoning)
    # ========================================================================
    model = run_phase2(model)
    validate_all_phases(model, 2)

    # ========================================================================
    # Phase 3: Create Embeddings (Note.md Phase 2)
    # ========================================================================
    var ext_model = run_phase3(model)

    # ========================================================================
    # Phase 4: Korean Text Training (Note.md Phase 3)
    # ========================================================================
    ext_model = run_phase4(ext_model)

    # ========================================================================
    # Phase 5: Korean Translation (Note.md Phase 4)
    # ========================================================================
    ext_model = run_phase5(ext_model)

    # ========================================================================
    # Phase 6: Korean Medical Exam Reasoning (Note.md Phase 5)
    # ========================================================================
    ext_model = run_phase6(ext_model)

    # ========================================================================
    # Pipeline Complete
    # ========================================================================
    print_header("PIPELINE COMPLETE - ALL 7 PHASES TRAINED ON CUDA")

    print("Final model capabilities (all merged into base weights):")
    print("  Phase 0: English medical baseline")
    print("  Phase 1: English medical LoRA (if needed)")
    print("  Phase 2: Medical reasoning (MCQ)")
    print("  Phase 3: Korean/English dual embeddings + RoPE")
    print("  Phase 4: Korean text knowledge")
    print("  Phase 5: Korean translation (front/back layers)")
    print("  Phase 6: Korean medical exam reasoning")
    print("")

    # Final English retention check
    val config = TrainConfig.default_config()
    val retained = check_english_retention(ext_model.base, config)
    if retained:
        print("FINAL: English knowledge retained (>= 95%)")
    else:
        print("WARNING: English knowledge degraded below 95%!")
    print("")

    ext_model.print_summary()
    report_gpu_memory()

    val peak_mem = rt_torch_cuda_max_memory_allocated(0)
    val peak_mb = peak_mem / (1024 * 1024)
    print("Peak GPU memory: {peak_mb}MB")

    rt_torch_cuda_empty_cache()
    print("")
    print("Training complete. Full 7-phase Korean medical reasoning pipeline finished.")


main()
