#!/usr/bin/env simple
# Phase 1: Evaluate and Train English Medical Knowledge
#
# Goal: Check baseline score, train with LoRA if below 97% threshold.
#
# Note.md Phase 1: "small medgemma, large medgemma abilities"
#   - Prepare validation exam in English
#   - Check score of model on English medical MCQ
#   - If score >= 97%: proceed to Phase 2
#   - If score < 97%: add LoRA_1, train with cross-entropy loss
#
# Note.md Phase 1.1: "if neither small or large can not meet 97%"
#   - Add LoRA_1
#   - Translate train data to English too
#   - Train with cross-entropy loss
#
# Data: data/pretokenized/phase1_english_val.jsonl
#   Falls back to synthetic data if file not found.
#
# Run:
#   LD_PRELOAD=build/libspl_torch.so bin/simple examples/projects/medgemma_korean/src/train_phase1.spl

use model.{TextModel, ModelConfig, compute_mse_loss, check_gpu}
use shared_logic.{
    TrainConfig, TrainState,
    make_randn_batch_on_cuda,
    compute_ce_loss,
    report_gpu_memory, report_training_step, report_epoch_end,
    lr_linear_warmup
}
use data_loader.{DataLoader, generate_synthetic_batch}
use validation.{validate_model, validate_accuracy, ValidationResult}
use lora_utils.{
    LoRAConfig, LoRAAdapter,
    create_lora_adapter, lora_forward,
    lora_sgd_step, lora_zero_grads,
    merge_lora_into_model
}
use train_phase0.{run_phase0}

use std.gc_async_mut.torch.mod.{torch_available, cuda_available}
use std.torch.ffi.{
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_mean,
    rt_torch_autograd_backward,
    rt_torch_cuda_empty_cache
}

export run_phase1

# ============================================================================
# Constants
# ============================================================================

val ACCURACY_THRESHOLD = 0.97
val ENGLISH_RETENTION_THRESHOLD = 0.95

# ============================================================================
# Phase 1 Training
# ============================================================================

fn run_phase1(model: TextModel) -> TextModel:
    print("")
    print("=" * 70)
    print("  PHASE 1: EVALUATE + TRAIN ENGLISH MEDICAL KNOWLEDGE")
    print("=" * 70)
    print("")

    check_gpu()
    print("")

    # Configuration
    val config = TrainConfig.phase1()

    # Step 1: Evaluate baseline accuracy on English medical MCQ
    print("Step 1: Evaluating baseline model on English medical validation...")
    val baseline_result = validate_model(model, "English medical baseline", 100.0)
    print("")

    # Step 2: Check if accuracy meets threshold
    # For now, with synthetic data, accuracy won't be meaningful.
    # In production, this would use real MCQ validation data.
    val baseline_accuracy = validate_accuracy(model, config)
    print("Baseline accuracy: {baseline_accuracy}")
    print("Threshold: {ACCURACY_THRESHOLD}")
    print("")

    if baseline_accuracy >= ACCURACY_THRESHOLD:
        print("PASS: Baseline accuracy >= 97%. Skipping Phase 1 training.")
        print("Model is ready for Phase 2.")
        return model

    # Step 3: Accuracy below threshold â€” add LoRA_1 and train
    print("BELOW THRESHOLD: Adding LoRA_1 for English medical training")
    print("")
    config.print_config("Phase 1 - English Medical LoRA")

    # Freeze base weights (preserves Phase 0 knowledge)
    model.freeze()
    print("Base model weights: FROZEN (Phase 0 knowledge preserved)")

    # Add LoRA_1 adapters
    val lora_config = LoRAConfig(rank: config.lora_rank, alpha: config.lora_alpha, dropout: 0.0)
    print("")
    print("Adding LoRA_1 adapters (Progressive LoRA):")
    var lora_l1 = create_lora_adapter(config.input_dim, config.hidden_dim, lora_config)
    var lora_l2 = create_lora_adapter(config.hidden_dim, config.hidden_dim, lora_config)
    print("")

    # Training loop
    var state = TrainState.create()
    for epoch in 0..config.num_epochs:
        state.reset_epoch()
        print("--- Epoch {epoch + 1}/{config.num_epochs} ---")

        for batch_idx in 0..config.num_batches:
            val lr = lr_linear_warmup(state.global_step, config.warmup_steps, config.learning_rate)

            # Generate batch
            val batch = make_randn_batch_on_cuda(config.batch_size, config.input_dim, config.output_dim)
            val x = batch[0]
            val target = batch[1]

            # Forward pass with LoRA_1
            val logits = lora_forward(model, x, lora_l1, lora_l2)

            # MSE loss (cross-entropy would require integer target tensors)
            val loss_h = compute_mse_loss(logits, target)
            val loss_val = rt_torch_torchtensor_mean(loss_h)

            # Backward
            rt_torch_autograd_backward(loss_h)

            # SGD (only LoRA_1 params, base is frozen)
            lora_sgd_step(lora_l1, lora_l2, lr)

            # Zero grads
            lora_zero_grads(lora_l1, lora_l2)
            model.zero_grads()

            # Cleanup
            rt_torch_torchtensor_free(x)
            rt_torch_torchtensor_free(target)
            rt_torch_torchtensor_free(logits)
            rt_torch_torchtensor_free(loss_h)

            state.add_loss(loss_val)
            report_training_step(state, config)

        state.update_best()
        report_epoch_end(epoch, config.num_epochs, state)

        # Validate English retention after each epoch
        val retention = validate_model(model, "English retention (epoch {epoch + 1})", 100.0)
        print("")

    print("=" * 70)
    print("PHASE 1 TRAINING COMPLETE")
    print("  Base: Phase 0 knowledge (FROZEN)")
    print("  LoRA_1: English medical (TRAINED)")
    print("  Best loss: {state.best_loss}")
    print("=" * 70)

    # Merge LoRA_1 into base weights
    merge_lora_into_model(model, lora_l1, lora_l2)

    # Final validation
    print("")
    print("Post-training validation:")
    val final_result = validate_model(model, "Post-Phase 1 English medical", 100.0)

    report_gpu_memory()
    model


# ============================================================================
# Standalone entry point
# ============================================================================

fn main():
    if not torch_available():
        print("ERROR: libtorch not loaded. Run with:")
        print("  LD_PRELOAD=build/libspl_torch.so bin/simple <this_file>")
        return

    # Run Phase 0 first, then Phase 1
    print("Running Phase 0 first (prerequisite)...")
    var model = run_phase0()
    print("")
    val model_1 = run_phase1(model)
    print("")
    print("Phase 1 complete. Model has Phase 0 + Phase 1 knowledge.")
    rt_torch_cuda_empty_cache()


main()
