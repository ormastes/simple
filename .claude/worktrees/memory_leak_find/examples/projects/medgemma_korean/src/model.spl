# TextModel - Real Neural Network for MedGemma Korean Training
#
# A feed-forward language model that runs on CUDA via libtorch FFI.
# Used across all progressive LoRA training phases.
#
# Architecture:
#   Input [batch, input_dim]
#   -> Linear(input_dim, hidden_dim) -> GELU
#   -> Linear(hidden_dim, hidden_dim) -> GELU
#   -> Linear(hidden_dim, output_dim)
#   -> Output [batch, output_dim]
#
# Requirements:
#   - libspl_torch.so must be built and loaded
#   - CUDA GPU required

use std.gc_async_mut.torch.mod.{
    Tensor,
    torch_available,
    cuda_available
}

use std.torch.ffi.{
    rt_torch_tensor_zeros,
    rt_torch_tensor_ones,
    rt_torch_tensor_randn,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_neg,
    rt_torch_torchtensor_matmul,
    rt_torch_torchtensor_relu,
    rt_torch_torchtensor_gelu,
    rt_torch_torchtensor_softmax,
    rt_torch_torchtensor_log_softmax,
    rt_torch_torchtensor_sum,
    rt_torch_torchtensor_sum_dim,
    rt_torch_torchtensor_mean,
    rt_torch_torchtensor_mean_dim,
    rt_torch_torchtensor_transpose,
    rt_torch_torchtensor_reshape,
    rt_torch_torchtensor_cuda,
    rt_torch_torchtensor_cpu,
    rt_torch_torchtensor_is_cuda,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_ndim,
    rt_torch_torchtensor_numel,
    rt_torch_torchtensor_shape,
    rt_torch_autograd_set_requires_grad,
    rt_torch_autograd_requires_grad,
    rt_torch_autograd_grad,
    rt_torch_autograd_backward,
    rt_torch_autograd_zero_grad,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end,
    rt_torch_cuda_memory_allocated,
    rt_torch_cuda_max_memory_allocated,
    rt_torch_cuda_empty_cache
}

export ModelConfig, TextModel, compute_mse_loss, sgd_update_tensor, check_gpu

# ============================================================================
# Model Configuration
# ============================================================================

class ModelConfig:
    input_dim: i64
    hidden_dim: i64
    output_dim: i64
    init_scale: f64
    num_layers: i64

    static fn default_config() -> ModelConfig:
        ModelConfig(
            input_dim: 64,
            hidden_dim: 128,
            output_dim: 32,
            init_scale: 0.1,
            num_layers: 3
        )

    static fn from_dims(input_dim: i64, hidden_dim: i64, output_dim: i64) -> ModelConfig:
        ModelConfig(
            input_dim: input_dim,
            hidden_dim: hidden_dim,
            output_dim: output_dim,
            init_scale: 0.1,
            num_layers: 3
        )

    fn total_params() -> i64:
        val l1 = self.input_dim * self.hidden_dim + self.hidden_dim
        val l2 = self.hidden_dim * self.hidden_dim + self.hidden_dim
        val l3 = self.hidden_dim * self.output_dim + self.output_dim
        l1 + l2 + l3

    fn print_config():
        print("ModelConfig:")
        print("  Input dim:  {self.input_dim}")
        print("  Hidden dim: {self.hidden_dim}")
        print("  Output dim: {self.output_dim}")
        print("  Layers: {self.num_layers}")
        print("  Total params: {self.total_params()}")


# ============================================================================
# GPU Check
# ============================================================================

fn check_gpu():
    if not torch_available():
        print("ERROR: libtorch not available. Build with:")
        print("  bash scripts/build/build-torch-ffi.sh")
        print("Run with:")
        print("  LD_PRELOAD=build/libspl_torch.so bin/simple <script>")
        return
    if not cuda_available():
        print("ERROR: CUDA not available. GPU required for training.")
        return
    print("GPU: CUDA available")
    val mem = rt_torch_cuda_memory_allocated(0)
    print("  Allocated: {mem} bytes")

# ============================================================================
# Utility: move tensor handle to CUDA
# ============================================================================

fn to_cuda(h: i64) -> i64:
    rt_torch_torchtensor_cuda(h, 0)

fn make_param(dims: [i64], init_scale: f64) -> i64:
    val h = rt_torch_tensor_randn(dims)
    val h_gpu = to_cuda(h)
    rt_torch_torchtensor_free(h)
    val h_scaled = rt_torch_torchtensor_mul_scalar(h_gpu, init_scale)
    rt_torch_torchtensor_free(h_gpu)
    rt_torch_autograd_set_requires_grad(h_scaled, true)
    h_scaled

fn make_zeros_param(dims: [i64]) -> i64:
    val h = rt_torch_tensor_zeros(dims)
    val h_gpu = to_cuda(h)
    rt_torch_torchtensor_free(h)
    rt_torch_autograd_set_requires_grad(h_gpu, true)
    h_gpu

# ============================================================================
# TextModel - 3-layer feed-forward network
# ============================================================================

class TextModel:
    # Layer 1 weights
    w1: i64
    b1: i64
    # Layer 2 weights
    w2: i64
    b2: i64
    # Output layer weights
    w3: i64
    b3: i64
    # Configuration
    config: ModelConfig

    static fn create(in_dim: i64, hid_dim: i64, out_dim: i64) -> TextModel:
        val cfg = ModelConfig.from_dims(in_dim, hid_dim, out_dim)
        TextModel.create_from_config(cfg)

    static fn create_from_config(cfg: ModelConfig) -> TextModel:
        print("Creating TextModel({cfg.input_dim} -> {cfg.hidden_dim} -> {cfg.hidden_dim} -> {cfg.output_dim}) on CUDA")
        val w1 = make_param([cfg.hidden_dim, cfg.input_dim], cfg.init_scale)
        val b1 = make_zeros_param([cfg.hidden_dim])
        val w2 = make_param([cfg.hidden_dim, cfg.hidden_dim], cfg.init_scale)
        val b2 = make_zeros_param([cfg.hidden_dim])
        val w3 = make_param([cfg.output_dim, cfg.hidden_dim], cfg.init_scale)
        val b3 = make_zeros_param([cfg.output_dim])
        print("  Total parameters: {cfg.total_params()}")
        TextModel(
            w1: w1, b1: b1,
            w2: w2, b2: b2,
            w3: w3, b3: b3,
            config: cfg
        )

    fn forward(x: i64) -> i64:
        # Layer 1: x @ W1^T + b1 -> GELU
        val w1t = rt_torch_torchtensor_transpose(self.w1, 0, 1)
        val xw1 = rt_torch_torchtensor_matmul(x, w1t)
        rt_torch_torchtensor_free(w1t)
        val h1_pre = rt_torch_torchtensor_add(xw1, self.b1)
        rt_torch_torchtensor_free(xw1)
        val h1 = rt_torch_torchtensor_gelu(h1_pre)
        rt_torch_torchtensor_free(h1_pre)

        # Layer 2: h1 @ W2^T + b2 -> GELU
        val w2t = rt_torch_torchtensor_transpose(self.w2, 0, 1)
        val hw2 = rt_torch_torchtensor_matmul(h1, w2t)
        rt_torch_torchtensor_free(w2t)
        rt_torch_torchtensor_free(h1)
        val h2_pre = rt_torch_torchtensor_add(hw2, self.b2)
        rt_torch_torchtensor_free(hw2)
        val h2 = rt_torch_torchtensor_gelu(h2_pre)
        rt_torch_torchtensor_free(h2_pre)

        # Output layer: h2 @ W3^T + b3
        val w3t = rt_torch_torchtensor_transpose(self.w3, 0, 1)
        val hw3 = rt_torch_torchtensor_matmul(h2, w3t)
        rt_torch_torchtensor_free(w3t)
        rt_torch_torchtensor_free(h2)
        val out = rt_torch_torchtensor_add(hw3, self.b3)
        rt_torch_torchtensor_free(hw3)
        out

    fn param_handles() -> [i64]:
        [self.w1, self.b1, self.w2, self.b2, self.w3, self.b3]

    fn param_count() -> i64:
        self.config.total_params()

    me sgd_step(lr: f64):
        rt_torch_autograd_no_grad_begin()
        self.w1 = sgd_update_tensor(self.w1, lr)
        self.b1 = sgd_update_tensor(self.b1, lr)
        self.w2 = sgd_update_tensor(self.w2, lr)
        self.b2 = sgd_update_tensor(self.b2, lr)
        self.w3 = sgd_update_tensor(self.w3, lr)
        self.b3 = sgd_update_tensor(self.b3, lr)
        rt_torch_autograd_no_grad_end()
        rt_torch_autograd_set_requires_grad(self.w1, true)
        rt_torch_autograd_set_requires_grad(self.b1, true)
        rt_torch_autograd_set_requires_grad(self.w2, true)
        rt_torch_autograd_set_requires_grad(self.b2, true)
        rt_torch_autograd_set_requires_grad(self.w3, true)
        rt_torch_autograd_set_requires_grad(self.b3, true)

    fn zero_grads():
        rt_torch_autograd_zero_grad(self.w1)
        rt_torch_autograd_zero_grad(self.b1)
        rt_torch_autograd_zero_grad(self.w2)
        rt_torch_autograd_zero_grad(self.b2)
        rt_torch_autograd_zero_grad(self.w3)
        rt_torch_autograd_zero_grad(self.b3)

    me freeze():
        rt_torch_autograd_set_requires_grad(self.w1, false)
        rt_torch_autograd_set_requires_grad(self.b1, false)
        rt_torch_autograd_set_requires_grad(self.w2, false)
        rt_torch_autograd_set_requires_grad(self.b2, false)
        rt_torch_autograd_set_requires_grad(self.w3, false)
        rt_torch_autograd_set_requires_grad(self.b3, false)

    me unfreeze():
        rt_torch_autograd_set_requires_grad(self.w1, true)
        rt_torch_autograd_set_requires_grad(self.b1, true)
        rt_torch_autograd_set_requires_grad(self.w2, true)
        rt_torch_autograd_set_requires_grad(self.b2, true)
        rt_torch_autograd_set_requires_grad(self.w3, true)
        rt_torch_autograd_set_requires_grad(self.b3, true)

    fn print_summary():
        print("TextModel Summary:")
        print("  Architecture: {self.config.input_dim} -> {self.config.hidden_dim} -> {self.config.hidden_dim} -> {self.config.output_dim}")
        print("  Parameters: {self.config.total_params()}")
        val on_gpu = rt_torch_torchtensor_is_cuda(self.w1)
        if on_gpu:
            print("  Device: CUDA")
        else:
            print("  Device: CPU")


# ============================================================================
# Checkpoint Stubs
# ============================================================================

# Save model checkpoint (stub - requires tensor serialization FFI).
# For now, just logs the action.
fn save_checkpoint(model: TextModel, path: text):
    print("CHECKPOINT: Would save model to {path}")
    print("  (Tensor serialization not yet available in FFI)")
    print("  Model config: {model.config.input_dim}x{model.config.hidden_dim}x{model.config.output_dim}")
    print("  Params: {model.config.total_params()}")

# Load model checkpoint (stub - requires tensor deserialization FFI).
fn load_checkpoint(path: text, config: ModelConfig) -> TextModel:
    print("CHECKPOINT: Would load model from {path}")
    print("  (Tensor deserialization not yet available in FFI)")
    print("  Creating fresh model instead")
    TextModel.create_from_config(config)


# ============================================================================
# Loss: Differentiable MSE (returns tensor handle for backward)
# ============================================================================

fn compute_mse_loss(pred: i64, target: i64) -> i64:
    val diff = rt_torch_torchtensor_sub(pred, target)
    val sq = rt_torch_torchtensor_mul(diff, diff)
    rt_torch_torchtensor_free(diff)
    val per_sample = rt_torch_torchtensor_sum_dim(sq, 1, false)
    rt_torch_torchtensor_free(sq)
    val loss = rt_torch_torchtensor_mean_dim(per_sample, 0, false)
    rt_torch_torchtensor_free(per_sample)
    loss


# ============================================================================
# SGD: Update a single tensor parameter
# ============================================================================

fn sgd_update_tensor(param_h: i64, lr: f64) -> i64:
    val grad_h = rt_torch_autograd_grad(param_h)
    if grad_h == 0:
        return rt_torch_torchtensor_clone(param_h)
    val scaled = rt_torch_torchtensor_mul_scalar(grad_h, lr)
    val updated = rt_torch_torchtensor_sub(param_h, scaled)
    rt_torch_torchtensor_free(scaled)
    rt_torch_torchtensor_free(param_h)
    updated
