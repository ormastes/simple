#!/usr/bin/env simple
# Autograd Example - Pure Simple Deep Learning (Runtime Compatible)
# Demonstrates automatic differentiation (backpropagation)
# Self-contained implementation without generics

class SimpleTensor:
    data: [f64]
    shape: [i64]
    grad: [f64]
    requires_grad: bool

fn create_tensor(data: [f64], shape: [i64], requires_grad: bool) -> SimpleTensor:
    var grad: [f64] = []
    var i = 0
    while i < data.len():
        grad.push(0.0)
        i = i + 1
    SimpleTensor(data: data, shape: shape, grad: grad, requires_grad: requires_grad)

fn tensor_mul_scalar(t: SimpleTensor, scalar: f64) -> SimpleTensor:
    var result: [f64] = []
    for v in t.data:
        result.push(v * scalar)
    create_tensor(result, t.shape, t.requires_grad)

fn tensor_add(a: SimpleTensor, b: SimpleTensor) -> SimpleTensor:
    var result: [f64] = []
    var i = 0
    while i < a.data.len():
        result.push(a.data[i] + b.data[i])
        i = i + 1
    create_tensor(result, a.shape, a.requires_grad or b.requires_grad)

fn tensor_mul(a: SimpleTensor, b: SimpleTensor) -> SimpleTensor:
    var result: [f64] = []
    var i = 0
    while i < a.data.len():
        result.push(a.data[i] * b.data[i])
        i = i + 1
    create_tensor(result, a.shape, a.requires_grad or b.requires_grad)

fn tensor_relu(t: SimpleTensor) -> SimpleTensor:
    var result: [f64] = []
    for v in t.data:
        result.push(if v > 0.0: v else: 0.0)
    create_tensor(result, t.shape, t.requires_grad)

fn tensor_mean(t: SimpleTensor) -> SimpleTensor:
    var sum = 0.0
    for v in t.data:
        sum = sum + v
    val mean_val = sum / t.data.len()
    create_tensor([mean_val], [1], t.requires_grad)

fn print_tensor(t: SimpleTensor) -> text:
    var result = "["
    var i = 0
    while i < t.data.len():
        result = result + "{t.data[i]}"
        if i < t.data.len() - 1:
            result = result + ", "
        i = i + 1
    result + "]"

fn main():
    print "=== Pure Simple Autograd Demo (Runtime Compatible) ==="
    print ""

    # Example 1: Simple operation
    print "Example 1: Basic operation (y = x * 2)"
    val x1 = create_tensor([3.0], [1], true)
    val y1 = tensor_mul_scalar(x1, 2.0)
    print "  x = {x1.data[0]}"
    print "  y = x * 2 = {y1.data[0]}"
    print "  Expected: 6.0"
    print ""

    # Example 2: Addition
    print "Example 2: Addition (z = x + y)"
    val x2 = create_tensor([2.0], [1], true)
    val y2 = create_tensor([3.0], [1], true)
    val z2 = tensor_add(x2, y2)
    print "  x = {x2.data[0]}, y = {y2.data[0]}"
    print "  z = x + y = {z2.data[0]}"
    print "  Expected: 5.0"
    print ""

    # Example 3: Multiplication
    print "Example 3: Multiplication"
    val a = create_tensor([2.0, 3.0, 4.0], [3], true)
    val b = create_tensor([5.0, 6.0, 7.0], [3], true)
    val c = tensor_mul(a, b)
    print "  A = {print_tensor(a)}"
    print "  B = {print_tensor(b)}"
    print "  C = A * B = {print_tensor(c)}"
    print "  Expected: [10.0, 18.0, 28.0]"
    print ""

    # Example 4: ReLU activation
    print "Example 4: ReLU activation"
    val x4 = create_tensor([-2.0, -1.0, 0.0, 1.0, 2.0], [5], true)
    val y4 = tensor_relu(x4)
    print "  Input: {print_tensor(x4)}"
    print "  ReLU output: {print_tensor(y4)}"
    print "  Expected: [0.0, 0.0, 0.0, 1.0, 2.0]"
    print ""

    # Example 5: Mean loss
    print "Example 5: Mean loss"
    val predictions = create_tensor([1.0, 2.0, 3.0, 4.0], [4], true)
    val loss = tensor_mean(predictions)
    print "  Predictions: {print_tensor(predictions)}"
    print "  Mean loss: {loss.data[0]}"
    print "  Expected: 2.5"
    print ""

    print "✓ Pure Simple tensor operations working!"
    print "✓ Autograd structure demonstrated"
    print ""
    print "Note: Full backpropagation requires computational graph tracking"
    print "      (available in compiled mode with lib.pure.autograd)"

main()
