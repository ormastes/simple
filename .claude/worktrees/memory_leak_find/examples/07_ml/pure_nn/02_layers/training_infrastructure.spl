# Training Infrastructure Demo - Pure Simple Deep Learning
# Demonstrates optimizers, loss functions, and training loops

use std.pure.autograd.{Tensor}
use std.pure.nn.{Linear, ReLU, Sigmoid, Tanh, Sequential, count_parameters, zero_grad}
use std.pure.training.{
    SGD, Adam, Trainer, TrainingHistory,
    mse_loss, mae_loss,
    accuracy
)

print "=== Pure Simple Training Infrastructure Demo ==="
print ""

# Example 1: SGD vs Adam Comparison
print "Example 1: Comparing Optimizers"
print ""

val model1 = Sequential.create([
    Linear.create(3, 5),
    ReLU.create(),
    Linear.create(5, 1)
])

print "Model: 3-5-1 MLP"
print "Parameters: {count_parameters(model1)}"
print ""

# SGD
val sgd = SGD.create(model1.parameters(), lr: 0.01, momentum: 0.9)
print "SGD optimizer:"
print "  {sgd.to_string()}"

# Adam
val adam = Adam.create(model1.parameters(), lr: 0.001)
print "Adam optimizer:"
print "  {adam.to_string()}"
print ""

# Example 2: Loss Functions
print "Example 2: Loss Functions"
print ""

val predictions = Tensor.from_data([1.5, 2.3, 3.1], [3], requires_grad: true)
val targets = Tensor.from_data([2.0, 2.0, 3.0], [3], requires_grad: false)

val mse = mse_loss(predictions, targets)
print "Predictions: [1.5, 2.3, 3.1]"
print "Targets:     [2.0, 2.0, 3.0]"
print "MSE Loss: {mse.value.data[0]:.4f}"

val mae = mae_loss(predictions, targets)
print "MAE Loss: {mae.value.data[0]:.4f}"
print ""

# Example 3: Training History
print "Example 3: Training History Tracking"
print ""

val history = TrainingHistory.create()
history.add_epoch(0, 1.5)
history.add_epoch(1, 0.8)
history.add_epoch(2, 0.4)
history.add_epoch(3, 0.2)

print "Epoch history:"
for (epoch, loss) in history.epochs.zip(history.losses):
    print "  Epoch {epoch}: loss = {loss:.2f}"

print "Final loss: {history.get_final_loss()}"
print ""

# Example 4: Training Loop with Different Optimizers
print "Example 4: Training Simple Linear Model"
print ""

# Simple regression: y = 2x + 1
val model2 = Sequential.create([
    Linear.create(1, 1, bias: true)
])

val train_data = [
    (Tensor.from_data([0.0], [1, 1], requires_grad: false),
     Tensor.from_data([1.0], [1, 1], requires_grad: false)),
    (Tensor.from_data([1.0], [1, 1], requires_grad: false),
     Tensor.from_data([3.0], [1, 1], requires_grad: false)),
    (Tensor.from_data([2.0], [1, 1], requires_grad: false),
     Tensor.from_data([5.0], [1, 1], requires_grad: false))
]

print "Training data: y = 2x + 1"
print "  x=0 -> y=1"
print "  x=1 -> y=3"
print "  x=2 -> y=5"
print ""

# Train with Adam
val adam_opt = Adam.create(model2.parameters(), lr: 0.1)
val trainer = Trainer.create(model2, adam_opt, mse_loss)

print "Training with Adam (20 epochs)..."
trainer.fit(train_data, epochs: 20, verbose: false)

val h = trainer.get_history()
print "Loss progression:"
print "  Initial: {h.losses[0]:.4f}"
print "  Final:   {h.get_final_loss():.4f}"
print "  Improvement: {((1.0 - h.get_final_loss() / h.losses[0]) * 100):.1f}%"
print ""

# Example 5: Binary Classification
print "Example 5: Binary Classification (AND gate)"
print ""

val and_model = Sequential.create([
    Linear.create(2, 4),
    Tanh.create(),
    Linear.create(4, 1),
    Sigmoid.create()
])

val and_data = [
    (Tensor.from_data([0.0, 0.0], [1, 2], requires_grad: false),
     Tensor.from_data([0.0], [1, 1], requires_grad: false)),
    (Tensor.from_data([0.0, 1.0], [1, 2], requires_grad: false),
     Tensor.from_data([0.0], [1, 1], requires_grad: false)),
    (Tensor.from_data([1.0, 0.0], [1, 2], requires_grad: false),
     Tensor.from_data([0.0], [1, 1], requires_grad: false)),
    (Tensor.from_data([1.0, 1.0], [1, 2], requires_grad: false),
     Tensor.from_data([1.0], [1, 1], requires_grad: false))
]

print "AND gate truth table:"
print "  (0,0) -> 0"
print "  (0,1) -> 0"
print "  (1,0) -> 0"
print "  (1,1) -> 1"
print ""

val and_optimizer = SGD.create(and_model.parameters(), lr: 0.5, momentum: 0.9)
val and_trainer = Trainer.create(and_model, and_optimizer, mse_loss)

print "Training AND gate (50 epochs)..."
and_trainer.fit(and_data, epochs: 50, verbose: false)

print "Evaluating..."
val and_acc = and_trainer.evaluate(and_data)
print "Accuracy: {and_acc * 100:.1f}%"
print ""

# Example 6: Manual Training Step
print "Example 6: Manual Training Step (one iteration)"
print ""

val manual_model = Sequential.create([
    Linear.create(2, 1)
])

val x = Tensor.from_data([1.0, 2.0], [1, 2], requires_grad: false)
val y_true = Tensor.from_data([5.0], [1, 1], requires_grad: false)

print "1. Forward pass"
val y_pred = manual_model.forward(x)
print "   Prediction: {y_pred.value.data[0]:.4f}"

print "2. Compute loss"
val loss = mse_loss(y_pred, y_true)
print "   Loss: {loss.value.data[0]:.4f}"

print "3. Backward pass"
val manual_opt = SGD.create(manual_model.parameters(), lr: 0.1)
manual_opt.zero_grad()
backward(loss)
print "   Gradients computed"

print "4. Update weights"
manual_opt.step()
print "   Weights updated"

print "5. Check new prediction"
val y_pred_new = manual_model.forward(x)
print "   New prediction: {y_pred_new.value.data[0]:.4f}"
print "   (Should be closer to {y_true.value.data[0]})"
print ""

# Summary
print "=== Summary ==="
print "✓ SGD and Adam optimizers working"
print "✓ MSE and MAE loss functions implemented"
print "✓ Training history tracking functional"
print "✓ Trainer class simplifies training loop"
print "✓ Binary classification working (AND gate)"
print "✓ Manual training step demonstrated"
print ""
print "Next: Train on real datasets (MNIST, etc.)"
