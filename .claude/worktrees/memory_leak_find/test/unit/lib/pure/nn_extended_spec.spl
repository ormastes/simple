# Tests for extended NN layers

use std.pure.tensor.{tensor_from_data}
use std.pure.nn.{LeakyReLU, ELU, GELU, BatchNorm1d, LayerNorm}

describe "nn extended layers":
    describe "LeakyReLU":
        it "passes positive values through":
            val layer = LeakyReLU.create(0.01)
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val y = layer.forward(x)
            expect(y.data[0]).to_equal(1.0)
            expect(y.data[1]).to_equal(2.0)
            expect(y.data[2]).to_equal(3.0)

        it "scales negative values":
            val layer = LeakyReLU.create(0.1)
            val x = tensor_from_data([-10.0, -5.0], [2])
            val y = layer.forward(x)
            expect(y.data[0]).to_equal(-1.0)
            expect(y.data[1]).to_equal(-0.5)

        it "has no parameters":
            val layer = LeakyReLU.create(0.01)
            expect(layer.parameters().len()).to_equal(0)

        it "has string representation":
            val layer = LeakyReLU.create(0.01)
            expect(layer.to_string()).to_contain("LeakyReLU")

    describe "ELU":
        it "passes positive values through":
            val layer = ELU.create(1.0)
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val y = layer.forward(x)
            expect(y.data[0]).to_equal(1.0)
            expect(y.data[1]).to_equal(2.0)

        it "applies exponential to negative values":
            val layer = ELU.create(1.0)
            val x = tensor_from_data([-1.0], [1])
            val y = layer.forward(x)
            # ELU(-1) = 1.0 * (exp(-1) - 1) ≈ -0.632
            expect(y.data[0]).to_be_less_than(0.0)
            expect(y.data[0]).to_be_greater_than(-0.7)

        it "has no parameters":
            val layer = ELU.create(1.0)
            expect(layer.parameters().len()).to_equal(0)

    describe "GELU":
        it "approximates identity for large positive":
            val layer = GELU.create()
            val x = tensor_from_data([5.0], [1])
            val y = layer.forward(x)
            # GELU(5) ≈ 5.0
            expect(y.data[0]).to_be_greater_than(4.9)

        it "outputs near zero for large negative":
            val layer = GELU.create()
            val x = tensor_from_data([-5.0], [1])
            val y = layer.forward(x)
            # GELU(-5) ≈ 0
            expect(y.data[0]).to_be_greater_than(-0.1)
            expect(y.data[0]).to_be_less_than(0.1)

        it "GELU(0) = 0":
            val layer = GELU.create()
            val x = tensor_from_data([0.0], [1])
            val y = layer.forward(x)
            expect(y.data[0]).to_be_greater_than(-0.01)
            expect(y.data[0]).to_be_less_than(0.01)

    describe "BatchNorm1d":
        it "creates with correct shapes":
            val bn = BatchNorm1d.create(4, 0.00001, 0.1)
            expect(bn.weight.data.len()).to_equal(4)
            expect(bn.bias.data.len()).to_equal(4)
            expect(bn.running_mean.data.len()).to_equal(4)
            expect(bn.running_var.data.len()).to_equal(4)

        it "weight initialized to ones":
            val bn = BatchNorm1d.create(3, 0.00001, 0.1)
            expect(bn.weight.data[0]).to_equal(1.0)
            expect(bn.weight.data[1]).to_equal(1.0)
            expect(bn.weight.data[2]).to_equal(1.0)

        it "has learnable parameters":
            val bn = BatchNorm1d.create(3, 0.00001, 0.1)
            expect(bn.parameters().len()).to_equal(2)

        it "normalizes input":
            val bn = BatchNorm1d.create(3, 0.00001, 0.1)
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val y = bn.forward(x)
            # With running_mean=0, running_var=1, output ≈ weight * (x - 0) / sqrt(1 + eps) + bias
            # ≈ 1.0 * x + 0.0 = x (approximately)
            expect(y.data[0]).to_be_greater_than(0.9)
            expect(y.data[1]).to_be_greater_than(1.9)

    describe "LayerNorm":
        it "creates with correct shapes":
            val ln = LayerNorm.create(4, 0.00001)
            expect(ln.weight.data.len()).to_equal(4)
            expect(ln.bias.data.len()).to_equal(4)

        it "weight initialized to ones":
            val ln = LayerNorm.create(3, 0.00001)
            expect(ln.weight.data[0]).to_equal(1.0)

        it "has learnable parameters":
            val ln = LayerNorm.create(3, 0.00001)
            expect(ln.parameters().len()).to_equal(2)

        it "normalizes to zero mean":
            val ln = LayerNorm.create(3, 0.00001)
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val y = ln.forward(x)
            # After normalization, mean should be near 0
            val mean_val = (y.data[0] + y.data[1] + y.data[2]) / 3.0
            expect(mean_val).to_be_greater_than(-0.1)
            expect(mean_val).to_be_less_than(0.1)

    describe "mode switching":
        it "LeakyReLU train/eval":
            var layer = LeakyReLU.create(0.01)
            expect(layer.training).to_equal(true)
            layer.eval()
            expect(layer.training).to_equal(false)
            layer.train()
            expect(layer.training).to_equal(true)

        it "BatchNorm1d train/eval":
            var bn = BatchNorm1d.create(3, 0.00001, 0.1)
            expect(bn.training).to_equal(true)
            bn.eval()
            expect(bn.training).to_equal(false)
