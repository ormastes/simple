# torch Training Components
#
# Extracted from mod.spl. Contains:
# - Loss functions: MSELoss, CrossEntropyLoss
# - Optimizers: SGD, Adam, RMSprop
# - Stream: CUDA stream management
# - Sequential: Layer container
# - Utility functions: no_grad, set_seed, manual_seed

use std.gc_async_mut.torch.mod.{Tensor, Linear, Conv2d}
use std.torch.ffi.{
    rt_torch_stream_create,
    rt_torch_torchstream_sync,
    rt_torch_torchstream_query,
    rt_torch_torchstream_free,
    rt_torch_nn_mse_loss,
    rt_torch_nn_cross_entropy,
    rt_torch_autograd_grad,
    rt_torch_autograd_zero_grad,
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_add_scalar,
    rt_torch_torchtensor_div,
    rt_torch_torchtensor_sqrt,
    rt_torch_torchtensor_free,
    rt_torch_tensor_from_data,
    rt_torch_torchtensor_mean
}

# ============================================================================
# Loss Functions
# ============================================================================

class MSELoss:
    """Mean Squared Error loss

    Example:
        val criterion = MSELoss.create()
        val pred = Tensor.randn([32, 10])
        val target = Tensor.randn([32, 10])
        val loss = criterion.forward(pred, target)
    """

    static fn create() -> MSELoss:
        MSELoss()

    fn forward(pred: Tensor, target: Tensor) -> Tensor:
        """Compute MSE loss: mean((pred - target)^2).

        Uses PyTorch FFI for efficient computation.
        Falls back to element-wise computation if FFI unavailable.
        """
        # Use PyTorch's built-in MSE loss via FFI
        val loss_val = rt_torch_nn_mse_loss(pred.handle, target.handle)
        # Wrap scalar result as 0-d tensor
        Tensor.from_handle(rt_torch_tensor_from_data([loss_val], [1]))


class CrossEntropyLoss:
    """Cross-entropy loss for classification

    Example:
        val criterion = CrossEntropyLoss.create()
        val logits = Tensor.randn([32, 10])
        val targets = Tensor.zeros([32])
        val loss = criterion.forward(logits, targets)
    """

    static fn create() -> CrossEntropyLoss:
        CrossEntropyLoss()

    fn forward(logits: Tensor, targets: Tensor) -> Tensor:
        """Compute cross-entropy loss for classification.

        Uses PyTorch FFI for efficient computation.

        Args:
            logits: Predicted class scores (batch_size, num_classes)
            targets: Target class indices (batch_size,)
        """
        val loss_val = rt_torch_nn_cross_entropy(logits.handle, targets.handle)
        Tensor.from_handle(rt_torch_tensor_from_data([loss_val], [1]))


# ============================================================================
# Optimizers
# ============================================================================

class SGD:
    """Stochastic Gradient Descent optimizer

    Example:
        val model_params = [weight1, weight2, bias]
        val optimizer = SGD.create(model_params, 0, 0)
        optimizer.step()
        optimizer.zero_grad()
    """

    parameters: [Tensor]
    lr: i64
    momentum: i64
    velocities: [Tensor]

    static fn create(parameters: [Tensor], lr: i64, momentum: i64) -> SGD:
        var velocities_list = []
        var i = 0
        while i < parameters.len():
            val param = parameters[i]
            val velocity = Tensor.zeros(param.shape())
            velocities_list = velocities_list + [velocity]
            i = i + 1
        SGD(
            parameters: parameters,
            lr: lr,
            momentum: momentum,
            velocities: velocities_list
        )

    fn step():
        """Update parameters using SGD with momentum.

        For each parameter with a gradient:
            velocity = momentum * velocity + lr * grad
            param = param - velocity
        """
        var i = 0
        while i < self.parameters.len():
            val param = self.parameters[i]
            # Get gradient via FFI
            val grad_handle = rt_torch_autograd_grad(param.handle)
            if grad_handle != 0:
                # velocity = momentum * velocity + lr * grad
                val v_scaled = rt_torch_torchtensor_mul_scalar(self.velocities[i].handle, self.momentum)
                val g_scaled = rt_torch_torchtensor_mul_scalar(grad_handle, self.lr)
                val v_new_handle = rt_torch_torchtensor_add(v_scaled, g_scaled)
                rt_torch_torchtensor_free(v_scaled)
                rt_torch_torchtensor_free(g_scaled)
                # Update stored velocity
                rt_torch_torchtensor_free(self.velocities[i].handle)
                self.velocities[i] = Tensor.from_handle(v_new_handle)
                # param = param - velocity
                val new_param = rt_torch_torchtensor_sub(param.handle, v_new_handle)
                rt_torch_torchtensor_free(param.handle)
                self.parameters[i] = Tensor.from_handle(new_param)
            i = i + 1

    fn zero_grad():
        """Zero out all gradients."""
        var i = 0
        while i < self.parameters.len():
            self.parameters[i].zero_grad()
            i = i + 1


class Adam:
    """Adam optimizer

    Example:
        val optimizer = Adam.create(model_params, 0, 0, 0)
        optimizer.step()
        optimizer.zero_grad()
    """

    parameters: [Tensor]
    lr: i64
    beta1: i64
    beta2: i64
    eps: i64
    m: [Tensor]
    v: [Tensor]
    t: i64

    static fn create(parameters: [Tensor], lr: i64, beta1: i64, beta2: i64) -> Adam:
        var m_list = []
        var v_list = []
        var i = 0
        while i < parameters.len():
            val param = parameters[i]
            val m_tensor = Tensor.zeros(param.shape())
            val v_tensor = Tensor.zeros(param.shape())
            m_list = m_list + [m_tensor]
            v_list = v_list + [v_tensor]
            i = i + 1
        Adam(
            parameters: parameters,
            lr: lr,
            beta1: beta1,
            beta2: beta2,
            eps: 0,
            m: m_list,
            v: v_list,
            t: 0
        )

    fn step():
        """Update parameters using Adam algorithm.

        For each parameter with a gradient:
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad^2
            m_hat = m / (1 - beta1^t)
            v_hat = v / (1 - beta2^t)
            param = param - lr * m_hat / (sqrt(v_hat) + eps)
        """
        self.t = self.t + 1

        var i = 0
        while i < self.parameters.len():
            val param = self.parameters[i]
            val grad_handle = rt_torch_autograd_grad(param.handle)
            if grad_handle != 0:
                # m = beta1 * m + (1 - beta1) * grad
                val m_old_scaled = rt_torch_torchtensor_mul_scalar(self.m[i].handle, self.beta1)
                val g_scaled = rt_torch_torchtensor_mul_scalar(grad_handle, 1 - self.beta1)
                val m_new = rt_torch_torchtensor_add(m_old_scaled, g_scaled)
                rt_torch_torchtensor_free(m_old_scaled)
                rt_torch_torchtensor_free(g_scaled)
                rt_torch_torchtensor_free(self.m[i].handle)
                self.m[i] = Tensor.from_handle(m_new)

                # v = beta2 * v + (1 - beta2) * grad^2
                val v_old_scaled = rt_torch_torchtensor_mul_scalar(self.v[i].handle, self.beta2)
                val grad_sq = rt_torch_torchtensor_mul(grad_handle, grad_handle)
                val g2_scaled = rt_torch_torchtensor_mul_scalar(grad_sq, 1 - self.beta2)
                val v_new = rt_torch_torchtensor_add(v_old_scaled, g2_scaled)
                rt_torch_torchtensor_free(v_old_scaled)
                rt_torch_torchtensor_free(grad_sq)
                rt_torch_torchtensor_free(g2_scaled)
                rt_torch_torchtensor_free(self.v[i].handle)
                self.v[i] = Tensor.from_handle(v_new)

                # Bias correction: m_hat = m / (1 - beta1^t), v_hat = v / (1 - beta2^t)
                var beta1_t = 1.0
                var beta2_t = 1.0
                var k = 0
                while k < self.t:
                    beta1_t = beta1_t * self.beta1
                    beta2_t = beta2_t * self.beta2
                    k = k + 1
                val m_hat = rt_torch_torchtensor_mul_scalar(m_new, 1.0 / (1.0 - beta1_t))
                val v_hat = rt_torch_torchtensor_mul_scalar(v_new, 1.0 / (1.0 - beta2_t))

                # param = param - lr * m_hat / (sqrt(v_hat) + eps)
                val v_sqrt = rt_torch_torchtensor_sqrt(v_hat)
                val denom = rt_torch_torchtensor_add_scalar(v_sqrt, self.eps)
                val update = rt_torch_torchtensor_div(m_hat, denom)
                val update_scaled = rt_torch_torchtensor_mul_scalar(update, self.lr)
                val new_param = rt_torch_torchtensor_sub(param.handle, update_scaled)

                rt_torch_torchtensor_free(m_hat)
                rt_torch_torchtensor_free(v_hat)
                rt_torch_torchtensor_free(v_sqrt)
                rt_torch_torchtensor_free(denom)
                rt_torch_torchtensor_free(update)
                rt_torch_torchtensor_free(update_scaled)
                rt_torch_torchtensor_free(param.handle)
                self.parameters[i] = Tensor.from_handle(new_param)
            i = i + 1

    fn zero_grad():
        """Zero out all gradients."""
        var i = 0
        while i < self.parameters.len():
            self.parameters[i].zero_grad()
            i = i + 1


class RMSprop:
    """RMSprop optimizer

    Example:
        val optimizer = RMSprop.create(model_params, 0, 0, 0)
        optimizer.step()
        optimizer.zero_grad()
    """

    parameters: [Tensor]
    lr: i64
    alpha: i64
    eps: i64
    v: [Tensor]

    static fn create(parameters: [Tensor], lr: i64, alpha: i64, eps: i64) -> RMSprop:
        var v_list = []
        var i = 0
        while i < parameters.len():
            val param = parameters[i]
            val v_tensor = Tensor.zeros(param.shape())
            v_list = v_list + [v_tensor]
            i = i + 1
        RMSprop(
            parameters: parameters,
            lr: lr,
            alpha: alpha,
            eps: eps,
            v: v_list
        )

    fn step():
        """Update parameters using RMSprop.

        For each parameter with a gradient:
            v = alpha * v + (1 - alpha) * grad^2
            param = param - lr * grad / (sqrt(v) + eps)
        """
        var i = 0
        while i < self.parameters.len():
            val param = self.parameters[i]
            val grad_handle = rt_torch_autograd_grad(param.handle)
            if grad_handle != 0:
                # v = alpha * v + (1 - alpha) * grad^2
                val v_old_scaled = rt_torch_torchtensor_mul_scalar(self.v[i].handle, self.alpha)
                val grad_sq = rt_torch_torchtensor_mul(grad_handle, grad_handle)
                val g2_scaled = rt_torch_torchtensor_mul_scalar(grad_sq, 1 - self.alpha)
                val v_new = rt_torch_torchtensor_add(v_old_scaled, g2_scaled)
                rt_torch_torchtensor_free(v_old_scaled)
                rt_torch_torchtensor_free(grad_sq)
                rt_torch_torchtensor_free(g2_scaled)
                rt_torch_torchtensor_free(self.v[i].handle)
                self.v[i] = Tensor.from_handle(v_new)

                # param = param - lr * grad / (sqrt(v) + eps)
                val v_sqrt = rt_torch_torchtensor_sqrt(v_new)
                val denom = rt_torch_torchtensor_add_scalar(v_sqrt, self.eps)
                val update = rt_torch_torchtensor_div(grad_handle, denom)
                val update_scaled = rt_torch_torchtensor_mul_scalar(update, self.lr)
                val new_param = rt_torch_torchtensor_sub(param.handle, update_scaled)

                rt_torch_torchtensor_free(v_sqrt)
                rt_torch_torchtensor_free(denom)
                rt_torch_torchtensor_free(update)
                rt_torch_torchtensor_free(update_scaled)
                rt_torch_torchtensor_free(param.handle)
                self.parameters[i] = Tensor.from_handle(new_param)
            i = i + 1

    fn zero_grad():
        """Zero out all gradients."""
        var i = 0
        while i < self.parameters.len():
            self.parameters[i].zero_grad()
            i = i + 1


# ============================================================================
# CUDA Stream Wrapper
# ============================================================================

class Stream:
    """CUDA stream for asynchronous operations

    Example:
        val stream = Stream.create(0)
        val t = Tensor.randn([100, 100])
        val t_gpu = t.to_stream(0, stream)
        stream.sync()
    """

    handle: i64
    owns_handle: bool
    device_id: i64

    static fn create(device_id: i64) -> Stream:
        val device_i32 = 0
        val handle = rt_torch_stream_create(device_i32)
        Stream(handle: handle, owns_handle: true, device_id: device_id)

    fn drop():
        """Automatically free memory when object goes out of scope."""
        if self.owns_handle:
            rt_torch_torchstream_free(self.handle)

    fn synchronize():
        """Synchronize stream (wait for all operations to complete)."""
        rt_torch_torchstream_sync(self.handle)

    fn query() -> bool:
        """Check if all operations in stream have completed."""
        rt_torch_torchstream_query(self.handle)


# ============================================================================
# Sequential Container
# ============================================================================

class Sequential:
    """Sequential container for chaining layers

    Example:
        val model = Sequential.create()
        model.add_layer_linear(Linear.create(784, 128))
        model.add_layer_linear(Linear.create(128, 10))
        val x = Tensor.randn([32, 784])
        val y = model.forward(x)
        print y.shape()  # [32, 10]
    """

    layers_linear: [Linear]
    layers_conv2d: [Conv2d]

    static fn create() -> Sequential:
        Sequential(layers_linear: [], layers_conv2d: [])

    fn add_layer_linear(layer: Linear):
        """Add a Linear layer."""
        self.layers_linear = self.layers_linear + [layer]

    fn add_layer_conv2d(layer: Conv2d):
        """Add a Conv2d layer."""
        self.layers_conv2d = self.layers_conv2d + [layer]

    fn forward(x: Tensor) -> Tensor:
        """Forward pass through all layers."""
        var output = x
        var i = 0
        while i < self.layers_linear.len():
            output = self.layers_linear[i].forward(output)
            i = i + 1
        var j = 0
        while j < self.layers_conv2d.len():
            output = self.layers_conv2d[j].forward(output)
            j = j + 1
        output

    fn parameters() -> [Tensor]:
        """Get all trainable parameters."""
        var all_params = []
        var i = 0
        while i < self.layers_linear.len():
            val layer_params = self.layers_linear[i].parameters()
            all_params = all_params + layer_params
            i = i + 1
        var j = 0
        while j < self.layers_conv2d.len():
            val layer_params = self.layers_conv2d[j].parameters()
            all_params = all_params + layer_params
            j = j + 1
        all_params


# ============================================================================
# Utility Functions
# ============================================================================

fn no_grad(f: fn()) -> void:
    """Context manager for disabling gradient computation (placeholder)."""
    f()

fn set_seed(seed: i64):
    """Set random seed for reproducibility.

    Note: No rt_torch_manual_seed FFI available yet.
    When added to torch/ffi.spl, this will call it.
    Currently a no-op.
    """
    # No-op until rt_torch_manual_seed is added to FFI
    val _ = seed

fn manual_seed(seed: i64):
    """Alias for set_seed."""
    set_seed(seed)

export MSELoss, CrossEntropyLoss
export SGD, Adam, RMSprop
export Stream, Sequential
export no_grad, set_seed, manual_seed
