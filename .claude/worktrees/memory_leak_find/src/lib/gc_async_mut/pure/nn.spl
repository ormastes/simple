# Pure Simple Neural Network Layers
#
# Implements common NN layers in pure Simple
# Zero external dependencies
# BatchNorm1d, LayerNorm, Embedding, and helpers are in nn_layers.spl
#
# NOTE: This module uses generics (PureTensor<f64>) and will only work
# in compiled mode, not in the interpreter (runtime parser limitation).

use std.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros, tensor_ones, tensor_randn}
use std.pure.tensor_ops.{add, mul, mul_scalar, relu, sigmoid, tanh, softmax, tensor_exp, tensor_sum, tensor_div_scalar}
use std.pure.nn.pooling.{MaxPool2d, AvgPool2d, maxpool2d_create, avgpool2d_create}
use std.gc_async_mut.pure.nn_layers.*

# ============================================================================
# Layer Classes
# ============================================================================

class Linear:
    """Fully-connected layer: y = xW^T + b"""
    weight: PureTensor<f64>
    bias: PureTensor<f64>?
    in_features: i64
    out_features: i64
    training: bool

    static fn create(in_features: i64, out_features: i64, bias: bool) -> Linear:
        """Create Linear layer with simple initialization."""
        var w_data: [f64] = []
        var i = 0
        while i < in_features * out_features:
            w_data.push(0.1)
            i = i + 1

        val weight = tensor_from_data(w_data, [out_features, in_features])
        val bias_tensor = if bias:
            Some(tensor_zeros([out_features]))
        else:
            nil

        Linear(weight: weight, bias: bias_tensor, in_features: in_features, out_features: out_features, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward: y = xW^T + b"""
        var result: [f64] = []
        var i = 0
        while i < self.out_features:
            var sum = 0.0
            var j = 0
            while j < self.in_features and j < x.data.len():
                sum = sum + self.weight.data[i * self.in_features + j] * x.data[j]
                j = j + 1

            if self.bias.?:
                sum = sum + self.bias.unwrap().data[i]

            result.push(sum)
            i = i + 1

        tensor_from_data(result, [self.out_features])

    fn parameters() -> [PureTensor<f64>]:
        """Get list of parameters (weight, bias if exists)."""
        if self.bias.?:
            [self.weight, self.bias.unwrap()]
        else:
            [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Linear(in_features={self.in_features}, out_features={self.out_features})"

class ReLU:
    """Rectified Linear Unit: max(0, x)"""
    training: bool

    static fn create() -> ReLU:
        ReLU(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        relu(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "ReLU()"

class Sigmoid:
    """Sigmoid activation"""
    training: bool

    static fn create() -> Sigmoid:
        Sigmoid(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        sigmoid(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Sigmoid()"

class Tanh:
    """Tanh activation"""
    training: bool

    static fn create() -> Tanh:
        Tanh(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        tanh(x)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Tanh()"

class Softmax:
    """Softmax activation: exp(x_i) / sum(exp(x_j))"""
    dim: i64
    training: bool

    static fn create(dim: i64) -> Softmax:
        Softmax(dim: dim, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        # Simple softmax: exp(x) / sum(exp(x))
        val exp_x = tensor_exp(x)
        val sum_exp = tensor_sum(exp_x)
        tensor_div_scalar(exp_x, sum_exp)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Softmax(dim={self.dim})"

class Dropout:
    """Dropout regularization"""
    p: f64
    training: bool

    static fn create(p: f64) -> Dropout:
        Dropout(p: p, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        # In training mode: randomly drop elements
        # In eval mode: return input unchanged
        if self.training:
            # Dropout mask implementation (Phase 3.4 - TODO âœ…)
            # Generate random values in [0, 1)
            val rand_tensor = tensor_randn(x.shape)

            # Create binary mask: keep if rand > p, drop if rand <= p
            var mask_data: [f64] = []
            val keep_prob = 1.0 - self.p
            for val in rand_tensor.data:
                # Shift random values from [-0.5, 0.5) to [0, 1)
                val normalized = val + 0.5
                if normalized > self.p:
                    mask_data.push(1.0)
                else:
                    mask_data.push(0.0)

            val mask = tensor_from_data(mask_data, x.shape)

            # Apply mask and scale by 1/(1-p) to maintain expected value
            val scale = 1.0 / keep_prob
            val masked = mul(x, mask)
            mul_scalar(masked, scale)
        else:
            x

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "Dropout(p={self.p})"

class Sequential:
    """Sequential container for chaining layers"""
    layers: [any]  # List of layers (using 'any' since no Layer trait yet)
    training: bool

    static fn create(layers: [any]) -> Sequential:
        Sequential(layers: layers, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass through all layers."""
        var result = x
        for layer in self.layers:
            result = layer.forward(result)
        result

    fn parameters() -> [PureTensor<f64>]:
        """Collect parameters from all layers."""
        var all_params: [PureTensor<f64>] = []
        for layer in self.layers:
            val layer_params = layer.parameters()
            all_params = all_params + layer_params
        all_params

    me train():
        """Set all layers to training mode."""
        self.training = true
        for layer in self.layers:
            layer.train()

    me eval():
        """Set all layers to evaluation mode."""
        self.training = false
        for layer in self.layers:
            layer.eval()

    fn to_string() -> text:
        """String representation."""
        var s = "Sequential(\n"
        for layer in self.layers:
            s = s + "  " + layer.to_string() + "\n"
        s + ")"

class Conv2d:
    """2D Convolutional layer: conv(x, weight) + bias

    Input shape: [batch, in_channels, height, width]
    Output shape: [batch, out_channels, out_height, out_width]
    """
    weight: PureTensor<f64>
    bias: PureTensor<f64>?
    in_channels: i64
    out_channels: i64
    kernel_size: i64
    stride: i64
    padding: i64
    training: bool

    static fn create(
        in_channels: i64,
        out_channels: i64,
        kernel_size: i64,
        stride: i64,
        padding: i64,
        bias: bool
    ) -> Conv2d:
        """Create Conv2d layer with simple initialization."""
        val weight_size = out_channels * in_channels * kernel_size * kernel_size
        var w_data: [f64] = []
        var i = 0
        while i < weight_size:
            w_data.push(0.1)
            i = i + 1

        val weight = tensor_from_data(
            w_data,
            [out_channels, in_channels, kernel_size, kernel_size]
        )

        val bias_tensor = if bias:
            Some(tensor_zeros([out_channels]))
        else:
            nil

        Conv2d(
            weight: weight,
            bias: bias_tensor,
            in_channels: in_channels,
            out_channels: out_channels,
            kernel_size: kernel_size,
            stride: stride,
            padding: padding,
            training: true
        )

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward pass: conv2d(x, weight) + bias"""
        val batch = x.shape[0]
        val in_c = x.shape[1]
        val in_h = x.shape[2]
        val in_w = x.shape[3]

        val out_h = (in_h + 2 * self.padding - self.kernel_size) / self.stride + 1
        val out_w = (in_w + 2 * self.padding - self.kernel_size) / self.stride + 1

        var result_data: [f64] = []

        var b = 0
        while b < batch:
            var oc = 0
            while oc < self.out_channels:
                var oh = 0
                while oh < out_h:
                    var ow = 0
                    while ow < out_w:
                        var sum = 0.0

                        var ic = 0
                        while ic < self.in_channels:
                            var kh = 0
                            while kh < self.kernel_size:
                                var kw = 0
                                while kw < self.kernel_size:
                                    val ih = oh * self.stride + kh - self.padding
                                    val iw = ow * self.stride + kw - self.padding

                                    if ih >= 0 and ih < in_h and iw >= 0 and iw < in_w:
                                        val x_idx = b * (in_c * in_h * in_w) +
                                                   ic * (in_h * in_w) +
                                                   ih * in_w +
                                                   iw

                                        val w_idx = oc * (in_c * self.kernel_size * self.kernel_size) +
                                                   ic * (self.kernel_size * self.kernel_size) +
                                                   kh * self.kernel_size +
                                                   kw

                                        sum = sum + x.data[x_idx] * self.weight.data[w_idx]

                                    kw = kw + 1
                                kh = kh + 1
                            ic = ic + 1

                        if self.bias.?:
                            sum = sum + self.bias.unwrap().data[oc]

                        result_data.push(sum)
                        ow = ow + 1
                    oh = oh + 1
                oc = oc + 1
            b = b + 1

        tensor_from_data(result_data, [batch, self.out_channels, out_h, out_w])

    fn backward(grad_output: PureTensor<f64>, x: PureTensor<f64>) -> PureTensor<f64>:
        """Backward pass for Conv2d"""
        val batch = x.shape[0]
        val in_h = x.shape[2]
        val in_w = x.shape[3]
        val out_h = grad_output.shape[2]
        val out_w = grad_output.shape[3]

        var grad_x_data: [f64] = []
        var i = 0
        val total_size = batch * self.in_channels * in_h * in_w
        while i < total_size:
            grad_x_data.push(0.0)
            i = i + 1

        var b = 0
        while b < batch:
            var oc = 0
            while oc < self.out_channels:
                var oh = 0
                while oh < out_h:
                    var ow = 0
                    while ow < out_w:
                        val grad_idx = b * (self.out_channels * out_h * out_w) +
                                      oc * (out_h * out_w) +
                                      oh * out_w +
                                      ow
                        val grad = grad_output.data[grad_idx]

                        var ic = 0
                        while ic < self.in_channels:
                            var kh = 0
                            while kh < self.kernel_size:
                                var kw = 0
                                while kw < self.kernel_size:
                                    val ih = oh * self.stride + kh - self.padding
                                    val iw = ow * self.stride + kw - self.padding

                                    if ih >= 0 and ih < in_h and iw >= 0 and iw < in_w:
                                        val w_idx = oc * (self.in_channels * self.kernel_size * self.kernel_size) +
                                                   ic * (self.kernel_size * self.kernel_size) +
                                                   kh * self.kernel_size +
                                                   kw

                                        val gx_idx = b * (self.in_channels * in_h * in_w) +
                                                    ic * (in_h * in_w) +
                                                    ih * in_w +
                                                    iw

                                        grad_x_data[gx_idx] = grad_x_data[gx_idx] + grad * self.weight.data[w_idx]

                                    kw = kw + 1
                                kh = kh + 1
                            ic = ic + 1

                        ow = ow + 1
                    oh = oh + 1
                oc = oc + 1
            b = b + 1

        tensor_from_data(grad_x_data, [batch, self.in_channels, in_h, in_w])

    fn parameters() -> [PureTensor<f64>]:
        """Get list of parameters (weight, bias if exists)."""
        if self.bias.?:
            [self.weight, self.bias.unwrap()]
        else:
            [self.weight]

    me train():
        """Set layer to training mode."""
        self.training = true

    me eval():
        """Set layer to evaluation mode."""
        self.training = false

    fn to_string() -> text:
        """String representation."""
        "Conv2d(in_channels={self.in_channels}, out_channels={self.out_channels}, kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding})"

class LeakyReLU:
    """Leaky Rectified Linear Unit: max(negative_slope * x, x)"""
    negative_slope: f64
    training: bool

    static fn create(negative_slope: f64) -> LeakyReLU:
        LeakyReLU(negative_slope: negative_slope, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        var result_data: [f64] = []
        for v in x.data:
            val activated = if v > 0.0: v else: self.negative_slope * v
            result_data.push(activated)
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "LeakyReLU(negative_slope={self.negative_slope})"

class ELU:
    """Exponential Linear Unit: x if x > 0, alpha * (exp(x) - 1) if x <= 0"""
    alpha: f64
    training: bool

    static fn create(alpha: f64) -> ELU:
        ELU(alpha: alpha, training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        var result_data: [f64] = []
        for v in x.data:
            if v > 0.0:
                result_data.push(v)
            else:
                # Compute exp(v) using Taylor series (20 terms)
                var exp_val = 1.0
                var term = 1.0
                var k = 1
                while k <= 20:
                    term = term * v / k
                    exp_val = exp_val + term
                    k = k + 1
                result_data.push(self.alpha * (exp_val - 1.0))
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "ELU(alpha={self.alpha})"

class GELU:
    """Gaussian Error Linear Unit: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))"""
    training: bool

    static fn create() -> GELU:
        GELU(training: true)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        val sqrt_2_over_pi = 0.7978845608
        var result_data: [f64] = []
        for v in x.data:
            # Compute inner = sqrt(2/pi) * (x + 0.044715 * x^3)
            val inner = sqrt_2_over_pi * (v + 0.044715 * v * v * v)
            # Compute tanh(inner) using (exp(2z)-1)/(exp(2z)+1) with Taylor exp
            val z2 = 2.0 * inner
            # Compute exp(z2) using Taylor series (20 terms)
            var exp_val = 1.0
            var term = 1.0
            var k = 1
            while k <= 20:
                term = term * z2 / k
                exp_val = exp_val + term
                k = k + 1
            val tanh_val = (exp_val - 1.0) / (exp_val + 1.0)
            result_data.push(0.5 * v * (1.0 + tanh_val))
        tensor_from_data(result_data, x.shape)

    fn parameters() -> [PureTensor<f64>]:
        []

    me train():
        self.training = true

    me eval():
        self.training = false

    fn to_string() -> text:
        "GELU()"


# ============================================================================
# Exports
# ============================================================================

export Linear, ReLU, Sigmoid, Tanh, Softmax, Dropout, Sequential
export Conv2d, LeakyReLU, ELU, GELU, BatchNorm1d, LayerNorm, Embedding
export MaxPool2d, AvgPool2d
export count_parameters, zero_grad
export linear_create  # Legacy API
export maxpool2d_create, avgpool2d_create  # Pooling factory functions
export calculate_conv2d_output_size
