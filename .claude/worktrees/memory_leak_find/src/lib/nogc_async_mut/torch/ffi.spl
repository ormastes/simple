# torch SFFI Bindings (NoGC — Duplicated)
#
# Duplicated from gc_async_mut/torch/ffi.spl to avoid cross-mode import warnings.
# Same extern fn signatures — both resolve to the same libtorch C++ symbols.

# ============================================================================
# Opaque Handle Types (represented as i64 pointers)
# ============================================================================

# TorchTensor: Opaque handle (pointer cast to i64)
# TorchStream: Opaque handle (pointer cast to i64)

# ============================================================================
# Library Information
# ============================================================================

extern fn rt_torch_available() -> bool
extern fn rt_torch_version() -> text
extern fn rt_torch_cuda_available() -> bool

# ============================================================================
# Tensor Creation Functions
# ============================================================================

extern fn rt_torch_tensor_zeros(dims: [i64]) -> i64
extern fn rt_torch_tensor_ones(dims: [i64]) -> i64
extern fn rt_torch_tensor_randn(dims: [i64]) -> i64
extern fn rt_torch_tensor_rand(dims: [i64]) -> i64
extern fn rt_torch_tensor_full(dims: [i64], value: f64) -> i64
extern fn rt_torch_tensor_from_data(data: [f64], dims: [i64]) -> i64
extern fn rt_torch_tensor_arange(start: f64, end: f64, step: f64) -> i64
extern fn rt_torch_tensor_linspace(start: f64, end: f64, steps: i64) -> i64
extern fn rt_torch_tensor_eye(n: i64) -> i64
extern fn rt_torch_tensor_empty(dims: [i64]) -> i64

# ============================================================================
# Element-wise Arithmetic Operations
# ============================================================================

extern fn rt_torch_torchtensor_add(handle: i64, other: i64) -> i64
extern fn rt_torch_torchtensor_sub(handle: i64, other: i64) -> i64
extern fn rt_torch_torchtensor_mul(handle: i64, other: i64) -> i64
extern fn rt_torch_torchtensor_div(handle: i64, other: i64) -> i64
extern fn rt_torch_torchtensor_pow(handle: i64, exponent: f64) -> i64
extern fn rt_torch_torchtensor_neg(handle: i64) -> i64
extern fn rt_torch_torchtensor_abs(handle: i64) -> i64
extern fn rt_torch_torchtensor_sqrt(handle: i64) -> i64
extern fn rt_torch_torchtensor_exp(handle: i64) -> i64
extern fn rt_torch_torchtensor_log(handle: i64) -> i64
extern fn rt_torch_torchtensor_add_scalar(handle: i64, scalar: f64) -> i64
extern fn rt_torch_torchtensor_mul_scalar(handle: i64, scalar: f64) -> i64

# ============================================================================
# Activation Functions
# ============================================================================

extern fn rt_torch_torchtensor_relu(handle: i64) -> i64
extern fn rt_torch_torchtensor_sigmoid(handle: i64) -> i64
extern fn rt_torch_torchtensor_tanh(handle: i64) -> i64
extern fn rt_torch_torchtensor_leaky_relu(handle: i64, negative_slope: f64) -> i64
extern fn rt_torch_torchtensor_gelu(handle: i64) -> i64
extern fn rt_torch_torchtensor_softmax(handle: i64, dim: i64) -> i64
extern fn rt_torch_torchtensor_log_softmax(handle: i64, dim: i64) -> i64

# ============================================================================
# Linear Algebra Operations
# ============================================================================

extern fn rt_torch_torchtensor_matmul(handle: i64, other: i64) -> i64
extern fn rt_torch_torchtensor_dot(handle: i64, other: i64) -> i64
extern fn rt_torch_torchtensor_transpose(handle: i64, dim0: i64, dim1: i64) -> i64
extern fn rt_torch_torchtensor_t(handle: i64) -> i64
extern fn rt_torch_torchtensor_norm(handle: i64) -> f64
extern fn rt_torch_torchtensor_det(handle: i64) -> f64
extern fn rt_torch_torchtensor_inverse(handle: i64) -> i64
extern fn rt_torch_torchtensor_svd(handle: i64) -> i64
extern fn rt_torch_torchtensor_eig(handle: i64) -> i64

# ============================================================================
# Reduction Operations
# ============================================================================

extern fn rt_torch_torchtensor_sum(handle: i64) -> f64
extern fn rt_torch_torchtensor_sum_dim(handle: i64, dim: i64, keepdim: bool) -> i64
extern fn rt_torch_torchtensor_mean(handle: i64) -> f64
extern fn rt_torch_torchtensor_mean_dim(handle: i64, dim: i64, keepdim: bool) -> i64
extern fn rt_torch_torchtensor_max(handle: i64) -> f64
extern fn rt_torch_torchtensor_max_dim(handle: i64, dim: i64, keepdim: bool) -> i64
extern fn rt_torch_torchtensor_min(handle: i64) -> f64
extern fn rt_torch_torchtensor_min_dim(handle: i64, dim: i64, keepdim: bool) -> i64
extern fn rt_torch_torchtensor_argmax(handle: i64, dim: i64, keepdim: bool) -> i64
extern fn rt_torch_torchtensor_argmin(handle: i64, dim: i64, keepdim: bool) -> i64
extern fn rt_torch_torchtensor_std(handle: i64) -> f64
extern fn rt_torch_torchtensor_var(handle: i64) -> f64

# ============================================================================
# Shape Manipulation
# ============================================================================

extern fn rt_torch_torchtensor_ndim(handle: i64) -> i64
extern fn rt_torch_torchtensor_numel(handle: i64) -> i64
extern fn rt_torch_torchtensor_shape(handle: i64) -> [i64]
extern fn rt_torch_torchtensor_reshape(handle: i64, dims: [i64]) -> i64
extern fn rt_torch_torchtensor_view(handle: i64, dims: [i64]) -> i64
extern fn rt_torch_torchtensor_permute(handle: i64, dims: [i64]) -> i64
extern fn rt_torch_torchtensor_squeeze(handle: i64) -> i64
extern fn rt_torch_torchtensor_squeeze_dim(handle: i64, dim: i64) -> i64
extern fn rt_torch_torchtensor_unsqueeze(handle: i64, dim: i64) -> i64
extern fn rt_torch_torchtensor_flatten(handle: i64) -> i64
extern fn rt_torch_torchtensor_contiguous(handle: i64) -> i64

# ============================================================================
# Indexing and Slicing
# ============================================================================

extern fn rt_torch_torchtensor_slice(handle: i64, dim: i64, start: i64, end: i64, step: i64) -> i64
extern fn rt_torch_torchtensor_index_select(handle: i64, dim: i64, indices: i64) -> i64
extern fn rt_torch_torchtensor_gather(handle: i64, dim: i64, indices: i64) -> i64
extern fn rt_torch_torchtensor_cat(tensors: [i64], dim: i64) -> i64
extern fn rt_torch_torchtensor_stack(tensors: [i64], dim: i64) -> i64
extern fn rt_torch_torchtensor_chunk(handle: i64, chunks: i64, dim: i64) -> [i64]

# ============================================================================
# Neural Network Operations
# ============================================================================

extern fn rt_torch_nn_conv2d(input: i64, weight: i64, bias: i64, stride: [i64], padding: [i64], dilation: [i64], groups: i64) -> i64
extern fn rt_torch_nn_max_pool2d(input: i64, kernel_size: [i64], stride: [i64], padding: [i64]) -> i64
extern fn rt_torch_nn_avg_pool2d(input: i64, kernel_size: [i64], stride: [i64], padding: [i64]) -> i64
extern fn rt_torch_nn_batch_norm(input: i64, running_mean: i64, running_var: i64, weight: i64, bias: i64, training: bool, momentum: f64, eps: f64) -> i64
extern fn rt_torch_nn_layer_norm(input: i64, normalized_shape: [i64], weight: i64, bias: i64, eps: f64) -> i64
extern fn rt_torch_nn_dropout(input: i64, p: f64, training: bool) -> i64
extern fn rt_torch_nn_linear(input: i64, weight: i64, bias: i64) -> i64
extern fn rt_torch_nn_embedding(input: i64, weight: i64) -> i64

# ============================================================================
# Loss Functions
# ============================================================================

extern fn rt_torch_nn_mse_loss(input: i64, target: i64) -> f64
extern fn rt_torch_nn_cross_entropy(input: i64, target: i64) -> f64
extern fn rt_torch_nn_binary_cross_entropy(input: i64, target: i64) -> f64
extern fn rt_torch_nn_nll_loss(input: i64, target: i64) -> f64

# ============================================================================
# Autograd Operations
# ============================================================================

extern fn rt_torch_autograd_set_requires_grad(handle: i64, requires_grad: bool)
extern fn rt_torch_autograd_requires_grad(handle: i64) -> bool
extern fn rt_torch_autograd_grad(handle: i64) -> i64
extern fn rt_torch_autograd_backward(handle: i64)
extern fn rt_torch_autograd_zero_grad(handle: i64)
extern fn rt_torch_autograd_detach(handle: i64) -> i64
extern fn rt_torch_autograd_no_grad_begin()
extern fn rt_torch_autograd_no_grad_end()

# ============================================================================
# Device Management
# ============================================================================

extern fn rt_torch_torchtensor_cuda(handle: i64, device_id: i32) -> i64
extern fn rt_torch_torchtensor_cpu(handle: i64) -> i64
extern fn rt_torch_torchtensor_is_cuda(handle: i64) -> bool
extern fn rt_torch_torchtensor_device(handle: i64) -> i32
extern fn rt_torch_torchtensor_to_stream(handle: i64, device_id: i32, stream: i64) -> i64
extern fn rt_torch_torchtensor_clone(handle: i64) -> i64

# ============================================================================
# CUDA Stream Operations
# ============================================================================

extern fn rt_torch_stream_create(device_id: i32) -> i64
extern fn rt_torch_torchstream_sync(handle: i64)
extern fn rt_torch_torchstream_query(handle: i64) -> bool
extern fn rt_torch_torchstream_free(handle: i64)

# ============================================================================
# Memory Management
# ============================================================================

extern fn rt_torch_torchtensor_free(handle: i64)
extern fn rt_torch_cuda_memory_allocated(device_id: i32) -> i64
extern fn rt_torch_cuda_max_memory_allocated(device_id: i32) -> i64
extern fn rt_torch_cuda_empty_cache()

# ============================================================================
# Trigonometric Functions
# ============================================================================

extern fn rt_torch_torchtensor_sin(handle: i64) -> i64
extern fn rt_torch_torchtensor_cos(handle: i64) -> i64
extern fn rt_torch_torchtensor_tan(handle: i64) -> i64
extern fn rt_torch_torchtensor_asin(handle: i64) -> i64
extern fn rt_torch_torchtensor_acos(handle: i64) -> i64
extern fn rt_torch_torchtensor_atan2(handle: i64, other: i64) -> i64

# ============================================================================
# Integer Tensor Creation
# ============================================================================

extern fn rt_torch_tensor_arange_int(start: i64, end: i64, step: i64) -> i64
extern fn rt_torch_tensor_zeros_int_1d(d0: i64) -> i64
extern fn rt_torch_tensor_zeros_int_2d(d0: i64, d1: i64) -> i64
extern fn rt_torch_tensor_ones_int_1d(d0: i64) -> i64
extern fn rt_torch_tensor_ones_int_2d(d0: i64, d1: i64) -> i64
extern fn rt_torch_tensor_full_int_1d(d0: i64, value: i64) -> i64
extern fn rt_torch_tensor_full_int_2d(d0: i64, d1: i64, value: i64) -> i64
extern fn rt_torch_tensor_from_i64_data(data: [i64], dims: [i64]) -> i64
extern fn rt_torch_torchtensor_to_float(handle: i64) -> i64
extern fn rt_torch_torchtensor_to_int(handle: i64) -> i64
extern fn rt_torch_torchtensor_to_float32(handle: i64) -> i64

# ============================================================================
# Tensor Serialization
# ============================================================================

extern fn rt_torch_tensor_save(handle: i64, path: text)
extern fn rt_torch_tensor_load(path: text) -> i64

# ============================================================================
# Safetensors Loading
# ============================================================================

extern fn rt_torch_safetensors_open(path: text) -> i64
extern fn rt_torch_safetensors_close(handle: i64)
extern fn rt_torch_safetensors_num_tensors(handle: i64) -> i64
extern fn rt_torch_safetensors_list_names(handle: i64) -> text
extern fn rt_torch_safetensors_get_tensor(sf_handle: i64, name: text) -> i64
