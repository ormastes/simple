# NoGC Pure Simple Higher-Level Torch Operations
#
# Same API as gc_async_mut/torch/ops.spl by convention.
# Uses duplicated FFI declarations from nogc ffi.spl.

use std.nogc_sync_mut.torch.ffi.{
    rt_torch_torchtensor_add,
    rt_torch_torchtensor_sub,
    rt_torch_torchtensor_mul,
    rt_torch_torchtensor_div,
    rt_torch_torchtensor_mul_scalar,
    rt_torch_torchtensor_add_scalar,
    rt_torch_torchtensor_neg,
    rt_torch_torchtensor_sqrt,
    rt_torch_torchtensor_pow,
    rt_torch_torchtensor_log_softmax,
    rt_torch_torchtensor_gather,
    rt_torch_torchtensor_sum,
    rt_torch_torchtensor_sum_dim,
    rt_torch_torchtensor_mean,
    rt_torch_torchtensor_mean_dim,
    rt_torch_torchtensor_free,
    rt_torch_torchtensor_clone,
    rt_torch_torchtensor_unsqueeze,
    rt_torch_torchtensor_argmax,
    rt_torch_autograd_grad,
    rt_torch_autograd_no_grad_begin,
    rt_torch_autograd_no_grad_end
}

export cross_entropy_loss_tensor, compute_mse_loss_tensor
export rms_norm, clip_grad_norm, compute_accuracy
export lr_warmup_linear, lr_warmup_cosine

# ============================================================================
# Cross-Entropy Loss (returns tensor for backward)
# ============================================================================

fn cross_entropy_loss_tensor(logits: i64, targets: i64) -> i64:
    val log_probs = rt_torch_torchtensor_log_softmax(logits, 1)
    val neg_log_probs = rt_torch_torchtensor_neg(log_probs)
    rt_torch_torchtensor_free(log_probs)
    val gathered = rt_torch_torchtensor_gather(neg_log_probs, 1, targets)
    rt_torch_torchtensor_free(neg_log_probs)
    val loss = rt_torch_torchtensor_mean_dim(gathered, 1, false)
    rt_torch_torchtensor_free(gathered)
    val scalar_loss = rt_torch_torchtensor_mean_dim(loss, 0, false)
    rt_torch_torchtensor_free(loss)
    scalar_loss


# ============================================================================
# MSE Loss (returns tensor for backward)
# ============================================================================

fn compute_mse_loss_tensor(pred: i64, target: i64) -> i64:
    val diff = rt_torch_torchtensor_sub(pred, target)
    val sq = rt_torch_torchtensor_mul(diff, diff)
    rt_torch_torchtensor_free(diff)
    val per_sample = rt_torch_torchtensor_sum_dim(sq, 1, false)
    rt_torch_torchtensor_free(sq)
    val loss = rt_torch_torchtensor_mean_dim(per_sample, 0, false)
    rt_torch_torchtensor_free(per_sample)
    loss


# ============================================================================
# RMSNorm
# ============================================================================

fn rms_norm(x: i64, weight: i64, eps: f64) -> i64:
    val x_sq = rt_torch_torchtensor_mul(x, x)
    val variance = rt_torch_torchtensor_mean_dim(x_sq, -1, true)
    rt_torch_torchtensor_free(x_sq)
    val var_eps = rt_torch_torchtensor_add_scalar(variance, eps)
    rt_torch_torchtensor_free(variance)
    val var_sqrt = rt_torch_torchtensor_sqrt(var_eps)
    rt_torch_torchtensor_free(var_eps)
    val normed = rt_torch_torchtensor_div(x, var_sqrt)
    rt_torch_torchtensor_free(var_sqrt)
    val result = rt_torch_torchtensor_mul(normed, weight)
    rt_torch_torchtensor_free(normed)
    result


# ============================================================================
# Gradient Clipping
# ============================================================================

fn clip_grad_norm(param_handles: [i64], max_norm: f64) -> f64:
    var total_sq = 0.0
    var idx = 0
    while idx < param_handles.len():
        val h = param_handles[idx]
        val g = rt_torch_autograd_grad(h)
        if g != 0:
            val g_sq = rt_torch_torchtensor_mul(g, g)
            val g_sum = rt_torch_torchtensor_sum(g_sq)
            rt_torch_torchtensor_free(g_sq)
            total_sq = total_sq + g_sum
        idx = idx + 1

    val total_norm = rt_sqrt(total_sq)
    if total_norm > max_norm:
        val scale = max_norm / total_norm
        rt_torch_autograd_no_grad_begin()
        var j = 0
        while j < param_handles.len():
            val h2 = param_handles[j]
            val g2 = rt_torch_autograd_grad(h2)
            if g2 != 0:
                val scaled = rt_torch_torchtensor_mul_scalar(g2, scale)
                rt_torch_torchtensor_free(scaled)
            j = j + 1
        rt_torch_autograd_no_grad_end()
    total_norm


fn rt_sqrt(x: f64) -> f64:
    if x <= 0.0:
        return 0.0
    var guess = x
    var i = 0
    while i < 15:
        guess = (guess + x / guess) / 2.0
        i = i + 1
    guess


# ============================================================================
# Accuracy Computation
# ============================================================================

fn compute_accuracy(logits: i64, targets: i64) -> f64:
    val preds = rt_torch_torchtensor_argmax(logits, 1, false)
    val diff = rt_torch_torchtensor_sub(preds, targets)
    rt_torch_torchtensor_free(preds)
    val diff_sq = rt_torch_torchtensor_mul(diff, diff)
    rt_torch_torchtensor_free(diff)
    val ones = rt_torch_torchtensor_add_scalar(diff_sq, 1.0)
    rt_torch_torchtensor_free(diff_sq)
    val inv = rt_torch_torchtensor_pow(ones, -1.0)
    rt_torch_torchtensor_free(ones)
    val accuracy = rt_torch_torchtensor_mean(inv)
    rt_torch_torchtensor_free(inv)
    accuracy


# ============================================================================
# Learning Rate Schedules
# ============================================================================

fn lr_warmup_linear(step: i64, warmup_steps: i64, total_steps: i64, base_lr: f64) -> f64:
    if step < warmup_steps:
        return base_lr * step / warmup_steps
    val remaining = total_steps - step
    val decay_steps = total_steps - warmup_steps
    if decay_steps <= 0:
        return base_lr
    base_lr * remaining / decay_steps


fn lr_warmup_cosine(step: i64, warmup_steps: i64, total_steps: i64, base_lr: f64, min_lr: f64) -> f64:
    if step < warmup_steps:
        return base_lr * step / warmup_steps
    val progress = (step - warmup_steps) * 1.0 / (total_steps - warmup_steps)
    val decay = 1.0 - progress
    val clamped = if decay < 0.0: 0.0 else: decay
    min_lr + (base_lr - min_lr) * clamped
