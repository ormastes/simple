"""GPU Computing Module - Unified GPU Interface (NoGC)

@tag:stdlib
@tag:api

Provides backend-agnostic GPU computing with automatic device selection,
memory management, and stream-based async operations. Supports CUDA (via PyTorch)
and Vulkan compute backends with CPU fallback.

NoGC variant â€” uses unique ownership (no owns_handle), unconditional drop().

## Architecture Overview

The GPU module uses a three-layer architecture:
1. **Device Layer** - Backend detection and device enumeration (imports from common/gpu/)
2. **Memory Layer** - Typed GPU arrays with automatic lifecycle management (RAII)
3. **Context Layer** - Unified API for device selection, allocation, and streams

## NoGC Differences from gc_async_mut/gpu

- `Tensor?` field instead of `TorchTensorWrapper?` (no owns_handle)
- `Stream?` field instead of `TorchStream?` (no owns_handle)
- `drop()` methods are unconditional
- `GpuBackend`/`Gpu` types imported from `std.common.gpu.device`

## See Also

- std.nogc_sync_mut.gpu.device: Backend detection and device handles
- std.nogc_sync_mut.gpu.memory: GPU array types and allocation
- std.nogc_sync_mut.gpu.context: Context API and stream management
- std.nogc_sync_mut.torch: PyTorch tensors for CUDA support
- std.common.gpu.device: Shared GpuBackend/Gpu type definitions
"""

# Re-export from submodules
use std.nogc_sync_mut.gpu.device.{GpuBackend, Gpu, detect_backends, preferred_backend, gpu_cuda, gpu_vulkan, gpu_none}
use std.nogc_sync_mut.gpu.memory.{GpuArray, gpu_alloc, gpu_alloc_upload, gpu_alloc_zeros}
use std.nogc_sync_mut.gpu.context.{Context, create_context_from_config}

# ============================================================================
# Exports
# ============================================================================

# Device types and functions - Backend selection and device handles
export GpuBackend    # Enum: Cuda, Vulkan, None_ (backend type)
export Gpu           # Struct: Device handle with backend, device_id, sync()
export detect_backends     # fn() -> [GpuBackend] - List available backends
export preferred_backend   # fn() -> GpuBackend - Best available backend
export gpu_cuda      # fn(device_id) -> Gpu - Create CUDA device handle
export gpu_vulkan    # fn(device_id) -> Gpu - Create Vulkan device handle (future)
export gpu_none      # fn() -> Gpu - Create CPU fallback handle

# Memory types and functions - GPU array allocation and transfer
export GpuArray      # class[T]: Typed GPU array with upload/download/copy_to
export gpu_alloc          # fn[T](gpu, count) -> GpuArray[T] - Allocate uninitialized
export gpu_alloc_upload   # fn[T](gpu, data) -> GpuArray[T] - Allocate + upload
export gpu_alloc_zeros    # fn[T](gpu, count) -> GpuArray[T] - Allocate zeros

# Context API (main interface) - High-level unified GPU API
export Context            # class: Manages device, streams, allocations
export create_context_from_config  # fn() -> Context - Create from dl.config.sdn
