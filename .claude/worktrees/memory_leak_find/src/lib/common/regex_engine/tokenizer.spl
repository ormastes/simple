# Regular Expression Engine - Tokenizer
#
# Lexical analysis of regex patterns.

from .types import RegexToken

# ============================================================================
# TOKENIZATION
# ============================================================================

fn tokenize_regex(pattern: text) -> [any]:
    """Tokenize regex pattern into tokens."""
    var tokens = []
    var i = 0
    while i < pattern.len():
        val ch = pattern[i:i + 1]
        if ch == "\\":
            if i + 1 < pattern.len():
                val next = pattern[i + 1:i + 2]
                val token = RegexToken(token_type: "escape", value: next, pos: i)
                tokens.push(token)
                i = i + 2
            else:
                val token = RegexToken(token_type: "literal", value: ch, pos: i)
                tokens.push(token)
                i = i + 1
        else:
            val token_type = get_token_type(ch)
            val token = RegexToken(token_type: token_type, value: ch, pos: i)
            tokens.push(token)
            i = i + 1
    val eof = RegexToken(token_type: "eof", value: "", pos: pattern.len())
    tokens.push(eof)
    tokens

fn get_token_type(ch: text) -> text:
    """Get token type for character."""
    if ch == ".": return "dot"
    if ch == "*": return "star"
    if ch == "+": return "plus"
    if ch == "?": return "question"
    if ch == "|": return "pipe"
    if ch == "(": return "lparen"
    if ch == ")": return "rparen"
    if ch == "[": return "lbracket"
    if ch == "]": return "rbracket"
    if ch == "{": return "lbrace"
    if ch == "}": return "rbrace"
    if ch == "^": return "caret"
    if ch == "$": return "dollar"
    "literal"

fn token_type(token: any) -> text:
    """Get type of token."""
    token.token_type

fn token_value(token: any) -> text:
    """Get value of token."""
    token.value

fn token_pos(token: any) -> i64:
    """Get position of token."""
    token.pos

export tokenize_regex, get_token_type, token_type, token_value, token_pos
