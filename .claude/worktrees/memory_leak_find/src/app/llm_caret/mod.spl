# LLM Caret - Public API Module
#
# Unified LLM access layer providing Claude-CLI-like chat API.
# Wraps multiple backends: Claude CLI, Claude API, OpenAI API,
# OpenAI-compatible endpoints, and local torch models.
#
# Usage:
#   use app.llm_caret.mod.{llm_init_defaults, llm_chat, llm_clear}
#   llm_init_defaults()
#   val response = llm_chat("Hello!")
#   print response

extern fn rt_env_get(key: text) -> text
extern fn rt_process_run(cmd: text, args: [text]) -> (text, text, i64)

# ============================================================================
# JSON helpers (inlined)
# ============================================================================

fn _LB() -> text:
    (123 as char).to_text()

fn _RB() -> text:
    (125 as char).to_text()

fn _Q() -> text:
    "\""

fn _unwrap_idx(opt) -> i64:
    match opt:
        Some(i): return i
        nil: return -1

fn _escape_json(s: text) -> text:
    var result = ""
    var i = 0
    while i < s.len():
        val ch = s[i]
        if ch == "\\":
            result = result + "\\\\"
        elif ch == "\"":
            result = result + "\\\""
        elif ch == "\n":
            result = result + "\\n"
        elif ch == "\r":
            result = result + "\\r"
        elif ch == "\t":
            result = result + "\\t"
        else:
            result = result + ch
        i = i + 1
    result

fn _extract_json_string(json: text, key: text) -> text:
    val quote = "\""
    val search = quote + key + quote + ":"
    val idx = _unwrap_idx(json.index_of(search))
    if idx < 0:
        return ""
    val slen = search.len()
    val start = idx + slen
    val after = json.substring(start)
    val trimmed = after.trim()
    if trimmed.starts_with(quote):
        val rest = trimmed.substring(1)
        var end = 0
        var escaped = false
        while end < rest.len():
            val ch = rest[end]
            if escaped:
                escaped = false
            elif ch == "\\":
                escaped = true
            elif ch == "\"":
                return rest.substring(0, end)
            end = end + 1
    ""

fn _extract_json_value(json: text, key: text) -> text:
    val search = _Q() + key + _Q() + ":"
    val idx = _unwrap_idx(json.index_of(search))
    if idx < 0:
        return "null"
    val slen = search.len()
    val start = idx + slen
    val after = json.substring(start)
    val trimmed = after.trim()
    var end = 0
    while end < trimmed.len():
        val ch = trimmed[end]
        if ch == "," or ch == _RB() or ch == "]":
            break
        end = end + 1
    trimmed.substring(0, end).trim()

fn _extract_json_bool(json: text, key: text) -> bool:
    val raw = _extract_json_value(json, key)
    raw == "true"

fn _extract_json_int(json: text, key: text) -> i64:
    val raw = _extract_json_value(json, key)
    if raw == "null" or raw == "":
        return 0
    int(raw)

# ============================================================================
# Module-level state
# ============================================================================

var LLM_INITIALIZED = false
var LLM_PROVIDER = "claude_cli"
var LLM_MODEL = ""
var LLM_CLI_PATH = "claude"
var LLM_API_KEY = ""
var LLM_BASE_URL = ""
var LLM_SYSTEM_PROMPT = ""
var LLM_SESSION_ID = ""
var LLM_MAX_TURNS = 0

# History
var LLM_ROLES: [text] = []
var LLM_CONTENTS: [text] = []

# ============================================================================
# Initialization
# ============================================================================

fn llm_init_defaults():
    LLM_INITIALIZED = true
    LLM_PROVIDER = "claude_cli"
    LLM_MODEL = ""
    LLM_CLI_PATH = "claude"
    LLM_ROLES = []
    LLM_CONTENTS = []

fn llm_init(provider: text, model: text):
    LLM_INITIALIZED = true
    LLM_PROVIDER = provider
    LLM_MODEL = model
    LLM_ROLES = []
    LLM_CONTENTS = []

fn llm_set_api_key(key: text):
    LLM_API_KEY = key

fn llm_set_base_url(url: text):
    LLM_BASE_URL = url

fn llm_set_cli_path(path: text):
    LLM_CLI_PATH = path

fn llm_system(prompt: text):
    LLM_SYSTEM_PROMPT = prompt

fn llm_provider() -> text:
    LLM_PROVIDER

fn llm_model() -> text:
    LLM_MODEL

fn llm_clear():
    LLM_ROLES = []
    LLM_CONTENTS = []
    LLM_SESSION_ID = ""

# ============================================================================
# History
# ============================================================================

fn llm_history_len() -> i64:
    LLM_ROLES.len()

fn llm_history_role(idx: i64) -> text:
    if idx >= 0 and idx < LLM_ROLES.len():
        return LLM_ROLES[idx]
    ""

fn llm_history_content(idx: i64) -> text:
    if idx >= 0 and idx < LLM_CONTENTS.len():
        return LLM_CONTENTS[idx]
    ""

# ============================================================================
# Build messages JSON from history
# ============================================================================

fn _build_messages_json() -> text:
    var items: [text] = []
    var i = 0
    while i < LLM_ROLES.len():
        var msg = _LB()
        msg = msg + _Q() + "role" + _Q() + ":" + _Q() + LLM_ROLES[i] + _Q() + ","
        msg = msg + _Q() + "content" + _Q() + ":" + _Q() + _escape_json(LLM_CONTENTS[i]) + _Q()
        msg = msg + _RB()
        items = items + [msg]
        i = i + 1
    var result = "["
    var j = 0
    for item in items:
        if j > 0:
            result = result + ","
        result = result + item
        j = j + 1
    result = result + "]"
    result

# ============================================================================
# Chat (main entry point)
# ============================================================================

fn llm_chat(user_msg: text) -> text:
    if not LLM_INITIALIZED:
        return "ERROR: call llm_init_defaults() or llm_init() first"
    # Add user message to history
    LLM_ROLES = LLM_ROLES + ["user"]
    LLM_CONTENTS = LLM_CONTENTS + [user_msg]
    # Dispatch based on provider
    var result_content = ""
    var result_error = ""
    if LLM_PROVIDER == "claude_cli":
        val r = _send_claude_cli(user_msg)
        result_content = r.0
        result_error = r.1
    elif LLM_PROVIDER == "claude_api":
        val r = _send_claude_api()
        result_content = r.0
        result_error = r.1
    elif LLM_PROVIDER == "openai":
        val r = _send_openai()
        result_content = r.0
        result_error = r.1
    else:
        result_error = "unsupported provider: " + LLM_PROVIDER
    if result_error != "":
        return "ERROR: " + result_error
    # Add assistant response to history
    LLM_ROLES = LLM_ROLES + ["assistant"]
    LLM_CONTENTS = LLM_CONTENTS + [result_content]
    result_content

# ============================================================================
# Send via Claude CLI
# ============================================================================

fn _send_claude_cli(prompt: text) -> (text, text):
    var args: [text] = []
    args = args + ["-p", prompt]
    args = args + ["--output-format", "json"]
    if LLM_MODEL != "":
        args = args + ["--model", LLM_MODEL]
    if LLM_SYSTEM_PROMPT != "":
        args = args + ["--system-prompt", LLM_SYSTEM_PROMPT]
    if LLM_SESSION_ID != "":
        args = args + ["--resume", LLM_SESSION_ID]
    if LLM_MAX_TURNS > 0:
        args = args + ["--max-turns", LLM_MAX_TURNS.to_text()]
    val result = rt_process_run(LLM_CLI_PATH, args)
    val stdout = result.0 ?? ""
    val stderr = result.1 ?? ""
    val exit_code = result.2
    if exit_code != 0:
        var err = "claude CLI error (code " + exit_code.to_text() + ")"
        if stderr != "":
            err = err + ": " + stderr
        return ("", err)
    # Parse JSON
    val content = _extract_json_string(stdout, "result")
    val sess = _extract_json_string(stdout, "session_id")
    val is_err = _extract_json_bool(stdout, "is_error")
    if sess != "":
        LLM_SESSION_ID = sess
    if is_err:
        return ("", content)
    (content, "")

# ============================================================================
# Send via Claude API
# ============================================================================

fn _send_claude_api() -> (text, text):
    var key = LLM_API_KEY
    if key == "":
        key = rt_env_get("ANTHROPIC_API_KEY") ?? ""
    if key == "":
        return ("", "ANTHROPIC_API_KEY not set")
    var url = LLM_BASE_URL
    if url == "":
        url = "https://api.anthropic.com"
    url = url + "/v1/messages"
    var model_val = LLM_MODEL
    if model_val == "":
        model_val = "claude-sonnet-4-20250514"
    val messages = _build_messages_json()
    # Build body
    var parts: [text] = []
    parts = parts + [_Q() + "model" + _Q() + ":" + _Q() + model_val + _Q()]
    parts = parts + [_Q() + "max_tokens" + _Q() + ":4096"]
    if LLM_SYSTEM_PROMPT != "":
        parts = parts + [_Q() + "system" + _Q() + ":" + _Q() + _escape_json(LLM_SYSTEM_PROMPT) + _Q()]
    parts = parts + [_Q() + "messages" + _Q() + ":" + messages]
    var body = _LB()
    var i = 0
    for part in parts:
        if i > 0:
            body = body + ","
        body = body + part
        i = i + 1
    body = body + _RB()
    var headers = "x-api-key: " + key + "\n"
    headers = headers + "anthropic-version: 2023-06-01\n"
    headers = headers + "content-type: application/json"
    val result = rt_http_request("POST", url, headers, body)
    val resp_body = result.1 ?? ""
    val http_error = result.2 ?? ""
    if http_error != "":
        return ("", "HTTP error: " + http_error)
    val content = _extract_json_string(resp_body, "text")
    (content, "")

# ============================================================================
# Send via OpenAI API
# ============================================================================

fn _send_openai() -> (text, text):
    var key = LLM_API_KEY
    if key == "":
        key = rt_env_get("OPENAI_API_KEY") ?? ""
    if key == "":
        return ("", "OPENAI_API_KEY not set")
    var url = LLM_BASE_URL
    if url == "":
        url = "https://api.openai.com"
    url = url + "/v1/chat/completions"
    var model_val = LLM_MODEL
    if model_val == "":
        model_val = "gpt-4o"
    val messages = _build_messages_json()
    var parts: [text] = []
    parts = parts + [_Q() + "model" + _Q() + ":" + _Q() + model_val + _Q()]
    parts = parts + [_Q() + "messages" + _Q() + ":" + messages]
    var body = _LB()
    var i = 0
    for part in parts:
        if i > 0:
            body = body + ","
        body = body + part
        i = i + 1
    body = body + _RB()
    var headers = "Authorization: Bearer " + key + "\n"
    headers = headers + "Content-Type: application/json"
    val result = rt_http_request("POST", url, headers, body)
    val resp_body = result.1 ?? ""
    val http_error = result.2 ?? ""
    if http_error != "":
        return ("", "HTTP error: " + http_error)
    val content = _extract_json_string(resp_body, "content")
    (content, "")

# ============================================================================
# Direct send (without history management)
# ============================================================================

fn llm_send(prompt: text) -> text:
    if not LLM_INITIALIZED:
        return "ERROR: call llm_init_defaults() or llm_init() first"
    if LLM_PROVIDER == "claude_cli":
        val r = _send_claude_cli(prompt)
        if r.1 != "":
            return "ERROR: " + r.1
        return r.0
    "ERROR: llm_send only supports claude_cli provider"
