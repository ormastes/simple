#!/usr/bin/env simple
# GPU Runtime API Test Specification
# Requires CUDA hardware
#
# Tests for runtime-compatible GPU interface

use std.gpu_runtime.{gpu_available, gpu_backend_name, gpu_device_count, gpu_tensor_zeros, gpu_tensor_ones, gpu_tensor_to_cuda, gpu_tensor_is_cuda, gpu_tensor_numel, gpu_alloc_zeros, gpu_alloc_ones, gpu_stream_create, gpu_stream_sync, gpu_stream_query}

describe "GPU Runtime API":
    describe "Backend Detection":
        it "detects if GPU is available":
            val available = gpu_available()
            # Should return bool (true or false)
            expect(available == true or available == false).to_equal(true)

        it "returns backend name":
            val name = gpu_backend_name()
            # Should be "CUDA" or "CPU"
            expect(name == "CUDA" or name == "CPU").to_equal(true)

        skip_it "returns device count":
            # Skipped: Requires PyTorch FFI loaded
            val count = gpu_device_count()
            expect(count >= 0).to_equal(true)

    describe "Tensor Creation":
        skip_it "creates zero tensor":
            # Skipped: Requires PyTorch FFI
            val handle = gpu_tensor_zeros(10, 10)
            expect(handle > 0).to_equal(true)

        skip_it "creates ones tensor":
            # Skipped: Requires PyTorch FFI
            val handle = gpu_tensor_ones(10, 10)
            expect(handle > 0).to_equal(true)

        skip_it "reports correct element count":
            # Skipped: Requires PyTorch FFI
            val handle = gpu_tensor_zeros(5, 4)
            val count = gpu_tensor_numel(handle)
            expect(count).to_equal(20)

    describe "CUDA Transfer":
        skip_it "moves tensor to CUDA":
            # Skipped: Requires CUDA available
            val cpu_handle = gpu_tensor_zeros(10, 10)
            val gpu_handle = gpu_tensor_to_cuda(cpu_handle, 0)
            val is_cuda = gpu_tensor_is_cuda(gpu_handle)
            expect(is_cuda).to_equal(true)

        skip_it "detects CPU tensor correctly":
            # Skipped: Requires PyTorch FFI
            val handle = gpu_tensor_zeros(10, 10)
            val is_cuda = gpu_tensor_is_cuda(handle)
            expect(is_cuda).to_equal(false)

    describe "Allocation Helpers":
        skip_it "allocates zeros on CPU":
            # Skipped: Requires PyTorch FFI
            val handle = gpu_alloc_zeros(10, 10, use_gpu: false, device_id: 0)
            val is_cuda = gpu_tensor_is_cuda(handle)
            expect(is_cuda).to_equal(false)

        skip_it "allocates zeros on GPU":
            # Skipped: Requires CUDA
            val handle = gpu_alloc_zeros(10, 10, use_gpu: true, device_id: 0)
            val is_cuda = gpu_tensor_is_cuda(handle)
            expect(is_cuda).to_equal(true)

        skip_it "allocates ones on GPU":
            # Skipped: Requires CUDA
            val handle = gpu_alloc_ones(10, 10, use_gpu: true, device_id: 0)
            val is_cuda = gpu_tensor_is_cuda(handle)
            expect(is_cuda).to_equal(true)

    describe "Stream Operations":
        skip_it "creates CUDA stream":
            # Skipped: Requires CUDA
            val stream = gpu_stream_create(0)
            expect(stream > 0).to_equal(true)

        skip_it "synchronizes stream":
            # Skipped: Requires CUDA
            val stream = gpu_stream_create(0)
            gpu_stream_sync(stream)
            # Should complete without error

        skip_it "queries stream status":
            # Skipped: Requires CUDA
            val stream = gpu_stream_create(0)
            val complete = gpu_stream_query(stream)
            expect(complete == true or complete == false).to_equal(true)

    describe "Multi-GPU":
        skip_it "allocates on different devices":
            # Skipped: Requires multiple GPUs
            val gpu0 = gpu_alloc_zeros(10, 10, use_gpu: true, device_id: 0)
            val gpu1 = gpu_alloc_zeros(10, 10, use_gpu: true, device_id: 1)

            val is_cuda_0 = gpu_tensor_is_cuda(gpu0)
            val is_cuda_1 = gpu_tensor_is_cuda(gpu1)

            expect(is_cuda_0).to_equal(true)
            expect(is_cuda_1).to_equal(true)
