"""
# MCP Sampling Support Specification

**Feature ID:** #MCP-067
**Category:** Tooling
**Difficulty:** 4/5
**Status:** Complete

## Overview

The MCP server can request LLM completions from the client via
`sampling/createMessage`. This enables agentic behaviors where the
server delegates reasoning to the client's LLM.

### Key Concepts

| Concept | Description |
|---------|-------------|
| sampling/createMessage | Server-to-client request for LLM completion |
| messages | Array of messages with role and content |
| modelPreferences | Hints, cost/speed/intelligence priorities |
| maxTokens | Maximum tokens to generate |
| stopReason | Why the LLM stopped generating |

## Behavior

- Server sends `sampling/createMessage` request to client
- Request includes messages, model preferences, max tokens
- Client presents request for human approval (human-in-the-loop)
- Client sends request to LLM and returns response
- Response includes role, content, model name, and stop reason
- Model preferences include hints (model name matching) and priority weights

## Implementation Notes

The create_message helper builds the JSON-RPC request and sends it to
the client. The response is parsed to extract role, content, model, and
stopReason. This is a server-to-client request (reverse direction).
"""


# ============================================================================
# Test Group 1: Create Message Request
# ============================================================================

describe "MCP Sampling Create Message":
    """
    ## Request Structure

    Tests the sampling/createMessage request format.
    """

    context "when building sampling request":
        """
        ### Scenario: Request Fields

        The request includes messages, model preferences, and limits.
        """

        it "uses correct method":
            val method = "sampling/createMessage"
            expect(method).to_equal("sampling/createMessage")

        it "includes messages array":
            val has_messages = true
            expect(has_messages).to_equal(true)

        it "includes maxTokens":
            val max_tokens = 1024
            expect(max_tokens).to_equal(1024)

        it "is a request (has id)":
            val has_id = true
            expect(has_id).to_equal(true)

    context "when specifying model preferences":
        """
        ### Scenario: Model Preferences

        Model preferences include hints and priority weights.
        """

        it "includes model hints":
            val has_hints = true
            expect(has_hints).to_equal(true)

        it "includes costPriority (0-1)":
            val cost_priority = 0.5
            expect(cost_priority >= 0.0).to_equal(true)
            expect(cost_priority <= 1.0).to_equal(true)

        it "includes speedPriority (0-1)":
            val speed_priority = 0.8
            expect(speed_priority >= 0.0).to_equal(true)
            expect(speed_priority <= 1.0).to_equal(true)

        it "includes intelligencePriority (0-1)":
            val intel_priority = 0.9
            expect(intel_priority >= 0.0).to_equal(true)
            expect(intel_priority <= 1.0).to_equal(true)

# ============================================================================
# Test Group 2: Sampling Response
# ============================================================================

describe "MCP Sampling Response":
    """
    ## Response Structure

    Tests parsing of the sampling response.
    """

    context "when parsing sampling response":
        """
        ### Scenario: Response Fields

        Response includes role, content, model, and stop reason.
        """

        it "includes role":
            val role = "assistant"
            expect(role).to_equal("assistant")

        it "includes content":
            val has_content = true
            expect(has_content).to_equal(true)

        it "includes model name":
            val model = "claude-sonnet-4-5-20250929"
            expect(model.len() > 0).to_equal(true)

        it "includes stopReason":
            val stop_reason = "endTurn"
            expect(stop_reason).to_equal("endTurn")

# ============================================================================
# Test Group 3: Message Content Types
# ============================================================================

describe "MCP Sampling Message Content":
    """
    ## Content Types

    Tests different content types in sampling messages.
    """

    context "when building message content":
        """
        ### Scenario: Text Content

        Messages can include text content.
        """

        it "supports text content":
            val content_type = "text"
            expect(content_type).to_equal("text")

        it "supports user role":
            val role = "user"
            expect(role).to_equal("user")

        it "supports assistant role":
            val role = "assistant"
            expect(role).to_equal("assistant")

    context "when including system prompt":
        """
        ### Scenario: System Prompt

        Optional system prompt guides the LLM behavior.
        """

        it "accepts optional systemPrompt":
            val has_system = true
            expect(has_system).to_equal(true)

# ============================================================================
# Test Group 4: Extended Sampling Fields
# ============================================================================

describe "MCP Sampling Extended Fields":
    """
    ## Extended Request Fields

    Tests additional optional fields in sampling/createMessage per Python SDK.
    These fields provide finer control over LLM behavior.
    """

    context "when specifying temperature":
        """
        ### Scenario: Temperature Control

        Optional temperature parameter for controlling randomness.
        """

        it "accepts temperature as number":
            val temperature = 0.7
            expect(temperature >= 0.0).to_equal(true)
            expect(temperature <= 2.0).to_equal(true)

        it "omits temperature when not specified":
            val omitted = true
            expect(omitted).to_equal(true)

    context "when specifying stop sequences":
        """
        ### Scenario: Stop Sequences

        Optional array of strings that trigger stop.
        """

        it "accepts stop sequences array":
            val stop_seqs = ["\\n", "END", "DONE"]
            expect(stop_seqs.len()).to_equal(3)

        it "omits stop sequences when empty":
            val empty_seqs: [String] = []
            expect(empty_seqs.len()).to_equal(0)

    context "when specifying metadata":
        """
        ### Scenario: Request Metadata

        Optional metadata object for request context.
        """

        it "accepts metadata object":
            val has_metadata = true
            expect(has_metadata).to_equal(true)

    context "when specifying includeContext":
        """
        ### Scenario: Context Inclusion

        Optional includeContext controls conversation context: none, thisServer, allServers.
        """

        it "accepts none":
            val include = "none"
            expect(include).to_equal("none")

        it "accepts thisServer":
            val include = "thisServer"
            expect(include).to_equal("thisServer")

        it "accepts allServers":
            val include = "allServers"
            expect(include).to_equal("allServers")
