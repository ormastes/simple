# Performance Module Tests
#
# Tests for unified performance configuration, statistics, and benchmarks.

use std.spec.{check, check_msg}
use std.perf.{
    PerfConfig, PerfConfig__default, PerfConfig__development, PerfConfig__testing, PerfConfig__production,
    PerfProfile__Development__to_config, PerfProfile__Production__to_config,
    PerfProfile__LowLatency__to_config, PerfProfile__HighThroughput__to_config, PerfProfile__MemoryConstrained__to_config,
    PerfProfile__Development__description, PerfProfile__Production__description,
    OptimizationLevel__Minimal__description, OptimizationLevel__Balanced__description, OptimizationLevel__Aggressive__description,
    OPT_MINIMAL, OPT_BALANCED, OPT_AGGRESSIVE,
    get_config, set_config,
    ComponentStats, ComponentStats__new,
    PerfStats, PerfStats__empty, collect_stats, reset_all_stats,
    format_bytes, format_duration,
    BenchmarkConfig, BenchmarkConfig__default, BenchmarkConfig__quick, BenchmarkConfig__thorough,
    BenchmarkResult, BenchmarkResult__new,
    BenchmarkComparison,
    Benchmark, Benchmark__new,
    BenchmarkSuite, BenchmarkSuite__new,
    run_benchmark
}
use std.types.{Count, Nanos, ByteSize, Capacity, Reductions}

describe "OptimizationLevel":
    it "provides descriptions":
        check(OptimizationLevel__Minimal__description().contains("Minimal"))
        check(OptimizationLevel__Balanced__description().contains("Balanced"))
        check(OptimizationLevel__Aggressive__description().contains("Maximum"))

describe "PerfProfile":
    it "converts to config":
        val config = PerfProfile__Development__to_config()
        check(config.level == OPT_MINIMAL)
        check(config.detailed_stats == true)

    it "creates production config":
        val config = PerfProfile__Production__to_config()
        check(config.level == OPT_BALANCED)
        check(config.stats_collection_enabled == false)

    it "creates low latency config":
        val config = PerfProfile__LowLatency__to_config()
        check(config.scheduler_config.reductions_per_timeslice.value == 500)

    it "creates high throughput config":
        val config = PerfProfile__HighThroughput__to_config()
        check(config.scheduler_config.reductions_per_timeslice.value == 8000)

    it "creates memory constrained config":
        val config = PerfProfile__MemoryConstrained__to_config()
        check(config.mailbox_config.capacity.value == 50)

    it "provides descriptions":
        check(PerfProfile__Development__description().contains("debugging"))
        check(PerfProfile__Production__description().contains("production"))

describe "PerfConfig - Creation":
    it "creates default config":
        val config = PerfConfig__default()
        check(config.level == OPT_BALANCED)
        check(config.symbol_interning_enabled)
        check(config.persistent_collections_enabled)
        check(config.per_actor_gc_enabled)
        check(config.lazy_evaluation_enabled)

    it "creates development config":
        val config = PerfConfig__development()
        check(config.level == OPT_MINIMAL)
        check(not config.persistent_collections_enabled)
        check(config.detailed_stats)

    it "creates testing config":
        val config = PerfConfig__testing()
        check(config.scheduler_config.scheduler_count.value == 1)

    it "creates production config":
        val config = PerfConfig__production()
        check(not config.stats_collection_enabled)

describe "PerfConfig - Builder":
    it "sets optimization level":
        val base = PerfConfig__default()
        val config = base.with_optimization_level(OPT_AGGRESSIVE)
        check(config.level == OPT_AGGRESSIVE)

    it "sets actor heap size":
        val base = PerfConfig__default()
        val config = base.with_actor_heap_size(1024 * 1024)
        check(config.actor_heap_config.initial_size.value == 1024 * 1024)

    it "sets reductions":
        val base = PerfConfig__default()
        val config = base.with_reductions(4000)
        check(config.scheduler_config.reductions_per_timeslice.value == 4000)

    it "enables and disables stats":
        val base = PerfConfig__default()
        val config = base.with_stats_enabled(false)
        check(not config.stats_collection_enabled)

    it "sets large binary threshold":
        val base = PerfConfig__default()
        val config = base.with_large_binary_threshold(128)
        check(config.large_binary_threshold == 128)

    it "chains builders":
        val base = PerfConfig__default()
        val c1 = base.with_optimization_level(OPT_AGGRESSIVE)
        val c2 = c1.with_reductions(4000)
        val config = c2.with_stats_enabled(true)
        check(config.level == OPT_AGGRESSIVE)
        check(config.scheduler_config.reductions_per_timeslice.value == 4000)
        check(config.stats_collection_enabled)

describe "PerfConfig - Global":
    it "gets default global config":
        val config = get_config()
        check(config.level == OPT_BALANCED)

describe "ComponentStats":
    it "creates empty stats":
        val stats = ComponentStats__new("test", true)
        check(stats.name == "test")
        check(stats.enabled)
        check(stats.metric_count() == 0)

    it "adds metrics":
        var stats = ComponentStats__new("test", true)
        stats.add_metric("count", 100.0)
        stats.add_metric("rate", 0.5)
        check(stats.metric_count() == 2)
        check(stats.get_metric("count") == 100.0)
        check(stats.get_metric("rate") == 0.5)

    it "formats enabled component":
        var stats = ComponentStats__new("memory", true)
        stats.add_metric("used", 1024.0)
        val formatted = stats.fmt()
        check(formatted.contains("memory"))
        check(formatted.contains("1024"))

    it "formats disabled component":
        val stats = ComponentStats__new("feature", false)
        check(stats.fmt().contains("disabled"))

describe "PerfStats":
    it "creates empty stats":
        val stats = PerfStats__empty()
        check(stats.total_memory_bytes.value == 0)
        check(stats.total_gc_count.value == 0)

    it "lists component names":
        val stats = PerfStats__empty()
        val names = stats.component_names()
        check(names.contains("symbols"))
        check(names.contains("lazy"))
        check(names.contains("shared_heap"))

    it "gets component by name":
        val stats = PerfStats__empty()
        val lazy = stats.get_component("lazy")
        check(lazy.?)
        check(lazy.name == "lazy")
        val invalid = stats.get_component("invalid")
        check(not invalid.?)

    it "generates summary":
        val stats = PerfStats__empty()
        val s = stats.summary()
        check(s.contains("Performance Statistics"))

describe "PerfStats - Collection":
    it "collects stats":
        val stats = collect_stats()
        val lazy = stats.get_component("lazy")
        check(lazy.?)

    it "resets stats":
        reset_all_stats()
        val stats = collect_stats()
        val lazy = stats.get_component("lazy")
        check(lazy.?)
        check(lazy.get_metric("total_created") == 0.0)

describe "Formatting Utilities":
    it "formats bytes":
        check(format_bytes(500) == "500 B")
        check(format_bytes(1024).contains("KB"))
        check(format_bytes(1024 * 1024).contains("MB"))
        check(format_bytes(1024 * 1024 * 1024).contains("GB"))

    it "formats duration":
        check(format_duration(500).contains("ns"))
        check(format_duration(5000).contains("Âµs"))
        check(format_duration(5000000).contains("ms"))
        check(format_duration(5000000000).contains("s"))

describe "BenchmarkConfig":
    it "creates default config":
        val config = BenchmarkConfig__default()
        check(config.warmup_iterations.value == 10)
        check(config.iterations.value == 100)

    it "creates quick config":
        val config = BenchmarkConfig__quick()
        check(config.warmup_iterations.value == 3)
        check(config.iterations.value == 20)

    it "creates thorough config":
        val config = BenchmarkConfig__thorough()
        check(config.warmup_iterations.value == 50)
        check(config.iterations.value == 1000)

describe "BenchmarkResult":
    it "creates empty result":
        val result = BenchmarkResult__new("test")
        check(result.name == "test")
        check(result.iterations.value == 0)

    it "calculates ops per second":
        var result = BenchmarkResult__new("test")
        result.mean_time_ns = Nanos(value: 1000000)
        val ops = result.ops_per_second()
        check(ops == 1000.0)

    it "calculates time in different units":
        var result = BenchmarkResult__new("test")
        result.mean_time_ns = Nanos(value: 1000000)
        check(result.mean_time_us() == 1000.0)
        check(result.mean_time_ms() == 1.0)

    it "calculates coefficient of variation":
        var result = BenchmarkResult__new("test")
        result.mean_time_ns = Nanos(value: 1000)
        result.stddev_ns = Nanos(value: 100)
        val cv = result.coefficient_of_variation()
        check(cv == 0.1)

    it "generates summary":
        var result = BenchmarkResult__new("test_op")
        result.iterations = Count(value: 100)
        result.mean_time_ns = Nanos(value: 1000000)
        result.min_time_ns = Nanos(value: 900000)
        result.max_time_ns = Nanos(value: 1100000)
        val s = result.summary()
        check(s.contains("test_op"))
        check(s.contains("Iterations: 100"))

describe "Benchmark":
    it "creates simple benchmark":
        val bench = Benchmark__new("increment", fn(): ())
        check(bench.name == "increment")

    it "adds description":
        val base = Benchmark__new("test", fn(): ())
        val bench = base.with_description("A test benchmark")
        check(bench.description_text == "A test benchmark")

describe "BenchmarkSuite":
    it "creates empty suite":
        val suite = BenchmarkSuite__new("test_suite")
        check(suite.name == "test_suite")
        check(suite.benchmarks.len() == 0)

    it "adds benchmarks":
        var suite = BenchmarkSuite__new("test_suite")
        suite.add_fn("bench1", fn(): ())
        suite.add_fn("bench2", fn(): ())
        check(suite.benchmarks.len() == 2)

    it "sets config for suite":
        val base = BenchmarkSuite__new("test")
        val suite = base.with_config(BenchmarkConfig__quick())
        check(suite.config.iterations.value == 20)

describe "BenchmarkComparison":
    it "compares faster result":
        var baseline = BenchmarkResult__new("baseline")
        baseline.mean_time_ns = Nanos(value: 2000)
        var candidate = BenchmarkResult__new("candidate")
        candidate.mean_time_ns = Nanos(value: 1000)
        val comparison = baseline.compare_to(candidate)
        check(comparison.is_faster())
        check(comparison.speedup == 2.0)

    it "compares slower result":
        var baseline = BenchmarkResult__new("baseline")
        baseline.mean_time_ns = Nanos(value: 1000)
        var candidate = BenchmarkResult__new("candidate")
        candidate.mean_time_ns = Nanos(value: 2000)
        val comparison = baseline.compare_to(candidate)
        check(comparison.is_slower())
        check(comparison.speedup == 0.5)

    it "generates summary":
        var baseline = BenchmarkResult__new("old")
        baseline.mean_time_ns = Nanos(value: 2000)
        var candidate = BenchmarkResult__new("new_bench")
        candidate.mean_time_ns = Nanos(value: 1000)
        val comparison = baseline.compare_to(candidate)
        val s = comparison.summary()
        check(s.contains("new_bench"))
        check(s.contains("old"))
        check(s.contains("faster"))

describe "run_benchmark":
    it "runs simple benchmark":
        val result = run_benchmark("noop", 10, fn(): ())
        check(result.name == "noop")
        check(result.iterations.value == 10)

    it "calculates statistics":
        val result = run_benchmark("noop", 100, fn(): ())
        check(result.min_time_ns.value >= 0)
        check(result.max_time_ns.value >= result.min_time_ns.value)
