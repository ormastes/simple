# LLM Caret Types Specification

# Re-define types inline for test isolation (import compatibility pattern)
struct Message:
    role: text
    content: text

struct ChatRequest:
    provider: text
    model: text
    messages: [Message]
    system_prompt: text
    max_tokens: i64
    temperature: f64
    session_id: text
    max_turns: i64
    stream: bool
    json_schema: text
    tools: [text]
    extra_args: [text]

struct ChatResponse:
    content: text
    model: text
    provider: text
    session_id: text
    stop_reason: text
    input_tokens: i64
    output_tokens: i64
    error: text
    is_error: bool
    raw: text

struct StreamEvent:
    event_type: text
    content: text
    session_id: text
    model: text
    stop_reason: text
    input_tokens: i64
    output_tokens: i64

struct ProviderConfig:
    provider_type: text
    base_url: text
    api_key: text
    model: text
    cli_path: text
    python_path: text
    model_path: text
    extra_args: [text]

# Factory functions
fn new_message(role: text, content: text) -> Message:
    Message(role: role, content: content)

fn new_user_message(content: text) -> Message:
    Message(role: "user", content: content)

fn new_assistant_message(content: text) -> Message:
    Message(role: "assistant", content: content)

fn new_system_message(content: text) -> Message:
    Message(role: "system", content: content)

fn new_chat_request() -> ChatRequest:
    ChatRequest(
        provider: "",
        model: "",
        messages: [],
        system_prompt: "",
        max_tokens: 0,
        temperature: -1.0,
        session_id: "",
        max_turns: 0,
        stream: false,
        json_schema: "",
        tools: [],
        extra_args: []
    )

fn new_chat_request_with_prompt(prompt: text) -> ChatRequest:
    ChatRequest(
        provider: "",
        model: "",
        messages: [Message(role: "user", content: prompt)],
        system_prompt: "",
        max_tokens: 0,
        temperature: -1.0,
        session_id: "",
        max_turns: 0,
        stream: false,
        json_schema: "",
        tools: [],
        extra_args: []
    )

fn new_chat_response() -> ChatResponse:
    ChatResponse(
        content: "",
        model: "",
        provider: "",
        session_id: "",
        stop_reason: "",
        input_tokens: 0,
        output_tokens: 0,
        error: "",
        is_error: false,
        raw: ""
    )

fn new_error_response(error_msg: text) -> ChatResponse:
    ChatResponse(
        content: "",
        model: "",
        provider: "",
        session_id: "",
        stop_reason: "error",
        input_tokens: 0,
        output_tokens: 0,
        error: error_msg,
        is_error: true,
        raw: ""
    )

fn new_success_response(content: text) -> ChatResponse:
    ChatResponse(
        content: content,
        model: "",
        provider: "",
        session_id: "",
        stop_reason: "end_turn",
        input_tokens: 0,
        output_tokens: 0,
        error: "",
        is_error: false,
        raw: ""
    )

fn new_stream_event(event_type: text, content: text) -> StreamEvent:
    StreamEvent(
        event_type: event_type,
        content: content,
        session_id: "",
        model: "",
        stop_reason: "",
        input_tokens: 0,
        output_tokens: 0
    )

fn new_text_delta(content: text) -> StreamEvent:
    StreamEvent(
        event_type: "text_delta",
        content: content,
        session_id: "",
        model: "",
        stop_reason: "",
        input_tokens: 0,
        output_tokens: 0
    )

fn new_message_stop(stop_reason: text) -> StreamEvent:
    StreamEvent(
        event_type: "message_stop",
        content: "",
        session_id: "",
        model: "",
        stop_reason: stop_reason,
        input_tokens: 0,
        output_tokens: 0
    )

fn new_provider_config(provider_type: text) -> ProviderConfig:
    ProviderConfig(
        provider_type: provider_type,
        base_url: "",
        api_key: "",
        model: "",
        cli_path: "",
        python_path: "",
        model_path: "",
        extra_args: []
    )

fn is_user_message(msg: Message) -> bool:
    msg.role == "user"

fn is_assistant_message(msg: Message) -> bool:
    msg.role == "assistant"

fn is_system_message(msg: Message) -> bool:
    msg.role == "system"

fn response_ok(resp: ChatResponse) -> bool:
    not resp.is_error

fn response_has_content(resp: ChatResponse) -> bool:
    resp.content != "" and not resp.is_error

# ============================================================================
# Tests
# ============================================================================

describe "Message":
    it "creates with role and content":
        val msg = new_message("user", "Hello")
        expect(msg.role).to_equal("user")
        expect(msg.content).to_equal("Hello")

    it "creates user message":
        val msg = new_user_message("Hi there")
        expect(msg.role).to_equal("user")
        expect(msg.content).to_equal("Hi there")

    it "creates assistant message":
        val msg = new_assistant_message("I can help")
        expect(msg.role).to_equal("assistant")
        expect(msg.content).to_equal("I can help")

    it "creates system message":
        val msg = new_system_message("You are helpful")
        expect(msg.role).to_equal("system")
        expect(msg.content).to_equal("You are helpful")

    it "detects user message":
        val msg = new_user_message("test")
        expect(is_user_message(msg)).to_equal(true)
        expect(is_assistant_message(msg)).to_equal(false)

    it "detects assistant message":
        val msg = new_assistant_message("test")
        expect(is_assistant_message(msg)).to_equal(true)
        expect(is_user_message(msg)).to_equal(false)

    it "detects system message":
        val msg = new_system_message("test")
        expect(is_system_message(msg)).to_equal(true)
        expect(is_user_message(msg)).to_equal(false)

describe "ChatRequest":
    it "creates empty request with defaults":
        val req = new_chat_request()
        expect(req.provider).to_equal("")
        expect(req.model).to_equal("")
        expect(req.max_tokens).to_equal(0)
        expect(req.stream).to_equal(false)
        expect(req.session_id).to_equal("")

    it "creates request with prompt":
        val req = new_chat_request_with_prompt("Hello world")
        expect(req.messages.len()).to_equal(1)
        expect(req.messages[0].role).to_equal("user")
        expect(req.messages[0].content).to_equal("Hello world")

    it "has empty tools by default":
        val req = new_chat_request()
        expect(req.tools.len()).to_equal(0)

    it "has empty extra_args by default":
        val req = new_chat_request()
        expect(req.extra_args.len()).to_equal(0)

    it "has negative temperature as unset marker":
        val req = new_chat_request()
        expect(req.temperature < 0.0).to_equal(true)

    it "has zero max_turns as unset marker":
        val req = new_chat_request()
        expect(req.max_turns).to_equal(0)

describe "ChatResponse":
    it "creates empty response":
        val resp = new_chat_response()
        expect(resp.content).to_equal("")
        expect(resp.is_error).to_equal(false)
        expect(resp.error).to_equal("")

    it "creates error response":
        val resp = new_error_response("Connection failed")
        expect(resp.is_error).to_equal(true)
        expect(resp.error).to_equal("Connection failed")
        expect(resp.stop_reason).to_equal("error")
        expect(resp.content).to_equal("")

    it "creates success response":
        val resp = new_success_response("Hello!")
        expect(resp.content).to_equal("Hello!")
        expect(resp.is_error).to_equal(false)
        expect(resp.stop_reason).to_equal("end_turn")

    it "checks response ok":
        val ok = new_success_response("test")
        val err = new_error_response("fail")
        expect(response_ok(ok)).to_equal(true)
        expect(response_ok(err)).to_equal(false)

    it "checks response has content":
        val ok = new_success_response("hello")
        val empty = new_chat_response()
        val err = new_error_response("fail")
        expect(response_has_content(ok)).to_equal(true)
        expect(response_has_content(empty)).to_equal(false)
        expect(response_has_content(err)).to_equal(false)

    it "has zero token counts by default":
        val resp = new_chat_response()
        expect(resp.input_tokens).to_equal(0)
        expect(resp.output_tokens).to_equal(0)

describe "StreamEvent":
    it "creates generic event":
        val evt = new_stream_event("content_block_start", "")
        expect(evt.event_type).to_equal("content_block_start")
        expect(evt.content).to_equal("")

    it "creates text delta":
        val evt = new_text_delta("Hello ")
        expect(evt.event_type).to_equal("text_delta")
        expect(evt.content).to_equal("Hello ")

    it "creates message stop":
        val evt = new_message_stop("end_turn")
        expect(evt.event_type).to_equal("message_stop")
        expect(evt.stop_reason).to_equal("end_turn")
        expect(evt.content).to_equal("")

    it "has empty defaults":
        val evt = new_stream_event("test", "data")
        expect(evt.session_id).to_equal("")
        expect(evt.model).to_equal("")
        expect(evt.input_tokens).to_equal(0)

describe "ProviderConfig":
    it "creates with provider type":
        val cfg = new_provider_config("claude_cli")
        expect(cfg.provider_type).to_equal("claude_cli")
        expect(cfg.base_url).to_equal("")
        expect(cfg.api_key).to_equal("")

    it "has empty defaults":
        val cfg = new_provider_config("openai")
        expect(cfg.model).to_equal("")
        expect(cfg.cli_path).to_equal("")
        expect(cfg.python_path).to_equal("")
        expect(cfg.model_path).to_equal("")

    it "has empty extra args":
        val cfg = new_provider_config("local")
        expect(cfg.extra_args.len()).to_equal(0)
