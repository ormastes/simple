# Tests for Pure Simple Functional API

use std.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros}
use std.pure.nn.functional.{fn_relu, fn_sigmoid, fn_tanh, fn_gelu, fn_leaky_relu, fn_elu, fn_selu, fn_softmax, fn_log_softmax, fn_linear, fn_dropout, fn_max_pool1d, fn_avg_pool1d, fn_batch_norm, fn_normalize}

describe "Functional API":
    describe "Activation Functions":
        it "fn_relu zeros out negative values":
            val x = tensor_from_data([-2.0, -1.0, 0.0, 1.0, 2.0], [5])
            val y = fn_relu(x)
            expect(y.shape).to_equal([5])
            expect(y.data[0]).to_equal(0.0)
            expect(y.data[1]).to_equal(0.0)
            expect(y.data[2]).to_equal(0.0)
            expect(y.data[3]).to_equal(1.0)
            expect(y.data[4]).to_equal(2.0)

        it "fn_relu preserves positive values unchanged":
            val x = tensor_from_data([0.5, 1.5, 3.0], [3])
            val y = fn_relu(x)
            expect(y.data[0]).to_equal(0.5)
            expect(y.data[1]).to_equal(1.5)
            expect(y.data[2]).to_equal(3.0)

        it "fn_sigmoid outputs values between 0 and 1":
            val x = tensor_from_data([-10.0, 0.0, 10.0], [3])
            val y = fn_sigmoid(x)
            expect(y.shape).to_equal([3])
            # sigmoid(-10) ~ 0, sigmoid(0) = 0.5, sigmoid(10) ~ 1
            expect(y.data[0]).to_be_less_than(0.01)
            expect(y.data[1]).to_equal(0.5)
            expect(y.data[2]).to_be_greater_than(0.99)

        it "fn_tanh outputs values between -1 and 1":
            val x = tensor_from_data([-10.0, 0.0, 10.0], [3])
            val y = fn_tanh(x)
            expect(y.shape).to_equal([3])
            # tanh(-10) ~ -1, tanh(0) = 0, tanh(10) ~ 1
            expect(y.data[0]).to_be_less_than(-0.99)
            expect(y.data[1]).to_equal(0.0)
            expect(y.data[2]).to_be_greater_than(0.99)

        it "fn_gelu is smooth approximation of relu":
            val x = tensor_from_data([-2.0, 0.0, 1.0, 2.0], [4])
            val y = fn_gelu(x)
            expect(y.shape).to_equal([4])
            # GELU(-2) ~ -0.045, GELU(0) = 0, GELU(1) ~ 0.841, GELU(2) ~ 1.955
            expect(y.data[0]).to_be_less_than(0.0)
            expect(y.data[0]).to_be_greater_than(-0.1)
            expect(y.data[1]).to_equal(0.0)
            expect(y.data[2]).to_be_greater_than(0.8)
            expect(y.data[2]).to_be_less_than(0.9)
            expect(y.data[3]).to_be_greater_than(1.9)

        it "fn_leaky_relu allows negative slope":
            val x = tensor_from_data([-2.0, -1.0, 0.0, 1.0, 2.0], [5])
            val y = fn_leaky_relu(x, 0.1)
            expect(y.shape).to_equal([5])
            expect(y.data[0]).to_equal(-0.2)   # -2.0 * 0.1
            expect(y.data[1]).to_equal(-0.1)   # -1.0 * 0.1
            expect(y.data[2]).to_equal(0.0)
            expect(y.data[3]).to_equal(1.0)
            expect(y.data[4]).to_equal(2.0)

        it "fn_elu applies exponential for negative values":
            val x = tensor_from_data([-1.0, 0.0, 1.0], [3])
            val y = fn_elu(x, 1.0)
            expect(y.shape).to_equal([3])
            # ELU(-1) = 1.0 * (exp(-1) - 1) ~ -0.632
            expect(y.data[0]).to_be_less_than(-0.6)
            expect(y.data[0]).to_be_greater_than(-0.7)
            expect(y.data[1]).to_equal(0.0)
            expect(y.data[2]).to_equal(1.0)

        it "fn_selu uses fixed scale and alpha constants":
            val x = tensor_from_data([0.0, 1.0, 2.0], [3])
            val y = fn_selu(x)
            expect(y.shape).to_equal([3])
            # SELU(0) = 0 (scale * alpha * (exp(0) - 1) = 0)
            expect(y.data[0]).to_equal(0.0)
            # SELU(1) = scale * 1 ~ 1.0507
            expect(y.data[1]).to_be_greater_than(1.04)
            expect(y.data[1]).to_be_less_than(1.06)
            # SELU(2) = scale * 2 ~ 2.1014
            expect(y.data[2]).to_be_greater_than(2.09)
            expect(y.data[2]).to_be_less_than(2.11)

        it "fn_selu handles negative inputs":
            val x = tensor_from_data([-1.0], [1])
            val y = fn_selu(x)
            # SELU(-1) = scale * alpha * (exp(-1) - 1) ~ 1.0507 * 1.6733 * (-0.632) ~ -1.111
            expect(y.data[0]).to_be_less_than(-1.0)
            expect(y.data[0]).to_be_greater_than(-1.2)

        it "fn_softmax produces probabilities summing to 1":
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val y = fn_softmax(x, 0)
            expect(y.shape).to_equal([3])
            # Values should be positive
            expect(y.data[0]).to_be_greater_than(0.0)
            expect(y.data[1]).to_be_greater_than(0.0)
            expect(y.data[2]).to_be_greater_than(0.0)
            # Sum should be ~1.0
            val total = y.data[0] + y.data[1] + y.data[2]
            expect(total).to_be_greater_than(0.99)
            expect(total).to_be_less_than(1.01)
            # Largest input should have largest probability
            expect(y.data[2]).to_be_greater_than(y.data[1])
            expect(y.data[1]).to_be_greater_than(y.data[0])

        it "fn_softmax 2D along dim=1 normalizes rows":
            val x = tensor_from_data([1.0, 2.0, 3.0, 1.0, 2.0, 3.0], [2, 3])
            val y = fn_softmax(x, 1)
            expect(y.shape).to_equal([2, 3])
            # Each row should sum to 1
            val row0_sum = y.data[0] + y.data[1] + y.data[2]
            val row1_sum = y.data[3] + y.data[4] + y.data[5]
            expect(row0_sum).to_be_greater_than(0.99)
            expect(row0_sum).to_be_less_than(1.01)
            expect(row1_sum).to_be_greater_than(0.99)
            expect(row1_sum).to_be_less_than(1.01)

        it "fn_log_softmax produces log probabilities":
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val y = fn_log_softmax(x, 0)
            expect(y.shape).to_equal([3])
            # All log-softmax values should be negative (log of probability < 1)
            expect(y.data[0]).to_be_less_than(0.0)
            expect(y.data[1]).to_be_less_than(0.0)
            expect(y.data[2]).to_be_less_than(0.0)
            # Largest input should have least negative log probability
            expect(y.data[2]).to_be_greater_than(y.data[1])
            expect(y.data[1]).to_be_greater_than(y.data[0])

        it "fn_log_softmax is consistent with fn_softmax":
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val sm = fn_softmax(x, 0)
            val lsm = fn_log_softmax(x, 0)
            # exp(log_softmax) should approximately equal softmax
            # We check that the largest probability matches
            expect(lsm.data[2]).to_be_greater_than(-0.1)
            expect(lsm.data[2]).to_be_less_than(0.0)

    describe "Linear and Pooling":
        it "fn_linear computes y = x @ W^T for 1D input":
            # x: [3], weight: [2, 3] -> output: [2]
            val x = tensor_from_data([1.0, 2.0, 3.0], [3])
            val weight = tensor_from_data([
                1.0, 0.0, 0.0,
                0.0, 1.0, 0.0
            ], [2, 3])
            val y = fn_linear(x, weight, [])
            expect(y.shape).to_equal([2])
            expect(y.data[0]).to_equal(1.0)  # 1*1 + 2*0 + 3*0
            expect(y.data[1]).to_equal(2.0)  # 1*0 + 2*1 + 3*0

        it "fn_linear adds bias when provided":
            val x = tensor_from_data([1.0, 1.0], [2])
            val weight = tensor_from_data([1.0, 1.0, 1.0, 1.0], [2, 2])
            val y = fn_linear(x, weight, [10.0, 20.0])
            expect(y.shape).to_equal([2])
            expect(y.data[0]).to_equal(12.0)  # (1+1) + 10
            expect(y.data[1]).to_equal(22.0)  # (1+1) + 20

        it "fn_linear handles batch input":
            # x: [2, 3], weight: [2, 3] -> output: [2, 2]
            val x = tensor_from_data([
                1.0, 0.0, 0.0,
                0.0, 1.0, 0.0
            ], [2, 3])
            val weight = tensor_from_data([
                1.0, 2.0, 3.0,
                4.0, 5.0, 6.0
            ], [2, 3])
            val y = fn_linear(x, weight, [])
            expect(y.shape).to_equal([2, 2])
            # Batch 0: [1,0,0] @ W^T -> [1, 4]
            expect(y.data[0]).to_equal(1.0)
            expect(y.data[1]).to_equal(4.0)
            # Batch 1: [0,1,0] @ W^T -> [2, 5]
            expect(y.data[2]).to_equal(2.0)
            expect(y.data[3]).to_equal(5.0)

        it "fn_dropout returns input unchanged in eval mode":
            val x = tensor_from_data([1.0, 2.0, 3.0, 4.0], [4])
            val y = fn_dropout(x, 0.5, false)
            expect(y.data[0]).to_equal(1.0)
            expect(y.data[1]).to_equal(2.0)
            expect(y.data[2]).to_equal(3.0)
            expect(y.data[3]).to_equal(4.0)

        it "fn_dropout zeros some elements in training mode":
            val x = tensor_from_data([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [8])
            val y = fn_dropout(x, 0.5, true)
            # Some elements should be zero, others scaled by 2.0
            var num_zero = 0
            var i = 0
            while i < 8:
                if y.data[i] == 0.0:
                    num_zero = num_zero + 1
                i = i + 1
            # With p=0.5, expect roughly half to be zeroed (deterministic pattern)
            expect(num_zero).to_be_greater_than(0)
            expect(num_zero).to_be_less_than(8)

        it "fn_max_pool1d selects maximum in each window":
            # Input: [1, 1, 6] - single batch, single channel, length 6
            val x = tensor_from_data([1.0, 3.0, 2.0, 5.0, 4.0, 6.0], [1, 1, 6])
            val y = fn_max_pool1d(x, 2, 2)
            expect(y.shape).to_equal([1, 1, 3])
            expect(y.data[0]).to_equal(3.0)  # max(1, 3)
            expect(y.data[1]).to_equal(5.0)  # max(2, 5)
            expect(y.data[2]).to_equal(6.0)  # max(4, 6)

        it "fn_max_pool1d handles stride 1":
            val x = tensor_from_data([1.0, 4.0, 2.0, 3.0], [1, 1, 4])
            val y = fn_max_pool1d(x, 2, 1)
            expect(y.shape).to_equal([1, 1, 3])
            expect(y.data[0]).to_equal(4.0)  # max(1, 4)
            expect(y.data[1]).to_equal(4.0)  # max(4, 2)
            expect(y.data[2]).to_equal(3.0)  # max(2, 3)

        it "fn_max_pool1d handles multiple channels":
            val x = tensor_from_data([
                1.0, 3.0, 2.0, 4.0,
                5.0, 7.0, 6.0, 8.0
            ], [1, 2, 4])
            val y = fn_max_pool1d(x, 2, 2)
            expect(y.shape).to_equal([1, 2, 2])
            # Channel 0: max(1,3)=3, max(2,4)=4
            expect(y.data[0]).to_equal(3.0)
            expect(y.data[1]).to_equal(4.0)
            # Channel 1: max(5,7)=7, max(6,8)=8
            expect(y.data[2]).to_equal(7.0)
            expect(y.data[3]).to_equal(8.0)

        it "fn_avg_pool1d computes average in each window":
            val x = tensor_from_data([1.0, 3.0, 2.0, 4.0, 5.0, 1.0], [1, 1, 6])
            val y = fn_avg_pool1d(x, 2, 2)
            expect(y.shape).to_equal([1, 1, 3])
            expect(y.data[0]).to_equal(2.0)  # avg(1, 3)
            expect(y.data[1]).to_equal(3.0)  # avg(2, 4)
            expect(y.data[2]).to_equal(3.0)  # avg(5, 1)

        it "fn_avg_pool1d handles kernel size 3":
            val x = tensor_from_data([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [1, 1, 6])
            val y = fn_avg_pool1d(x, 3, 3)
            expect(y.shape).to_equal([1, 1, 2])
            expect(y.data[0]).to_equal(2.0)  # avg(1, 2, 3)
            expect(y.data[1]).to_equal(5.0)  # avg(4, 5, 6)

    describe "Normalization":
        it "fn_batch_norm normalizes with given statistics":
            # Input: [3] with mean=2, var=4, weight=1, bias=0
            val x = tensor_from_data([0.0, 2.0, 4.0], [3])
            val mean_t = tensor_from_data([2.0, 2.0, 2.0], [3])
            val var_t = tensor_from_data([4.0, 4.0, 4.0], [3])
            val weight_t = tensor_from_data([1.0, 1.0, 1.0], [3])
            val bias_t = tensor_from_data([0.0, 0.0, 0.0], [3])
            val y = fn_batch_norm(x, mean_t, var_t, weight_t, bias_t, 0.0)
            expect(y.shape).to_equal([3])
            # (0 - 2) / sqrt(4) = -1.0
            expect(y.data[0]).to_equal(-1.0)
            # (2 - 2) / sqrt(4) = 0.0
            expect(y.data[1]).to_equal(0.0)
            # (4 - 2) / sqrt(4) = 1.0
            expect(y.data[2]).to_equal(1.0)

        it "fn_batch_norm applies weight and bias":
            val x = tensor_from_data([0.0, 2.0], [2])
            val mean_t = tensor_from_data([0.0, 0.0], [2])
            val var_t = tensor_from_data([1.0, 1.0], [2])
            val weight_t = tensor_from_data([2.0, 3.0], [2])
            val bias_t = tensor_from_data([1.0, -1.0], [2])
            val y = fn_batch_norm(x, mean_t, var_t, weight_t, bias_t, 0.0)
            # (0 - 0) / sqrt(1) * 2 + 1 = 1.0
            expect(y.data[0]).to_equal(1.0)
            # (2 - 0) / sqrt(1) * 3 + (-1) = 5.0
            expect(y.data[1]).to_equal(5.0)

        it "fn_batch_norm handles batch input":
            val x = tensor_from_data([
                1.0, 2.0,
                3.0, 4.0
            ], [2, 2])
            val mean_t = tensor_from_data([0.0, 0.0], [2])
            val var_t = tensor_from_data([1.0, 1.0], [2])
            val weight_t = tensor_from_data([1.0, 1.0], [2])
            val bias_t = tensor_from_data([0.0, 0.0], [2])
            val y = fn_batch_norm(x, mean_t, var_t, weight_t, bias_t, 0.0)
            expect(y.shape).to_equal([2, 2])
            expect(y.data[0]).to_equal(1.0)
            expect(y.data[1]).to_equal(2.0)
            expect(y.data[2]).to_equal(3.0)
            expect(y.data[3]).to_equal(4.0)

        it "fn_normalize produces unit L2 norm":
            val x = tensor_from_data([3.0, 4.0], [2])
            val y = fn_normalize(x, 2.0)
            expect(y.shape).to_equal([2])
            # norm = sqrt(9 + 16) = 5
            # [3/5, 4/5] = [0.6, 0.8]
            expect(y.data[0]).to_equal(0.6)
            expect(y.data[1]).to_equal(0.8)

        it "fn_normalize handles L1 norm":
            val x = tensor_from_data([2.0, -3.0], [2])
            val y = fn_normalize(x, 1.0)
            expect(y.shape).to_equal([2])
            # L1 norm = |2| + |-3| = 5
            expect(y.data[0]).to_equal(0.4)   # 2/5
            expect(y.data[1]).to_equal(-0.6)  # -3/5

        it "fn_normalize handles zero vector gracefully":
            val x = tensor_from_data([0.0, 0.0], [2])
            val y = fn_normalize(x, 2.0)
            expect(y.shape).to_equal([2])
            # With eps = 1e-12, dividing 0 by eps yields ~0
            expect(y.data[0]).to_equal(0.0)
            expect(y.data[1]).to_equal(0.0)
