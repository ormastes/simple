# Tests for Pure Simple Attention Mechanism

use lib.pure.tensor.{PureTensor, tensor_from_data, tensor_zeros}
use lib.pure.nn.attention.{AttentionOutput, scaled_dot_product_attention, scaled_dot_product_attention_forward, row_softmax, MultiHeadAttention, multi_head_attention_create}

describe "Attention Mechanism":
    describe "row_softmax":
        it "computes softmax for a single row":
            val t = tensor_from_data([1.0, 2.0, 3.0], [1, 3])
            val result = row_softmax(t)
            expect(result.shape).to_equal([1, 3])
            # Softmax values should sum to 1.0
            val total = result.data[0] + result.data[1] + result.data[2]
            # Allow small floating point error
            expect(total).to_be_greater_than(0.99)
            expect(total).to_be_less_than(1.01)

        it "produces monotonically increasing outputs for increasing inputs":
            val t = tensor_from_data([1.0, 2.0, 3.0], [1, 3])
            val result = row_softmax(t)
            expect(result.data[0]).to_be_less_than(result.data[1])
            expect(result.data[1]).to_be_less_than(result.data[2])

        it "handles multiple rows independently":
            val t = tensor_from_data([1.0, 0.0, 0.0, 1.0], [2, 2])
            val result = row_softmax(t)
            expect(result.shape).to_equal([2, 2])
            # Row 0: softmax([1, 0]) -> [>0.5, <0.5]
            expect(result.data[0]).to_be_greater_than(0.5)
            expect(result.data[1]).to_be_less_than(0.5)
            # Row 1: softmax([0, 1]) -> [<0.5, >0.5]
            expect(result.data[2]).to_be_less_than(0.5)
            expect(result.data[3]).to_be_greater_than(0.5)

        it "handles equal values (uniform distribution)":
            val t = tensor_from_data([1.0, 1.0, 1.0, 1.0], [1, 4])
            val result = row_softmax(t)
            # All values should be 0.25
            expect(result.data[0]).to_be_greater_than(0.24)
            expect(result.data[0]).to_be_less_than(0.26)
            expect(result.data[1]).to_be_greater_than(0.24)
            expect(result.data[1]).to_be_less_than(0.26)

        it "handles large differences without overflow":
            val t = tensor_from_data([0.0, 10.0], [1, 2])
            val result = row_softmax(t)
            # exp(0-10) is very small, so [~0, ~1]
            expect(result.data[0]).to_be_less_than(0.01)
            expect(result.data[1]).to_be_greater_than(0.99)

    describe "scaled_dot_product_attention":
        it "computes attention with identity-like query-key alignment":
            # Q and K are identity-like: each query attends to matching key
            val query = tensor_from_data([
                1.0, 0.0,
                0.0, 1.0
            ], [2, 2])
            val key = tensor_from_data([
                1.0, 0.0,
                0.0, 1.0
            ], [2, 2])
            val value = tensor_from_data([
                10.0, 20.0,
                30.0, 40.0
            ], [2, 2])

            val result = scaled_dot_product_attention(query, key, value, nil)
            val output = result.output
            val weights = result.weights

            # Output shape should be [2, 2]
            expect(output.shape).to_equal([2, 2])
            # Weights shape should be [2, 2]
            expect(weights.shape).to_equal([2, 2])
            # Query 0 should attend more to key 0 (higher dot product)
            expect(weights.data[0]).to_be_greater_than(weights.data[1])
            # Query 1 should attend more to key 1
            expect(weights.data[3]).to_be_greater_than(weights.data[2])

        it "returns correct output dimensions":
            val query = tensor_from_data([1.0, 0.0, 0.0, 1.0, 1.0, 1.0], [3, 2])
            val key = tensor_from_data([1.0, 0.0, 0.0, 1.0], [2, 2])
            val value = tensor_from_data([1.0, 2.0, 3.0, 3.0, 4.0, 5.0], [2, 3])

            val result = scaled_dot_product_attention(query, key, value, nil)
            # Output: [seq_q=3, d_v=3]
            expect(result.output.shape).to_equal([3, 3])
            # Weights: [seq_q=3, seq_k=2]
            expect(result.weights.shape).to_equal([3, 2])

        it "applies mask to block positions":
            val query = tensor_from_data([1.0, 0.0], [1, 2])
            val key = tensor_from_data([1.0, 0.0, 0.0, 1.0], [2, 2])
            val value = tensor_from_data([10.0, 30.0], [2, 1])

            # Mask: block key position 1 (second key)
            val mask = tensor_from_data([0.0, -999999.0], [1, 2])

            val result = scaled_dot_product_attention(query, key, value, mask)
            # With key 1 blocked, all attention should go to key 0
            expect(result.weights.data[0]).to_be_greater_than(0.99)
            expect(result.weights.data[1]).to_be_less_than(0.01)
            # Output should be close to value[0] = 10.0
            expect(result.output.data[0]).to_be_greater_than(9.5)

        it "attention weights sum to one per query":
            val query = tensor_from_data([1.0, 2.0, 3.0, 4.0], [2, 2])
            val key = tensor_from_data([0.5, 0.5, 1.0, 0.0, 0.0, 1.0], [3, 2])
            val value = tensor_from_data([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [3, 2])

            val result = scaled_dot_product_attention(query, key, value, nil)
            # Row 0 weights sum
            val sum0 = result.weights.data[0] + result.weights.data[1] + result.weights.data[2]
            expect(sum0).to_be_greater_than(0.99)
            expect(sum0).to_be_less_than(1.01)
            # Row 1 weights sum
            val sum1 = result.weights.data[3] + result.weights.data[4] + result.weights.data[5]
            expect(sum1).to_be_greater_than(0.99)
            expect(sum1).to_be_less_than(1.01)

    describe "scaled_dot_product_attention_forward":
        it "computes attention without mask":
            val query = tensor_from_data([1.0, 0.0], [1, 2])
            val key = tensor_from_data([1.0, 0.0, 0.0, 1.0], [2, 2])
            val value = tensor_from_data([10.0, 30.0], [2, 1])

            val result = scaled_dot_product_attention_forward(query, key, value)
            expect(result.output.shape).to_equal([1, 1])
            expect(result.weights.shape).to_equal([1, 2])

    describe "MultiHeadAttention":
        describe "Creation":
            it "creates multi-head attention with correct parameters":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)
                expect(mha.d_model).to_equal(4)
                expect(mha.num_heads).to_equal(2)
                expect(mha.d_k).to_equal(2)
                expect(mha.training).to_equal(true)

            it "initializes weight matrices with correct shapes":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)
                expect(mha.w_q.shape).to_equal([4, 4])
                expect(mha.w_k.shape).to_equal([4, 4])
                expect(mha.w_v.shape).to_equal([4, 4])
                expect(mha.w_o.shape).to_equal([4, 4])

            it "creates via factory function":
                val mha = multi_head_attention_create(8, 4)
                expect(mha.d_model).to_equal(8)
                expect(mha.num_heads).to_equal(4)
                expect(mha.d_k).to_equal(2)

        describe "Forward Pass":
            it "produces correct output shape":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)

                val query = tensor_from_data([
                    1.0, 0.0, 0.0, 0.0,
                    0.0, 1.0, 0.0, 0.0,
                    0.0, 0.0, 1.0, 0.0
                ], [3, 4])
                val key = tensor_from_data([
                    1.0, 0.0, 0.0, 0.0,
                    0.0, 1.0, 0.0, 0.0
                ], [2, 4])
                val value = tensor_from_data([
                    1.0, 2.0, 3.0, 4.0,
                    5.0, 6.0, 7.0, 8.0
                ], [2, 4])

                val output = mha.forward(query, key, value)
                # Output shape: [seq_q=3, d_model=4]
                expect(output.shape).to_equal([3, 4])
                expect(output.data.len()).to_equal(12)

            it "produces finite output values":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)

                val query = tensor_from_data([1.0, 0.5, 0.3, 0.1], [1, 4])
                val key = tensor_from_data([0.5, 1.0, 0.2, 0.8], [1, 4])
                val value = tensor_from_data([2.0, 3.0, 4.0, 5.0], [1, 4])

                val output = mha.forward(query, key, value)
                expect(output.shape).to_equal([1, 4])
                # Values should be finite (not NaN or huge)
                var i = 0
                while i < 4:
                    expect(output.data[i]).to_be_greater_than(-1000.0)
                    expect(output.data[i]).to_be_less_than(1000.0)
                    i = i + 1

            it "handles self-attention (Q=K=V)":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)

                val x = tensor_from_data([
                    1.0, 0.0, 1.0, 0.0,
                    0.0, 1.0, 0.0, 1.0
                ], [2, 4])

                val output = mha.forward(x, x, x)
                expect(output.shape).to_equal([2, 4])

        describe "Parameters":
            it "has four trainable weight matrices":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)
                val params = mha.parameters()
                expect(params.len()).to_equal(4)

            it "weight parameters have correct shapes":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)
                val params = mha.parameters()
                expect(params[0].shape).to_equal([4, 4])
                expect(params[1].shape).to_equal([4, 4])
                expect(params[2].shape).to_equal([4, 4])
                expect(params[3].shape).to_equal([4, 4])

        describe "Layer Interface":
            it "switches between train and eval modes":
                val mha = MultiHeadAttention.create(d_model: 4, num_heads: 2)
                expect(mha.training).to_equal(true)

                mha.eval_mode()
                expect(mha.training).to_equal(false)

                mha.train_mode()
                expect(mha.training).to_equal(true)

            it "has string representation":
                val mha = MultiHeadAttention.create(d_model: 8, num_heads: 4)
                val s = mha.to_string()
                expect(s.contains("MultiHeadAttention")).to_equal(true)
                expect(s.contains("8")).to_equal(true)
                expect(s.contains("4")).to_equal(true)
                expect(s.contains("2")).to_equal(true)
