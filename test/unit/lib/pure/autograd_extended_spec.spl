# Tests for extended autograd operations

use std.pure.autograd.{Tensor, backward, tensor_from_data, tensor_div, tensor_sigmoid, tensor_tanh, tensor_exp_op, tensor_log_op, tensor_neg, tensor_softmax, tensor_add, tensor_mul}

describe "autograd extended operations":
    describe "tensor_div":
        it "computes forward pass correctly":
            val a = tensor_from_data([6.0, 8.0], [2], true)
            val b = tensor_from_data([2.0, 4.0], [2], true)
            val c = tensor_div(a, b)
            expect(c.value.data[0]).to_equal(3.0)
            expect(c.value.data[1]).to_equal(2.0)

        it "computes gradients for division":
            val a = tensor_from_data([6.0], [1], true)
            val b = tensor_from_data([3.0], [1], true)
            val c = tensor_div(a, b)
            backward(c)
            # dc/da = 1/b = 1/3
            expect(a.grad.?).to_equal(true)
            expect(a.grad.unwrap().data[0]).to_be_greater_than(0.32)
            expect(a.grad.unwrap().data[0]).to_be_less_than(0.34)

    describe "tensor_sigmoid":
        it "computes forward pass":
            val x = tensor_from_data([0.0], [1], true)
            val y = tensor_sigmoid(x)
            # sigmoid(0) = 0.5
            expect(y.value.data[0]).to_be_greater_than(0.49)
            expect(y.value.data[0]).to_be_less_than(0.51)

        it "computes gradients":
            val x = tensor_from_data([0.0], [1], true)
            val y = tensor_sigmoid(x)
            backward(y)
            # d(sigmoid(0))/dx = sigmoid(0) * (1 - sigmoid(0)) = 0.25
            expect(x.grad.?).to_equal(true)
            expect(x.grad.unwrap().data[0]).to_be_greater_than(0.24)
            expect(x.grad.unwrap().data[0]).to_be_less_than(0.26)

    describe "tensor_tanh":
        it "computes forward pass":
            val x = tensor_from_data([0.0], [1], true)
            val y = tensor_tanh(x)
            # tanh(0) = 0
            expect(y.value.data[0]).to_be_greater_than(-0.01)
            expect(y.value.data[0]).to_be_less_than(0.01)

        it "computes gradients at zero":
            val x = tensor_from_data([0.0], [1], true)
            val y = tensor_tanh(x)
            backward(y)
            # d(tanh(0))/dx = 1 - tanh^2(0) = 1
            expect(x.grad.?).to_equal(true)
            expect(x.grad.unwrap().data[0]).to_be_greater_than(0.99)
            expect(x.grad.unwrap().data[0]).to_be_less_than(1.01)

    describe "tensor_exp_op":
        it "computes exp(0) = 1":
            val x = tensor_from_data([0.0], [1], true)
            val y = tensor_exp_op(x)
            expect(y.value.data[0]).to_equal(1.0)

        it "computes gradients for exp":
            val x = tensor_from_data([0.0], [1], true)
            val y = tensor_exp_op(x)
            backward(y)
            # d(exp(0))/dx = exp(0) = 1
            expect(x.grad.?).to_equal(true)
            expect(x.grad.unwrap().data[0]).to_equal(1.0)

    describe "tensor_log_op":
        it "computes log(1) = 0":
            val x = tensor_from_data([1.0], [1], true)
            val y = tensor_log_op(x)
            expect(y.value.data[0]).to_be_greater_than(-0.01)
            expect(y.value.data[0]).to_be_less_than(0.01)

        it "computes gradients for log":
            val x = tensor_from_data([2.0], [1], true)
            val y = tensor_log_op(x)
            backward(y)
            # d(log(2))/dx = 1/2 = 0.5
            expect(x.grad.?).to_equal(true)
            expect(x.grad.unwrap().data[0]).to_be_greater_than(0.49)
            expect(x.grad.unwrap().data[0]).to_be_less_than(0.51)

    describe "tensor_neg":
        it "negates values":
            val x = tensor_from_data([3.0, -2.0], [2], true)
            val y = tensor_neg(x)
            expect(y.value.data[0]).to_equal(-3.0)
            expect(y.value.data[1]).to_equal(2.0)

        it "computes gradients for negation":
            val x = tensor_from_data([5.0], [1], true)
            val y = tensor_neg(x)
            backward(y)
            # d(-x)/dx = -1
            expect(x.grad.?).to_equal(true)
            expect(x.grad.unwrap().data[0]).to_equal(-1.0)

    describe "tensor_softmax":
        it "outputs sum to 1":
            val x = tensor_from_data([1.0, 2.0, 3.0], [3], false)
            val y = tensor_softmax(x)
            val total = y.value.data[0] + y.value.data[1] + y.value.data[2]
            expect(total).to_be_greater_than(0.99)
            expect(total).to_be_less_than(1.01)

        it "preserves ordering":
            val x = tensor_from_data([1.0, 2.0, 3.0], [3], false)
            val y = tensor_softmax(x)
            expect(y.value.data[2]).to_be_greater_than(y.value.data[1])
            expect(y.value.data[1]).to_be_greater_than(y.value.data[0])

    describe "chain rule":
        it "chains add and sigmoid":
            val a = tensor_from_data([0.0], [1], true)
            val b = tensor_from_data([0.0], [1], true)
            val c = tensor_add(a, b)
            val d = tensor_sigmoid(c)
            backward(d)
            # sigmoid(0) backward: grad = 0.25 for both inputs
            expect(a.grad.?).to_equal(true)
            expect(b.grad.?).to_equal(true)

        it "chains mul and exp":
            val a = tensor_from_data([1.0], [1], true)
            val b = tensor_from_data([0.0], [1], true)
            val c = tensor_mul(a, b)
            val d = tensor_exp_op(c)
            backward(d)
            # exp(0) = 1, d(exp(a*b))/da = b * exp(a*b) = 0 * 1 = 0
            expect(a.grad.?).to_equal(true)
