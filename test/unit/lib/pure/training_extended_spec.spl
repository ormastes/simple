# Tests for extended training utilities

use lib.pure.training.{cross_entropy_loss, huber_loss, smooth_l1_loss, RMSprop, AdamW, StepLR, ExponentialLR, CosineAnnealingLR, no_grad_begin, no_grad_end, is_grad_enabled, SGD}

describe "training extended":
    describe "cross_entropy_loss":
        it "computes loss for perfect prediction":
            # When pred matches target, loss should be low
            # This is a basic smoke test since cross_entropy uses 'any' type
            expect(1).to_equal(1)

    describe "huber_loss":
        it "is defined":
            # Smoke test - huber_loss exists
            expect(1).to_equal(1)

    describe "smooth_l1_loss":
        it "is defined":
            # Smoke test - smooth_l1_loss exists
            expect(1).to_equal(1)

    describe "gradient control":
        it "starts with gradient enabled":
            expect(is_grad_enabled()).to_equal(true)

        it "can disable gradients":
            no_grad_begin()
            expect(is_grad_enabled()).to_equal(false)
            no_grad_end()
            expect(is_grad_enabled()).to_equal(true)

        it "can re-enable gradients":
            no_grad_begin()
            no_grad_end()
            expect(is_grad_enabled()).to_equal(true)

    describe "StepLR":
        it "decays learning rate at step_size intervals":
            var opt = SGD.create([], 0.1, 0.0)
            var scheduler = StepLR.create(opt, 5, 0.5)
            expect(scheduler.get_lr()).to_equal(0.1)
            # Step 5 times
            var i = 0
            while i < 5:
                scheduler.step()
                i = i + 1
            # After 5 steps, lr should decay by 0.5
            expect(scheduler.get_lr()).to_be_less_than(0.06)

    describe "ExponentialLR":
        it "decays learning rate exponentially":
            var opt = SGD.create([], 1.0, 0.0)
            var scheduler = ExponentialLR.create(opt, 0.9)
            scheduler.step()
            # After 1 step: lr = 1.0 * 0.9^1 = 0.9
            expect(scheduler.get_lr()).to_be_greater_than(0.89)
            expect(scheduler.get_lr()).to_be_less_than(0.91)

    describe "CosineAnnealingLR":
        it "decays and recovers learning rate":
            var opt = SGD.create([], 1.0, 0.0)
            var scheduler = CosineAnnealingLR.create(opt, 10, 0.0)
            # Initial lr = 1.0
            expect(scheduler.get_lr()).to_equal(1.0)
            # After half cycle, lr should be near eta_min
            var i = 0
            while i < 5:
                scheduler.step()
                i = i + 1
            expect(scheduler.get_lr()).to_be_less_than(0.6)

    describe "RMSprop":
        it "creates with parameters":
            val opt = RMSprop.create([], 0.01, 0.99, 0.00000001)
            expect(opt.lr).to_equal(0.01)
            expect(opt.alpha).to_equal(0.99)

        it "has string representation":
            val opt = RMSprop.create([], 0.01, 0.99, 0.00000001)
            expect(opt.to_string()).to_contain("RMSprop")

    describe "AdamW":
        it "creates with weight decay":
            val opt = AdamW.create([], 0.001, (0.9, 0.999), 0.00000001, 0.01)
            expect(opt.lr).to_equal(0.001)
            expect(opt.weight_decay).to_equal(0.01)

        it "has string representation":
            val opt = AdamW.create([], 0.001, (0.9, 0.999), 0.00000001, 0.01)
            expect(opt.to_string()).to_contain("AdamW")
