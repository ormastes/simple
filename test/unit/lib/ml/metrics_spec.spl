# ML Engine Metrics BDD Test Specification
# Feature: Classification and Regression Metrics for Training Engine
#
# Tests the Accuracy, Loss, MSE, MAE, and RMSE metrics
# using the reset/update/compute pattern.

class Metric:
    fn compute() -> f64:
        return 0.0

class Accuracy:
    correct: f64
    total: f64

impl Accuracy:
    me reset():
        self.correct = 0.0
        self.total = 0.0

    me update(output):
        var preds = output["pred"]
        var labels = output["labels"]
        if preds == nil:
            preds = output["y_pred"]
            labels = output["y_true"]
        if preds == nil:
            return
        var i = 0
        val n = preds.len()
        while i < n:
            if preds[i] == labels[i]:
                self.correct = self.correct + 1.0
            self.total = self.total + 1.0
            i = i + 1

    fn compute() -> f64:
        if self.total == 0.0:
            return 0.0
        return self.correct / self.total

class Loss:
    total_loss: f64
    count: f64

impl Loss:
    me reset():
        self.total_loss = 0.0
        self.count = 0.0

    me update(output):
        val v = output["loss"]
        if v != nil:
            self.total_loss = self.total_loss + v
            self.count = self.count + 1.0

    fn compute() -> f64:
        if self.count == 0.0:
            return 0.0
        return self.total_loss / self.count

class MSE:
    sum_sq: f64
    count: f64

impl MSE:
    me reset():
        self.sum_sq = 0.0
        self.count = 0.0

    me update(output):
        var preds = output["pred"]
        var actuals = output["actual"]
        if preds == nil:
            preds = output["y_pred"]
            actuals = output["y_true"]
        if preds == nil:
            return
        var i = 0
        val n = preds.len()
        while i < n:
            val diff = preds[i] - actuals[i]
            self.sum_sq = self.sum_sq + diff * diff
            self.count = self.count + 1.0
            i = i + 1

    fn compute() -> f64:
        if self.count == 0.0:
            return 0.0
        return self.sum_sq / self.count

class MAE:
    sum_abs: f64
    count: f64

impl MAE:
    me reset():
        self.sum_abs = 0.0
        self.count = 0.0

    me update(output):
        var preds = output["pred"]
        var actuals = output["actual"]
        if preds == nil:
            preds = output["y_pred"]
            actuals = output["y_true"]
        if preds == nil:
            return
        var i = 0
        val n = preds.len()
        while i < n:
            var diff = preds[i] - actuals[i]
            if diff < 0.0:
                diff = 0.0 - diff
            self.sum_abs = self.sum_abs + diff
            self.count = self.count + 1.0
            i = i + 1

    fn compute() -> f64:
        if self.count == 0.0:
            return 0.0
        return self.sum_abs / self.count

class RMSE:
    sum_sq: f64
    count: f64

impl RMSE:
    me reset():
        self.sum_sq = 0.0
        self.count = 0.0

    me update(output):
        var preds = output["pred"]
        var actuals = output["actual"]
        if preds == nil:
            preds = output["y_pred"]
            actuals = output["y_true"]
        if preds == nil:
            return
        var i = 0
        val n = preds.len()
        while i < n:
            val diff = preds[i] - actuals[i]
            self.sum_sq = self.sum_sq + diff * diff
            self.count = self.count + 1.0
            i = i + 1

    fn compute() -> f64:
        if self.count == 0.0:
            return 0.0
        val mse = self.sum_sq / self.count
        return mse ** 0.5


describe "ML Engine Metrics":
    """
    ## Metrics System Overview

    The ML metrics system provides automatic computation of training and
    evaluation metrics. All metrics follow the reset/update/compute pattern:

    - **reset()**: Clear accumulated state for new epoch
    - **update(output)**: Process batch output and accumulate statistics
    - **compute()**: Calculate final metric value from accumulated state

    This design enables streaming computation without storing all data.
    """

    describe "Accuracy metric":
        """
        Classification accuracy: correct predictions / total predictions.

        Supported output formats:
        - Dict: {"pred": [...], "labels": [...]}
        - Dict: {"y_pred": [...], "y_true": [...]}
        - Tuple: ([predictions], [labels])
        """

        it "computes accuracy for perfect predictions":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"pred": [0, 1, 2], "labels": [0, 1, 2]})
            expect acc.compute() == 1.0

        it "computes accuracy for partial matches":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"pred": [0, 1, 0], "labels": [0, 0, 0]})
            # 2 correct out of 3: 0.666...
            val result = acc.compute()
            expect result > 0.66
            expect result < 0.67

        it "computes accuracy for all wrong predictions":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"pred": [1, 2, 3], "labels": [0, 0, 0]})
            expect acc.compute() == 0.0

        it "accumulates across multiple batches":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"pred": [0, 1], "labels": [0, 1]})  # 2 correct
            acc.update({"pred": [0, 1], "labels": [1, 0]})  # 0 correct
            # Total: 2 correct out of 4 = 0.5
            expect acc.compute() == 0.5

        it "handles empty output gracefully":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"pred": [], "labels": []})
            expect acc.compute() == 0.0

        it "supports y_pred/y_true format":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"y_pred": [0, 1, 2], "y_true": [0, 1, 2]})
            expect acc.compute() == 1.0

        it "resets properly between epochs":
            var acc = Accuracy(correct: 0.0, total: 0.0)
            acc.reset()
            acc.update({"pred": [0], "labels": [0]})
            expect acc.compute() == 1.0

            acc.reset()
            acc.update({"pred": [0], "labels": [1]})
            expect acc.compute() == 0.0


    describe "Loss metric":
        """
        Average loss metric: tracks mean of loss values from output dict.

        Expected output format: {"loss": <scalar value>}
        """

        it "computes average loss":
            var loss = Loss(total_loss: 0.0, count: 0.0)
            loss.reset()
            loss.update({"loss": 1.0})
            loss.update({"loss": 2.0})
            loss.update({"loss": 3.0})
            expect loss.compute() == 2.0

        it "handles single batch":
            var loss = Loss(total_loss: 0.0, count: 0.0)
            loss.reset()
            loss.update({"loss": 0.5})
            expect loss.compute() == 0.5

        it "handles empty updates":
            var loss = Loss(total_loss: 0.0, count: 0.0)
            loss.reset()
            expect loss.compute() == 0.0

        it "ignores output without loss key":
            var loss = Loss(total_loss: 0.0, count: 0.0)
            loss.reset()
            loss.update({"other": 1.0})
            expect loss.compute() == 0.0

        it "resets properly between epochs":
            var loss = Loss(total_loss: 0.0, count: 0.0)
            loss.reset()
            loss.update({"loss": 10.0})
            expect loss.compute() == 10.0

            loss.reset()
            loss.update({"loss": 1.0})
            expect loss.compute() == 1.0


    describe "MSE metric":
        """
        Mean Squared Error: (1/n) * sum((pred - actual)^2)

        Useful for regression tasks where large errors should be
        penalized more than small errors.
        """

        it "computes MSE for perfect predictions":
            var mse = MSE(sum_sq: 0.0, count: 0.0)
            mse.reset()
            mse.update({"pred": [1.0, 2.0, 3.0], "actual": [1.0, 2.0, 3.0]})
            expect mse.compute() == 0.0

        it "computes MSE for predictions with errors":
            var mse = MSE(sum_sq: 0.0, count: 0.0)
            mse.reset()
            # Errors: 1, 1 -> Squared: 1, 1 -> Mean: 1.0
            mse.update({"pred": [2.0, 3.0], "actual": [1.0, 2.0]})
            expect mse.compute() == 1.0

        it "penalizes large errors more":
            var mse = MSE(sum_sq: 0.0, count: 0.0)
            mse.reset()
            # Error of 2 -> Squared: 4
            mse.update({"pred": [3.0], "actual": [1.0]})
            expect mse.compute() == 4.0

        it "handles y_pred/y_true format":
            var mse = MSE(sum_sq: 0.0, count: 0.0)
            mse.reset()
            mse.update({"y_pred": [1.0], "y_true": [1.0]})
            expect mse.compute() == 0.0


    describe "MAE metric":
        """
        Mean Absolute Error: (1/n) * sum(|pred - actual|)

        More robust to outliers than MSE as it doesn't square errors.
        """

        it "computes MAE for perfect predictions":
            var mae = MAE(sum_abs: 0.0, count: 0.0)
            mae.reset()
            mae.update({"pred": [1.0, 2.0, 3.0], "actual": [1.0, 2.0, 3.0]})
            expect mae.compute() == 0.0

        it "computes MAE for predictions with errors":
            var mae = MAE(sum_abs: 0.0, count: 0.0)
            mae.reset()
            # Errors: |2-1| + |3-2| = 1 + 1 = 2 -> Mean: 1.0
            mae.update({"pred": [2.0, 3.0], "actual": [1.0, 2.0]})
            expect mae.compute() == 1.0

        it "handles negative errors correctly":
            var mae = MAE(sum_abs: 0.0, count: 0.0)
            mae.reset()
            # Error: |0-2| = 2
            mae.update({"pred": [0.0], "actual": [2.0]})
            expect mae.compute() == 2.0


    describe "RMSE metric":
        """
        Root Mean Squared Error: sqrt((1/n) * sum((pred - actual)^2))

        Same unit as the target variable, making it more interpretable than MSE.
        """

        it "computes RMSE for perfect predictions":
            var rmse = RMSE(sum_sq: 0.0, count: 0.0)
            rmse.reset()
            rmse.update({"pred": [1.0, 2.0], "actual": [1.0, 2.0]})
            expect rmse.compute() == 0.0

        it "computes RMSE as square root of MSE":
            var rmse = RMSE(sum_sq: 0.0, count: 0.0)
            rmse.reset()
            # MSE = 4.0 -> RMSE = 2.0
            rmse.update({"pred": [3.0], "actual": [1.0]})
            expect rmse.compute() == 2.0


    describe "Metric base class":
        """
        Base class for custom metrics with reset/update/compute pattern.
        """

        it "provides default compute returning zero":
            val metric = Metric()
            expect metric.compute() == 0.0
