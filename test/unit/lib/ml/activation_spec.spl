# @skip
"""
ML Neural Network Activation Functions Tests
Feature: ML Activation Functions (ReLU, Sigmoid, GELU)
Category: ML, Neural Networks
Status: Complete

Tests for ml.torch.nn activation functions including ReLU variants,
sigmoid-based activations, and modern activations like GELU.
"""

use ml.torch as torch
use ml.torch.nn.activations as act
use ml.torch.ffi_helpers as helpers
use ml.torch.dtype.DType.*

# Local Device enum (runtime can't handle imported enum constructors with data)
enum Device:
    CPU
    CUDA(index: i64)

describe "Activation Functions":
    """
    Tests for neural network activation functions.
    """
    context "ReLU and variants":
        it "applies ReLU":
            val device = Device.CUDA(1)
            val x = helpers.randn_1d(20, device=device)
            val output = act.relu(x)
            expect output.shape() == [20]
            expect output.device().cuda_id() == Some(1)

        it "applies LeakyReLU":
            val device = Device.CUDA(1)
            val x = helpers.randn_1d(20, device=device)
            val output = act.leaky_relu(x, negative_slope=0.01)
            expect output.shape() == [20]
            expect output.device().cuda_id() == Some(1)

    context "sigmoid-based":
        it "applies Sigmoid":
            val device = Device.CUDA(1)
            val x = helpers.randn_1d(15, device=device)
            val output = act.sigmoid(x)
            expect output.shape() == [15]
            expect output.device().cuda_id() == Some(1)

        it "applies Tanh":
            val device = Device.CUDA(1)
            val x = helpers.randn_1d(15, device=device)
            val output = act.tanh(x)
            expect output.shape() == [15]
            expect output.device().cuda_id() == Some(1)

        it "applies Softmax":
            val device = Device.CUDA(1)
            val x = helpers.randn_1d(10, device=device)
            val output = act.softmax(x, dim=0)
            expect output.shape() == [10]
            expect output.device().cuda_id() == Some(1)

    context "modern activations":
        it "applies GELU":
            val device = Device.CUDA(1)
            val x = helpers.randn_1d(25, device=device)
            val output = act.gelu(x)
            expect output.shape() == [25]
            expect output.device().cuda_id() == Some(1)
