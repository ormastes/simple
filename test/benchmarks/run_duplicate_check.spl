#!/usr/bin/env simple
# Benchmark utility for duplicate detection
#
# Usage:
#   bin/simple test/benchmarks/run_duplicate_check.spl [path] [options]
#
# Options:
#   --iterations N        Number of iterations (default: 3)
#   --save PATH          Save results to file
#   --compare PATH       Compare with previous results
#   --verbose            Show detailed output

use app.duplicate_check.config.{default_config, DuplicationConfig}
use app.duplicate_check.detector.{collect_files}
use app.duplicate_check.benchmark.{run_benchmark, run_benchmark_iterations, format_benchmark_result, format_benchmark_stats, save_benchmark_results, load_benchmark_results, compare_benchmarks}
use app.io.mod.{cli_get_args, file_exists}

fn main():
    val args = cli_get_args()

    # Parse arguments
    var target_path = "src/"
    var iterations = 3
    var save_path = ""
    var compare_path = ""
    var verbose = false

    var i = 2  # Skip program name and script path
    while i < args.len():
        val arg = args[i]

        if arg == "--iterations" and i + 1 < args.len():
            iterations = int(args[i + 1])
            i = i + 2
        elif arg == "--save" and i + 1 < args.len():
            save_path = args[i + 1]
            i = i + 2
        elif arg == "--compare" and i + 1 < args.len():
            compare_path = args[i + 1]
            i = i + 2
        elif arg == "--verbose":
            verbose = true
            i = i + 1
        elif arg == "--help" or arg == "-h":
            print "Duplicate Detection Benchmark"
            print ""
            print "Usage: bin/simple run_benchmark.spl [path] [options]"
            print ""
            print "Options:"
            print "  --iterations N      Number of iterations (default: 3)"
            print "  --save PATH        Save results to file"
            print "  --compare PATH     Compare with previous results"
            print "  --verbose          Show detailed output"
            print "  --help, -h         Show this help"
            exit(0)
        elif not arg.starts_with("-"):
            target_path = arg
            i = i + 1
        else:
            i = i + 1

    print "==================================="
    print "Duplicate Detection Benchmark"
    print "==================================="
    print ""

    # Setup config
    var config = default_config()
    config.quiet = not verbose

    # Collect files
    val files = collect_files(target_path, config)
    print "Target: {target_path}"
    print "Files: {files.len()}"
    print "Iterations: {iterations}"
    print ""

    # Run benchmark
    if iterations == 1:
        print "Running single benchmark..."
        val result = run_benchmark("duplicate_check_benchmark", files, config)
        print format_benchmark_result(result)

        if save_path.len() > 0:
            save_benchmark_results([result], save_path)
            print ""
            print "Results saved to: {save_path}"

        if compare_path.len() > 0 and file_exists(compare_path):
            val previous = load_benchmark_results(compare_path)
            if previous.len() > 0:
                print ""
                print "Comparison with previous run:"
                print compare_benchmarks(previous[0], result)
    else:
        print "Running {iterations} iterations..."
        val stats = run_benchmark_iterations("duplicate_check_benchmark", files, config, iterations)
        print ""
        print format_benchmark_stats(stats)

        if save_path.len() > 0:
            save_benchmark_results(stats.runs, save_path)
            print ""
            print "Results saved to: {save_path}"

        if compare_path.len() > 0 and file_exists(compare_path):
            val previous = load_benchmark_results(compare_path)
            if previous.len() > 0:
                print ""
                print "Comparison with previous average:"
                # Compute average of current runs
                var total_current = 0
                for run in stats.runs:
                    total_current = total_current + run.duration_ms
                val avg_current = total_current / stats.runs.len()

                # Use first previous run for comparison
                print compare_benchmarks(previous[0], stats.runs[0])

    print ""
    print "Benchmark complete!"
