# Phase 1 Integration Tests - Verify cache, interner, lazy features, and progress reporting

use std.spec.*
use app.duplicate_check.config.{DuplicationConfig}
use app.duplicate_check.detector.{find_duplicates, collect_files}
use app.duplicate_check.cache.{new_token_cache_manager}
use app.io.mod.{file_write, shell}

fn create_test_file(path: text, content: text):
    shell("mkdir -p /tmp/test_duplicate_phase1")
    file_write(path, content)

fn cleanup_test_files():
    shell("rm -rf /tmp/test_duplicate_phase1")

describe "Phase 1: File caching optimization":
    it "uses cache manager for tokenization":
        create_test_file("/tmp/test_duplicate_phase1/test1.spl", "fn test(): 42")
        create_test_file("/tmp/test_duplicate_phase1/test2.spl", "fn test(): 42")

        val config = DuplicationConfig(
            min_tokens: 5,
            min_lines: 1,
            ignore_identifiers: false,
            ignore_literals: false,
            ignore_comments: true,
            ignore_whitespace: true,
            similarity_threshold: 0.85,
            output_format: "text",
            report_path: "doc/analysis/duplicate_db.sdn",
            max_allowed: 0,
            min_impact: 10,
            exclude_patterns: [],
            use_ast_features: false,
            use_cosine_similarity: false,
            verbose: false,
            quiet: true
        )

        val cache_manager = new_token_cache_manager()
        val files = ["/tmp/test_duplicate_phase1/test1.spl", "/tmp/test_duplicate_phase1/test2.spl"]
        val groups = find_duplicates(files, config, cache_manager)

        expect(groups.len()).to_be_greater_than(0)

        cleanup_test_files()

describe "Phase 1: Verbose and quiet modes":
    it "respects quiet mode":
        create_test_file("/tmp/test_duplicate_phase1/quiet_test.spl", "fn quiet_test(): 1")

        val config = DuplicationConfig(
            min_tokens: 5,
            min_lines: 1,
            ignore_identifiers: false,
            ignore_literals: false,
            ignore_comments: true,
            ignore_whitespace: true,
            similarity_threshold: 0.85,
            output_format: "text",
            report_path: "doc/analysis/duplicate_db.sdn",
            max_allowed: 0,
            min_impact: 10,
            exclude_patterns: [],
            use_ast_features: false,
            use_cosine_similarity: false,
            verbose: false,
            quiet: true
        )

        val cache_manager = new_token_cache_manager()
        val files = ["/tmp/test_duplicate_phase1/quiet_test.spl"]
        val groups = find_duplicates(files, config, cache_manager)

        expect(config.quiet).to_equal(true)

        cleanup_test_files()

    it "enables verbose mode":
        create_test_file("/tmp/test_duplicate_phase1/verbose_test.spl", "fn verbose_test(): 2")

        val config = DuplicationConfig(
            min_tokens: 5,
            min_lines: 1,
            ignore_identifiers: false,
            ignore_literals: false,
            ignore_comments: true,
            ignore_whitespace: true,
            similarity_threshold: 0.85,
            output_format: "text",
            report_path: "doc/analysis/duplicate_db.sdn",
            max_allowed: 0,
            min_impact: 10,
            exclude_patterns: [],
            use_ast_features: false,
            use_cosine_similarity: false,
            verbose: true,
            quiet: false
        )

        val cache_manager = new_token_cache_manager()
        val files = ["/tmp/test_duplicate_phase1/verbose_test.spl"]
        val groups = find_duplicates(files, config, cache_manager)

        expect(config.verbose).to_equal(true)

        cleanup_test_files()

describe "Phase 1: StringInterner integration":
    it "processes files with token interning":
        create_test_file("/tmp/test_duplicate_phase1/interner1.spl", "fn test1(): x + y + z")
        create_test_file("/tmp/test_duplicate_phase1/interner2.spl", "fn test2(): x + y + z")

        val config = DuplicationConfig(
            min_tokens: 5,
            min_lines: 1,
            ignore_identifiers: false,
            ignore_literals: false,
            ignore_comments: true,
            ignore_whitespace: true,
            similarity_threshold: 0.85,
            output_format: "text",
            report_path: "doc/analysis/duplicate_db.sdn",
            max_allowed: 0,
            min_impact: 10,
            exclude_patterns: [],
            use_ast_features: false,
            use_cosine_similarity: false,
            verbose: false,
            quiet: true
        )

        val cache_manager = new_token_cache_manager()
        val files = ["/tmp/test_duplicate_phase1/interner1.spl", "/tmp/test_duplicate_phase1/interner2.spl"]
        val groups = find_duplicates(files, config, cache_manager)

        expect(groups.len()).to_be_greater_than(0)

        cleanup_test_files()

describe "Phase 1: Lazy feature extraction":
    skip_it "extracts features only when needed for cosine similarity":
        # SKIP: Requires use_cosine_similarity which is slow in interpreter mode
        create_test_file("/tmp/test_duplicate_phase1/lazy1.spl", "fn lazy1(): val x = 42; x")
        create_test_file("/tmp/test_duplicate_phase1/lazy2.spl", "fn lazy2(): val y = 43; y")

        val config = DuplicationConfig(
            min_tokens: 5,
            min_lines: 1,
            ignore_identifiers: false,
            ignore_literals: false,
            ignore_comments: true,
            ignore_whitespace: true,
            similarity_threshold: 0.85,
            output_format: "text",
            report_path: "doc/analysis/duplicate_db.sdn",
            max_allowed: 0,
            min_impact: 10,
            exclude_patterns: [],
            use_ast_features: false,
            use_cosine_similarity: true,
            verbose: true,
            quiet: false
        )

        val cache_manager = new_token_cache_manager()
        val files = ["/tmp/test_duplicate_phase1/lazy1.spl", "/tmp/test_duplicate_phase1/lazy2.spl"]
        val groups = find_duplicates(files, config, cache_manager)

        cleanup_test_files()
