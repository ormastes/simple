# Performance Module Tests
#
# Tests for unified performance configuration, statistics, and benchmarks.

# @skip
from perf_config import {
    PerfConfig,
    PerfProfile,
    OptimizationLevel,
    get_config,
    set_config
}
from perf_stats import {
    PerfStats,
    ComponentStats,
    collect_stats,
    reset_all_stats,
    format_bytes
}
from benchmark import {
    Benchmark,
    BenchmarkResult,
    BenchmarkSuite,
    BenchmarkConfig,
    run_benchmark,
    format_duration
}

describe "OptimizationLevel":
    it "provides descriptions":
        check(OptimizationLevel.Minimal.description().contains("Minimal"))
        check(OptimizationLevel.Balanced.description().contains("Balanced"))
        check(OptimizationLevel.Aggressive.description().contains("Maximum"))

describe "PerfProfile":
    it "converts to config":
        val config = PerfProfile.Development.to_config()

        check(config.level == OptimizationLevel.Minimal)
        check(config.detailed_stats == true)

    it "creates production config":
        val config = PerfProfile.Production.to_config()

        check(config.level == OptimizationLevel.Balanced)
        check(config.stats_collection_enabled == false)

    it "creates low latency config":
        val config = PerfProfile.LowLatency.to_config()

        check(config.scheduler_config.reductions_per_timeslice == 500)

    it "creates high throughput config":
        val config = PerfProfile.HighThroughput.to_config()

        check(config.scheduler_config.reductions_per_timeslice == 8000)

    it "creates memory constrained config":
        val config = PerfProfile.MemoryConstrained.to_config()

        check(config.mailbox_config.capacity == 50)

    it "provides descriptions":
        check(PerfProfile.Development.description().contains("debugging"))
        check(PerfProfile.Production.description().contains("production"))

describe "PerfConfig - Creation":
    it "creates default config":
        val config = PerfConfig.default()

        check(config.level == OptimizationLevel.Balanced)
        check(config.symbol_interning_enabled)
        check(config.persistent_collections_enabled)
        check(config.per_actor_gc_enabled)
        check(config.lazy_evaluation_enabled)

    it "creates development config":
        val config = PerfConfig.development()

        check(config.level == OptimizationLevel.Minimal)
        check(not config.persistent_collections_enabled)
        check(config.detailed_stats)

    it "creates testing config":
        val config = PerfConfig.testing()

        check(config.scheduler_config.scheduler_count == 1)

    it "creates production config":
        val config = PerfConfig.production()

        check(not config.stats_collection_enabled)

describe "PerfConfig - Builder":
    it "sets optimization level":
        val config = PerfConfig.default()
            .with_optimization_level(OptimizationLevel.Aggressive)

        check(config.level == OptimizationLevel.Aggressive)

    it "sets actor heap size":
        val config = PerfConfig.default()
            .with_actor_heap_size(1024 * 1024)

        check(config.actor_heap_config.initial_size == 1024 * 1024)

    it "sets reductions":
        val config = PerfConfig.default()
            .with_reductions(4000)

        check(config.scheduler_config.reductions_per_timeslice == 4000)

    it "enables/disables stats":
        val config = PerfConfig.default()
            .with_stats_enabled(false)

        check(not config.stats_collection_enabled)

    it "sets large binary threshold":
        val config = PerfConfig.default()
            .with_large_binary_threshold(128)

        check(config.large_binary_threshold == 128)

    it "chains builders":
        val config = PerfConfig.default()
            .with_optimization_level(OptimizationLevel.Aggressive)
            .with_reductions(4000)
            .with_stats_enabled(true)

        check(config.level == OptimizationLevel.Aggressive)
        check(config.scheduler_config.reductions_per_timeslice == 4000)
        check(config.stats_collection_enabled)

describe "PerfConfig - Global":
    it "gets default global config":
        val config = get_config()

        check(config.level == OptimizationLevel.Balanced)

    it "sets global config":
        val custom = PerfConfig.development()
        set_config(custom)

        val retrieved = get_config()
        check(retrieved.level == OptimizationLevel.Minimal)

        # Reset
        set_config(PerfConfig.default())

describe "ComponentStats":
    it "creates empty stats":
        val stats = ComponentStats.new("test", true)

        check(stats.name == "test")
        check(stats.enabled)
        check(stats.metric_count() == 0)

    it "adds metrics":
        var stats = ComponentStats.new("test", true)

        stats.add_metric("count", 100.0)
        stats.add_metric("rate", 0.5)

        check(stats.metric_count() == 2)
        check(stats.get_metric("count") == Some(100.0))
        check(stats.get_metric("rate") == Some(0.5))

    it "formats enabled component":
        var stats = ComponentStats.new("memory", true)
        stats.add_metric("used", 1024.0)

        val formatted = stats.fmt()
        check(formatted.contains("memory"))
        check(formatted.contains("1024"))

    it "formats disabled component":
        val stats = ComponentStats.new("feature", false)

        check(stats.fmt().contains("disabled"))

describe "PerfStats":
    it "creates empty stats":
        val stats = PerfStats.empty()

        check(stats.total_memory_bytes == 0)
        check(stats.total_gc_count == 0)

    it "lists component names":
        val stats = PerfStats.empty()
        val names = stats.component_names()

        check(names.contains("symbols"))
        check(names.contains("lazy"))
        check(names.contains("shared_heap"))

    it "gets component by name":
        val stats = PerfStats.empty()

        val lazy = stats.get_component("lazy")
        check(lazy.?)
        check(lazy.unwrap().name == "lazy")

        val invalid = stats.get_component("invalid")
        check(not invalid.?)

    it "generates summary":
        val stats = PerfStats.empty()
        val summary = stats.summary()

        check(summary.contains("Performance Statistics"))
        check(summary.contains("Symbol Interning"))
        check(summary.contains("Lazy Evaluation"))

describe "PerfStats - Collection":
    it "collects stats":
        val stats = collect_stats()

        # Should have collected from lazy subsystem
        val lazy = stats.get_component("lazy")
        check(lazy.?)

    it "resets stats":
        reset_all_stats()
        val stats = collect_stats()

        # Stats should be reset
        val lazy = stats.get_component("lazy")
        check(lazy.?.get_metric("total_created") == Some(0.0))

describe "Formatting Utilities":
    it "formats bytes":
        check(format_bytes(500) == "500 B")
        check(format_bytes(1024).contains("KB"))
        check(format_bytes(1024 * 1024).contains("MB"))
        check(format_bytes(1024 * 1024 * 1024).contains("GB"))

    it "formats duration":
        check(format_duration(500).contains("ns"))
        check(format_duration(5000).contains("Âµs"))
        check(format_duration(5000000).contains("ms"))
        check(format_duration(5000000000).contains("s"))

describe "BenchmarkConfig":
    it "creates default config":
        val config = BenchmarkConfig.default()

        check(config.warmup_iterations == 10)
        check(config.iterations == 100)

    it "creates quick config":
        val config = BenchmarkConfig.quick()

        check(config.warmup_iterations == 3)
        check(config.iterations == 20)

    it "creates thorough config":
        val config = BenchmarkConfig.thorough()

        check(config.warmup_iterations == 50)
        check(config.iterations == 1000)

describe "BenchmarkResult":
    it "creates empty result":
        val result = BenchmarkResult.new("test")

        check(result.name == "test")
        check(result.iterations == 0)

    it "calculates ops per second":
        var result = BenchmarkResult.new("test")
        result.mean_time_ns = 1000000  # 1ms

        val ops = result.ops_per_second()
        check(ops == 1000.0)

    it "calculates time in different units":
        var result = BenchmarkResult.new("test")
        result.mean_time_ns = 1000000  # 1ms

        check(result.mean_time_us() == 1000.0)
        check(result.mean_time_ms() == 1.0)

    it "calculates coefficient of variation":
        var result = BenchmarkResult.new("test")
        result.mean_time_ns = 1000
        result.stddev_ns = 100

        val cv = result.coefficient_of_variation()
        check(cv == 0.1)

    it "generates summary":
        var result = BenchmarkResult.new("test_op")
        result.iterations = 100
        result.mean_time_ns = 1000000
        result.min_time_ns = 900000
        result.max_time_ns = 1100000

        val summary = result.summary()
        check(summary.contains("test_op"))
        check(summary.contains("Iterations: 100"))

describe "Benchmark":
    it "creates simple benchmark":
        var counter = 0
        val bench = Benchmark.new("increment", \:
            counter = counter + 1
        )

        check(bench.name == "increment")

    it "adds setup function":
        var setup_called = false
        val bench = Benchmark.new("test", \: ())
            .with_setup(\:
                setup_called = true
            )

        bench.setup()
        check(setup_called)

    it "adds teardown function":
        var teardown_called = false
        val bench = Benchmark.new("test", \: ())
            .with_teardown(\:
                teardown_called = true
            )

        bench.teardown()
        check(teardown_called)

    it "adds description":
        val bench = Benchmark.new("test", \: ())
            .with_description("A test benchmark")

        check(bench.description == "A test benchmark")

describe "BenchmarkSuite":
    it "creates empty suite":
        val suite = BenchmarkSuite.new("test_suite")

        check(suite.name == "test_suite")
        check(suite.benchmarks.len() == 0)

    it "adds benchmarks":
        var suite = BenchmarkSuite.new("test_suite")

        suite.add_fn("bench1", \: ())
        suite.add_fn("bench2", \: ())

        check(suite.benchmarks.len() == 2)

    it "sets config for suite":
        val suite = BenchmarkSuite.new("test")
            .with_config(BenchmarkConfig.quick())

        check(suite.config.iterations == 20)

describe "BenchmarkComparison":
    it "compares faster result":
        var baseline = BenchmarkResult.new("baseline")
        baseline.mean_time_ns = 2000

        var candidate = BenchmarkResult.new("candidate")
        candidate.mean_time_ns = 1000

        val comparison = baseline.compare_to(candidate)

        check(comparison.is_faster())
        check(comparison.speedup == 2.0)

    it "compares slower result":
        var baseline = BenchmarkResult.new("baseline")
        baseline.mean_time_ns = 1000

        var candidate = BenchmarkResult.new("candidate")
        candidate.mean_time_ns = 2000

        val comparison = baseline.compare_to(candidate)

        check(comparison.is_slower())
        check(comparison.speedup == 0.5)

    it "generates summary":
        var baseline = BenchmarkResult.new("old")
        baseline.mean_time_ns = 2000

        var candidate = BenchmarkResult.new("new")
        candidate.mean_time_ns = 1000

        val comparison = baseline.compare_to(candidate)
        val summary = comparison.summary()

        check(summary.contains("new"))
        check(summary.contains("old"))
        check(summary.contains("faster"))

describe "run_benchmark":
    it "runs simple benchmark":
        var counter = 0
        val result = run_benchmark("counter", 10, \:
            counter = counter + 1
        )

        check(result.name == "counter")
        check(result.iterations == 10)
        check(counter >= 10  # Includes warmup)

    it "calculates statistics":
        val result = run_benchmark("noop", 100, \: ())

        check(result.min_time_ns >= 0)
        check(result.max_time_ns >= result.min_time_ns)
        check(result.mean_time_ns >= result.min_time_ns)
        check(result.mean_time_ns <= result.max_time_ns)
