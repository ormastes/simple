"""
# TreeSitter Lexer Specification

**Feature IDs:** #TS-LEX-001 to #TS-LEX-020
**Category:** Infrastructure | Parser
**Status:** Implemented

Tests the TreeSitter lexer/tokenizer implementation, including token
recognition, spans, and edge cases.

## API

```simple
use std.parser.treesitter.{Lexer, Token, TokenKind}

val lexer = Lexer.new(source)
val tokens = lexer.tokenize()?
```
"""

use std.parser.treesitter.{Lexer, Token, TokenKind, Span}


# ============================================================================
# Test Group 1: Basic Tokenization
# ============================================================================

describe "TreeSitter Lexer Basic Tokenization":
    """
    ## Simple Token Recognition

    Tests basic token recognition for literals and keywords.
    """

    context "integer literals":
        it "tokenizes decimal integer":
            var lexer = Lexer.new("42")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1
            # First token should be integer

        it "tokenizes zero":
            var lexer = Lexer.new("0")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

    context "identifiers":
        it "tokenizes identifier":
            var lexer = Lexer.new("foo")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes identifier with underscore":
            var lexer = Lexer.new("_bar")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes identifier with digits":
            var lexer = Lexer.new("foo123")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

    context "keywords":
        it "tokenizes fn keyword":
            var lexer = Lexer.new("fn")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes let keyword":
            var lexer = Lexer.new("let")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes if keyword":
            var lexer = Lexer.new("if")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1


# ============================================================================
# Test Group 2: Operator Tokenization
# ============================================================================

describe "TreeSitter Lexer Operators":
    """
    ## Operator Recognition

    Tests operator token recognition.
    """

    context "arithmetic operators":
        it "tokenizes plus":
            var lexer = Lexer.new("+")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes minus":
            var lexer = Lexer.new("-")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes star":
            var lexer = Lexer.new("*")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes slash":
            var lexer = Lexer.new("/")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

    context "comparison operators":
        it "tokenizes equals":
            var lexer = Lexer.new("==")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes not equals":
            var lexer = Lexer.new("!=")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes less than":
            var lexer = Lexer.new("<")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes greater than":
            var lexer = Lexer.new(">")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes less than or equal":
            var lexer = Lexer.new("<=")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

        it "tokenizes greater than or equal":
            var lexer = Lexer.new(">=")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1

    context "arrow operators":
        it "tokenizes arrow":
            var lexer = Lexer.new("->")
            val tokens = lexer.tokenize().unwrap()
            expect tokens.len() >= 1


# ============================================================================
# Test Group 3: Delimiter Tokenization
# ============================================================================

describe "TreeSitter Lexer Delimiters":
    """
    ## Delimiter Recognition

    Tests delimiter token recognition.
    """

    it "tokenizes parentheses":
        var lexer = Lexer.new("()")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 2

    it "tokenizes braces":
        var lexer = Lexer.new("{}")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 2

    it "tokenizes brackets":
        var lexer = Lexer.new("[]")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 2

    it "tokenizes colon":
        var lexer = Lexer.new(":")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 1

    it "tokenizes comma":
        var lexer = Lexer.new(",")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 1

    it "tokenizes dot":
        var lexer = Lexer.new(".")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 1


# ============================================================================
# Test Group 4: Expression Tokenization
# ============================================================================

describe "TreeSitter Lexer Expressions":
    """
    ## Multi-Token Expressions

    Tests tokenization of complete expressions.
    """

    it "tokenizes variable declaration":
        var lexer = Lexer.new("val x = 42")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 4  # val, x, =, 42, EOF

    it "tokenizes binary expression":
        var lexer = Lexer.new("1 + 2")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 3  # 1, +, 2, EOF

    it "tokenizes function call":
        var lexer = Lexer.new("foo(x, y)")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 5  # foo, (, x, ,, y, ), EOF

    it "tokenizes method call":
        var lexer = Lexer.new("obj.method()")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 5  # obj, ., method, (, ), EOF


# ============================================================================
# Test Group 5: Token Spans
# ============================================================================

describe "TreeSitter Lexer Token Spans":
    """
    ## Source Position Tracking

    Tests that tokens have correct source positions.
    """

    it "tracks byte positions":
        var lexer = Lexer.new("x + y")
        val tokens = lexer.tokenize().unwrap()
        # First token 'x' starts at byte 0
        expect tokens[0].span.start_byte == 0

    it "tracks line numbers":
        var lexer = Lexer.new("x\ny")
        val tokens = lexer.tokenize().unwrap()
        expect tokens[0].span.start_line == 1

    it "tracks column positions":
        var lexer = Lexer.new("  x")
        val tokens = lexer.tokenize().unwrap()
        # 'x' starts at column 3 (after two spaces)
        expect tokens[0].span.start_column >= 1


# ============================================================================
# Test Group 6: Token Text
# ============================================================================

describe "TreeSitter Lexer Token Text":
    """
    ## Token Text Extraction

    Tests that token text is correctly captured.
    """

    it "captures identifier text":
        var lexer = Lexer.new("myVariable")
        val tokens = lexer.tokenize().unwrap()
        expect tokens[0].text == "myVariable"

    it "captures number text":
        var lexer = Lexer.new("12345")
        val tokens = lexer.tokenize().unwrap()
        expect tokens[0].text == "12345"

    it "captures operator text":
        var lexer = Lexer.new("==")
        val tokens = lexer.tokenize().unwrap()
        expect tokens[0].text == "=="


# ============================================================================
# Test Group 7: Whitespace Handling
# ============================================================================

describe "TreeSitter Lexer Whitespace":
    """
    ## Whitespace Processing

    Tests whitespace handling during tokenization.
    """

    it "skips spaces between tokens":
        var lexer = Lexer.new("x   +   y")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 3  # x, +, y, EOF

    it "handles multiple whitespace types":
        var lexer = Lexer.new("x\t+\ty")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 3


# ============================================================================
# Test Group 8: EOF Token
# ============================================================================

describe "TreeSitter Lexer EOF":
    """
    ## End of File Token

    Tests that EOF token is correctly added.
    """

    it "ends with EOF token":
        var lexer = Lexer.new("x")
        val tokens = lexer.tokenize().unwrap()
        val last = tokens[tokens.len() - 1]
        # Last token should be EOF
        expect true  # Tokens generated

    it "empty input produces EOF":
        var lexer = Lexer.new("")
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 1  # At least EOF


# ============================================================================
# Test Group 9: Complex Source
# ============================================================================

describe "TreeSitter Lexer Complex Source":
    """
    ## Real-World Source Code

    Tests tokenization of realistic code.
    """

    it "tokenizes function definition":
        val source = "fn add(a, b):\n    return a + b"
        var lexer = Lexer.new(source)
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 10

    it "tokenizes struct definition":
        val source = "struct Point:\n    x: i64\n    y: i64"
        var lexer = Lexer.new(source)
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 8

    it "tokenizes if-else":
        val source = "if x > 0:\n    y = 1\nelse:\n    y = 0"
        var lexer = Lexer.new(source)
        val tokens = lexer.tokenize().unwrap()
        expect tokens.len() >= 12
