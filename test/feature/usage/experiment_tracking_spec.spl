"""
# Experiment Tracking Integration Specification

**Feature IDs:** #exp-integration
**Category:** Stdlib
**Difficulty:** 3/5
**Status:** In Progress

## Overview

Integration tests for the full experiment tracking workflow:
config loading, run lifecycle, metric logging, artifact storage,
and querying.

## Key Concepts

| Concept | Description |
|---------|-------------|
| Run | A single experiment execution with config + metrics |
| Artifact | Content-addressed file stored with a run |
| Sweep | Hyperparameter optimization across multiple runs |
| Config | SDN-based configuration with composition and overrides |
"""

use std.exp.config.*
use std.exp.storage.*
use std.exp.run.*
use std.exp.artifact.*
use std.exp.query.*

# ============================================================================
# Full Workflow
# ============================================================================

describe "Experiment Tracking Workflow":
    """
    ## End-to-End Workflow

    Tests the complete experiment tracking lifecycle from config
    through run completion and querying.
    """

    context "basic workflow":
        it "creates a run, logs metrics, and completes":
            # Config
            var values: Dict<text, ConfigValue> = {}
            values["lr"] = ConfigValue.Float(0.001)
            values["epochs"] = ConfigValue.Int(10)
            val config = ExpConfig(values: values, source_files: [], overrides: {})

            # Start run
            var run = start_run(config, ["test", "integration"])
            expect run.status == RunStatus.Running

            # Log metrics
            run.log_metric("loss", 0.9, 0)
            run.log_metric("loss", 0.5, 1)
            run.log_metric("loss", 0.2, 2)
            run.log_metric("accuracy", 0.95, 2)

            # Complete
            run.complete()
            expect run.status == RunStatus.Completed

        it "stores and retrieves artifacts":
            val config = ExpConfig__empty()
            var run = start_run(config, [])
            var store = ArtifactStore__for_run(run.run_id)

            # Store data artifact
            val hash = store.register_data("results.sdn", "loss: 0.1\nacc: 0.95", {})
            expect hash.?

            # Retrieve
            val content = store.get_blob("results.sdn")
            expect content == Some("loss: 0.1\nacc: 0.95")

    context "config composition":
        it "merges configs with overlay winning":
            var base_vals: Dict<text, ConfigValue> = {}
            base_vals["lr"] = ConfigValue.Float(0.001)
            base_vals["batch_size"] = ConfigValue.Int(32)
            val base = ExpConfig(values: base_vals, source_files: [], overrides: {})

            var overlay_vals: Dict<text, ConfigValue> = {}
            overlay_vals["lr"] = ConfigValue.Float(0.01)
            val overlay = ExpConfig(values: overlay_vals, source_files: [], overrides: {})

            val merged = merge_configs(base, overlay)
            # Use match workaround for enum method dispatch
            val lr_val = merged.get("lr")
            val lr_ok = match lr_val:
                case Some(Float(f)): f == 0.01
                case _: false
            expect lr_ok

            val bs_val = merged.get("batch_size")
            val bs_ok = match bs_val:
                case Some(Int(n)): n == 32
                case _: false
            expect bs_ok

    context "querying":
        it "lists runs by status":
            val filter = RunFilter__by_status("completed")
            val runs = list_runs(filter)
            # Result depends on existing .exp/ state
            expect runs.len() >= 0

        it "lists all runs":
            val runs = list_runs(RunFilter__all())
            expect runs.len() >= 0

# ============================================================================
# Run Comparison
# ============================================================================

describe "Run Comparison":
    """
    ## Comparing Runs

    Tests for diffing config and metrics between runs.
    """

    it "diffs two runs with different configs":
        var vals_a: Dict<text, ConfigValue> = {}
        vals_a["lr"] = ConfigValue.Float(0.001)
        val config_a = ExpConfig(values: vals_a, source_files: [], overrides: {})

        var vals_b: Dict<text, ConfigValue> = {}
        vals_b["lr"] = ConfigValue.Float(0.01)
        val config_b = ExpConfig(values: vals_b, source_files: [], overrides: {})

        var run_a = start_run(config_a, ["baseline"])
        run_a.log_metric("loss", 0.5, 0)
        run_a.complete()

        var run_b = start_run(config_b, ["experiment"])
        run_b.log_metric("loss", 0.3, 0)
        run_b.complete()

        val result = diff_runs(run_a.run_id, run_b.run_id)
        expect result.is_ok()
