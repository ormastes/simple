"""
# TreeSitter Lexer Specification

**Feature IDs:** #TS-LEX-001 to #TS-LEX-020
**Category:** Infrastructure | Parser
**Status:** Implemented

Tests the core.lexer tokenization used by compiler.treesitter,
including keyword, identifier, number, operator, and delimiter tokens.

## API

```simple
use compiler.core.lexer.{Lexer, lexer_new, lexer_next_token, Token, TokenKind}

var lexer = lexer_new(source)
val token = lexer_next_token(lexer)
```
"""

use compiler.core.lexer.*


# ============================================================================
# Test Group 1: Keyword Tokenization
# ============================================================================

describe "Core Lexer Keyword Tokenization":
    """
    Tests that language keywords produce correct TokenKind.
    """

    it "tokenizes fn keyword":
        var lexer = lexer_new("fn")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwFn

    it "tokenizes val keyword":
        var lexer = lexer_new("val")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwVal

    it "tokenizes var keyword":
        var lexer = lexer_new("var")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwVar

    it "tokenizes if keyword":
        var lexer = lexer_new("if")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwIf

    it "tokenizes struct keyword":
        var lexer = lexer_new("struct")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwStruct

    it "tokenizes enum keyword":
        var lexer = lexer_new("enum")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwEnum

    it "tokenizes impl keyword":
        var lexer = lexer_new("impl")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwImpl

    it "tokenizes trait keyword":
        var lexer = lexer_new("trait")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.KwTrait


# ============================================================================
# Test Group 2: Identifier Tokenization
# ============================================================================

describe "Core Lexer Identifier Tokenization":
    """
    Tests that identifiers produce TokenKind.Ident.
    """

    it "tokenizes simple identifier":
        var lexer = lexer_new("foo")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Ident

    it "tokenizes identifier with underscore":
        var lexer = lexer_new("_bar")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Ident

    it "tokenizes identifier with digits":
        var lexer = lexer_new("foo123")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Ident


# ============================================================================
# Test Group 3: Number Tokenization
# ============================================================================

describe "Core Lexer Number Tokenization":
    """
    Tests number literal tokenization.
    """

    it "tokenizes integer":
        var lexer = lexer_new("42")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Integer

    it "tokenizes float":
        var lexer = lexer_new("3.14")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Float

    it "tokenizes zero":
        var lexer = lexer_new("0")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Integer


# ============================================================================
# Test Group 4: Operator Tokenization
# ============================================================================

describe "Core Lexer Operator Tokenization":
    """
    Tests operator token recognition.
    """

    it "tokenizes plus":
        var lexer = lexer_new("+")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Plus

    it "tokenizes minus":
        var lexer = lexer_new("-")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Minus

    it "tokenizes star":
        var lexer = lexer_new("*")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Star

    it "tokenizes colon":
        var lexer = lexer_new(":")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Colon

    it "tokenizes arrow":
        var lexer = lexer_new("->")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Arrow


# ============================================================================
# Test Group 5: Delimiter Tokenization
# ============================================================================

describe "Core Lexer Delimiter Tokenization":
    """
    Tests delimiter token recognition.
    """

    it "tokenizes left paren":
        var lexer = lexer_new("(")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.LParen

    it "tokenizes right paren":
        var lexer = lexer_new(")")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.RParen

    it "tokenizes left brace":
        var lexer = lexer_new("{")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.LBrace

    it "tokenizes right brace":
        var lexer = lexer_new("}")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.RBrace

    it "tokenizes left bracket":
        var lexer = lexer_new("[")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.LBracket

    it "tokenizes right bracket":
        var lexer = lexer_new("]")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.RBracket


# ============================================================================
# Test Group 6: Multi-Token Sequence
# ============================================================================

describe "Core Lexer Multi-Token Sequence":
    """
    Tests tokenizing sequences of multiple tokens.
    """

    it "tokenizes function signature":
        var lexer = lexer_new("fn add(a: i64):")
        val t1 = lexer_next_token(lexer)
        expect t1.kind to_equal TokenKind.KwFn
        val t2 = lexer_next_token(lexer)
        expect t2.kind to_equal TokenKind.Ident

    it "tokenizes variable declaration":
        var lexer = lexer_new("val x = 42")
        val t1 = lexer_next_token(lexer)
        expect t1.kind to_equal TokenKind.KwVal
        val t2 = lexer_next_token(lexer)
        expect t2.kind to_equal TokenKind.Ident


# ============================================================================
# Test Group 7: EOF Token
# ============================================================================

describe "Core Lexer EOF Token":
    """
    Tests that lexer produces EOF at end of input.
    """

    it "produces EOF for empty input":
        var lexer = lexer_new("")
        val token = lexer_next_token(lexer)
        expect token.kind to_equal TokenKind.Eof

    it "produces EOF after all tokens consumed":
        var lexer = lexer_new("x")
        val t1 = lexer_next_token(lexer)
        val t2 = lexer_next_token(lexer)
        expect t2.kind to_equal TokenKind.Eof
