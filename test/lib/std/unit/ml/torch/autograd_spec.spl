# PyTorch Autograd Tests
# Tests for ml.torch.autograd module

import std.spec
import ml.torch as torch
import ml.torch.ffi_helpers as helpers
import ml.torch.device.Device
import ml.torch.dtype.DType

class MockGradientCheckpoint:
    fn supports_checkpointing() -> bool:
        return true

    fn checkpoint_segment(start: i64, end: i64) -> bool:
        return start < end

describe "Autograd":
    context "gradient tracking":
        it "tracks gradients":
            val device = Device::CUDA(1)
            val x = helpers.ones_1d(5, device=device)
            x.set_requires_grad(true)
            expect x.requires_grad()

    context "backward pass":
        it "computes backward pass":
            val device = Device::CUDA(1)
            val x = helpers.ones_1d(5, device=device)
            x.set_requires_grad(true)
            val y = x.mul_scalar(2.0)
            y.backward()
            val grads = x.grad()
            expect grads.numel() == 5
            expect grads.device().cuda_id() == Some(1)

    context "gradient operations":
        it "accumulates gradients":
            val device = Device::CUDA(1)
            val x = helpers.ones_1d(3, device=device)
            x.set_requires_grad(true)
            val y = x.mul_scalar(3.0)
            y.backward()
            val grads = x.grad()
            expect grads.numel() == 3

    context "gradient checkpointing":
        it "supports gradient checkpointing":
            val checkpoint = MockGradientCheckpoint()
            expect checkpoint.supports_checkpointing()
            expect checkpoint.checkpoint_segment(0, 100)
