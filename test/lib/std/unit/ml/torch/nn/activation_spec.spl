# ML Neural Network Activation Functions Tests
# SKIPPED: ml.torch.nn activation functions not yet implemented.

import std.spec

describe "Activation Functions":
    skip "applies ReLU":
        pass
    skip "applies Sigmoid":
        pass
    skip "applies Tanh":
        pass
    skip "applies Softmax":
        pass
    skip "applies LeakyReLU":
        pass
    skip "applies GELU":
        pass
