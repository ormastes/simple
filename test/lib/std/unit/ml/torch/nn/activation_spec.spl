# ML Neural Network Activation Functions Tests
# Tests for ml.torch.nn activation functions

import std.spec
import ml.torch as torch
import ml.torch.nn.activations as act
import ml.torch.device.Device
import ml.torch.dtype.DType

describe "Activation Functions":
    context "ReLU and variants":
        it "applies ReLU":
            val device = Device::CUDA(1)
            val x = torch.from_data([-2.0, -1.0, 0.0, 1.0, 2.0], device=device)
            val output = act.relu(x)
            expect output.shape() == [5]
            expect output.device().cuda_id() == Some(1)

        it "applies LeakyReLU":
            val device = Device::CUDA(1)
            val x = torch.randn([20], device=device)
            val output = act.leaky_relu(x, negative_slope=0.01)
            expect output.shape() == [20]
            expect output.device().cuda_id() == Some(1)

    context "sigmoid-based":
        it "applies Sigmoid":
            val device = Device::CUDA(1)
            val x = torch.randn([15], device=device)
            val output = act.sigmoid(x)
            expect output.shape() == [15]
            expect output.device().cuda_id() == Some(1)

        it "applies Tanh":
            val device = Device::CUDA(1)
            val x = torch.randn([15], device=device)
            val output = act.tanh(x)
            expect output.shape() == [15]
            expect output.device().cuda_id() == Some(1)

        it "applies Softmax":
            val device = Device::CUDA(1)
            val x = torch.randn([10], device=device)
            val output = act.softmax(x, dim=0)
            expect output.shape() == [10]
            expect output.device().cuda_id() == Some(1)

    context "modern activations":
        it "applies GELU":
            val device = Device::CUDA(1)
            val x = torch.randn([25], device=device)
            val output = act.gelu(x)
            expect output.shape() == [25]
            expect output.device().cuda_id() == Some(1)
