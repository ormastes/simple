# PyTorch Transformer Tests
# Tests for ml.torch.nn transformer modules

import std.spec
import ml.torch as torch
import ml.torch.nn.transformer as tfm
import ml.torch.ffi_helpers as helpers
import ml.torch.device.Device
import ml.torch.dtype.DType

describe "Transformer":
    context "attention":
        it "creates multi-head attention":
            val mha = tfm.MultiheadAttention(embed_dim=256, num_heads=8)
            expect mha.embed_dim == 256
            expect mha.num_heads == 8

    context "encoder/decoder":
        it "creates transformer encoder layer":
            val encoder = tfm.TransformerEncoderLayer(d_model=512, nhead=8)
            expect encoder.d_model == 512
            expect encoder.nhead == 8

        it "creates transformer decoder layer":
            val decoder = tfm.TransformerDecoderLayer(d_model=512, nhead=8)
            expect decoder.d_model == 512
            expect decoder.nhead == 8

    context "sequence modeling":
        it "processes sequences with positional encoding":
            val pe = tfm.PositionalEncoding(d_model=256, max_len=1024)
            expect pe.d_model == 256
            expect pe.max_len == 1024

    context "advanced":
        skip "handles masking":
            pass
