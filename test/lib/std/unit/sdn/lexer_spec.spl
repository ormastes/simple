# SDN Lexer Tests
#
# Tests for lexer tokenization with INDENT/DEDENT handling.
#
# NOTE: BDD framework scoping bugs were FIXED on 2026-01-04
# These tests can now be implemented properly

use std.spec.*
use sdn.lexer.{Lexer}
use token.{TokenKind}

describe "SDN Lexer":
    context "primitive values":
        it "tokenizes integers":
            val lexer = Lexer.new("42")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Integer + EOF
            match tokens[0].kind:
                case TokenKind.Integer(value):
                    expect(value).to eq(42)
                case _:
                    fail("Expected Integer token")

        it "tokenizes negative integers":
            val lexer = Lexer.new("-123")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Integer + EOF
            match tokens[0].kind:
                case TokenKind.Integer(value):
                    expect(value).to eq(-123)
                case _:
                    fail("Expected Integer token")

        it "tokenizes integers with underscores":
            val lexer = Lexer.new("1_000_000")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Integer + EOF
            match tokens[0].kind:
                case TokenKind.Integer(value):
                    expect(value).to eq(1000000)
                case _:
                    fail("Expected Integer token")

        it "tokenizes floats":
            val lexer = Lexer.new("3.14")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Float + EOF
            match tokens[0].kind:
                case TokenKind.Float(value):
                    # Check if close to 3.14
                    expect(value > 3.13 and value < 3.15).to eq(true)
                case _:
                    fail("Expected Float token")

        it "tokenizes scientific notation":
            val lexer = Lexer.new("1.23e-4")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Float + EOF
            match tokens[0].kind:
                case TokenKind.Float(value):
                    # Check if close to 0.000123
                    expect(value > 0.0001 and value < 0.0002).to eq(true)
                case _:
                    fail("Expected Float token")

        it "tokenizes bare strings":
            val lexer = Lexer.new("hello_world")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Identifier + EOF
            match tokens[0].kind:
                case TokenKind.Identifier(value):
                    expect(value).to eq("hello_world")
                case _:
                    fail("Expected Identifier token")

        it "tokenizes quoted strings":
            val lexer = Lexer.new("\"hello world\"")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # String + EOF
            match tokens[0].kind:
                case TokenKind.String(value):
                    expect(value).to eq("hello world")
                case _:
                    fail("Expected String token")

        it "tokenizes booleans":
            val lexer = Lexer.new("true false")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(3)  # Bool + Bool + EOF
            match tokens[0].kind:
                case TokenKind.Bool(value):
                    expect(value).to eq(true)
                case _:
                    fail("Expected Bool token")
            match tokens[1].kind:
                case TokenKind.Bool(value):
                    expect(value).to eq(false)
                case _:
                    fail("Expected Bool token")

        it "tokenizes null values":
            val lexer = Lexer.new("null")
            val tokens = lexer.tokenize()
            expect(tokens.len()).to eq(2)  # Null + EOF
            match tokens[0].kind:
                case TokenKind.Null:
                    pass  # Success
                case _:
                    fail("Expected Null token")

    context "indentation":
        it "tracks INDENT tokens":
            val source = "parent:\n    child"
            val lexer = Lexer.new(source)
            val tokens = lexer.tokenize()
            # Should have: Identifier, Colon, Newline, INDENT, Identifier, EOF
            var found_indent = false
            for token in tokens:
                match token.kind:
                    case TokenKind.Indent:
                        found_indent = true
                    case _:
                        pass
            expect(found_indent).to eq(true)

        it "tracks DEDENT tokens":
            val source = "parent:\n    child\nsibling"
            val lexer = Lexer.new(source)
            val tokens = lexer.tokenize()
            # Should have DEDENT before "sibling"
            var found_dedent = false
            for token in tokens:
                match token.kind:
                    case TokenKind.Dedent:
                        found_dedent = true
                    case _:
                        pass
            expect(found_dedent).to eq(true)

        it "emits multiple DEDENTs for deep unindent":
            val source = "a:\n    b:\n        c\nd"
            val lexer = Lexer.new(source)
            val tokens = lexer.tokenize()
            # Should have 2 DEDENTs before "d"
            var dedent_count = 0
            for token in tokens:
                match token.kind:
                    case TokenKind.Dedent:
                        dedent_count = dedent_count + 1
                    case _:
                        pass
            expect(dedent_count >= 2).to eq(true)

    context "punctuation":
        it "tokenizes dict/array delimiters":
            val lexer = Lexer.new("[1, 2] {a: b}")
            val tokens = lexer.tokenize()
            # Check for brackets and braces
            var has_lbracket = false
            var has_rbracket = false
            var has_lbrace = false
            var has_rbrace = false
            for token in tokens:
                match token.kind:
                    case TokenKind.LBracket:
                        has_lbracket = true
                    case TokenKind.RBracket:
                        has_rbracket = true
                    case TokenKind.LBrace:
                        has_lbrace = true
                    case TokenKind.RBrace:
                        has_rbrace = true
                    case _:
                        pass
            expect(has_lbracket).to eq(true)
            expect(has_rbracket).to eq(true)
            expect(has_lbrace).to eq(true)
            expect(has_rbrace).to eq(true)

        it "tokenizes table pipe delimiters":
            val lexer = Lexer.new("table |col1, col2|")
            val tokens = lexer.tokenize()
            # Check for pipe tokens
            var pipe_count = 0
            for token in tokens:
                match token.kind:
                    case TokenKind.Pipe:
                        pipe_count = pipe_count + 1
                    case _:
                        pass
            expect(pipe_count).to eq(2)

        it "tokenizes assignment operators":
            val lexer = Lexer.new("key = value")
            val tokens = lexer.tokenize()
            # Check for equals token
            var has_equals = false
            for token in tokens:
                match token.kind:
                    case TokenKind.Equals:
                        has_equals = true
                    case _:
                        pass
            expect(has_equals).to eq(true)

    context "comments":
        it "skips line comments":
            val lexer = Lexer.new("# This is a comment\nvalue")
            val tokens = lexer.tokenize()
            # Comment should be skipped, only value and EOF
            expect(tokens.len()).to eq(3)  # Newline + Identifier + EOF

        it "handles comments at end of line":
            val lexer = Lexer.new("value # comment")
            val tokens = lexer.tokenize()
            # Should have identifier and EOF (comment skipped)
            expect(tokens.len()).to eq(2)

    context "simple key-value pairs":
        it "tokenizes colon-separated pairs":
            val lexer = Lexer.new("name: Alice")
            val tokens = lexer.tokenize()
            # Should have: Identifier, Colon, Identifier, EOF
            expect(tokens.len()).to eq(4)
            match tokens[1].kind:
                case TokenKind.Colon:
                    pass  # Success
                case _:
                    fail("Expected Colon token")

        it "tokenizes multiple values":
            val lexer = Lexer.new("age: 30, score: 95")
            val tokens = lexer.tokenize()
            # Should have multiple colons and commas
            var colon_count = 0
            var comma_count = 0
            for token in tokens:
                match token.kind:
                    case TokenKind.Colon:
                        colon_count = colon_count + 1
                    case TokenKind.Comma:
                        comma_count = comma_count + 1
                    case _:
                        pass
            expect(colon_count).to eq(2)
            expect(comma_count).to eq(1)

    context "bracket depth tracking":
        it "disables indentation inside brackets":
            val source = "[1,\n    2,\n    3]"
            val lexer = Lexer.new(source)
            val tokens = lexer.tokenize()
            # Should NOT have INDENT/DEDENT tokens inside brackets
            var indent_count = 0
            for token in tokens:
                match token.kind:
                    case TokenKind.Indent:
                        indent_count = indent_count + 1
                    case TokenKind.Dedent:
                        indent_count = indent_count + 1
                    case _:
                        pass
            expect(indent_count).to eq(0)

    context "escape sequences":
        it "handles newline escape":
            val lexer = Lexer.new("\"line1\\nline2\"")
            val tokens = lexer.tokenize()
            match tokens[0].kind:
                case TokenKind.String(value):
                    expect(value.contains("\n")).to eq(true)
                case _:
                    fail("Expected String token")

        it "handles tab escape":
            val lexer = Lexer.new("\"col1\\tcol2\"")
            val tokens = lexer.tokenize()
            match tokens[0].kind:
                case TokenKind.String(value):
                    expect(value.contains("\t")).to eq(true)
                case _:
                    fail("Expected String token")

        it "handles quote escape":
            val lexer = Lexer.new("\"He said \\\"hello\\\"\"")
            val tokens = lexer.tokenize()
            match tokens[0].kind:
                case TokenKind.String(value):
                    expect(value.contains("\"")).to eq(true)
                case _:
                    fail("Expected String token")

    context "EOF handling":
        it "emits EOF token":
            val lexer = Lexer.new("test")
            val tokens = lexer.tokenize()
            val last_token = tokens[tokens.len() - 1]
            match last_token.kind:
                case TokenKind.Eof:
                    pass  # Success
                case _:
                    fail("Expected EOF token at end")

        it "emits DEDENTs before EOF":
            val source = "parent:\n    child:\n        value"
            val lexer = Lexer.new(source)
            val tokens = lexer.tokenize()
            # Should have DEDENTs before EOF
            val last_index = tokens.len() - 1
            var dedents_before_eof = 0
            var idx = last_index - 1
            loop:
                if idx < 0:
                    break
                match tokens[idx].kind:
                    case TokenKind.Dedent:
                        dedents_before_eof = dedents_before_eof + 1
                        idx = idx - 1
                    case _:
                        break
            expect(dedents_before_eof >= 1).to eq(true)
