# @skip
use std.spec

"""
# Simple Compiler Lexer Specification

**Feature IDs:** #001-050
**Category:** Compiler
**Difficulty:** 3/5
**Status:** In Progress

## Overview

The lexer (tokenizer) is the first phase of the Simple compiler pipeline.
It converts source text into a stream of tokens, handling:
- All literal types (int, float, string, bool, nil)
- Keywords and identifiers
- Operators (arithmetic, comparison, logical, special)
- Indentation-based syntax (Python-style INDENT/DEDENT)
- Math block mode with special operators (caret for power)
- Error recovery for invalid characters

## Key Concepts

| Concept | Description |
|---------|-------------|
| Token | Atomic unit of source code (keyword, operator, literal, etc.) |
| Span | Source location info (line, column, offset) |
| Indentation Tracking | Automatic INDENT/DEDENT token generation |
| Math Block Mode | Special lexer mode for `m{}` blocks with caret operator |
| Implicit Multiplication | Auto-insert multiplication in math mode (e.g., `2x` → `2 * x`) |

## Behavior

- Tokenizes all Simple language constructs
- Tracks indentation levels and generates INDENT/DEDENT tokens
- Supports multiple string literal formats (normal, raw, multi-line)
- Handles numeric literals in multiple bases (decimal, hex, binary, octal)
- Provides precise source location tracking for error messages
- Enters special modes for math blocks and custom blocks

## Related Specifications

- [Parser](parser_spec.spl) - Consumes token stream to build AST
- [Blocks](blocks_spec.spl) - Custom block handling

## Implementation Notes

The lexer is stateful and tracks:
- Current position in source text
- Indentation stack for INDENT/DEDENT generation
- Current lexer mode (Normal, MathBlock, RawBlock)
- Pending tokens for lookahead
"""

# Note: Lexer is a struct, not a class, so we use Lexer.new() to create instances


# ============================================================================
# Helper Functions
# ============================================================================

fn lex(source: text) -> [Token]:
    """Lex source code and return all tokens."""
    val lexer = Lexer(source: source)
    var tokens = []
    loop:
        val tok = lexer.next_token()
        tokens.push(tok)
        if tok.kind == TokenKind.Eof:
            break
    tokens

fn lex_one(source: text) -> Token:
    """Lex source and return first token (excluding EOF)."""
    val lexer = Lexer(source: source)
    lexer.next_token()

fn token_kinds(source: text) -> [TokenKind]:
    """Return just the token kinds from source."""
    lex(source).map(\t: t.kind)

fn expect_token_kinds(source: text, expected: [TokenKind]):
    """Assert that source lexes to expected token kinds."""
    val actual = token_kinds(source)
    expect(actual).to(eq(expected))


# ============================================================================
# Test Group 1: Literal Tokens
# ============================================================================

describe "Lexer - Integer Literals":
    """
    ## Integer Literal Tokenization

    Tests all integer literal formats:
    - Decimal: `42`, `0`, `123456789`
    - Hexadecimal: `0x2A`, `0xFF`, `0x0`
    - Binary: `0b101010`, `0b1111`, `0b0`
    - Octal: `0o52`, `0o77`, `0o0`
    - Underscores: `1_000_000`, `0xFF_FF_FF`
    """

    it "lexes decimal integers":
        val tok = lex_one("42")
        expect(tok.kind).to(eq(TokenKind.IntLit))
        expect(tok.text).to(eq("42"))

    it "lexes zero":
        val tok = lex_one("0")
        expect(tok.kind).to(eq(TokenKind.IntLit))

    it "lexes large decimal integers":
        val tok = lex_one("123456789")
        expect(tok.kind).to(eq(TokenKind.IntLit))

    it "lexes hexadecimal integers":
        val tok = lex_one("0x2A")
        expect(tok.kind).to(eq(TokenKind.IntLit))
        expect(tok.text).to(eq("0x2A"))

    it "lexes binary integers":
        val tok = lex_one("0b101010")
        expect(tok.kind).to(eq(TokenKind.IntLit))

    it "lexes octal integers":
        val tok = lex_one("0o52")
        expect(tok.kind).to(eq(TokenKind.IntLit))

    it "lexes integers with underscores":
        val tok = lex_one("1_000_000")
        expect(tok.kind).to(eq(TokenKind.IntLit))


describe "Lexer - Float Literals":
    """
    ## Floating-Point Literal Tokenization

    Tests all float literal formats:
    - Simple: `3.14`, `0.5`, `2.0`
    - Scientific: `1e10`, `2.5e-3`, `6.022e23`
    - Underscores: `1_000.5`, `1e1_000`
    """

    it "lexes simple float":
        val tok = lex_one("3.14")
        expect(tok.kind).to(eq(TokenKind.FloatLit))

    it "lexes float with leading zero":
        val tok = lex_one("0.5")
        expect(tok.kind).to(eq(TokenKind.FloatLit))

    it "lexes float with trailing zero":
        val tok = lex_one("2.0")
        expect(tok.kind).to(eq(TokenKind.FloatLit))

    it "lexes scientific notation positive exponent":
        val tok = lex_one("1e10")
        expect(tok.kind).to(eq(TokenKind.FloatLit))

    it "lexes scientific notation negative exponent":
        val tok = lex_one("2.5e-3")
        expect(tok.kind).to(eq(TokenKind.FloatLit))

    it "lexes float with underscores":
        val tok = lex_one("1_000.5")
        expect(tok.kind).to(eq(TokenKind.FloatLit))


describe "Lexer - String Literals":
    """
    ## String Literal Tokenization

    Tests all string literal formats:
    - Double quotes: `"hello"`
    - Single quotes: `'world'`
    - Raw strings: `r"no\nescapes"`
    - Multi-line: `\"\"\"text\"\"\"`
    - Interpolation: `"Hello {name}"`
    """

    it "lexes double-quoted string":
        val tok = lex_one("\"hello\"")
        expect(tok.kind).to(eq(TokenKind.StringLit))
        expect(tok.text).to(eq("\"hello\""))

    it "lexes single-quoted string":
        val tok = lex_one("'world'")
        expect(tok.kind).to(eq(TokenKind.StringLit))

    it "lexes empty string":
        val tok = lex_one("\"\"")
        expect(tok.kind).to(eq(TokenKind.StringLit))

    it "lexes string with escapes":
        val tok = lex_one("\"hello\\nworld\"")
        expect(tok.kind).to(eq(TokenKind.StringLit))

    it "lexes raw string":
        val tok = lex_one("r\"no\\nescapes\"")
        expect(tok.kind).to(eq(TokenKind.RawStringLit))

    it "lexes multi-line string":
        val tok = lex_one("\"\"\"line1\nline2\"\"\"")
        expect(tok.kind).to(eq(TokenKind.StringLit))

    it "lexes string with interpolation":
        val tok = lex_one("\"Hello {name}\"")
        expect(tok.kind).to(eq(TokenKind.StringLit))


describe "Lexer - Boolean and Nil Literals":
    """
    ## Boolean and Nil Tokenization

    Tests literal keywords:
    - `true` → BoolLit
    - `false` → BoolLit
    - `nil` → NilLit
    """

    it "lexes true":
        val tok = lex_one("true")
        expect(tok.kind).to(eq(TokenKind.BoolLit))

    it "lexes false":
        val tok = lex_one("false")
        expect(tok.kind).to(eq(TokenKind.BoolLit))

    it "lexes nil":
        val tok = lex_one("nil")
        expect(tok.kind).to(eq(TokenKind.NilLit))


# ============================================================================
# Test Group 2: Keywords
# ============================================================================

describe "Lexer - Declaration Keywords":
    """
    ## Declaration Keyword Tokenization

    Tests keywords for declarations:
    - `fn`, `val`, `var`, `struct`, `class`, `enum`, `trait`, `impl`
    - `type`, `mod`, `pub`, `static`, `me`, `extern`
    """

    it "lexes fn":
        expect(lex_one("fn").kind).to(eq(TokenKind.KwFn))

    it "lexes val":
        expect(lex_one("val").kind).to(eq(TokenKind.KwVal))

    it "lexes var":
        expect(lex_one("var").kind).to(eq(TokenKind.KwVar))

    it "lexes struct":
        expect(lex_one("struct").kind).to(eq(TokenKind.KwStruct))

    it "lexes class":
        expect(lex_one("class").kind).to(eq(TokenKind.KwClass))

    it "lexes enum":
        expect(lex_one("enum").kind).to(eq(TokenKind.KwEnum))

    it "lexes trait":
        expect(lex_one("trait").kind).to(eq(TokenKind.KwTrait))

    it "lexes impl":
        expect(lex_one("impl").kind).to(eq(TokenKind.KwImpl))

    it "lexes type":
        expect(lex_one("type").kind).to(eq(TokenKind.KwType))

    it "lexes mod":
        expect(lex_one("mod").kind).to(eq(TokenKind.KwMod))

    it "lexes pub":
        expect(lex_one("pub").kind).to(eq(TokenKind.KwPub))

    it "lexes static":
        expect(lex_one("static").kind).to(eq(TokenKind.KwStatic))

    it "lexes me":
        expect(lex_one("me").kind).to(eq(TokenKind.KwMe))

    it "lexes extern":
        expect(lex_one("extern").kind).to(eq(TokenKind.KwExtern))


describe "Lexer - Control Flow Keywords":
    """
    ## Control Flow Keyword Tokenization

    Tests keywords for control flow:
    - `if`, `else`, `elif`, `match`, `case`
    - `for`, `while`, `loop`, `break`, `continue`
    - `return`, `yield`, `await`, `async`
    """

    it "lexes if":
        expect(lex_one("if").kind).to(eq(TokenKind.KwIf))

    it "lexes else":
        expect(lex_one("else").kind).to(eq(TokenKind.KwElse))

    it "lexes elif":
        expect(lex_one("elif").kind).to(eq(TokenKind.KwElif))

    it "lexes match":
        expect(lex_one("match").kind).to(eq(TokenKind.KwMatch))

    it "lexes for":
        expect(lex_one("for").kind).to(eq(TokenKind.KwFor))

    it "lexes while":
        expect(lex_one("while").kind).to(eq(TokenKind.KwWhile))

    it "lexes loop":
        expect(lex_one("loop").kind).to(eq(TokenKind.KwLoop))

    it "lexes break":
        expect(lex_one("break").kind).to(eq(TokenKind.KwBreak))

    it "lexes continue":
        expect(lex_one("continue").kind).to(eq(TokenKind.KwContinue))

    it "lexes return":
        expect(lex_one("return").kind).to(eq(TokenKind.KwReturn))


describe "Lexer - Expression Keywords":
    """
    ## Expression Keyword Tokenization

    Tests keywords used in expressions:
    - `in`, `is`, `as`, `not`, `and`, `or`, `xor`
    - `try`, `catch`, `throw`, `with`
    - `self`, `super`, `None`, `Some`, `Ok`, `Err`
    """

    it "lexes in":
        expect(lex_one("in").kind).to(eq(TokenKind.KwIn))

    it "lexes is":
        expect(lex_one("is").kind).to(eq(TokenKind.KwIs))

    it "lexes as":
        expect(lex_one("as").kind).to(eq(TokenKind.KwAs))

    it "lexes not":
        expect(lex_one("not").kind).to(eq(TokenKind.KwNot))

    it "lexes and":
        expect(lex_one("and").kind).to(eq(TokenKind.KwAnd))

    it "lexes or":
        expect(lex_one("or").kind).to(eq(TokenKind.KwOr))

    it "lexes xor":
        expect(lex_one("xor").kind).to(eq(TokenKind.KwXor))

    it "lexes self":
        expect(lex_one("self").kind).to(eq(TokenKind.KwSelf))


# ============================================================================
# Test Group 3: Operators
# ============================================================================

describe "Lexer - Arithmetic Operators":
    """
    ## Arithmetic Operator Tokenization

    Tests: `+`, `-`, `*`, `/`, `%`, `**`
    """

    it "lexes plus":
        expect(lex_one("+").kind).to(eq(TokenKind.Plus))

    it "lexes minus":
        expect(lex_one("-").kind).to(eq(TokenKind.Minus))

    it "lexes star":
        expect(lex_one("*").kind).to(eq(TokenKind.Star))

    it "lexes slash":
        expect(lex_one("/").kind).to(eq(TokenKind.Slash))

    it "lexes percent":
        expect(lex_one("%").kind).to(eq(TokenKind.Percent))

    it "lexes power operator":
        expect(lex_one("**").kind).to(eq(TokenKind.StarStar))


describe "Lexer - Comparison Operators":
    """
    ## Comparison Operator Tokenization

    Tests: `==`, `!=`, `<`, `>`, `<=`, `>=`
    """

    it "lexes equal":
        expect(lex_one("==").kind).to(eq(TokenKind.Eq))

    it "lexes not equal":
        expect(lex_one("!=").kind).to(eq(TokenKind.NotEq))

    it "lexes less than":
        expect(lex_one("<").kind).to(eq(TokenKind.Lt))

    it "lexes greater than":
        expect(lex_one(">").kind).to(eq(TokenKind.Gt))

    it "lexes less than or equal":
        expect(lex_one("<=").kind).to(eq(TokenKind.LtEq))

    it "lexes greater than or equal":
        expect(lex_one(">=").kind).to(eq(TokenKind.GtEq))


describe "Lexer - Special Operators":
    """
    ## Special Operator Tokenization

    Tests:
    - Optional chaining: `?.`, `.?`, `??`
    - Pipeline: `|>`, `>>`, `<<`, `//`, `~>`
    - Broadcast: `.+`, `.-`, `.*`, `./`, `.^`
    - Range: `..`, `..=`
    - Other: `@`, `!`, `\`
    """

    it "lexes optional chaining":
        expect(lex_one("?.").kind).to(eq(TokenKind.QuestionDot))

    it "lexes existence check":
        expect(lex_one(".?").kind).to(eq(TokenKind.DotQuestion))

    it "lexes null coalescing":
        expect(lex_one("??").kind).to(eq(TokenKind.QuestionQuestion))

    it "lexes pipe forward":
        expect(lex_one("|>").kind).to(eq(TokenKind.PipeForward))

    it "lexes compose":
        expect(lex_one(">>").kind).to(eq(TokenKind.Compose))

    it "lexes compose back":
        expect(lex_one("<<").kind).to(eq(TokenKind.ComposeBack))

    it "lexes layer connect":
        expect(lex_one("~>").kind).to(eq(TokenKind.LayerConnect))

    it "lexes broadcast add":
        expect(lex_one(".+").kind).to(eq(TokenKind.DotPlus))

    it "lexes broadcast sub":
        expect(lex_one(".-").kind).to(eq(TokenKind.DotMinus))

    it "lexes broadcast mul":
        expect(lex_one(".*").kind).to(eq(TokenKind.DotStar))

    it "lexes broadcast div":
        expect(lex_one("./").kind).to(eq(TokenKind.DotSlash))

    it "lexes broadcast power":
        expect(lex_one(".^").kind).to(eq(TokenKind.DotCaret))

    it "lexes exclusive range":
        expect(lex_one("..").kind).to(eq(TokenKind.DotDot))

    it "lexes inclusive range":
        expect(lex_one("..=").kind).to(eq(TokenKind.DotDotEq))

    it "lexes at operator":
        expect(lex_one("@").kind).to(eq(TokenKind.At))


# ============================================================================
# Test Group 4: Indentation Tracking
# ============================================================================

describe "Lexer - Indentation Tracking":
    """
    ## Indentation-Based Syntax

    The lexer automatically generates INDENT and DEDENT tokens
    based on indentation levels (Python-style).

    **Rules:**
    - Increase in indentation → INDENT token
    - Decrease in indentation → DEDENT token(s)
    - Multiple dedents can occur on one line
    """

    it "generates INDENT for increased indentation":
        val source = "if true:\n    pass"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.Indent))

    it "generates DEDENT for decreased indentation":
        val source = "if true:\n    pass\nelse:\n    pass"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.Dedent))

    it "generates multiple DEDENTs for large decrease":
        val source = "if true:\n    if true:\n        pass\nelse:\n    pass"
        val kinds = token_kinds(source)
        val dedent_count = kinds.filter(\k: k == TokenKind.Dedent).len()
        expect(dedent_count).to(gt(0))

    it "ignores blank lines for indentation":
        val source = "val x = 1\n\nval y = 2"
        val kinds = token_kinds(source)
        expect(kinds).not_to(include(TokenKind.Indent))

    it "ignores comment-only lines for indentation":
        val source = "val x = 1\n# comment\nval y = 2"
        val kinds = token_kinds(source)
        expect(kinds).not_to(include(TokenKind.Indent))

    it "handles tabs as indentation":
        val source = "if true:\n\tpass"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.Indent))

    it "handles mixed spaces and tabs":
        # Should produce error or normalize
        val source = "if true:\n\t    pass"
        val kinds = token_kinds(source)
        # At least it should lex without crashing
        expect(kinds.len()).to(gt(0))


# ============================================================================
# Test Group 5: Math Block Mode
# ============================================================================

describe "Lexer - Math Block Mode":
    """
    ## Math Block Special Lexing

    Inside `m{}` blocks, the lexer enters a special mode:
    - `^` is power operator (not error)
    - Implicit multiplication: `2x` → `2 * x`
    - Special handling for math notation
    """

    it "lexes caret as power in math block":
        val source = "m{ x^2 }"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.Caret))

    it "rejects caret outside math block":
        val source = "x^2"
        val tok = lex_one(source)
        # Should be Ident("x") followed by Error
        expect(lex(source)[1].kind).to(eq(TokenKind.Error))

    it "generates implicit multiplication in math block":
        val source = "m{ 2x }"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.ImplicitMul))

    it "handles complex math expressions":
        val source = "m{ x^2 + 2*x*y + y^2 }"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.Caret))

    it "exits math mode at closing brace":
        val source = "m{ x^2 } + y"
        val tokens = lex(source)
        # After closing brace, should be back to normal mode
        # so + is Plus, not error
        expect(tokens.any(\t: t.kind == TokenKind.Plus)).to(be_true())

    it "handles nested braces in math block":
        val source = "m{ f(x^2) }"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.Caret))


# ============================================================================
# Test Group 6: Identifiers
# ============================================================================

describe "Lexer - Identifiers":
    """
    ## Identifier Tokenization

    Valid identifiers:
    - Start with letter or underscore
    - Contain letters, digits, underscores
    - Case-sensitive
    - Can have trailing `_` (mutable implicit var)
    """

    it "lexes simple identifier":
        val tok = lex_one("foo")
        expect(tok.kind).to(eq(TokenKind.Ident))
        expect(tok.text).to(eq("foo"))

    it "lexes identifier with underscore":
        val tok = lex_one("foo_bar")
        expect(tok.kind).to(eq(TokenKind.Ident))

    it "lexes identifier with digits":
        val tok = lex_one("var1")
        expect(tok.kind).to(eq(TokenKind.Ident))

    it "lexes private identifier":
        val tok = lex_one("_private")
        expect(tok.kind).to(eq(TokenKind.Ident))

    it "lexes mutable implicit var":
        val tok = lex_one("count_")
        expect(tok.kind).to(eq(TokenKind.Ident))

    it "distinguishes keywords from identifiers":
        val tok = lex_one("function")  # Not "fn"
        expect(tok.kind).to(eq(TokenKind.Ident))


# ============================================================================
# Test Group 7: Error Recovery
# ============================================================================

describe "Lexer - Error Recovery":
    """
    ## Error Handling

    The lexer should gracefully handle errors:
    - Invalid characters → Error token
    - Unterminated strings → Error token
    - Invalid numeric literals → Error token
    """

    it "produces error token for invalid character":
        val tok = lex_one("$invalid")
        expect(tok.kind).to(eq(TokenKind.Dollar))  # $ is valid
        val tokens = lex("~invalid")
        expect(tokens[0].kind).to(eq(TokenKind.Tilde))  # ~ is valid

    it "handles unterminated string":
        val source = "\"unterminated"
        val tok = lex_one(source)
        # Should produce either Error or StringLit with error flag
        val is_valid = [TokenKind.Error, TokenKind.StringLit].include(tok.kind)
        expect(is_valid).to(be_true())

    it "handles invalid numeric literal":
        val source = "0x"  # Hex with no digits
        val tokens = lex(source)
        # Should produce error or int token
        expect(tokens.len()).to(gt(0))

    it "continues after error":
        val source = "val x = @@ 42"
        val tokens = lex(source)
        # Should lex val, x, =, @@, 42 despite error
        expect(tokens.any(\t: t.kind == TokenKind.IntLit)).to(be_true())


# ============================================================================
# Test Group 8: Delimiters and Punctuation
# ============================================================================

describe "Lexer - Delimiters":
    """
    ## Delimiter Tokenization

    Tests: `(`, `)`, `{`, `}`, `[`, `]`
    """

    it "lexes left paren":
        expect(lex_one("(").kind).to(eq(TokenKind.LParen))

    it "lexes right paren":
        expect(lex_one(")").kind).to(eq(TokenKind.RParen))

    it "lexes left brace":
        expect(lex_one("{").kind).to(eq(TokenKind.LBrace))

    it "lexes right brace":
        expect(lex_one("}").kind).to(eq(TokenKind.RBrace))

    it "lexes left bracket":
        expect(lex_one("[").kind).to(eq(TokenKind.LBracket))

    it "lexes right bracket":
        expect(lex_one("]").kind).to(eq(TokenKind.RBracket))


describe "Lexer - Punctuation":
    """
    ## Punctuation Tokenization

    Tests: `,`, `:`, `.`, `;`, `.`, `->`, `=>`
    """

    it "lexes comma":
        expect(lex_one(",").kind).to(eq(TokenKind.Comma))

    it "lexes colon":
        expect(lex_one(":").kind).to(eq(TokenKind.Colon))

    it "lexes double colon":
        expect(lex_one(".").kind).to(eq(TokenKind.ColonColon))

    it "lexes semicolon":
        expect(lex_one(";").kind).to(eq(TokenKind.Semicolon))

    it "lexes dot":
        expect(lex_one(".").kind).to(eq(TokenKind.Dot))

    it "lexes arrow":
        expect(lex_one("->").kind).to(eq(TokenKind.Arrow))

    it "lexes fat arrow":
        expect(lex_one("=>").kind).to(eq(TokenKind.FatArrow))


# ============================================================================
# Test Group 9: Source Location Tracking
# ============================================================================

describe "Lexer - Source Location Tracking":
    """
    ## Span Information

    Every token has a Span with:
    - Start/end offset
    - Line number (1-based)
    - Column number (1-based)
    """

    it "tracks single-line token position":
        val tok = lex_one("foo")
        expect(tok.span.line).to(eq(1))
        expect(tok.span.col).to(eq(1))

    it "tracks column offset":
        val source = "val x = 42"
        val tokens = lex(source)
        val x_tok = tokens[1]  # "x"
        expect(x_tok.span.col).to(eq(5))  # After "val "

    it "tracks multi-line positions":
        val source = "val x = 1\nval y = 2"
        val tokens = lex(source)
        val y_line = tokens.filter(\t: t.text == "y")[0].span.line
        expect(y_line).to(eq(2))

    it "provides correct span length":
        val tok = lex_one("function")
        expect(tok.span.len()).to(eq(8))

    it "merges spans correctly":
        val span1 = Span.new(0, 5, 1, 1)
        val span2 = Span.new(10, 15, 1, 11)
        val merged = span1.merge(span2)
        expect(merged.start).to(eq(0))
        expect(merged.end).to(eq(15))


# ============================================================================
# Test Group 10: Integration Tests
# ============================================================================

describe "Lexer - Integration Tests":
    """
    ## Complete Code Lexing

    Tests lexing complete Simple code snippets.
    """

    it "lexes simple function definition":
        val source = """
fn square(x):
    x * x
"""
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.KwFn))
        expect(kinds).to(include(TokenKind.Ident))
        expect(kinds).to(include(TokenKind.Indent))
        expect(kinds).to(include(TokenKind.Star))

    it "lexes class with methods":
        val source = """
class Point:
    x: i64
    y: i64

    fn distance():
        (self.x ** 2 + self.y ** 2) ** 0.5
"""
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.KwClass))
        expect(kinds).to(include(TokenKind.KwFn))
        expect(kinds).to(include(TokenKind.KwSelf))

    it "lexes match expression":
        val source = """
match value:
    Some(x):
        print(x)
    None:
        print("none")
"""
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.KwMatch))
        expect(kinds).to(include(TokenKind.KwSome))
        expect(kinds).to(include(TokenKind.KwNone))

    it "lexes pipeline expression":
        val source = "data |> normalize |> transform |> predict"
        val kinds = token_kinds(source)
        val pipe_count = kinds.filter(\k: k == TokenKind.PipeForward).len()
        expect(pipe_count).to(eq(3))

    it "lexes ML dimension checking":
        val source = "val model = Linear(784, 256) ~> ReLU() ~> Linear(256, 10)"
        val kinds = token_kinds(source)
        expect(kinds).to(include(TokenKind.LayerConnect))
