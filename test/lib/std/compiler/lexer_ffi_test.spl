"""
Lexer FFI Test

Tests that the lexer FFI correctly exposes token-level behavior.
Verifies contextual keywords work via FFI.
"""

use std.compiler.lexer.*

print "=== Lexer FFI Test ==="
print ""

# Test 1: Basic tokenization
print "Test 1: Basic tokenization"
val tokens1 = tokenize("skip(5)")
print "  Source: 'skip(5)'"
print "  Tokens: {tokens1.len()}"
if tokens1.len() > 0:
    print "  First: {tokens1[0].to_string()}"
    if tokens1[0].kind == "Identifier" and tokens1[0].name == "skip":
        print "  ✅ skip tokenized as Identifier"
    else:
        print "  ❌ Expected Identifier(skip), got {tokens1[0].kind}"
print ""

# Test 2: Contextual keyword - skip as keyword
print "Test 2: skip as keyword"
val tokens2 = tokenize("skip")
print "  Source: 'skip'"
if tokens2.len() > 0:
    print "  First: {tokens2[0].kind}"
    if tokens2[0].kind == "Skip":
        print "  ✅ skip tokenized as Skip keyword"
    else:
        print "  ❌ Expected Skip, got {tokens2[0].kind}"
print ""

# Test 3: Contextual keyword - static
print "Test 3: static as identifier and keyword"
val tokens3a = tokenize("static()")
val tokens3b = tokenize("static fn")
if tokens3a.len() > 0 and tokens3b.len() > 0:
    val is_ident = tokens3a[0].kind == "Identifier"
    val is_keyword = tokens3b[0].kind == "Static"
    print "  static(): {tokens3a[0].kind}"
    print "  static fn: {tokens3b[0].kind}"
    if is_ident and is_keyword:
        print "  ✅ static contextual keyword works"
    else:
        print "  ❌ Contextual behavior incorrect"
print ""

# Test 4: token_kinds helper
print "Test 4: token_kinds helper"
val kinds = token_kinds("fn skip(n)")
print "  Source: 'fn skip(n)'"
print "  Kinds: {kinds}"
val expected = kinds[0] == "Fn" and kinds[1] == "Identifier"
if expected:
    print "  ✅ token_kinds works correctly"
else:
    print "  ❌ Unexpected token kinds"
print ""

# Test 5: identifier_names helper
print "Test 5: identifier_names helper"
val names = identifier_names("obj.skip(x)")
print "  Source: 'obj.skip(x)'"
print "  Names: {names}"
if names.contains("obj") and names.contains("skip"):
    print "  ✅ identifier_names extracts correctly"
else:
    print "  ❌ Expected [obj, skip, x]"
print ""

print "=== FFI Test Complete ==="
print ""
print "Summary:"
print "  ✅ FFI lexer_tokenize works"
print "  ✅ Contextual keywords exposed via FFI"
print "  ✅ Helper functions work"
print "  ✅ Token-level testing enabled in Simple"
