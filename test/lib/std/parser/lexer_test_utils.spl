"""
Lexer Test Utilities

Helper functions for testing Simple lexer behavior.
Pure Simple implementation - no FFI.
"""

use std.parser.treesitter.lexer.{Lexer}
use std.parser.treesitter.grammar.tokens.{TokenKind}

# Tokenize source and return list of token kind names
pub fn tokenize_names(source: str) -> List<str>:
    val lexer = Lexer.new(source)
    val result = lexer.tokenize()

    match result:
        case Ok(tokens):
            var names = []
            for token in tokens:
                names.append(token_name(token.kind))
            return names
        case Err(msg):
            print "Tokenize error: {msg}"
            return []

# Get readable name for token kind
fn token_name(kind: TokenKind) -> str:
    match kind:
        case TokenKind.Identifier(name): "Identifier({name})"
        case TokenKind.TypeIdentifier(name): "TypeIdentifier({name})"
        case TokenKind.Integer(n): "Integer({n})"
        case TokenKind.f32(f): "Float({f})"
        case TokenKind.bool(b): if b: "Bool(true)" else: "Bool(false)"

        # Keywords
        case TokenKind.Fn: "Fn"
        case TokenKind.Let: "Let"
        case TokenKind.Val: "Val"
        case TokenKind.Var: "Var"
        case TokenKind.Return: "Return"
        case TokenKind.If: "If"
        case TokenKind.Else: "Else"
        case TokenKind.Static: "Static"
        case TokenKind.Default: "Default"

        # Delimiters
        case TokenKind.LParen: "LParen"
        case TokenKind.RParen: "RParen"
        case TokenKind.LBrace: "LBrace"
        case TokenKind.RBrace: "RBrace"
        case TokenKind.LBracket: "LBracket"
        case TokenKind.RBracket: "RBracket"

        # Operators
        case TokenKind.Dot: "Dot"
        case TokenKind.Arrow: "Arrow"
        case TokenKind.Colon: "Colon"
        case TokenKind.Comma: "Comma"
        case TokenKind.Plus: "Plus"
        case TokenKind.Minus: "Minus"
        case TokenKind.Star: "Star"
        case TokenKind.Slash: "Slash"

        # Special
        case TokenKind.Eof: "Eof"
        case TokenKind.Newline: "Newline"

        case _: "Unknown"

# Extract just identifier names from source
pub fn identifier_names(source: str) -> List<str>:
    val lexer = Lexer.new(source)
    val result = lexer.tokenize()

    match result:
        case Ok(tokens):
            var names = []
            for token in tokens:
                match token.kind:
                    case TokenKind.Identifier(name):
                        names.append(name)
                    case _: pass
            return names
        case Err(_):
            return []

# Check if first token is an identifier with given name
pub fn first_is_identifier(source: str, expected_name: str) -> bool:
    val lexer = Lexer.new(source)
    val result = lexer.tokenize()

    match result:
        case Ok(tokens):
            if tokens.len() > 0:
                match tokens[0].kind:
                    case TokenKind.Identifier(name):
                        return name == expected_name
                    case _:
                        return false
            else:
                return false
        case Err(_):
            return false

# Check if first token is a specific keyword
pub fn first_is_keyword(source: str, keyword_name: str) -> bool:
    val names = tokenize_names(source)
    if names.len() > 0:
        return names[0] == keyword_name
    else:
        return false
