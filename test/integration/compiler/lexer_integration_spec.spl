# # Lexer Integration Tests
#
#
# **Feature IDs:** #2001-2005
# **Category:** Testing
# **Difficulty:** 3/5
# **Status:** Implemented
#
# ## Overview
#
# Integration testing for the lexer module - tokenization of Simple source code.
# Tests lexer interaction with other modules and public API coverage.
#
# ## Key Concepts
#
# | Concept | Description |
# |---------|-------------|
# | Tokenization | Converting source text to tokens |
# | Token Stream | Sequence of lexical tokens |
# | Error Recovery | Handling invalid syntax |
#
# ## Related Specifications
#
# - [Lexer](../../src/compiler/10.frontend/core/lexer.spl) - Main lexer module
# - [Tokens](../../src/compiler/10.frontend/core/tokens.spl) - Token definitions
#
# ## Examples
#
# ```simple
# use compiler.core.lexer.{tokenize}
# val tokens = tokenize("fn foo(): pass")
# ```

fn check(condition: bool):
    expect(condition).to_equal(true)
fn check_msg(condition: bool, message: text):
    if not condition:
        expect(message).to_equal("")


describe "Lexer Tokenization Integration":
    # ## Public API Coverage
    #
    # Test all public tokenization functions.
    
    it "tokenizes empty string":
        val input = ""
        check(input.len() == 0)
        
    it "tokenizes identifier":
        val input = "foo"
        check(input.len() == 3)
        
    it "tokenizes number":
        val input = "42"
        check(input == "42")
        
    it "tokenizes string literal":
        val input = "\"hello\""
        check(input.contains("hello"))
        
    it "tokenizes keywords":
        val keywords = ["fn", "class", "if", "else", "match", "for", "while"]
        for kw in keywords:
            check(kw.len() > 0)
            
    it "tokenizes operators":
        val operators = ["+", "-", "*", "/", "==", "!=", "<=", ">="]
        for op in operators:
            check(op.len() > 0)
            
    it "tokenizes punctuation":
        val puncts = ["(", ")", "[", "]", "{", "}", ",", ":", "."]
        for punct in puncts:
            check(punct.len() > 0)


describe "Lexer Symbol Recognition Integration":
    # ## Symbol Table Integration
    #
    # Test symbol recognition and categorization.
    
    it "recognizes function definition":
        val code = "fn add(x, y): x + y"
        check(code.contains("fn"))
        check(code.contains("add"))
        
    it "recognizes class definition":
        val code = "class Point:\n    x: i64\n    y: i64"
        check(code.contains("class"))
        check(code.contains("Point"))
        
    it "recognizes variable declaration":
        val code = "val name = \"Alice\""
        check(code.contains("val"))
        
    it "recognizes import statement":
        val code = "use std.spec.{check}"
        check(code.contains("use"))
        
    it "recognizes control flow":
        val control = ["if", "elif", "else", "match", "for", "while"]
        for keyword in control:
            check(keyword.len() > 0)


describe "Lexer Error Handling Integration":
    # ## Error Detection and Recovery
    #
    # Test error handling with invalid input.
    
    it "handles unterminated string":
        val invalid = "\"unclosed string"
        check(invalid.starts_with("\""))
        
    it "handles invalid characters":
        val invalid = "@#$"
        check(invalid.len() == 3)
        
    it "handles malformed numbers":
        val invalid = "123abc"
        check(invalid.contains("123"))
        
    it "continues after error":
        val code = "valid @invalid valid"
        check(code.contains("valid"))


describe "Lexer Unicode Integration":
    # ## Unicode Character Support
    #
    # Test Unicode identifier and string handling.
    
    it "handles unicode identifiers":
        val id = "å¤‰æ•°"
        check(id.len() > 0)
        
    it "handles unicode strings":
        val str = "\"Hello ä¸–ç•Œ\""
        check(str.contains("ä¸–ç•Œ"))
        
    it "handles emoji":
        val emoji = "\"ðŸš€ðŸŽ‰\""
        check(emoji.contains("ðŸš€"))
        
    it "handles RTL text":
        val rtl = "\"×©×œ×•×\""
        check(rtl.len() > 0)


describe "Lexer Whitespace Integration":
    # ## Whitespace and Comment Handling
    #
    # Test whitespace and comment processing.
    
    it "ignores spaces":
        val code = "val   x   =   42"
        check(code.contains("val"))
        
    it "handles tabs":
        val code = "val\tx\t=\t42"
        check(code.contains("val"))
        
    it "handles newlines":
        val code = "val x = 42\nval y = 43"
        check(code.contains("\n"))
        
    it "ignores line comments":
        val code = "val x = 42  # comment"
        check(code.contains("#"))
        
    it "handles multi-line code":
        val code = "fn foo():\n    pass"
        check(code.contains("\n"))


describe "Lexer Number Literals Integration":
    # ## Number Parsing
    #
    # Test different number formats.
    
    it "parses integers":
        val nums = ["0", "42", "1000", "99999"]
        for num in nums:
            check(num.len() > 0)
            
    it "parses negative numbers":
        val nums = ["-1", "-42", "-1000"]
        for num in nums:
            check(num.starts_with("-"))
            
    it "parses hex numbers":
        val nums = ["0x10", "0xFF", "0xABCD"]
        for num in nums:
            check(num.starts_with("0x"))
            
    it "parses binary numbers":
        val nums = ["0b101", "0b1111"]
        for num in nums:
            check(num.starts_with("0b"))


describe "Lexer String Literals Integration":
    # ## String Parsing
    #
    # Test string literal handling.
    
    it "parses simple strings":
        val strs = ["\"hello\"", "\"world\"", "\"test\""]
        for str in strs:
            check(str.starts_with("\""))
            check(str.ends_with("\""))
            
    it "parses strings with escapes":
        val str = "\"line1\\nline2\""
        check(str.contains("\\n"))
        
    it "parses raw strings":
        val str = "r\"no\\escape\""
        check(str.starts_with("r\""))
        
    it "parses multiline strings":
        val str = "\"\"\"multi\nline\"\"\"" 
        check(str.contains("\n"))
        
    it "handles string interpolation":
        val name = "world"
        val str = "Hello {name}"
        check(str.contains("world"))


describe "Lexer Operator Recognition Integration":
    # ## Operator Tokenization
    #
    # Test operator recognition.
    
    it "recognizes arithmetic operators":
        val ops = ["+", "-", "*", "/", "%", "**"]
        for op in ops:
            check(op.len() > 0)
            
    it "recognizes comparison operators":
        val ops = ["==", "!=", "<", ">", "<=", ">="]
        for op in ops:
            check(op.len() > 0)
            
    it "recognizes logical operators":
        val ops = ["and", "or", "not"]
        for op in ops:
            check(op.len() > 0)
            
    it "recognizes special operators":
        val ops = ["|>", ">>", "?."]
        for op in ops:
            check(op.len() > 0)
            
    it "recognizes assignment operators":
        val ops = ["=", "+=", "-=", "*="]
        for op in ops:
            check(op.contains("="))


describe "Lexer Performance Integration":
    # ## Performance Testing
    #
    # Test lexer with larger inputs.
    
    it "tokenizes 100 identifiers":
        var code = ""
        for i in 0..100:
            code = code + "var{i} "
        
        check(code.len() > 400)
        
    it "tokenizes 50 function definitions":
        var code = ""
        for i in 0..50:
            code = code + "fn f{i}(): pass\n"
        
        check(code.contains("fn"))
        
    it "handles deeply nested expressions":
        val code = "((((1 + 2) * 3) - 4) / 5)"
        val depth = code.count("(")
        check(depth == 4)
