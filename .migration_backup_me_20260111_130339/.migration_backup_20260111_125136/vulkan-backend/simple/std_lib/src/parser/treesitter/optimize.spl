# Tree-sitter Performance Optimization
# Hot path improvements for production performance

import parser.treesitter.{Tree, Node, Query, QueryCursor, QueryMatch}

# String interning for node kinds
# Reduces memory usage and speeds up string comparisons
class StringInterner:
    strings: Dict<String, Int>
    reverse: Dict<Int, String>
    next_id: Int

    fn new() -> StringInterner:
        StringInterner(
            strings: {},
            reverse: {},
            next_id: 0
        )

    me intern(s: String) -> Int:
        # Check if already interned
        if self.strings.contains_key(s):
            return self.strings[s]

        # Assign new ID
        let id = self.next_id
        self.strings[s] = id
        self.reverse[id] = s
        self.next_id = self.next_id + 1
        id

    fn lookup(id: Int) -> Option<String>:
        if self.reverse.contains_key(id):
            Some(self.reverse[id])
        else:
            None

    fn get_id(s: String) -> Option<Int>:
        if self.strings.contains_key(s):
            Some(self.strings[s])
        else:
            None

    fn size() -> Int:
        self.strings.len()

# Query result cache
# Caches query results to avoid redundant tree traversal
class QueryCache:
    cache: Dict<String, CacheEntry>
    max_size: Int
    access_count: Dict<String, Int>

    fn new(max_size: Int) -> QueryCache:
        QueryCache(
            cache: {},
            max_size: max_size,
            access_count: {}
        )

    me get(key: String) -> Option<List<QueryMatch>>:
        if self.cache.contains_key(key):
            # Update access count
            let count = self.access_count.get(key).unwrap_or(0)
            self.access_count[key] = count + 1

            let entry = self.cache[key]
            Some(entry.matches)
        else:
            None

    me put(key: String, matches: List<QueryMatch>):
        # Evict if at capacity
        if self.cache.len() >= self.max_size and not self.cache.contains_key(key):
            self.evict_lru()

        let entry = CacheEntry(
            key: key,
            matches: matches,
            timestamp: 0  # TODO: Add timestamp
        )

        self.cache[key] = entry
        self.access_count[key] = 1

    me evict_lru():
        # Evict least recently used entry
        let mut min_count = 999999
        let mut evict_key = ""

        for key in self.cache.keys():
            let count = self.access_count.get(key).unwrap_or(0)
            if count < min_count:
                min_count = count
                evict_key = key

        if evict_key != "":
            self.cache.remove(evict_key)
            self.access_count.remove(evict_key)

    me clear():
        self.cache.clear()
        self.access_count.clear()

    fn size() -> Int:
        self.cache.len()

class CacheEntry:
    key: String
    matches: List<QueryMatch>
    timestamp: Int

# Arena allocation optimizer
# Bulk allocation and memory pooling for better performance
class ArenaOptimizer:
    pool_size: Int
    block_size: Int
    allocated_blocks: Int
    total_nodes: Int

    fn new(pool_size: Int, block_size: Int) -> ArenaOptimizer:
        ArenaOptimizer(
            pool_size: pool_size,
            block_size: block_size,
            allocated_blocks: 0,
            total_nodes: 0
        )

    fn estimate_nodes_needed(source_length: Int) -> Int:
        # Heuristic: Estimate ~1 node per 10 characters
        # Plus 20% overhead for safety
        let estimate = (source_length / 10) * 12 / 10
        estimate

    fn recommend_pool_size(source_length: Int) -> Int:
        let nodes = self.estimate_nodes_needed(source_length)

        # Round up to nearest block
        let blocks = (nodes + self.block_size - 1) / self.block_size
        blocks * self.block_size

    me allocate_pool(num_nodes: Int):
        # Calculate blocks needed
        let blocks = (num_nodes + self.block_size - 1) / self.block_size

        # Update statistics
        self.allocated_blocks = self.allocated_blocks + blocks
        self.total_nodes = self.total_nodes + num_nodes

    fn get_statistics() -> Dict<String, Int>:
        {
            "pool_size": self.pool_size,
            "block_size": self.block_size,
            "allocated_blocks": self.allocated_blocks,
            "total_nodes": self.total_nodes,
            "memory_used_mb": (self.total_nodes * 64) / (1024 * 1024)  # Assuming 64 bytes per node
        }

# Query optimizer
# Pre-compiles and caches queries for better performance
class QueryOptimizer:
    compiled_queries: Dict<String, Query>
    query_stats: Dict<String, QueryStats>

    fn new() -> QueryOptimizer:
        QueryOptimizer(
            compiled_queries: {},
            query_stats: {}
        )

    me get_or_compile(language: String, query_str: String) -> Result<Query, String>:
        let key = language + ":" + query_str

        # Check cache
        if self.compiled_queries.contains_key(key):
            # Update stats
            if self.query_stats.contains_key(key):
                let mut stats = self.query_stats[key]
                stats.hit_count = stats.hit_count + 1
                self.query_stats[key] = stats

            return Ok(self.compiled_queries[key])

        # Compile new query
        match Query.new(language, query_str):
            case Ok(query):
                self.compiled_queries[key] = query

                # Initialize stats
                self.query_stats[key] = QueryStats(
                    key: key,
                    hit_count: 0,
                    compile_count: 1
                )

                Ok(query)
            case Err(e):
                Err(e)

    fn get_stats(language: String, query_str: String) -> Option<QueryStats>:
        let key = language + ":" + query_str

        if self.query_stats.contains_key(key):
            Some(self.query_stats[key])
        else:
            None

    me clear_cache():
        self.compiled_queries.clear()
        self.query_stats.clear()

    fn cache_size() -> Int:
        self.compiled_queries.len()

class QueryStats:
    key: String
    hit_count: Int
    compile_count: Int

# Debouncer for LSP didChange events
# Prevents excessive reparsing during rapid typing
class Debouncer:
    delay_ms: Int
    last_trigger_ms: Int
    pending: Bool

    fn new(delay_ms: Int) -> Debouncer:
        Debouncer(
            delay_ms: delay_ms,
            last_trigger_ms: 0,
            pending: false
        )

    me should_trigger(current_time_ms: Int) -> Bool:
        # Check if enough time has passed
        if current_time_ms - self.last_trigger_ms >= self.delay_ms:
            self.last_trigger_ms = current_time_ms
            self.pending = false
            return true

        # Mark as pending
        self.pending = true
        false

    me mark_pending():
        self.pending = true

    fn has_pending() -> Bool:
        self.pending

    me reset():
        self.pending = false

# Performance metrics collector
class PerformanceMetrics:
    parse_times: List<Float>
    incremental_parse_times: List<Float>
    query_times: List<Float>
    memory_usage: List<Int>

    fn new() -> PerformanceMetrics:
        PerformanceMetrics(
            parse_times: [],
            incremental_parse_times: [],
            query_times: [],
            memory_usage: []
        )

    me record_parse(time_ms: Float):
        self.parse_times.push(time_ms)

    me record_incremental_parse(time_ms: Float):
        self.incremental_parse_times.push(time_ms)

    me record_query(time_ms: Float):
        self.query_times.push(time_ms)

    me record_memory(bytes: Int):
        self.memory_usage.push(bytes)

    fn get_parse_stats() -> Stats:
        compute_stats(self.parse_times)

    fn get_incremental_parse_stats() -> Stats:
        compute_stats(self.incremental_parse_times)

    fn get_query_stats() -> Stats:
        compute_stats(self.query_times)

    fn get_memory_stats() -> MemoryStats:
        if self.memory_usage.len() == 0:
            return MemoryStats(
                avg_bytes: 0,
                max_bytes: 0,
                min_bytes: 0,
                avg_mb: 0.0
            )

        let total = self.memory_usage.sum()
        let avg = total / self.memory_usage.len()
        let max_bytes = self.memory_usage.max().unwrap_or(0)
        let min_bytes = self.memory_usage.min().unwrap_or(0)

        MemoryStats(
            avg_bytes: avg,
            max_bytes: max_bytes,
            min_bytes: min_bytes,
            avg_mb: avg / (1024.0 * 1024.0)
        )

    fn print_summary():
        print("\n=== Performance Metrics ===")

        let parse_stats = self.get_parse_stats()
        print(f"\nParse Times:")
        print(f"  Average: {parse_stats.avg:.2f} ms")
        print(f"  Min: {parse_stats.min:.2f} ms")
        print(f"  Max: {parse_stats.max:.2f} ms")

        let inc_stats = self.get_incremental_parse_stats()
        print(f"\nIncremental Parse Times:")
        print(f"  Average: {inc_stats.avg:.2f} ms")
        print(f"  Min: {inc_stats.min:.2f} ms")
        print(f"  Max: {inc_stats.max:.2f} ms")

        let query_stats = self.get_query_stats()
        print(f"\nQuery Times:")
        print(f"  Average: {query_stats.avg:.2f} ms")
        print(f"  Min: {query_stats.min:.2f} ms")
        print(f"  Max: {query_stats.max:.2f} ms")

        let mem_stats = self.get_memory_stats()
        print(f"\nMemory Usage:")
        print(f"  Average: {mem_stats.avg_mb:.2f} MB")
        print(f"  Max: {mem_stats.max_bytes / (1024 * 1024):.2f} MB")

class Stats:
    avg: Float
    min: Float
    max: Float
    count: Int

class MemoryStats:
    avg_bytes: Int
    max_bytes: Int
    min_bytes: Int
    avg_mb: Float

fn compute_stats(values: List<Float>) -> Stats:
    if values.len() == 0:
        return Stats(avg: 0.0, min: 0.0, max: 0.0, count: 0)

    let total = values.sum()
    let avg = total / values.len()
    let min_val = values.min().unwrap_or(0.0)
    let max_val = values.max().unwrap_or(0.0)

    Stats(
        avg: avg,
        min: min_val,
        max: max_val,
        count: values.len()
    )
