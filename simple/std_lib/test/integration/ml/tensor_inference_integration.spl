# Tensor Dimension Inference - Integration Tests
# Tests full workflows with dimension tracking through multi-layer networks

import ml.torch.typed_tensor.{TypedTensor, TensorType, DimSpec}
import ml.torch.dtype.{DType}
import ml.torch.device.{Device}
import verification.models.tensor_dimensions.{
    Dim, TensorShape, DimInferenceContext, ShapeError,
    unify_dims, unify_shapes, infer_matmul_shape
}

# ============================================================================
# Multi-Layer Perceptron Integration
# ============================================================================

describe "Multi-Layer Perceptron":

    describe "Dimension Propagation":
        it "propagates dimensions through 3-layer MLP":
            # Input: [batch, 784] - MNIST flattened images
            let input = TypedTensor.randn([
                DimSpec.ranged("batch", 32, 1, 64),
                DimSpec.exact(784)
            ])

            # Layer 1: [784, 256]
            let w1 = TypedTensor.randn([
                DimSpec.exact(784),
                DimSpec.exact(256)
            ])

            # Layer 2: [256, 128]
            let w2 = TypedTensor.randn([
                DimSpec.exact(256),
                DimSpec.exact(128)
            ])

            # Layer 3: [128, 10]
            let w3 = TypedTensor.randn([
                DimSpec.exact(128),
                DimSpec.exact(10)
            ])

            # Forward pass
            let h1 = input.matmul(w1)?
            assert h1.ndim() == 2
            assert h1.verify().is_ok()

            let h2 = h1.matmul(w2)?
            assert h2.ndim() == 2
            assert h2.verify().is_ok()

            let output = h2.matmul(w3)?
            assert output.ndim() == 2
            assert output.verify().is_ok()

            # Output should be [batch, 10]
            let actual_shape = output.actual_shape()
            assert actual_shape[1] == 10

    describe "Batch Size Variation":
        it "handles different batch sizes within range":
            # Model with ranged batch dimension
            let w = TypedTensor.randn([DimSpec.exact(784), DimSpec.exact(10)])

            # Try batch sizes within range [1, 64]
            for batch_size in [1, 16, 32, 64]:
                let input = TypedTensor.randn([
                    DimSpec.ranged("batch", batch_size, 1, 64),
                    DimSpec.exact(784)
                ])

                let output = input.matmul(w)?
                assert output.verify().is_ok()
                assert output.actual_shape()[0] == batch_size
                assert output.actual_shape()[1] == 10

    describe "Error Handling":
        it "catches dimension mismatch at compile time":
            let input = TypedTensor.randn([
                DimSpec.exact(32),
                DimSpec.exact(784)
            ])

            # Wrong weight matrix (incompatible K dimension)
            let w_wrong = TypedTensor.randn([
                DimSpec.exact(512),  # Should be 784!
                DimSpec.exact(10)
            ])

            # This should fail
            let result = input.matmul(w_wrong)
            assert result.is_err()

            match result:
                case Err(ShapeError.MatmulShapeMismatch(left, right)):
                    # Expected error
                    assert true
                case _:
                    assert false  # Unexpected error type

# ============================================================================
# CNN Integration (NCHW Format)
# ============================================================================

describe "Convolutional Neural Network":

    describe "NCHW Format":
        it "tracks 4D tensor dimensions through CNN":
            # Input: [batch, channels, height, width]
            let input = TypedTensor.randn([
                DimSpec.ranged("batch", 32, 1, 128),
                DimSpec.exact(3),    # RGB channels
                DimSpec.exact(224),  # Height
                DimSpec.exact(224)   # Width
            ])

            assert input.ndim() == 4
            assert input.verify().is_ok()

            # Typical CNN operations would follow (convolution, pooling)
            # For now, verify shape structure
            let shape = input.actual_shape()
            assert shape[1] == 3    # 3 channels
            assert shape[2] == 224  # Height
            assert shape[3] == 224  # Width

    describe "Reshape for FC Layer":
        it "reshapes CNN output to fully connected input":
            # After conv/pool: [batch, 512, 7, 7]
            let conv_output = TypedTensor.randn([
                DimSpec.ranged("batch", 32, 1, 64),
                DimSpec.exact(512),
                DimSpec.exact(7),
                DimSpec.exact(7)
            ])

            # Reshape to [batch, 512*7*7] = [batch, 25088]
            let flat = conv_output.reshape([
                DimSpec.ranged("batch", 32, 1, 64),
                DimSpec.exact(25088)
            ])?

            assert flat.ndim() == 2
            assert flat.verify().is_ok()

            # Can now apply FC layer
            let w_fc = TypedTensor.randn([
                DimSpec.exact(25088),
                DimSpec.exact(1000)
            ])

            let output = flat.matmul(w_fc)?
            assert output.ndim() == 2
            assert output.actual_shape()[1] == 1000

# ============================================================================
# Transformer Integration
# ============================================================================

describe "Transformer":

    describe "Attention Mechanism":
        it "computes attention scores with correct dimensions":
            # Q, K, V: [batch, heads, seq_len, head_dim]
            let batch = DimSpec.ranged("batch", 32, 1, 64)
            let heads = DimSpec.exact(12)
            let seq_len = DimSpec.ranged("seq", 128, 1, 512)
            let head_dim = DimSpec.exact(64)

            let q = TypedTensor.randn([batch, heads, seq_len, head_dim])
            let k = TypedTensor.randn([batch, heads, seq_len, head_dim])
            let v = TypedTensor.randn([batch, heads, seq_len, head_dim])

            # All should have 4 dimensions
            assert q.ndim() == 4
            assert k.ndim() == 4
            assert v.ndim() == 4

            # Transpose K for attention: [batch, heads, seq, head_dim] -> [batch, heads, head_dim, seq]
            let k_t = k.transpose(2, 3)?
            assert k_t.ndim() == 4

            # Attention scores: Q @ K^T
            # [batch, heads, seq, head_dim] @ [batch, heads, head_dim, seq] -> [batch, heads, seq, seq]
            let scores = q.matmul(k_t)?
            assert scores.ndim() == 4
            assert scores.verify().is_ok()

# ============================================================================
# Memory Estimation Integration
# ============================================================================

describe "Memory Estimation":

    describe "Model Memory Planning":
        it "estimates memory for full model":
            # MLP: 784 -> 256 -> 128 -> 10

            # Parameter memory
            let w1_type = TensorType.new([DimSpec.exact(784), DimSpec.exact(256)], DType.Float32)
            let w2_type = TensorType.new([DimSpec.exact(256), DimSpec.exact(128)], DType.Float32)
            let w3_type = TensorType.new([DimSpec.exact(128), DimSpec.exact(10)], DType.Float32)

            let w1_mem = w1_type.min_memory_bytes()  # 784 * 256 * 4 = 802,816 bytes
            let w2_mem = w2_type.min_memory_bytes()  # 256 * 128 * 4 = 131,072 bytes
            let w3_mem = w3_type.min_memory_bytes()  # 128 * 10 * 4 = 5,120 bytes

            let param_mem = w1_mem + w2_mem + w3_mem
            assert param_mem == 802816 + 131072 + 5120  # 939,008 bytes

            # Activation memory (varies with batch)
            let act_type = TensorType.new([
                DimSpec.ranged("batch", 32, 1, 64),
                DimSpec.exact(256)  # Largest activation
            ], DType.Float32)

            let min_act_mem = act_type.min_memory_bytes()  # 1 * 256 * 4 = 1,024
            let max_act_mem = act_type.max_memory_bytes()  # 64 * 256 * 4 = 65,536

            assert min_act_mem == 1024
            assert max_act_mem == 65536

            # Total memory range
            let total_min = param_mem + min_act_mem
            let total_max = param_mem + max_act_mem

            # Should be around 939 KB to 1004 KB
            assert total_min < 1_000_000
            assert total_max < 1_100_000

    describe "Training Memory":
        it "estimates training memory with gradients and optimizer":
            # Parameters
            let param_type = TensorType.new([DimSpec.exact(1000), DimSpec.exact(100)], DType.Float32)
            let param_mem = param_type.min_memory_bytes()  # 1000 * 100 * 4 = 400,000

            # Gradients (same size as parameters)
            let grad_mem = param_mem

            # Optimizer state (Adam: 2x parameters for momentum and variance)
            let opt_mem = 2 * param_mem

            # Activations (batch-dependent)
            let act_type = TensorType.new([
                DimSpec.ranged("batch", 32, 1, 128),
                DimSpec.exact(100)
            ], DType.Float32)
            let min_act = act_type.min_memory_bytes()  # 1 * 100 * 4 = 400
            let max_act = act_type.max_memory_bytes()  # 128 * 100 * 4 = 51,200

            # Total training memory
            let training_min = param_mem + grad_mem + opt_mem + min_act
            let training_max = param_mem + grad_mem + opt_mem + max_act

            # param + grad + opt = 400k + 400k + 800k = 1.6MB
            # + activations: 1.6MB to 1.65MB
            assert training_min >= 1_600_000
            assert training_max <= 1_700_000

# ============================================================================
# Dynamic Batch Size
# ============================================================================

describe "Dynamic Batch Size":

    describe "Inference with Variable Batches":
        it "handles single example inference":
            # Model
            let w = TypedTensor.randn([DimSpec.exact(784), DimSpec.exact(10)])

            # Single example: batch=1
            let single = TypedTensor.randn([
                DimSpec.exact(1),
                DimSpec.exact(784)
            ])

            let output = single.matmul(w)?
            assert output.actual_shape() == [1, 10]

        it "handles full batch inference":
            # Model
            let w = TypedTensor.randn([DimSpec.exact(784), DimSpec.exact(10)])

            # Full batch: batch=64
            let batch = TypedTensor.randn([
                DimSpec.exact(64),
                DimSpec.exact(784)
            ])

            let output = batch.matmul(w)?
            assert output.actual_shape() == [64, 10]

        it "verifies batch dimension in range":
            # Input with range constraint
            let input = TypedTensor.randn([
                DimSpec.ranged("batch", 32, 1, 64),
                DimSpec.exact(784)
            ])

            # Should pass verification
            assert input.verify().is_ok()

            # Actual batch size should be within range
            let actual_batch = input.actual_shape()[0]
            assert actual_batch >= 1
            assert actual_batch <= 64

# ============================================================================
# Error Recovery
# ============================================================================

describe "Error Handling and Recovery":

    describe "Graceful Degradation":
        it "provides clear error messages on shape mismatch":
            let a = TypedTensor.randn([DimSpec.exact(4), DimSpec.exact(8)])
            let b = TypedTensor.randn([DimSpec.exact(10), DimSpec.exact(16)])

            match a.matmul(b):
                case Err(ShapeError.MatmulShapeMismatch(left, right)):
                    # Error should contain both shapes
                    assert true
                case _:
                    assert false  # Should be matmul error

        it "validates reshape operations":
            let t = TypedTensor.randn([DimSpec.exact(4), DimSpec.exact(6)])

            # Valid reshape: 24 elements -> 24 elements
            let valid = t.reshape([DimSpec.exact(2), DimSpec.exact(12)])
            assert valid.is_ok()

            # Invalid reshape: 24 elements -> 30 elements
            let invalid = t.reshape([DimSpec.exact(3), DimSpec.exact(10)])
            assert invalid.is_err()

            match invalid:
                case Err(ShapeError.ReshapeElementsMismatch(input, output)):
                    assert true
                case _:
                    assert false

    describe "Dimension Out of Range":
        it "detects out-of-range dimensions":
            # Create tensor with range [1, 64]
            let t = TypedTensor.randn([
                DimSpec.ranged("batch", 32, 1, 64),
                DimSpec.exact(256)
            ])

            # Verify current shape is in range
            let result = t.verify()
            assert result.is_ok()

# ============================================================================
# Composition and Reuse
# ============================================================================

describe "Model Composition":

    describe "Layer Functions":
        it "composes reusable layers":
            # Define reusable linear layer function
            fn linear(input: TypedTensor, weight: TypedTensor) -> Result<TypedTensor, ShapeError>:
                return input.matmul(weight)

            # Input
            let x = TypedTensor.randn([DimSpec.exact(32), DimSpec.exact(784)])

            # Layer 1
            let w1 = TypedTensor.randn([DimSpec.exact(784), DimSpec.exact(256)])
            let h1 = linear(x, w1)?

            # Layer 2 (uses output of layer 1)
            let w2 = TypedTensor.randn([DimSpec.exact(256), DimSpec.exact(10)])
            let out = linear(h1, w2)?

            assert out.actual_shape() == [32, 10]

    describe "Model Classes":
        it "builds models with dimension tracking":
            struct SimpleModel:
                w1: TypedTensor
                w2: TypedTensor

            fn SimpleModel.forward(self, input: TypedTensor) -> Result<TypedTensor, ShapeError>:
                let h = input.matmul(self.w1)?
                let out = h.matmul(self.w2)?
                return Ok(out)

            # Create model
            let model = SimpleModel {
                w1: TypedTensor.randn([DimSpec.exact(784), DimSpec.exact(128)]),
                w2: TypedTensor.randn([DimSpec.exact(128), DimSpec.exact(10)])
            }

            # Forward pass
            let input = TypedTensor.randn([DimSpec.exact(32), DimSpec.exact(784)])
            let output = model.forward(input)?

            assert output.ndim() == 2
            assert output.actual_shape() == [32, 10]

# ============================================================================
# Summary Output
# ============================================================================

print("")
print("============================================================")
print("  TENSOR DIMENSION INFERENCE - INTEGRATION TESTS")
print("============================================================")
print("")
print("  ✅ Multi-Layer Perceptron dimension propagation")
print("  ✅ CNN 4D tensor handling (NCHW)")
print("  ✅ Transformer attention mechanism")
print("  ✅ Memory estimation for training")
print("  ✅ Dynamic batch size support")
print("  ✅ Error handling and recovery")
print("  ✅ Model composition and reuse")
print("")
print("  All integration tests demonstrate typed tensor dimension")
print("  inference working correctly across realistic ML workflows.")
print("")
