# Training Engine Feature Specification
"""
# Event-Driven Training Engine

**Feature ID:** TBD
**Category:** ML Infrastructure
**Difficulty:** 4/5
**Status:** In Progress

## Overview

Simple's Training Engine provides a flexible, event-driven framework for orchestrating
machine learning training loops. Inspired by PyTorch Ignite, the engine abstracts away
boilerplate training code while providing powerful hooks for metrics, checkpointing, early
stopping, and custom callbacks.

## Key Features

**Engine Core:**
- Execute user-defined process_function per batch
- Track training state (epoch, iteration, metrics, outputs)
- Support both finite (max_epochs) and infinite training loops

**Event System:**
- Built-in lifecycle events (STARTED, EPOCH_COMPLETED, ITERATION_COMPLETED, etc.)
- Attach multiple handlers to events with guaranteed execution order
- Periodic events (every=N)

**Metrics:**
- Automatic metric computation during training/evaluation
- Built-in classification metrics (Accuracy)
- Built-in regression metrics (MSE, MAE, RMSE)
- Custom metrics via Metric base class (reset/update/compute pattern)

## Implementation

**Primary Files:**
- `simple/std_lib/src/ml/engine/__init__.spl` - Engine, State, Metric classes
"""
import std.spec

# Import the ML engine module - use braced import for specific classes
import ml.engine.{State, Metric, Accuracy, Loss, MSE, MAE, RMSE, abs, sqrt, min}
import ml.torch.training.{EarlyStopping, ParameterStats, ParameterTracker}


# ============================================================================
# Engine Core Tests
# ============================================================================

describe "Engine core":
    """
    ## Engine Core Functionality

    The Engine class is the heart of the training system. It executes a user-defined
    process_function for each batch in the dataset, manages training state, and fires
    events at key lifecycle points.
    """

    it "creates State with default values":
        """
        **Given** a State class from ml.engine
        **When** creating an instance
        **Then** all state fields are initialized correctly
        """
        val state = State()
        expect state.epoch == 0
        expect state.iteration == 0
        expect state.epoch_iteration == 0
        expect state.max_epochs == 0

    it "tracks metric base class defaults":
        """
        **Given** a Metric base class
        **When** calling compute() without updates
        **Then** returns default value of 0.0
        """
        val metric = Metric()
        expect metric.compute() == 0.0


# ============================================================================
# Helper Function Tests
# ============================================================================

describe "Helper functions":
    """
    ## Math Helper Functions

    The ml.engine module provides helper functions for metric calculations.
    """

    it "computes absolute value":
        """
        **Given** the abs() function
        **When** computing absolute values
        **Then** returns correct results for positive and negative inputs
        """
        # Test abs using comparison operators instead of equality
        expect abs(5.0) > 4.9
        expect abs(-5.0) > 4.9
        expect abs(0.0) < 0.1
        expect abs(-10.0) > 9.9

    it "computes square root":
        """
        **Given** the sqrt() function
        **When** computing square roots
        **Then** returns correct results
        """
        expect sqrt(4.0) == 2.0
        expect sqrt(9.0) == 3.0
        expect sqrt(0.0) == 0.0
        expect sqrt(1.0) == 1.0

    it "computes minimum of two values":
        """
        **Given** the min() function
        **When** comparing two integers
        **Then** returns the smaller value
        """
        expect min(3, 5) == 3
        expect min(10, 2) == 2
        expect min(7, 7) == 7
        expect min(-5, 5) == -5


# ============================================================================
# Metric Tests
# ============================================================================

describe "Metrics":
    """
    ## Metrics

    The metric system follows a reset/update/compute pattern for tracking
    training and evaluation metrics.
    """

    it "computes accuracy metric":
        """
        **Given** an Accuracy metric instance
        **When** updating with predictions and labels
        **Then** computes correct accuracy value
        """
        val accuracy = Accuracy()
        accuracy.reset()

        # Update with batch 1: 3 correct out of 4
        accuracy.update({"pred": [0, 1, 2, 3], "labels": [0, 1, 2, 0]})

        val result = accuracy.compute()
        # 3 out of 4 correct = 0.75
        expect result == 0.75

    it "computes loss metric":
        """
        **Given** a Loss metric instance
        **When** updating with loss values
        **Then** computes average loss
        """
        val loss_metric = Loss()
        loss_metric.reset()

        # Update with batches - use values that don't have floating point issues
        loss_metric.update({"loss": 1.0})
        loss_metric.update({"loss": 2.0})
        loss_metric.update({"loss": 3.0})

        val avg_loss = loss_metric.compute()
        # Average of 1.0, 2.0, 3.0 = 2.0
        expect avg_loss == 2.0

    it "computes MSE metric":
        """
        **Given** an MSE metric instance
        **When** updating with predictions and actual values
        **Then** computes mean squared error
        """
        val mse = MSE()
        mse.reset()

        # pred=[1.0, 2.0], actual=[1.0, 3.0]
        # errors: 0, 1 -> squared: 0, 1 -> mean: 0.5
        mse.update({"pred": [1.0, 2.0], "actual": [1.0, 3.0]})

        val result = mse.compute()
        expect result == 0.5

    it "computes MAE metric":
        """
        **Given** an MAE metric instance
        **When** updating with predictions and actual values
        **Then** computes mean absolute error
        """
        val mae = MAE()
        mae.reset()

        # pred=[1.0, 2.0, 4.0], actual=[1.0, 3.0, 6.0]
        # errors: 0, 1, 2 -> mean: 1.0
        mae.update({"pred": [1.0, 2.0, 4.0], "actual": [1.0, 3.0, 6.0]})

        val result = mae.compute()
        expect result == 1.0

    it "resets metrics between epochs":
        """
        **Given** a metric with accumulated values
        **When** calling reset()
        **Then** all accumulated state is cleared
        """
        val accuracy = Accuracy()

        # First epoch
        accuracy.update({"pred": [1, 1], "labels": [1, 0]})
        val first_result = accuracy.compute()
        expect first_result == 0.5

        # Reset for new epoch
        accuracy.reset()

        # Second epoch with different data
        accuracy.update({"pred": [1, 1, 1], "labels": [1, 1, 1]})
        val second_result = accuracy.compute()
        expect second_result == 1.0

    it "computes RMSE metric":
        """
        **Given** an RMSE metric instance
        **When** updating with predictions and actual values
        **Then** computes root mean squared error (sqrt of MSE)
        """
        val rmse = RMSE()
        rmse.reset()

        # pred=[3.0], actual=[1.0]
        # error: 2 -> squared: 4 -> mean: 4 -> sqrt: 2
        rmse.update({"pred": [3.0], "actual": [1.0]})

        val result = rmse.compute()
        expect result == 2.0

    it "handles perfect predictions with zero error":
        """
        **Given** predictions that match actual values exactly
        **When** computing MSE, MAE, and RMSE
        **Then** all return 0.0
        """
        val mse = MSE()
        val mae = MAE()
        val rmse = RMSE()

        mse.reset()
        mae.reset()
        rmse.reset()

        mse.update({"pred": [1.0, 2.0, 3.0], "actual": [1.0, 2.0, 3.0]})
        mae.update({"pred": [1.0, 2.0, 3.0], "actual": [1.0, 2.0, 3.0]})
        rmse.update({"pred": [1.0, 2.0, 3.0], "actual": [1.0, 2.0, 3.0]})

        expect mse.compute() == 0.0
        expect mae.compute() == 0.0
        expect rmse.compute() == 0.0

    it "accumulates accuracy across multiple batches":
        """
        **Given** an Accuracy metric
        **When** updating with multiple batches
        **Then** computes accuracy across all samples
        """
        val acc = Accuracy()
        acc.reset()

        # Batch 1: 2 out of 2 correct
        acc.update({"pred": [1, 0], "labels": [1, 0]})

        # Batch 2: 1 out of 2 correct
        acc.update({"pred": [1, 1], "labels": [1, 0]})

        # Total: 3 out of 4 correct = 0.75
        expect acc.compute() == 0.75

    it "accumulates MSE across multiple batches":
        """
        **Given** an MSE metric
        **When** updating with multiple batches
        **Then** computes MSE across all samples
        """
        val mse = MSE()
        mse.reset()

        # Batch 1: errors [0, 0] -> squared [0, 0]
        mse.update({"pred": [1.0, 2.0], "actual": [1.0, 2.0]})

        # Batch 2: errors [1, 1] -> squared [1, 1]
        mse.update({"pred": [2.0, 3.0], "actual": [1.0, 2.0]})

        # Total: 4 samples, sum of squared errors = 2
        # MSE = 2 / 4 = 0.5
        expect mse.compute() == 0.5

    it "supports y_pred/y_true format for accuracy":
        """
        **Given** an Accuracy metric
        **When** using y_pred/y_true keys instead of pred/labels
        **Then** computes correctly
        """
        val acc = Accuracy()
        acc.reset()
        acc.update({"y_pred": [0, 1, 2], "y_true": [0, 1, 2]})
        expect acc.compute() == 1.0

    it "supports y_pred/y_true format for regression metrics":
        """
        **Given** MSE, MAE, RMSE metrics
        **When** using y_pred/y_true keys instead of pred/actual
        **Then** computes correctly
        """
        val mse = MSE()
        mse.reset()
        mse.update({"y_pred": [2.0], "y_true": [1.0]})
        expect mse.compute() == 1.0

        val mae = MAE()
        mae.reset()
        mae.update({"y_pred": [2.0], "y_true": [1.0]})
        expect mae.compute() == 1.0


# ============================================================================
# Event System Tests
# ============================================================================

describe "Event system":
    """
    ## Event System

    The event system enables attaching handlers to lifecycle events during
    training. Handlers are executed when events are fired.
    """

    it "provides event name constants":
        """
        **Given** the event system module
        **When** accessing event constants
        **Then** they are defined as string identifiers

        Note: Event constants are strings like "started", "epoch_completed", etc.
        These are used to register and fire handlers.
        """
        # The event system uses string constants for event names
        # Event examples: "started", "epoch_started", "epoch_completed", etc.
        expect true  # Event system is available via ml.engine module

    it "supports handler registration pattern":
        """
        **Given** an Engine class with event system
        **When** handlers are registered for events
        **Then** handlers are called when events fire

        **Pattern:**
        ```simple
        val engine = ml.engine.Engine(process_fn)
        engine.attach_handler("epoch_completed", handler, priority=0)
        engine.fire_event("epoch_completed")
        ```
        """
        # Event handler registration is available via Engine.attach_handler()
        expect true  # Pattern documented


# ============================================================================
# Engine Integration Tests
# ============================================================================

describe "Engine integration":
    """
    ## Engine Integration

    Tests that verify the Engine class provides metric and handler integration.
    """

    it "engine class provides add_metric method":
        """
        **Given** an Engine class
        **When** checking available methods
        **Then** add_metric() is available for attaching metrics

        **Usage Pattern:**
        ```simple
        val engine = ml.engine.Engine(process_fn)
        val acc = ml.engine.Accuracy()
        engine.add_metric(acc, "accuracy")
        ```
        """
        # Engine provides add_metric() for attaching metrics
        expect true  # API documented

    it "engine class provides terminate method":
        """
        **Given** an Engine class
        **When** training needs to stop early
        **Then** terminate() method sets termination flag

        **Usage Pattern:**
        ```simple
        val engine = ml.engine.Engine(process_fn)
        engine.terminate()  # Stops training loop
        ```
        """
        # Engine provides terminate() for early stopping
        expect true  # API documented


# ============================================================================
# Handlers (Planned)
# ============================================================================

describe "Handlers":
    """
    ## Handlers

    Handlers are reusable components that encapsulate common training infrastructure.

    **Status:** Planned - handlers not yet fully implemented
    """

    it "checkpoint handler (planned)":
        """
        **Given** a Checkpoint handler with N-best retention
        **When** training completes multiple epochs
        **Then** only the N best checkpoints are retained

        **Usage:**
        ```simple
        var checkpoint = Checkpoint(n_best=2, metric_name="val_loss")
        checkpoint.save(epoch=1, metric=0.5, model_state={})
        checkpoint.save(epoch=2, metric=0.3, model_state={})
        checkpoint.save(epoch=3, metric=0.4, model_state={})
        # Only keeps epochs 2 and 3 (the 2 best)
        ```
        """
        # Checkpoint handler implemented in ml.torch.training
        expect true  # Handler available, full test requires file I/O

    it "early stopping handler (planned)":
        """
        **Given** an EarlyStopping handler with patience parameter
        **When** validation metric doesn't improve
        **Then** training stops early

        **Usage:**
        ```simple
        var early_stop = EarlyStopping(patience=3)
        early_stop.check(0.5)  # Sets best_loss=0.5
        early_stop.check(0.6)  # counter=1
        early_stop.check(0.6)  # counter=2
        early_stop.check(0.6)  # counter=3, should_stop=true
        ```
        """
        # EarlyStopping handler fully implemented and tested below
        expect true  # See EarlyStopping describe block for full tests


# ============================================================================
# Engine Composition (Planned)
# ============================================================================

describe "Engine composition":
    """
    ## Engine Composition

    Multiple engines can work together in a single workflow.

    **Status:** Planned - composition patterns documented but not fully tested
    """

    it "supports multiple independent engines":
        """
        **Given** multiple Engine instances
        **When** created with different process functions
        **Then** each maintains independent state

        **Pattern:**
        ```simple
        fn train_step(engine, batch) -> any:
            return {"loss": 0.1}

        fn eval_step(engine, batch) -> any:
            return {"loss": 0.05}

        val trainer = ml.engine.Engine(train_step)
        val evaluator = ml.engine.Engine(eval_step)
        # Each engine has its own State
        ```
        """
        # Engine composition pattern is supported
        expect true  # Pattern documented

    it "supports trainer-evaluator workflow":
        """
        **Given** a trainer engine and evaluator engine
        **When** evaluator runs after each training epoch
        **Then** validation metrics are computed separately

        **Common Pattern:**
        - Trainer engine runs training loop
        - Handler attached to EPOCH_COMPLETED runs evaluator
        - Each engine maintains separate metrics

        **Integration Example:**
        ```simple
        import ml.engine.{Engine, Events, State}
        import ml.tracking.{run, TrackMode}

        # Define training step
        fn train_step(state: State) -> any:
            val batch = state.batch
            val loss = model.forward(batch).loss()
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            return {"loss": loss.item()}

        # Define evaluation step
        fn eval_step(state: State) -> any:
            val batch = state.batch
            val output = model.forward(batch)
            return {"val_loss": output.loss().item()}

        # Create engines
        val trainer = Engine(train_step)
        val evaluator = Engine(eval_step)

        # Run evaluator after each training epoch
        trainer.on(Events::EPOCH_COMPLETED, |engine|:
            evaluator.run(val_dataloader)
            val metrics = evaluator.state.metrics
            print("Validation loss: {metrics.get('val_loss')}")
        )

        # Optional: integrate with experiment tracking
        with run(project='my_project', mode=TrackMode::Offline) as exp:
            trainer.on(Events::ITERATION_COMPLETED, |engine|:
                exp.log(engine.state.output, step=engine.state.iteration)
            )
            trainer.run(train_dataloader, max_epochs=10)
        ```
        """
        # Pattern documented with integration example
        expect true


# ============================================================================
# EarlyStopping Tests
# ============================================================================

describe "EarlyStopping":
    """
    ## EarlyStopping

    Early stopping prevents overfitting by monitoring validation loss
    and stopping training when loss stops improving.

    **Status:** Placeholder tests - `me` method issues with test framework
    """

    it "initializes with custom patience":
        """
        **Given** an EarlyStopping instance
        **When** created with custom patience
        **Then** patience reflects the provided value

        **Usage:**
        ```simple
        var early_stop = EarlyStopping(patience=10)
        expect early_stop.patience == 10
        ```
        """
        var early_stop = EarlyStopping(10)
        expect early_stop.patience == 10
        expect early_stop.counter == 0
        expect early_stop.should_stop == false

    it "tracks loss improvement and stops after patience exceeded":
        """
        **Given** an EarlyStopping instance with patience=3
        **When** loss fails to improve for 3 consecutive checks
        **Then** check returns false and should_stop is true

        **Expected Behavior:**
        ```simple
        var early_stop = EarlyStopping(patience=3)
        early_stop.check(1.0)  # Sets best_loss
        early_stop.check(1.5)  # counter=1, returns true
        early_stop.check(1.5)  # counter=2, returns true
        early_stop.check(1.5)  # counter=3, returns false, should_stop=true
        ```
        """
        var early_stop = EarlyStopping(3)
        early_stop.check(1.0)  # Sets best_loss
        early_stop.check(1.5)  # counter=1
        early_stop.check(1.5)  # counter=2
        early_stop.check(1.5)  # counter=3, patience exceeded
        expect early_stop.should_stop == true
        expect early_stop.counter == 3

    it "resets counter when loss improves":
        """
        **Given** an EarlyStopping instance with incremented counter
        **When** loss improves below best_loss
        **Then** counter resets to 0 and best_loss updates

        **Expected Behavior:**
        ```simple
        var early_stop = EarlyStopping(patience=5)
        early_stop.check(1.0)  # best_loss=1.0
        early_stop.check(1.5)  # counter=1
        early_stop.check(0.8)  # best_loss=0.8, counter=0
        ```
        """
        var early_stop = EarlyStopping(5)
        early_stop.check(1.0)  # Sets best_loss to 1.0
        early_stop.check(1.5)  # Worse, counter=1
        expect early_stop.counter == 1
        early_stop.check(0.8)  # Better! Resets counter
        expect early_stop.counter == 0
        expect early_stop.best_loss == 0.8


# ============================================================================
# ParameterStats Tests
# ============================================================================

describe "ParameterStats":
    """
    ## ParameterStats

    Simple data class for holding parameter statistics.
    """

    it "creates stats with all fields":
        """
        **Given** a ParameterStats instance
        **When** created with name, mean, std, norm
        **Then** all fields are accessible
        """
        val stats = ParameterStats(name="weight", mean=0.01, std=0.1, norm=1.5)
        expect stats.name == "weight"
        expect stats.mean == 0.01
        expect stats.std == 0.1
        expect stats.norm == 1.5

    it "supports different parameter names":
        """
        **Given** ParameterStats instances
        **When** created with different names
        **Then** names are stored correctly
        """
        val w_stats = ParameterStats(name="layer1.weight", mean=0.0, std=0.1, norm=2.0)
        val b_stats = ParameterStats(name="layer1.bias", mean=0.5, std=0.01, norm=0.5)

        expect w_stats.name == "layer1.weight"
        expect b_stats.name == "layer1.bias"


# ============================================================================
# ParameterTracker Tests
# ============================================================================

describe "ParameterTracker":
    """
    ## ParameterTracker

    Tracks parameter and gradient statistics during training.

    **Status:** Fully implemented with array field access
    """

    it "tracks parameter statistics":
        """
        **Given** a ParameterTracker instance
        **When** adding ParameterStats
        **Then** stats are accumulated in insertion order

        **Usage:**
        ```simple
        var tracker = ParameterTracker()
        tracker.add_stat(ParameterStats(name="w1", mean=0.0, std=0.1, norm=1.0))
        tracker.add_stat(ParameterStats(name="w2", mean=0.01, std=0.2, norm=2.0))
        # tracker.stats.len() == 2
        # tracker.stats[0].name == "w1"
        ```
        """
        var tracker = ParameterTracker()
        expect tracker.stats.len() == 0

        tracker.add_stat(ParameterStats(name="w1", mean=0.0, std=0.1, norm=1.0))
        tracker.add_stat(ParameterStats(name="w2", mean=0.01, std=0.2, norm=2.0))

        expect tracker.stats.len() == 2
        val first = tracker.stats[0]
        val second = tracker.stats[1]
        expect first.name == "w1"
        expect second.name == "w2"

    it "monitors gradient norms for exploding/vanishing gradients":
        """
        **Given** a ParameterTracker tracking gradient statistics
        **When** adding gradient stats
        **Then** norms can be checked for healthy training

        **Usage:**
        ```simple
        var tracker = ParameterTracker()
        tracker.add_stat(ParameterStats(name="grad_layer1", mean=0.0, std=0.1, norm=0.8))
        # Healthy norm: 0.1 < norm < 10.0
        # Vanishing gradient: norm < 1e-7
        # Exploding gradient: norm > 1e3
        ```
        """
        var tracker = ParameterTracker()
        tracker.add_stat(ParameterStats(name="grad_layer1", mean=0.0, std=0.1, norm=0.8))

        val stat = tracker.stats[0]
        expect stat.name == "grad_layer1"
        expect stat.norm > 0.1
        expect stat.norm < 10.0