# Lexer Feature Specification
# Feature #1: Tokenizes Simple language source code
# Category: Infrastructure | Difficulty: 3 | Status: Complete

"""
# Lexer

**Feature ID:** #1
**Category:** Infrastructure
**Difficulty:** 3/5
**Status:** Complete

## Overview

Tokenizes Simple language source code into a stream of tokens. Handles indentation-based syntax with INDENT/DEDENT tokens, string literals, numbers, identifiers, operators, and keywords.

## Syntax

The lexer tokenizes:
```simple
val x = 42
if x > 0:
    print(x)
```

Into tokens: `LET`, `IDENT(x)`, `EQ`, `INT(42)`, `NEWLINE`, `IF`, `IDENT(x)`, `GT`, `INT(0)`, `COLON`, `NEWLINE`, `INDENT`, `IDENT(print)`, `LPAREN`, `IDENT(x)`, `RPAREN`, `DEDENT`

**Token Categories:**
- Literals: integers, floats, strings, f-strings
- Identifiers: variable and function names
- Operators: arithmetic (+, -, *, /), comparison (>, <, ==), logical (and, or, not)
- Keywords: val, var, fn, if, match, etc.
- Indentation: INDENT/DEDENT for block structure

## Implementation

**Files:**
- `src/parser/src/lexer/mod.rs`
- `src/parser/src/token.rs`

**Test Files:**
- `src/parser/tests/lexer_tests.rs`

**Dependencies:** (none)
**Required By:** #2 (Parser)

## Notes

First stage of compilation pipeline. Uses INDENT/DEDENT for Python-like significant whitespace.
"""

# =====================================================
# Feature Metadata
# =====================================================

class FeatureMetadata:
    id: i32
    name: text
    category: text
    difficulty: i32
    status: text
    impl_type: text
    spec_ref: text
    files: List<text>
    tests: List<text>
    description: text
    code_examples: List<text>
    dependencies: List<i32>
    required_by: List<i32>
    notes: text

# Feature Definition
val FEATURE = FeatureMetadata {
    id: 1,
    name: "Lexer",
    category: "Infrastructure",
    difficulty: 3,
    status: "Complete",
    impl_type: "Rust",
    spec_ref: "doc/spec/lexer_parser.md",
    files: [
        "src/parser/src/lexer/mod.rs",
        "src/parser/src/token.rs"
    ],
    tests: [
        "src/parser/tests/lexer_tests.rs"
    ],
    description: "Tokenizes Simple language source code into a stream of tokens. Handles indentation-based syntax with INDENT/DEDENT tokens, string literals, numbers, identifiers, operators, and keywords.",
    code_examples: [
        "# The lexer converts this:",
        "val x = 42",
        "if x > 0:",
        "    print(x)",
        "",
        "# Into tokens: LET, IDENT(x), EQ, INT(42), NEWLINE,",
        "# IF, IDENT(x), GT, INT(0), COLON, NEWLINE, INDENT,",
        "# IDENT(print), LPAREN, IDENT(x), RPAREN, DEDENT"
    ],
    dependencies: [],
    required_by: [2],
    notes: "First stage of compilation pipeline. Uses INDENT/DEDENT for Python-like significant whitespace."
}

# =====================================================
# BDD Specification Tests
# =====================================================

print("============================================================")
print("  LEXER FEATURE SPECIFICATION (#1)")
print("  Category: Infrastructure | Difficulty: 3 | Status: Complete")
print("============================================================")
print("")

var passed = 0
var failed = 0

# describe "Lexer tokenization"
print("describe Lexer tokenization:")

# it "tokenizes integer literals"
print("  it tokenizes integer literals:")
val int_val = 42
if int_val == 42:
    print("    [PASS] integer literal 42")
    passed = passed + 1
else:
    print("    [FAIL] integer literal")
    failed = failed + 1

# it "tokenizes string literals"
print("  it tokenizes string literals:")
val str_val = "hello world"
if str_val == "hello world":
    print("    [PASS] string literal")
    passed = passed + 1
else:
    print("    [FAIL] string literal")
    failed = failed + 1

# it "tokenizes f-string literals"
print("  it tokenizes f-string literals:")
val name = "Simple"
val fstr = "Hello, {name}!"
if fstr == "Hello, Simple!":
    print("    [PASS] f-string interpolation")
    passed = passed + 1
else:
    print("    [FAIL] f-string interpolation")
    failed = failed + 1

# it "tokenizes identifiers"
print("  it tokenizes identifiers:")
val my_variable = 100
if my_variable == 100:
    print("    [PASS] identifier with underscore")
    passed = passed + 1
else:
    print("    [FAIL] identifier")
    failed = failed + 1

# describe "Lexer operators"
print("")
print("describe Lexer operators:")

# it "tokenizes arithmetic operators"
print("  it tokenizes arithmetic operators:")
val arith = 2 + 3 * 4 - 1
if arith == 13:
    print("    [PASS] arithmetic operators +, *, -")
    passed = passed + 1
else:
    print("    [FAIL] arithmetic operators")
    failed = failed + 1

# it "tokenizes comparison operators"
print("  it tokenizes comparison operators:")
val cmp1 = 5 > 3
val cmp2 = 3 < 5
val cmp3 = 5 >= 5
val cmp4 = 5 <= 5
val cmp5 = 5 == 5
val cmp6 = 5 != 3
if cmp1 and cmp2 and cmp3 and cmp4 and cmp5 and cmp6:
    print("    [PASS] comparison operators >, <, >=, <=, ==, !=")
    passed = passed + 1
else:
    print("    [FAIL] comparison operators")
    failed = failed + 1

# it "tokenizes logical operators"
print("  it tokenizes logical operators:")
val log1 = true and true
val log2 = true or false
val log3 = not false
if log1 and log2 and log3:
    print("    [PASS] logical operators and, or, not")
    passed = passed + 1
else:
    print("    [FAIL] logical operators")
    failed = failed + 1

# describe "Lexer indentation"
print("")
print("describe Lexer indentation:")

# it "handles indented blocks with INDENT/DEDENT"
print("  it handles indented blocks with INDENT/DEDENT:")
var indent_ok = false
if true:
    val nested = 1
    if nested == 1:
        indent_ok = true
if indent_ok:
    print("    [PASS] nested indentation")
    passed = passed + 1
else:
    print("    [FAIL] nested indentation")
    failed = failed + 1

# it "handles multiple indentation levels"
print("  it handles multiple indentation levels:")
var level3_ok = false
if true:
    if true:
        if true:
            level3_ok = true
if level3_ok:
    print("    [PASS] 3-level nesting")
    passed = passed + 1
else:
    print("    [FAIL] 3-level nesting")
    failed = failed + 1

# =====================================================
# Documentation Output
# =====================================================

print("")
print("============================================================")
print("  GENERATED DOCUMENTATION")
print("============================================================")
print("")
print("# {FEATURE.name}")
print("")
print("**Feature ID:** #{FEATURE.id}")
print("**Category:** {FEATURE.category}")
print("**Difficulty:** Level {FEATURE.difficulty}/5")
print("**Status:** {FEATURE.status}")
print("**Implementation:** {FEATURE.impl_type}")
print("")
print("## Description")
print("")
print(FEATURE.description)
print("")
print("## Implementation Files")
print("")
for file in FEATURE.files:
    print("- `{file}`")
print("")
print("## Test Files")
print("")
for test in FEATURE.tests:
    print("- `{test}`")
print("")
print("## Code Examples")
print("")
print("```simple")
for example in FEATURE.code_examples:
    print(example)
print("```")
print("")
if FEATURE.required_by.len() > 0:
    print("## Required By")
    print("")
    print("Features: {FEATURE.required_by}")
    print("")
print("## Notes")
print("")
print(FEATURE.notes)

# =====================================================
# Test Summary
# =====================================================

print("")
print("============================================================")
print("  TEST SUMMARY")
print("============================================================")
print("Passed: {passed}")
print("Failed: {failed}")
print("Total:  {passed + failed}")
if failed == 0:
    print("")
    print("All tests PASSED!")
print("============================================================")
