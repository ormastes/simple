# Custom Autograd Function Tests
#
# Tests for custom autograd functions, context management, and gradient checkpointing.

import spec
import ml.torch as torch
import ml.torch.nn as nn
import ml.torch.autograd as autograd


describe "Custom Autograd Functions":
    describe "Context - Saving tensors":
        it "should create context":
            let ctx = autograd.Context()
            spec.expect(ctx.handle).to_not_be(0)

        it "should save single tensor":
            let ctx = autograd.Context()
            let x = torch.randn([3, 3])

            ctx.save_for_backward(x)

            let (saved_x,) = ctx.saved_tensors()
            spec.expect(saved_x.numel()).to_be(9)

        it "should save multiple tensors":
            let ctx = autograd.Context()
            let x = torch.randn([2, 2])
            let y = torch.randn([3, 3])
            let z = torch.randn([4, 4])

            ctx.save_for_backward(x, y, z)

            let (saved_x, saved_y, saved_z) = ctx.saved_tensors()
            spec.expect(saved_x.numel()).to_be(4)
            spec.expect(saved_y.numel()).to_be(9)
            spec.expect(saved_z.numel()).to_be(16)

        it "should save scalar values":
            let ctx = autograd.Context()

            ctx.save_value("alpha", 0.5)
            ctx.save_value("beta", 1.5)

            let alpha = ctx.get_value("alpha")
            let beta = ctx.get_value("beta")

            spec.expect(alpha).to_be_close_to(0.5, 0.0001)
            spec.expect(beta).to_be_close_to(1.5, 0.0001)

        it "should save both tensors and values":
            let ctx = autograd.Context()
            let x = torch.randn([5])

            ctx.save_for_backward(x)
            ctx.save_value("scale", 2.0)

            let (saved_x,) = ctx.saved_tensors()
            let scale = ctx.get_value("scale")

            spec.expect(saved_x.numel()).to_be(5)
            spec.expect(scale).to_be_close_to(2.0, 0.0001)

    describe "Custom Function - Basic operations":
        it "should define custom ReLU function":
            class CustomReLU(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor) -> torch.Tensor:
                    ctx.save_for_backward(x)
                    return x.clamp(min=0.0)

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> torch.Tensor:
                    let (x,) = ctx.saved_tensors()
                    let mask = (x > 0.0).to_float()
                    return grad_output * mask

            let x = torch.from_data([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=true)
            let y = CustomReLU.apply(x)

            # Forward should clamp negative values to 0
            spec.expect(y[0].item()).to_be_close_to(0.0, 0.0001)
            spec.expect(y[1].item()).to_be_close_to(0.0, 0.0001)
            spec.expect(y[2].item()).to_be_close_to(0.0, 0.0001)
            spec.expect(y[3].item()).to_be_close_to(1.0, 0.0001)
            spec.expect(y[4].item()).to_be_close_to(2.0, 0.0001)

        it "should define custom exponential function":
            class CustomExp(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor) -> torch.Tensor:
                    let result = x.exp()
                    ctx.save_for_backward(result)
                    return result

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> torch.Tensor:
                    # Derivative of exp(x) is exp(x)
                    let (result,) = ctx.saved_tensors()
                    return grad_output * result

            let x = torch.zeros([3], requires_grad=true)
            let y = CustomExp.apply(x)

            # exp(0) = 1
            for i in range(3):
                spec.expect(y[i].item()).to_be_close_to(1.0, 0.0001)

        it "should define custom linear transformation":
            class CustomLinear(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
                    ctx.save_for_backward(x, weight)
                    return (x @ weight) + bias

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor):
                    let (x, weight) = ctx.saved_tensors()

                    # Gradient w.r.t. input
                    let grad_x = grad_output @ weight.T

                    # Gradient w.r.t. weight
                    let grad_weight = x.T @ grad_output

                    # Gradient w.r.t. bias
                    let grad_bias = grad_output.sum(dim=0)

                    return (grad_x, grad_weight, grad_bias)

            let x = torch.randn([2, 3], requires_grad=true)
            let weight = torch.randn([3, 4], requires_grad=true)
            let bias = torch.randn([4], requires_grad=true)

            let y = CustomLinear.apply(x, weight, bias)

            spec.expect(y.shape()[0]).to_be(2)
            spec.expect(y.shape()[1]).to_be(4)

    describe "Custom Function - Advanced operations":
        it "should implement custom activation with saved value":
            class ParametricReLU(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor, alpha: float) -> torch.Tensor:
                    ctx.save_for_backward(x)
                    ctx.save_value("alpha", alpha)

                    # y = x if x > 0 else alpha * x
                    let mask = (x > 0.0).to_float()
                    return x * mask + x * (1.0 - mask) * alpha

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> (torch.Tensor, None):
                    let (x,) = ctx.saved_tensors()
                    let alpha = ctx.get_value("alpha")

                    # Gradient: 1 if x > 0 else alpha
                    let mask = (x > 0.0).to_float()
                    let grad = mask + (1.0 - mask) * alpha

                    return (grad_output * grad, None)

            let x = torch.from_data([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=true)
            let y = ParametricReLU.apply(x, alpha=0.2)

            # Negative values should be scaled by alpha
            spec.expect(y[0].item()).to_be_close_to(-0.4, 0.001)  # -2 * 0.2
            spec.expect(y[1].item()).to_be_close_to(-0.2, 0.001)  # -1 * 0.2
            spec.expect(y[3].item()).to_be_close_to(1.0, 0.001)
            spec.expect(y[4].item()).to_be_close_to(2.0, 0.001)

        it "should implement custom normalization":
            class CustomLayerNorm(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor, eps: float) -> torch.Tensor:
                    let mean = x.mean()
                    let var = x.var()
                    let std = (var + eps).sqrt()

                    let normalized = (x - mean) / std

                    ctx.save_for_backward(x, normalized)
                    ctx.save_value("std", std.item())
                    ctx.save_value("eps", eps)

                    return normalized

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> (torch.Tensor, None):
                    let (x, normalized) = ctx.saved_tensors()
                    let std = ctx.get_value("std")

                    # Simplified gradient computation
                    let N = x.numel()
                    let grad = grad_output / std

                    return (grad, None)

            let x = torch.randn([10], requires_grad=true)
            let y = CustomLayerNorm.apply(x, eps=1e-5)

            # Normalized output should have mean ≈ 0 and std ≈ 1
            spec.expect(y.mean().item()).to_be_close_to(0.0, 0.1)

    describe "Gradient Checkpointing":
        it "should checkpoint simple function":
            fn square_and_sum(x: torch.Tensor) -> torch.Tensor:
                let squared = x * x
                return squared.sum()

            let x = torch.randn([10], requires_grad=true)

            # Without checkpointing
            let y1 = square_and_sum(x)

            # With checkpointing
            let y2 = autograd.checkpoint(square_and_sum, x)

            # Results should be the same
            spec.expect(y1.item()).to_be_close_to(y2.item(), 0.001)

        it "should checkpoint complex function":
            fn multi_layer(x: torch.Tensor) -> torch.Tensor:
                let h1 = x @ x.T
                let h2 = h1 @ h1
                let h3 = h2.sum()
                return h3

            let x = torch.randn([5, 5], requires_grad=true)

            # With checkpointing (saves memory)
            let y = autograd.checkpoint(multi_layer, x)

            spec.expect(y.numel()).to_be(1)

        it "should checkpoint in training loop":
            fn expensive_block(x: torch.Tensor) -> torch.Tensor:
                # Simulate expensive computation
                let mut result = x
                for i in range(5):
                    result = result @ result.T
                    result = result.sum().unsqueeze(0)
                return result

            let x = torch.randn([3, 3], requires_grad=true)

            # Use checkpointing to save memory
            let y = autograd.checkpoint(expensive_block, x)

            spec.expect(y.numel()).to_be_greater_than(0)

    describe "Gradient context managers":
        it "should disable gradients with no_grad":
            let x = torch.randn([10], requires_grad=true)

            # Gradients tracked
            let y = x * 2
            spec.expect(y.requires_grad()).to_be(true)

            # Gradients disabled
            with autograd.no_grad():
                let z = x * 2
                spec.expect(z.requires_grad()).to_be(false)

            # Gradients tracked again
            let w = x * 2
            spec.expect(w.requires_grad()).to_be(true)

        it "should enable gradients with enable_grad":
            with autograd.no_grad():
                let x = torch.randn([10])

                # Inside no_grad
                let y = x * 2
                spec.expect(y.requires_grad()).to_be(false)

                # Temporarily enable gradients
                with autograd.enable_grad():
                    let z = torch.randn([10], requires_grad=true)
                    let w = z * 2
                    spec.expect(w.requires_grad()).to_be(true)

                # Back to no_grad
                let v = x * 2
                spec.expect(v.requires_grad()).to_be(false)

        it "should set gradient state conditionally":
            fn train_step(x: torch.Tensor, is_training: bool) -> torch.Tensor:
                with autograd.set_grad_enabled(is_training):
                    let y = x * 2
                    return y

            let x = torch.randn([10], requires_grad=true)

            # Training mode
            let y_train = train_step(x, is_training=true)
            spec.expect(y_train.requires_grad()).to_be(true)

            # Inference mode
            let y_eval = train_step(x, is_training=false)
            spec.expect(y_eval.requires_grad()).to_be(false)

        it "should nest gradient contexts":
            with autograd.no_grad():
                let x = torch.randn([5])

                with autograd.enable_grad():
                    let y = torch.randn([5], requires_grad=true)
                    spec.expect(y.requires_grad()).to_be(true)

                    with autograd.no_grad():
                        let z = y * 2
                        spec.expect(z.requires_grad()).to_be(false)

    describe "Custom autograd in training":
        it "should use custom function in neural network":
            class CustomSigmoid(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor) -> torch.Tensor:
                    let sigmoid = 1.0 / (1.0 + (-x).exp())
                    ctx.save_for_backward(sigmoid)
                    return sigmoid

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> torch.Tensor:
                    let (sigmoid,) = ctx.saved_tensors()
                    # sigmoid'(x) = sigmoid(x) * (1 - sigmoid(x))
                    return grad_output * sigmoid * (1.0 - sigmoid)

            let model = nn.Linear(10, 1)
            let x = torch.randn([5, 10], requires_grad=true)

            let logits = model(x)
            let probs = CustomSigmoid.apply(logits)

            # Probabilities should be in [0, 1]
            for i in range(5):
                let p = probs[i, 0].item()
                spec.expect(p).to_be_greater_than_or_equal(0.0)
                spec.expect(p).to_be_less_than_or_equal(1.0)

        it "should compute gradients through custom function":
            class Square(autograd.Function):
                @staticmethod
                fn forward(ctx: autograd.Context, x: torch.Tensor) -> torch.Tensor:
                    ctx.save_for_backward(x)
                    return x * x

                @staticmethod
                fn backward(ctx: autograd.Context, grad_output: torch.Tensor) -> torch.Tensor:
                    let (x,) = ctx.saved_tensors()
                    # d/dx(x^2) = 2x
                    return grad_output * 2.0 * x

            let x = torch.ones([3], requires_grad=true)
            let y = Square.apply(x)
            let loss = y.sum()

            loss.backward()

            let grad = x.grad()
            # Gradient of x^2 at x=1 is 2
            for i in range(3):
                spec.expect(grad[i].item()).to_be_close_to(2.0, 0.0001)

    describe "Performance and memory":
        it "should reduce memory with checkpointing":
            fn large_computation(x: torch.Tensor) -> torch.Tensor:
                # Many intermediate tensors
                let mut result = x
                for i in range(10):
                    result = result @ result.T
                    result = result + torch.ones(result.shape())
                return result.sum()

            let x = torch.randn([10, 10], requires_grad=true)

            # Checkpoint saves memory by not storing intermediate activations
            let y = autograd.checkpoint(large_computation, x)

            spec.expect(y.numel()).to_be(1)

        it "should skip gradient computation with no_grad":
            let x = torch.randn([100, 100], requires_grad=true)

            # With gradients (slower, more memory)
            let y1 = x @ x

            # Without gradients (faster, less memory)
            with autograd.no_grad():
                let y2 = x @ x
                spec.expect(y2.requires_grad()).to_be(false)
