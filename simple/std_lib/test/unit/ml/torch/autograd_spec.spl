# PyTorch Autograd Tests
#
# Tests for automatic differentiation functionality including gradient tracking,
# backpropagation, and gradient accumulation.

import spec
import ml.torch as torch
import ml.torch.nn as nn


describe("PyTorch Autograd"):
    describe("Tensor gradient tracking"):
        it("should enable gradient tracking with requires_grad parameter"):
            let x = torch.randn([3, 3], requires_grad=true)
            spec.expect(x.requires_grad()).to_be(true)

        it("should disable gradient tracking by default"):
            let x = torch.randn([3, 3])
            spec.expect(x.requires_grad()).to_be(false)

        it("should allow setting requires_grad after creation"):
            let x = torch.zeros([2, 2])
            spec.expect(x.requires_grad()).to_be(false)

            x.set_requires_grad(true)
            spec.expect(x.requires_grad()).to_be(true)

    describe("Backward pass"):
        it("should compute gradients for simple operations"):
            # y = 2*x + 3, dy/dx = 2
            let x = torch.ones([5], requires_grad=true)
            let y = (x * 2.0).add_scalar(3.0)
            let loss = y.sum()

            loss.backward()

            let grad = x.grad()
            spec.expect(grad).to_not_be(None)
            # Each element should have gradient of 2
            spec.expect(grad.item()).to_be_close_to(2.0, 0.0001)

        it("should compute gradients for squared operations"):
            # y = x^2, dy/dx = 2*x
            let x = torch.randn([3], requires_grad=true)
            let x_val = x.clone().detach()

            let y = (x * x).sum()
            y.backward()

            let grad = x.grad()
            # Gradient should be approximately 2*x
            let expected = x_val * 2.0

            for i in range(3):
                let g = grad[i].item()
                let e = expected[i].item()
                spec.expect(g).to_be_close_to(e, 0.001)

        it("should accumulate gradients across multiple backward calls"):
            let x = torch.ones([3], requires_grad=true)

            # First backward
            let y1 = (x * 2.0).sum()
            y1.backward()

            let grad1 = x.grad()
            let first_val = grad1[0].item()
            spec.expect(first_val).to_be_close_to(2.0, 0.0001)

            # Second backward without zero_grad - should accumulate
            let y2 = (x * 3.0).sum()
            y2.backward()

            let grad2 = x.grad()
            let second_val = grad2[0].item()
            spec.expect(second_val).to_be_close_to(5.0, 0.0001)  # 2 + 3

        it("should reset gradients with zero_grad"):
            let x = torch.ones([3], requires_grad=true)

            # First backward
            let y = (x * 2.0).sum()
            y.backward()

            # Reset gradients
            x.zero_grad()

            let grad_after_reset = x.grad()
            # After reset, gradient should be None or zeros
            if grad_after_reset is not None:
                spec.expect(grad_after_reset[0].item()).to_be_close_to(0.0, 0.0001)

    describe("Detach operation"):
        it("should create tensor without gradient tracking"):
            let x = torch.randn([3], requires_grad=true)
            let y = x.detach()

            spec.expect(x.requires_grad()).to_be(true)
            spec.expect(y.requires_grad()).to_be(false)

        it("should break gradient flow when detached"):
            let x = torch.ones([3], requires_grad=true)
            let y = x.detach()
            let z = y * 2.0

            # z should not require gradients
            spec.expect(z.requires_grad()).to_be(false)

    describe("Neural network training"):
        it("should compute gradients through linear layer"):
            let layer = nn.Linear(10, 5, bias=true)
            let x = torch.randn([1, 10], requires_grad=true)

            let output = layer(x)
            let loss = output.sum()

            loss.backward()

            # Input should have gradients
            let grad = x.grad()
            spec.expect(grad).to_not_be(None)
            spec.expect(grad.numel()).to_be(10)

        it("should compute gradients through multiple layers"):
            let fc1 = nn.Linear(10, 20)
            let fc2 = nn.Linear(20, 5)

            let x = torch.randn([2, 10], requires_grad=true)

            let h = nn.relu(fc1(x))
            let output = fc2(h)
            let loss = output.sum()

            loss.backward()

            let grad = x.grad()
            spec.expect(grad).to_not_be(None)
            spec.expect(grad.shape()[0]).to_be(2)
            spec.expect(grad.shape()[1]).to_be(10)

    describe("No-grad context"):
        it("should disable gradient tracking in no_grad context"):
            import ml.torch.autograd as autograd

            let x = torch.randn([3], requires_grad=true)

            with autograd.no_grad():
                let y = x * 2.0
                spec.expect(y.requires_grad()).to_be(false)

            # Outside context, should work normally
            let z = x * 3.0
            spec.expect(z.requires_grad()).to_be(true)

    describe("Matrix multiplication gradients"):
        it("should compute gradients for matrix multiplication"):
            # y = x @ W, dy/dx = W^T (simplified)
            let x = torch.randn([2, 3], requires_grad=true)
            let W = torch.randn([3, 4], requires_grad=true)

            let y = x @ W
            let loss = y.sum()

            loss.backward()

            let x_grad = x.grad()
            let W_grad = W.grad()

            spec.expect(x_grad).to_not_be(None)
            spec.expect(W_grad).to_not_be(None)
            spec.expect(x_grad.shape()[0]).to_be(2)
            spec.expect(x_grad.shape()[1]).to_be(3)
            spec.expect(W_grad.shape()[0]).to_be(3)
            spec.expect(W_grad.shape()[1]).to_be(4)

    describe("Gradient clipping"):
        it("should clip gradients by value"):
            let x = torch.ones([5], requires_grad=true)
            let y = (x * 100.0).sum()

            y.backward()

            # Clip gradients
            nn.clip_grad_value([x], clip_value=10.0)

            let grad = x.grad()
            for i in range(5):
                let g = grad[i].item()
                spec.expect(g).to_be_less_than_or_equal(10.0)
                spec.expect(g).to_be_greater_than_or_equal(-10.0)

        it("should clip gradients by norm"):
            let params = [
                torch.ones([3], requires_grad=true),
                torch.ones([3], requires_grad=true)
            ]

            # Create large gradients
            for param in params:
                let y = (param * 100.0).sum()
                y.backward()

            # Clip by global norm
            let total_norm = nn.clip_grad_norm(params, max_norm=5.0)

            # Total norm should be clipped to max_norm
            spec.expect(total_norm).to_be_greater_than(5.0)  # Was larger before clipping

            # After clipping, compute new norm
            let mut new_norm = 0.0
            for param in params:
                let grad = param.grad()
                if grad is not None:
                    let norm_val = grad.norm(2.0).item()
                    new_norm = new_norm + (norm_val ** 2.0)
            new_norm = new_norm ** 0.5

            spec.expect(new_norm).to_be_less_than_or_equal(5.1)  # Should be ~5.0

    describe("Optimizer integration"):
        it("should work with SGD optimizer"):
            import ml.torch.optim as optim

            let model = nn.Linear(10, 1)
            let optimizer = optim.SGD(model.parameters(), lr=0.01)

            # Forward pass
            let x = torch.randn([5, 10])
            let y = model(x)
            let loss = y.sum()

            # Backward pass
            loss.backward()

            # Check that gradients exist
            let params = model.parameters()
            for param in params:
                let grad = param.grad()
                spec.expect(grad).to_not_be(None)

            # Optimizer step
            optimizer.step()

            # Zero gradients for next iteration
            optimizer.zero_grad()

    describe("Device transfer with gradients"):
        it("should maintain gradient tracking after device transfer"):
            let x = torch.randn([3, 3], requires_grad=true)

            # Transfer to CPU (already on CPU, but tests the API)
            let y = x.to_cpu()

            spec.expect(y.requires_grad()).to_be(true)

            let z = (y * 2.0).sum()
            z.backward()

            let grad = y.grad()
            spec.expect(grad).to_not_be(None)
