# Unit tests for tree-sitter lexer
# Phase 1: Basic tokenization tests

import spec.{describe, it, expect}
import parser.treesitter.lexer.{Lexer}
import parser.treesitter.grammar.{TokenKind}

describe("Lexer"):
    it("tokenizes empty source"):
        let mut lexer = Lexer.new("")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        # Should have only EOF token
        expect(tokens.len()).to_equal(1)
        match tokens[0].kind:
            case TokenKind.Eof: expect(true).to_be(true)
            case _: expect(false).to_be(true)

    it("tokenizes integer literal"):
        let mut lexer = Lexer.new("42")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        expect(tokens.len()).to_equal(2)  # Integer + EOF
        match tokens[0].kind:
            case TokenKind.Integer(val):
                expect(val).to_equal(42)
            case _: expect(false).to_be(true)

    it("tokenizes identifier"):
        let mut lexer = Lexer.new("foo")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        expect(tokens.len()).to_equal(2)  # Identifier + EOF
        match tokens[0].kind:
            case TokenKind.Identifier(name):
                expect(name).to_equal("foo")
            case _: expect(false).to_be(true)

    it("tokenizes type identifier"):
        let mut lexer = Lexer.new("String")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        match tokens[0].kind:
            case TokenKind.TypeIdentifier(name):
                expect(name).to_equal("String")
            case _: expect(false).to_be(true)

    it("tokenizes keywords"):
        let mut lexer = Lexer.new("fn let return")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        expect(tokens.len()).to_equal(4)  # fn, let, return, EOF

        match tokens[0].kind:
            case TokenKind.Fn: expect(true).to_be(true)
            case _: expect(false).to_be(true)

        match tokens[1].kind:
            case TokenKind.Let: expect(true).to_be(true)
            case _: expect(false).to_be(true)

        match tokens[2].kind:
            case TokenKind.Return: expect(true).to_be(true)
            case _: expect(false).to_be(true)

    it("tokenizes operators"):
        let mut lexer = Lexer.new("+ - * / == !=")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        expect(tokens.len()).to_equal(7)  # 6 operators + EOF

        match tokens[0].kind:
            case TokenKind.Plus: expect(true).to_be(true)
            case _: expect(false).to_be(true)

        match tokens[4].kind:
            case TokenKind.Eq: expect(true).to_be(true)
            case _: expect(false).to_be(true)

        match tokens[5].kind:
            case TokenKind.NotEq: expect(true).to_be(true)
            case _: expect(false).to_be(true)

    it("tokenizes delimiters"):
        let mut lexer = Lexer.new("( ) { } [ ] , : ;")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        expect(tokens.len()).to_equal(10)  # 9 delimiters + EOF

    it("tokenizes expression"):
        let mut lexer = Lexer.new("1 + 2")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        expect(tokens.len()).to_equal(4)  # 1, +, 2, EOF

        match tokens[0].kind:
            case TokenKind.Integer(1): expect(true).to_be(true)
            case _: expect(false).to_be(true)

        match tokens[1].kind:
            case TokenKind.Plus: expect(true).to_be(true)
            case _: expect(false).to_be(true)

        match tokens[2].kind:
            case TokenKind.Integer(2): expect(true).to_be(true)
            case _: expect(false).to_be(true)

    it("tokenizes function signature"):
        let mut lexer = Lexer.new("fn add(x: i32)")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        # fn, add, (, x, :, i32, ), EOF
        expect(tokens.len()).to_equal(8)

    it("tracks line and column numbers"):
        let mut lexer = Lexer.new("foo\nbar")
        let result = lexer.tokenize()
        expect(result.is_ok()).to_be(true)

        let tokens = result.unwrap()
        # foo, newline, bar, EOF
        expect(tokens.len()).to_equal(4)

        # First token on line 1
        expect(tokens[0].span.start_line).to_equal(1)

        # Newline token
        match tokens[1].kind:
            case TokenKind.Newline: expect(true).to_be(true)
            case _: expect(false).to_be(true)

        # Second token on line 2
        expect(tokens[2].span.start_line).to_equal(2)
