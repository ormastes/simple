# Matrix Multiplication - 2D GPU Kernel Example
#
# This example demonstrates 2D work groups for matrix operations.
# Each GPU thread computes one output element.
#
# Compilation: ./simple examples/gpu/vulkan/matrix_multiply.spl

import std.gpu
import std.io
import std.math

# GPU kernel - matrix multiplication (naive version)
#[gpu]
fn matmul_kernel(a: []f32, b: []f32, c: []f32, m: i32, n: i32, k: i32):
    """
    Compute C = A × B where:
    - A is m×k matrix
    - B is k×n matrix
    - C is m×n matrix (output)

    Each thread computes one element of C.
    """
    # Get 2D thread coordinates
    row = gpu.global_id(0)  # Which row of C
    col = gpu.global_id(1)  # Which column of C

    # Bounds check
    if row >= m or col >= n:
        return

    # Compute dot product of row from A and column from B
    sum = 0.0f32
    for i in range(k):
        a_elem = a[row * k + i]  # A[row, i]
        b_elem = b[i * n + col]  # B[i, col]
        sum += a_elem * b_elem

    # Store result
    c[row * n + col] = sum

# GPU kernel - optimized version with shared memory
#[gpu]
fn matmul_kernel_optimized(a: []f32, b: []f32, c: []f32, m: i32, n: i32, k: i32):
    """
    Optimized matrix multiplication using shared memory tiles.
    ~10x faster than naive version for large matrices.
    """
    # Work group size (must match launch parameters)
    tile_size = 16

    # Allocate shared memory tiles
    tile_a = gpu.shared_memory<f32>(tile_size * tile_size)
    tile_b = gpu.shared_memory<f32>(tile_size * tile_size)

    # Thread coordinates
    row = gpu.global_id(0)
    col = gpu.global_id(1)
    local_row = gpu.local_id(0)
    local_col = gpu.local_id(1)

    sum = 0.0f32

    # Loop over tiles
    num_tiles = (k + tile_size - 1) / tile_size
    for tile in range(num_tiles):
        # Load tile of A into shared memory
        a_row = row
        a_col = tile * tile_size + local_col
        if a_row < m and a_col < k:
            tile_a[local_row * tile_size + local_col] = a[a_row * k + a_col]
        else:
            tile_a[local_row * tile_size + local_col] = 0.0

        # Load tile of B into shared memory
        b_row = tile * tile_size + local_row
        b_col = col
        if b_row < k and b_col < n:
            tile_b[local_row * tile_size + local_col] = b[b_row * n + b_col]
        else:
            tile_b[local_row * tile_size + local_col] = 0.0

        # Wait for all threads to finish loading
        gpu.barrier()

        # Compute partial dot product from tiles
        for i in range(tile_size):
            sum += tile_a[local_row * tile_size + i] * tile_b[i * tile_size + local_col]

        # Wait before loading next tile
        gpu.barrier()

    # Store result
    if row < m and col < n:
        c[row * n + col] = sum

fn matrix_multiply_example():
    """
    Example: Multiply two matrices on GPU
    """
    # Matrix dimensions (m×k) × (k×n) = (m×n)
    m = 512  # Rows of A
    k = 512  # Columns of A, rows of B
    n = 512  # Columns of B

    io.println("Matrix Multiplication Example")
    io.println("Dimensions: (" + str(m) + "×" + str(k) + ") × (" + str(k) + "×" + str(n) + ")")
    io.println("")

    # Create random input matrices
    a = [math.random() for _ in range(m * k)]
    b = [math.random() for _ in range(k * n)]

    # Check Vulkan availability
    if !gpu.device_available():
        io.println("Error: Vulkan not available")
        return

    # Create device
    device = gpu.Device()

    # Allocate buffers
    buf_a = device.alloc_buffer(a)
    buf_b = device.alloc_buffer(b)
    buf_c = device.alloc_buffer<f32>(m * n)  # Output buffer

    io.println("Running naive kernel...")
    start = time.now()

    # Launch naive kernel
    device.launch_2d(
        matmul_kernel,
        [buf_a, buf_b, buf_c, m, n, k],
        global_size=(m, n),
        local_size=(16, 16)  # 16×16 = 256 threads per work group
    )

    device.sync()
    naive_time = time.now() - start
    io.println("Naive kernel: " + str(naive_time * 1000.0) + " ms")

    # Download and verify first result
    result_naive = device.download(buf_c)

    io.println("")
    io.println("Running optimized kernel...")
    start = time.now()

    # Launch optimized kernel
    device.launch_2d(
        matmul_kernel_optimized,
        [buf_a, buf_b, buf_c, m, n, k],
        global_size=(m, n),
        local_size=(16, 16)  # Must match tile_size in kernel
    )

    device.sync()
    optimized_time = time.now() - start
    io.println("Optimized kernel: " + str(optimized_time * 1000.0) + " ms")

    # Download optimized result
    result_optimized = device.download(buf_c)

    # Verify results match
    io.println("")
    io.println("Verification:")
    max_diff = 0.0
    for i in range(min(100, m * n)):
        diff = abs(result_naive[i] - result_optimized[i])
        max_diff = max(max_diff, diff)

    if max_diff < 0.001:
        io.println("✓ Results match (max diff: " + str(max_diff) + ")")
        speedup = naive_time / optimized_time
        io.println("✓ Speedup: " + str(speedup) + "×")
    else:
        io.println("✗ Results differ (max diff: " + str(max_diff) + ")")

    # Performance metrics
    flops = 2.0 * f64(m) * f64(n) * f64(k)  # 2 ops (mul+add) per output element
    gflops = flops / (optimized_time * 1e9)

    io.println("")
    io.println("Performance:")
    io.println("  Operations: " + str(flops / 1e9) + " GFLOP")
    io.println("  Performance: " + str(gflops) + " GFLOP/s")

fn main():
    matrix_multiply_example()
