# Tensor Dimensions - Lean verification generation
# Generates Lean proofs for tensor dimension inference

import verification.lean.codegen as codegen

fn regenerate_tensor_dimensions() -> text:
    gen = codegen.LeanCodegen.create("TensorDimensions")

    gen = gen.add_namespace("TensorDimensions")

    # Doc comment
    gen = gen.add_doc_comment("Tensor dimension inference and verification.\n    Provides compile-time dimension tracking with range constraints.")

    # ========================================================================
    # Dimension type
    # ========================================================================

    dim = codegen.build_enum_with_deriving("Dim", [
        ("literal", [("value", codegen.make_nat_type())]),
        ("variable", [("id", codegen.make_nat_type()), ("name", codegen.make_option_type(codegen.make_string_type()))]),
        ("named", [("name", codegen.make_string_type()), ("lo", codegen.make_option_type(codegen.make_nat_type())), ("hi", codegen.make_option_type(codegen.make_nat_type()))]),
        ("dynamic", []),
        ("broadcast", [])
    ], ["Repr", "DecidableEq"])
    gen = gen.add_inductive(dim)

    # ========================================================================
    # TensorShape type
    # ========================================================================

    gen = gen.add_raw_line("abbrev TensorShape := List Dim")
    gen = gen.add_blank()

    # ========================================================================
    # Dimension constraint type
    # ========================================================================

    constraint = codegen.build_enum_with_deriving("DimConstraint", [
        ("equal", [("d1", codegen.make_simple_type("Dim")), ("d2", codegen.make_simple_type("Dim"))]),
        ("greaterThan", [("d", codegen.make_simple_type("Dim")), ("min", codegen.make_nat_type())]),
        ("lessThan", [("d", codegen.make_simple_type("Dim")), ("max", codegen.make_nat_type())]),
        ("inRange", [("d", codegen.make_simple_type("Dim")), ("lo", codegen.make_nat_type()), ("hi", codegen.make_nat_type())]),
        ("productEquals", [("dims", codegen.make_list_type(codegen.make_simple_type("Dim"))), ("value", codegen.make_nat_type())])
    ], ["Repr"])
    gen = gen.add_inductive(constraint)

    # ========================================================================
    # Dimension equality
    # ========================================================================

    gen = gen.add_doc_comment("Check if two dimensions are equal (for unification).")
    gen = gen.add_raw_line("def dimEq : Dim → Dim → bool")
    gen = gen.add_raw_line("  | Dim.literal v1, Dim.literal v2 => v1 = v2")
    gen = gen.add_raw_line("  | Dim.variable id1 _, Dim.variable id2 _ => id1 = id2")
    gen = gen.add_raw_line("  | Dim.named n1 _ _, Dim.named n2 _ _ => n1 = n2")
    gen = gen.add_raw_line("  | Dim.dynamic, Dim.dynamic => true")
    gen = gen.add_raw_line("  | Dim.broadcast, Dim.broadcast => true")
    gen = gen.add_raw_line("  | _, _ => false")
    gen = gen.add_blank()

    # ========================================================================
    # Shape compatibility
    # ========================================================================

    gen = gen.add_doc_comment("Check if two shapes are compatible (same rank, compatible dims).")
    gen = gen.add_raw_line("def shapesCompatible : TensorShape → TensorShape → bool")
    gen = gen.add_raw_line("  | [], [] => true")
    gen = gen.add_raw_line("  | d1 :: s1, d2 :: s2 => dimEq d1 d2 && shapesCompatible s1 s2")
    gen = gen.add_raw_line("  | _, _ => false")
    gen = gen.add_blank()

    # ========================================================================
    # Dimension value extraction
    # ========================================================================

    gen = gen.add_doc_comment("Get concrete value from dimension if literal.")
    gen = gen.add_raw_line("def getDimValue : Dim → Option ℕ")
    gen = gen.add_raw_line("  | Dim.literal v => some v")
    gen = gen.add_raw_line("  | _ => none")
    gen = gen.add_blank()

    # ========================================================================
    # Range checking
    # ========================================================================

    gen = gen.add_doc_comment("Check if dimension satisfies range constraint.")
    gen = gen.add_raw_line("def dimInRange (d : Dim) (lo hi : ℕ) : Prop :=")
    gen = gen.add_raw_line("  match getDimValue d with")
    gen = gen.add_raw_line("  | some v => lo ≤ v ∧ v ≤ hi")
    gen = gen.add_raw_line("  | none => True  -- Dynamic dims always satisfy")
    gen = gen.add_blank()

    # ========================================================================
    # Unification result
    # ========================================================================

    gen = gen.add_doc_comment("Result of dimension unification.")
    unify_result = codegen.build_enum_with_deriving("UnifyResult", [
        ("success", [("d", codegen.make_simple_type("Dim"))]),
        ("failure", [("expected", codegen.make_simple_type("Dim")), ("actual", codegen.make_simple_type("Dim"))])
    ], ["Repr"])
    gen = gen.add_inductive(unify_result)

    # ========================================================================
    # Dimension unification
    # ========================================================================

    gen = gen.add_doc_comment("Unify two dimensions, returning unified dim or error.")
    gen = gen.add_raw_line("def unifyDim : Dim → Dim → UnifyResult")
    gen = gen.add_raw_line("  -- Same literal")
    gen = gen.add_raw_line("  | Dim.literal v1, Dim.literal v2 =>")
    gen = gen.add_raw_line("    if v1 = v2 then UnifyResult.success (Dim.literal v1)")
    gen = gen.add_raw_line("    else UnifyResult.failure (Dim.literal v1) (Dim.literal v2)")
    gen = gen.add_raw_line("  -- Variable binds to anything")
    gen = gen.add_raw_line("  | Dim.variable _ _, d => UnifyResult.success d")
    gen = gen.add_raw_line("  | d, Dim.variable _ _ => UnifyResult.success d")
    gen = gen.add_raw_line("  -- Dynamic matches anything")
    gen = gen.add_raw_line("  | Dim.dynamic, d => UnifyResult.success d")
    gen = gen.add_raw_line("  | d, Dim.dynamic => UnifyResult.success d")
    gen = gen.add_raw_line("  -- Broadcast rules")
    gen = gen.add_raw_line("  | Dim.broadcast, Dim.literal 1 => UnifyResult.success (Dim.literal 1)")
    gen = gen.add_raw_line("  | Dim.literal 1, Dim.broadcast => UnifyResult.success (Dim.literal 1)")
    gen = gen.add_raw_line("  | Dim.broadcast, d => UnifyResult.success d")
    gen = gen.add_raw_line("  | d, Dim.broadcast => UnifyResult.success d")
    gen = gen.add_raw_line("  -- Named dims with same name")
    gen = gen.add_raw_line("  | Dim.named n1 lo1 hi1, Dim.named n2 lo2 hi2 =>")
    gen = gen.add_raw_line("    if n1 = n2 then UnifyResult.success (Dim.named n1 lo1 hi1)")
    gen = gen.add_raw_line("    else UnifyResult.failure (Dim.named n1 lo1 hi1) (Dim.named n2 lo2 hi2)")
    gen = gen.add_raw_line("  -- Default: failure")
    gen = gen.add_raw_line("  | d1, d2 => UnifyResult.failure d1 d2")
    gen = gen.add_blank()

    # ========================================================================
    # Shape unification
    # ========================================================================

    gen = gen.add_doc_comment("Unify two shapes dimension by dimension.")
    gen = gen.add_raw_line("def unifyShapes : TensorShape → TensorShape → Option TensorShape")
    gen = gen.add_raw_line("  | [], [] => some []")
    gen = gen.add_raw_line("  | d1 :: s1, d2 :: s2 =>")
    gen = gen.add_raw_line("    match unifyDim d1 d2, unifyShapes s1 s2 with")
    gen = gen.add_raw_line("    | UnifyResult.success d, some s => some (d :: s)")
    gen = gen.add_raw_line("    | _, _ => none")
    gen = gen.add_raw_line("  | _, _ => none")
    gen = gen.add_blank()

    # ========================================================================
    # Matrix multiplication shape inference
    # ========================================================================

    gen = gen.add_doc_comment("Infer shape of matrix multiplication [M,K] @ [K,N] -> [M,N].")
    gen = gen.add_raw_line("def matmulShape (left right : TensorShape) : Option TensorShape :=")
    gen = gen.add_raw_line("  match left, right with")
    gen = gen.add_raw_line("  | [m, k1], [k2, n] =>")
    gen = gen.add_raw_line("    match unifyDim k1 k2 with")
    gen = gen.add_raw_line("    | UnifyResult.success _ => some [m, n]")
    gen = gen.add_raw_line("    | UnifyResult.failure _ _ => none")
    gen = gen.add_raw_line("  | _, _ => none")
    gen = gen.add_blank()

    # ========================================================================
    # Theorems
    # ========================================================================

    # Reflexivity of shape compatibility
    gen = gen.add_doc_comment("Shape compatibility is reflexive.")
    gen = gen.add_theorem(codegen.build_theorem(
        "shapesCompatible_refl",
        [("s", "TensorShape")],
        "shapesCompatible s s = true",
        "induction s with\n| nil => rfl\n| cons d s' ih => simp [shapesCompatible, dimEq]; exact ih"
    ))

    # Unification success means dims are compatible
    gen = gen.add_doc_comment("Successful unification implies dimension equality.")
    gen = gen.add_theorem(codegen.build_theorem(
        "unifyDim_success_eq",
        [("d1", "Dim"), ("d2", "Dim"), ("d", "Dim")],
        "unifyDim d1 d2 = UnifyResult.success d → dimEq d1 d ∨ dimEq d2 d",
        "intro h\ncases d1 <;> cases d2 <;> simp [unifyDim] at h <;> try (left; simp [dimEq]; rfl) <;> try (right; simp [dimEq]; rfl)"
    ))

    # Matmul shape determinism
    gen = gen.add_doc_comment("Matmul shape inference is deterministic.")
    gen = gen.add_theorem(codegen.build_theorem(
        "matmulShape_deterministic",
        [("l", "TensorShape"), ("r", "TensorShape"), ("s1", "TensorShape"), ("s2", "TensorShape")],
        "matmulShape l r = some s1 → matmulShape l r = some s2 → s1 = s2",
        "intro h1 h2\nhave : some s1 = some s2 := by simpa [h1] using h2\ncases this\nrfl"
    ))

    # Shape product preservation (for reshape)
    gen = gen.add_doc_comment("Product of dimensions helper.")
    gen = gen.add_raw_line("def dimProduct : TensorShape → Option ℕ")
    gen = gen.add_raw_line("  | [] => some 1")
    gen = gen.add_raw_line("  | d :: ds =>")
    gen = gen.add_raw_line("    match getDimValue d, dimProduct ds with")
    gen = gen.add_raw_line("    | some v, some p => some (v * p)")
    gen = gen.add_raw_line("    | _, _ => none")
    gen = gen.add_blank()

    # Memory estimation
    gen = gen.add_doc_comment("Minimum memory for shape (in elements).")
    gen = gen.add_raw_line("def minElements : TensorShape → Option ℕ :=")
    gen = gen.add_raw_line("  dimProduct")
    gen = gen.add_blank()

    gen = gen.add_doc_comment("Maximum memory for shape (using range upper bounds).")
    gen = gen.add_raw_line("def maxElementsAux : List Dim → Option ℕ")
    gen = gen.add_raw_line("  | [] => some 1")
    gen = gen.add_raw_line("  | d :: ds =>")
    gen = gen.add_raw_line("    match d, maxElementsAux ds with")
    gen = gen.add_raw_line("    | Dim.literal v, some p => some (v * p)")
    gen = gen.add_raw_line("    | Dim.named _ _ (some hi), some p => some (hi * p)")
    gen = gen.add_raw_line("    | _, _ => none")
    gen = gen.add_blank()

    gen = gen.add_raw_line("def maxElements : TensorShape → Option ℕ := maxElementsAux")
    gen = gen.add_blank()

    # Memory bound theorem
    gen = gen.add_doc_comment("Min elements ≤ max elements when both exist.")
    gen = gen.add_theorem(codegen.build_theorem(
        "min_le_max_elements",
        [("s", "TensorShape")],
        "∀ min max, minElements s = some min → maxElements s = some max → min ≤ max",
        "intro min max h_min h_max\nsorry  -- Requires induction on s with product monotonicity"
    ))

    gen = gen.end_namespace("TensorDimensions")

    return gen.emit()


# ============================================================================
# Memory Verification
# ============================================================================

fn regenerate_tensor_memory() -> text:
    gen = codegen.LeanCodegen.create("TensorMemory")

    gen = gen.add_namespace("TensorMemory")

    gen = gen.add_doc_comment("Memory estimation and verification for tensor training.")

    # Import TensorDimensions
    gen = gen.add_raw_line("open TensorDimensions")
    gen = gen.add_blank()

    # Memory bound structure
    gen = gen.add_raw_line("structure MemoryBound where")
    gen = gen.add_raw_line("  minBytes : ℕ")
    gen = gen.add_raw_line("  maxBytes : ℕ")
    gen = gen.add_raw_line("  valid : minBytes ≤ maxBytes")
    gen = gen.add_blank()

    # Device memory
    gen = gen.add_raw_line("structure DeviceMemory where")
    gen = gen.add_raw_line("  totalBytes : ℕ")
    gen = gen.add_raw_line("  availableBytes : ℕ")
    gen = gen.add_raw_line("  valid : availableBytes ≤ totalBytes")
    gen = gen.add_blank()

    # Training memory components
    gen = gen.add_raw_line("structure TrainingMemory where")
    gen = gen.add_raw_line("  parameters : MemoryBound")
    gen = gen.add_raw_line("  gradients : MemoryBound")
    gen = gen.add_raw_line("  optimizerState : MemoryBound")
    gen = gen.add_raw_line("  activations : MemoryBound")
    gen = gen.add_blank()

    # Total memory
    gen = gen.add_raw_line("def TrainingMemory.totalMax (tm : TrainingMemory) : ℕ :=")
    gen = gen.add_raw_line("  tm.parameters.maxBytes + tm.gradients.maxBytes +")
    gen = gen.add_raw_line("  tm.optimizerState.maxBytes + tm.activations.maxBytes")
    gen = gen.add_blank()

    gen = gen.add_raw_line("def TrainingMemory.totalMin (tm : TrainingMemory) : ℕ :=")
    gen = gen.add_raw_line("  tm.parameters.minBytes + tm.gradients.minBytes +")
    gen = gen.add_raw_line("  tm.optimizerState.minBytes + tm.activations.minBytes")
    gen = gen.add_blank()

    # Fits theorem
    gen = gen.add_doc_comment("If max estimate fits, any actual usage fits.")
    gen = gen.add_theorem(codegen.build_theorem(
        "training_fits_if_max_fits",
        [("tm", "TrainingMemory"), ("device", "DeviceMemory"), ("actual", "ℕ")],
        "tm.totalMax ≤ device.availableBytes → tm.totalMin ≤ actual → actual ≤ tm.totalMax → actual ≤ device.availableBytes",
        "intro h_max h_min h_actual\nomega"
    ))

    gen = gen.end_namespace("TensorMemory")

    return gen.emit()
