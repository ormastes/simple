# Checkpoint - Model Save/Load Functions
#
# Functions for saving and loading model state and training checkpoints.

export save, load

import device.{Device, device_code}


# ============================================================================
# Save/Load Functions
# ============================================================================

fn save(obj: {str: any}, path: str):
    """Save model state or checkpoint to file.

    Saves a dictionary of tensors, model states, or training checkpoints to disk.
    Common use cases:
    - Save model weights: `torch.save(model.state_dict(), "model.pth")`
    - Save checkpoint: `torch.save({"model": model.state_dict(), "optimizer": optimizer.state_dict(), "epoch": 10}, "checkpoint.pth")`

    Args:
        obj: Dictionary to save (typically state_dict or checkpoint)
        path: File path to save to

    Example:
        ```simple
        # Save model only
        torch.save(model.state_dict(), "model_weights.pth")

        # Save full training checkpoint
        val checkpoint = {
            "model_state": model.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "epoch": current_epoch,
            "loss": best_loss
        }
        torch.save(checkpoint, "checkpoint.pth")
        ```

    Note:
        Uses PyTorch's native serialization format (pickle).
    """
    # Convert Simple dict to format suitable for FFI
    val keys = []
    val values = []

    for (key, value) in obj.items():
        keys.append(key)
        values.append(value)

    # Call FFI to save
    val result = rt_torch_save(keys.as_ptr(), values.as_ptr(), keys.len() as i32, path.as_ptr(), path.len() as i32)

    if result != 0:
        panic("Failed to save checkpoint to {path}")


fn load(path: str, device: Device = Device::CPU) -> {str: any}:
    """Load model state or checkpoint from file.

    Loads a previously saved dictionary from disk.

    Args:
        path: File path to load from
        device: Device to load tensors to (default: CPU)

    Returns:
        Dictionary with loaded data

    Example:
        ```simple
        # Load model weights
        val state = torch.load("model_weights.pth")
        model.load_state_dict(state)

        # Load training checkpoint
        val checkpoint = torch.load("checkpoint.pth")
        model.load_state_dict(checkpoint["model_state"])
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        val start_epoch = checkpoint["epoch"]
        ```

    Note:
        Loaded tensors will be on the specified device.
    """
    # Call FFI to load
    val handle = rt_torch_load(path.as_ptr(), path.len() as i32, device_code(device))

    if handle == 0:
        panic("Failed to load checkpoint from {path}")

    # Deserialize data from handle to Simple dict
    val result = deserialize_checkpoint(handle)

    # Free the handle after deserialization
    rt_torch_checkpoint_free(handle)

    return result


fn deserialize_checkpoint(handle: u64) -> dict:
    """Deserialize a loaded checkpoint handle to a Simple dict.

    Iterates through all keys in the checkpoint and converts
    tensors/values to Simple equivalents.

    Args:
        handle: Checkpoint handle from rt_torch_load

    Returns:
        Dict containing checkpoint data
    """
    var result: dict = {}

    # Get number of entries in checkpoint
    val count = rt_torch_checkpoint_len(handle)

    for i in 0..count:
        # Get key name
        val key = rt_torch_checkpoint_key(handle, i)

        # Get value type (0=tensor, 1=int, 2=float, 3=string, 4=nested_dict)
        val value_type = rt_torch_checkpoint_value_type(handle, i)

        match value_type:
            0 ->
                # Tensor - get as raw data and wrap
                val tensor_handle = rt_torch_checkpoint_get_tensor(handle, i)
                result[key] = TensorWrapper(handle: tensor_handle)
            1 ->
                # Integer
                val int_val = rt_torch_checkpoint_get_int(handle, i)
                result[key] = int_val
            2 ->
                # Float
                val float_val = rt_torch_checkpoint_get_float(handle, i)
                result[key] = float_val
            3 ->
                # String
                val str_val = rt_torch_checkpoint_get_string(handle, i)
                result[key] = str_val
            4 ->
                # Nested dict - recursively deserialize
                val nested_handle = rt_torch_checkpoint_get_nested(handle, i)
                result[key] = deserialize_checkpoint(nested_handle)
            _ ->
                # Unknown type, skip
                pass

    return result


# Wrapper for tensor data that preserves the handle for model loading
class TensorWrapper:
    handle: u64

    fn __init__(handle: u64):
        self.handle = handle

    fn to_tensor(device: Device = Device.Cpu) -> Tensor:
        """Convert wrapper to a Tensor on the specified device."""
        return tensor_from_handle(self.handle, device)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_save(keys: *str, values: *any, count: i32, path: *u8, path_len: i32) -> i32
extern fn rt_torch_load(path: *u8, path_len: i32, device: i32) -> u64
extern fn rt_torch_checkpoint_free(handle: u64)
extern fn rt_torch_checkpoint_len(handle: u64) -> i32
extern fn rt_torch_checkpoint_key(handle: u64, index: i32) -> text
extern fn rt_torch_checkpoint_value_type(handle: u64, index: i32) -> i32
extern fn rt_torch_checkpoint_get_tensor(handle: u64, index: i32) -> u64
extern fn rt_torch_checkpoint_get_int(handle: u64, index: i32) -> i64
extern fn rt_torch_checkpoint_get_float(handle: u64, index: i32) -> f64
extern fn rt_torch_checkpoint_get_string(handle: u64, index: i32) -> text
extern fn rt_torch_checkpoint_get_nested(handle: u64, index: i32) -> u64
