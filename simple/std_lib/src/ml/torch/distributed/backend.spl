# Distributed Backend and Reduction Operations
#
# Backend types and reduction operations for distributed training.

export Backend, ReduceOp

# ============================================================================
# Backend Enum
# ============================================================================

enum Backend:
    """Distributed backend type.

    - NCCL: NVIDIA Collective Communications Library (GPU-only, recommended)
    - GLOO: CPU and GPU support (cross-platform)
    - MPI: Message Passing Interface (research/HPC)
    """
    NCCL   # GPU-only, fastest for multi-GPU
    GLOO   # CPU and GPU, cross-platform
    MPI    # Research/HPC environments

    fn to_str(self) -> str:
        """Convert backend to string."""
        match self:
            Backend::NCCL -> "nccl"
            Backend::GLOO -> "gloo"
            Backend::MPI -> "mpi"


# ============================================================================
# Reduction Operations
# ============================================================================

enum ReduceOp:
    """Reduction operation for collective communication.

    - SUM: Sum values across processes
    - PRODUCT: Multiply values across processes
    - MIN: Minimum value across processes
    - MAX: Maximum value across processes
    - BAND: Bitwise AND across processes
    - BOR: Bitwise OR across processes
    - BXOR: Bitwise XOR across processes
    """
    SUM
    PRODUCT
    MIN
    MAX
    BAND   # Bitwise AND
    BOR    # Bitwise OR
    BXOR   # Bitwise XOR

    fn code(self) -> i32:
        """Convert to FFI code."""
        match self:
            ReduceOp::SUM -> 0
            ReduceOp::PRODUCT -> 1
            ReduceOp::MIN -> 2
            ReduceOp::MAX -> 3
            ReduceOp::BAND -> 4
            ReduceOp::BOR -> 5
            ReduceOp::BXOR -> 6
