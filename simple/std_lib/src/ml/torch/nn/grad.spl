# Neural Network - Gradient Clipping
#
# Gradient clipping utilities for training neural networks.

export clip_grad_value, clip_grad_norm

import ml.torch.tensor_class.{Tensor}

fn clip_grad_value(parameters: [Tensor], clip_value: f64):
    """Clip gradients by value (element-wise clipping).

    Clips each gradient component to the range [-clip_value, clip_value].
    Useful for preventing exploding gradients in RNNs and deep networks.

    Args:
        parameters: List of parameters (tensors with .grad attribute)
        clip_value: Maximum absolute value for gradients

    Example:
        ```simple
        # During training loop
        let model = MyModel()
        let optimizer = optim.Adam(model.parameters(), lr=0.001)

        for batch in training_data:
            optimizer.zero_grad()
            let loss = compute_loss(batch)
            loss.backward()

            # Clip gradients before optimizer step
            nn.clip_grad_value(model.parameters(), clip_value=1.0)
            optimizer.step()
        ```

    Note:
        This modifies gradients in-place. Should be called after backward()
        but before optimizer.step().
    """
    for param in parameters:
        if param.grad is not None:
            # Clamp gradient values to [-clip_value, clip_value]
            let grad_handle = @rt_torch_clip_grad_value(
                param.grad.handle,
                clip_value
            )
            if grad_handle == 0:
                panic("Gradient value clipping failed")
            param.grad = Tensor(grad_handle)


fn clip_grad_norm(parameters: [Tensor], max_norm: f64, norm_type: f64 = 2.0) -> f64:
    """Clip gradients by global norm.

    Computes the norm of all gradients together and scales them down if the
    total norm exceeds max_norm. Preserves gradient direction.

    More commonly used than clip_grad_value as it considers the overall
    gradient magnitude rather than individual components.

    Args:
        parameters: List of parameters (tensors with .grad attribute)
        max_norm: Maximum allowed norm
        norm_type: Type of norm (default: 2.0 for L2 norm)

    Returns:
        Total norm of gradients before clipping

    Example:
        ```simple
        # Typical usage in training loop
        let model = MyModel()
        let optimizer = optim.Adam(model.parameters(), lr=0.001)

        for batch in training_data:
            optimizer.zero_grad()
            let loss = compute_loss(batch)
            loss.backward()

            # Clip by global norm (common for transformers)
            let total_norm = nn.clip_grad_norm(model.parameters(), max_norm=1.0)
            if total_norm > 10.0:
                print("Warning: gradient norm was {total_norm}")

            optimizer.step()
        ```
    """
    # Collect all gradient handles
    let mut grad_handles: [u64] = []
    for param in parameters:
        if param.grad is not None:
            grad_handles.push(param.grad.handle)

    if grad_handles.is_empty():
        return 0.0

    # Call FFI to compute and clip
    let total_norm = @rt_torch_clip_grad_norm(
        grad_handles.data_ptr(),
        grad_handles.len() as i32,
        max_norm,
        norm_type
    )

    return total_norm
