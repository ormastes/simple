# Neural Network - Layers and Modules
#
# Provides neural network building blocks including layers, activation functions,
# normalization layers, and the Module base class for building custom models.
#
# ## Classes
# - `Module`: Base class for all neural network modules
# - `Sequential`: Container for chaining layers
# - `Linear`: Fully connected (dense) layer
# - `Conv2d`: 2D convolutional layer
# - `Dropout`: Dropout regularization layer
# - `BatchNorm1d`: 1D batch normalization
# - `BatchNorm2d`: 2D batch normalization
# - `LayerNorm`: Layer normalization
#
# ## Activation Functions
# - `relu()`, `gelu()`, `silu()`, `tanh()`, `sigmoid()`
# - `mish()`, `elu()`, `softplus()`, `leaky_relu()`
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.nn as nn
#
# class MyModel(nn.Module):
#     fn __init__(self):
#         super().__init__()
#         self.fc1 = nn.Linear(784, 128)
#         self.fc2 = nn.Linear(128, 10)
#
#     fn forward(self, x):
#         x = nn.relu(self.fc1(x))
#         x = self.fc2(x)
#         return x
# ```

# Re-export all components
pub use base.*
pub use linear.*
pub use dropout.*
pub use normalization.*
pub use embedding.*
pub use activations.*
pub use conv.*
pub use recurrent.*
pub use loss.*
pub use transformer.*
pub use grad.*
