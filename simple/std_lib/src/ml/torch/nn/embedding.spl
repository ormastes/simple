# Neural Network - Embedding Layer
#
# Embedding layer for mapping indices to dense vectors.

export Embedding

import ml.torch.tensor_class.{Tensor}
import base.{Module}

class Embedding(Module):
    """Embedding layer for mapping indices to dense vectors.

    Commonly used for word embeddings in NLP and item embeddings in recommendation systems.

    Example:
        ```simple
        # Vocabulary size 1000, embedding dimension 128
        val embedding = nn.Embedding(num_embeddings=1000, embedding_dim=128)

        # Input: [batch_size, sequence_length] of integer indices
        val indices = torch.tensor([[1, 2, 3], [4, 5, 6]])
        val embedded = embedding(indices)  # [batch, seq_len, embedding_dim]
        ```
    """
    module_handle: u64
    num_embeddings: i32
    embedding_dim: i32

    fn __init__(num_embeddings: i32, embedding_dim: i32, padding_idx: i32 = -1):
        """Initialize embedding layer.

        Args:
            num_embeddings: Size of vocabulary (number of unique indices)
            embedding_dim: Dimension of embedding vectors
            padding_idx: If specified, entries at this index are zero vectors (default: -1 = none)
        """
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim

        # Create module via FFI
        self.module_handle = @rt_torch_embedding_new(
            num_embeddings,
            embedding_dim,
            padding_idx
        )
        if self.module_handle == 0:
            panic("Failed to create Embedding layer")

    fn __del__():
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(indices: Tensor) -> Tensor:
        """Look up embeddings for input indices.

        Args:
            indices: Integer tensor of indices [batch, seq_len] or [batch]

        Returns:
            Embedded tensor [batch, seq_len, embedding_dim] or [batch, embedding_dim]
        """
        val handle = @rt_torch_embedding_forward(self.module_handle, indices.handle)
        if handle == 0:
            panic("Embedding forward pass failed")
        return Tensor(handle)
