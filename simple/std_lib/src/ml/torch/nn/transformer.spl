# Transformer Components
#
# Provides transformer architecture building blocks including multi-head attention,
# positional encoding, and encoder/decoder layers.
#
# ## Classes
# - `MultiheadAttention`: Multi-head attention mechanism
# - `PositionalEncoding`: Positional encoding for sequence models
# - `TransformerEncoderLayer`: Single transformer encoder layer
# - `TransformerDecoderLayer`: Single transformer decoder layer
#
# ## Example
# ```simple
# import ml.torch as torch
# import ml.torch.nn as nn
# from ml.torch.nn.transformer import MultiheadAttention, TransformerEncoderLayer
#
# # Self-attention
# let mha = MultiheadAttention(embed_dim=512, num_heads=8)
# let output, attn_weights = mha(query, key, value)
#
# # Encoder layer
# let encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)
# let encoded = encoder_layer(src)
# ```

export MultiheadAttention, PositionalEncoding
export TransformerEncoderLayer, TransformerDecoderLayer

import base.{Module}


# ============================================================================
# Multi-head Attention
# ============================================================================

class MultiheadAttention(Module):
    """Multi-head attention mechanism.

    Core component of Transformers for attending to different positions.

    Example:
        ```simple
        # Self-attention
        let mha = nn.MultiheadAttention(embed_dim=512, num_heads=8)
        let output, attn_weights = mha(query, key, value)
        ```
    """
    module_handle: u64
    embed_dim: i32
    num_heads: i32

    fn __init__(self, embed_dim: i32, num_heads: i32, dropout: f64 = 0.0):
        """Initialize multi-head attention.

        Args:
            embed_dim: Embedding dimension (must be divisible by num_heads)
            num_heads: Number of attention heads
            dropout: Dropout probability (default: 0.0)
        """
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads

        # Create module via FFI
        self.module_handle = @rt_torch_multihead_attention_new(
            embed_dim,
            num_heads,
            dropout
        )
        if self.module_handle == 0:
            panic("Failed to create MultiheadAttention")

    fn __del__(self):
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(self, query: Tensor, key: Tensor, value: Tensor, attn_mask: Tensor = None) -> (Tensor, Tensor):
        """Apply multi-head attention.

        Args:
            query: Query tensor [seq_len, batch, embed_dim]
            key: Key tensor [seq_len, batch, embed_dim]
            value: Value tensor [seq_len, batch, embed_dim]
            attn_mask: Attention mask (default: None)

        Returns:
            (output, attn_weights): Output and attention weights
        """
        let mask_handle = 0u64
        if attn_mask is not None:
            mask_handle = attn_mask.handle

        let mut out_handle = 0u64
        let mut attn_handle = 0u64

        @rt_torch_multihead_attention_forward(
            self.module_handle,
            query.handle,
            key.handle,
            value.handle,
            mask_handle,
            &out_handle,
            &attn_handle
        )

        if out_handle == 0 or attn_handle == 0:
            panic("MultiheadAttention forward failed")

        return (Tensor(out_handle), Tensor(attn_handle))


# ============================================================================
# Positional Encoding
# ============================================================================

class PositionalEncoding(Module):
    """Positional encoding for Transformer models.

    Adds positional information to token embeddings using sine/cosine functions.

    Example:
        ```simple
        let pe = nn.PositionalEncoding(d_model=512, max_len=5000)
        let x = embedding(tokens)  # [seq_len, batch, d_model]
        let x_with_pos = pe(x)
        ```
    """
    d_model: i32
    max_len: i32
    pe_table: Tensor

    fn __init__(self, d_model: i32, max_len: i32 = 5000, dropout: f64 = 0.1):
        """Initialize positional encoding.

        Args:
            d_model: Embedding dimension
            max_len: Maximum sequence length (default: 5000)
            dropout: Dropout probability (default: 0.1)
        """
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len

        # Create positional encoding table via FFI
        let handle = @rt_torch_positional_encoding_new(d_model, max_len)
        if handle == 0:
            panic("Failed to create PositionalEncoding")
        self.pe_table = Tensor(handle)

    fn forward(self, x: Tensor) -> Tensor:
        """Add positional encoding to input.

        Args:
            x: Input tensor [seq_len, batch, d_model]

        Returns:
            Output tensor with positional encoding added
        """
        # Add positional encoding (broadcasting)
        let seq_len = x.shape()[0]
        let pe_slice = self.pe_table[0..seq_len]
        return x + pe_slice


# ============================================================================
# Transformer Encoder Layer
# ============================================================================

class TransformerEncoderLayer(Module):
    """Single Transformer encoder layer.

    Consists of multi-head self-attention and feedforward network.

    Example:
        ```simple
        let layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=2048)
        let output = layer(src)
        ```
    """
    module_handle: u64
    d_model: i32
    nhead: i32

    fn __init__(self, d_model: i32, nhead: i32, dim_feedforward: i32 = 2048, dropout: f64 = 0.1):
        """Initialize Transformer encoder layer.

        Args:
            d_model: Embedding dimension
            nhead: Number of attention heads
            dim_feedforward: Dimension of feedforward network (default: 2048)
            dropout: Dropout probability (default: 0.1)
        """
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead

        # Create module via FFI
        self.module_handle = @rt_torch_transformer_encoder_layer_new(
            d_model,
            nhead,
            dim_feedforward,
            dropout
        )
        if self.module_handle == 0:
            panic("Failed to create TransformerEncoderLayer")

    fn __del__(self):
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:
        """Apply Transformer encoder layer.

        Args:
            src: Source tensor [seq_len, batch, d_model]
            src_mask: Source mask (default: None)

        Returns:
            Output tensor [seq_len, batch, d_model]
        """
        let mask_handle = 0u64
        if src_mask is not None:
            mask_handle = src_mask.handle

        let handle = @rt_torch_transformer_encoder_layer_forward(
            self.module_handle,
            src.handle,
            mask_handle
        )

        if handle == 0:
            panic("TransformerEncoderLayer forward failed")

        return Tensor(handle)


# ============================================================================
# Transformer Decoder Layer
# ============================================================================

class TransformerDecoderLayer(Module):
    """Single Transformer decoder layer.

    Consists of masked self-attention, encoder-decoder attention, and feedforward network.

    Example:
        ```simple
        let layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, dim_feedforward=2048)
        let output = layer(tgt, memory)
        ```
    """
    module_handle: u64
    d_model: i32
    nhead: i32

    fn __init__(self, d_model: i32, nhead: i32, dim_feedforward: i32 = 2048, dropout: f64 = 0.1):
        """Initialize Transformer decoder layer.

        Args:
            d_model: Embedding dimension
            nhead: Number of attention heads
            dim_feedforward: Dimension of feedforward network (default: 2048)
            dropout: Dropout probability (default: 0.1)
        """
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead

        # Create module via FFI
        self.module_handle = @rt_torch_transformer_decoder_layer_new(
            d_model,
            nhead,
            dim_feedforward,
            dropout
        )
        if self.module_handle == 0:
            panic("Failed to create TransformerDecoderLayer")

    fn __del__(self):
        """Free module resources."""
        if self.module_handle != 0:
            @rt_torch_module_free(self.module_handle)

    fn forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor = None, memory_mask: Tensor = None) -> Tensor:
        """Apply Transformer decoder layer.

        Args:
            tgt: Target tensor [seq_len, batch, d_model]
            memory: Memory tensor from encoder [seq_len, batch, d_model]
            tgt_mask: Target mask (default: None)
            memory_mask: Memory mask (default: None)

        Returns:
            Output tensor [seq_len, batch, d_model]
        """
        let tgt_mask_handle = 0u64
        let memory_mask_handle = 0u64

        if tgt_mask is not None:
            tgt_mask_handle = tgt_mask.handle
        if memory_mask is not None:
            memory_mask_handle = memory_mask.handle

        let handle = @rt_torch_transformer_decoder_layer_forward(
            self.module_handle,
            tgt.handle,
            memory.handle,
            tgt_mask_handle,
            memory_mask_handle
        )

        if handle == 0:
            panic("TransformerDecoderLayer forward failed")

        return Tensor(handle)


# ============================================================================
# External FFI Functions
# ============================================================================

extern fn rt_torch_multihead_attention_new(embed_dim: i32, num_heads: i32, dropout: f64) -> u64
extern fn rt_torch_multihead_attention_forward(module: u64, query: u64, key: u64, value: u64, attn_mask: u64, out_ptr: *u64, attn_ptr: *u64) -> i32

extern fn rt_torch_positional_encoding_new(d_model: i32, max_len: i32) -> u64

extern fn rt_torch_transformer_encoder_layer_new(d_model: i32, nhead: i32, dim_feedforward: i32, dropout: f64) -> u64
extern fn rt_torch_transformer_encoder_layer_forward(module: u64, src: u64, src_mask: u64) -> u64

extern fn rt_torch_transformer_decoder_layer_new(d_model: i32, nhead: i32, dim_feedforward: i32, dropout: f64) -> u64
extern fn rt_torch_transformer_decoder_layer_forward(module: u64, tgt: u64, memory: u64, tgt_mask: u64, memory_mask: u64) -> u64

extern fn rt_torch_module_free(module: u64) -> i32
