# Tensor - Multi-dimensional Array with GPU Support (Simplified)

export Tensor

import device.{Device, device_code}
import dtype.{DType, dtype_code}
import tensor_ffi.{rt_torch_zeros, rt_torch_ones, rt_torch_randn, rt_torch_arange, rt_torch_free, rt_torch_clone, rt_torch_shape, rt_torch_numel, rt_torch_dtype, rt_torch_device, rt_torch_add, rt_torch_sub, rt_torch_mul, rt_torch_div, rt_torch_matmul, rt_torch_reshape, rt_torch_sum, rt_torch_mean, rt_torch_item, rt_torch_set_requires_grad, rt_torch_requires_grad, rt_torch_backward, rt_torch_grad, rt_torch_detach}

class Tensor:
    """Multi-dimensional array with GPU support."""
    handle: u64

    fn __init__(handle: u64):
        """Initialize tensor from handle."""
        self.handle = handle

    fn __del__():
        """Free tensor memory."""
        if self.handle != 0:
            rt_torch_free(self.handle)

    # Factory Methods
    static fn zeros(shape: [i64], dtype: DType = DType::Float32, device: Device = Device::CPU) -> Tensor:
        """Create tensor filled with zeros."""
        val handle = rt_torch_zeros(shape.data_ptr(), shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    static fn ones(shape: [i64], dtype: DType = DType::Float32, device: Device = Device::CPU) -> Tensor:
        """Create tensor filled with ones."""
        val handle = rt_torch_ones(shape.data_ptr(), shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    static fn randn(shape: [i64], dtype: DType = DType::Float32, device: Device = Device::CPU) -> Tensor:
        """Create tensor with random values from N(0, 1)."""
        val handle = rt_torch_randn(shape.data_ptr(), shape.len() as i32, dtype_code(dtype), device_code(device))
        return Tensor(handle)

    # Properties
    fn numel() -> i64:
        """Get total number of elements."""
        return rt_torch_numel(self.handle)

    # Arithmetic Operations
    fn add(other: Tensor) -> Tensor:
        """Add two tensors."""
        return Tensor(rt_torch_add(self.handle, other.handle))

    fn sub(other: Tensor) -> Tensor:
        """Subtract two tensors."""
        return Tensor(rt_torch_sub(self.handle, other.handle))

    fn mul(other: Tensor) -> Tensor:
        """Multiply two tensors."""
        return Tensor(rt_torch_mul(self.handle, other.handle))

    fn div(other: Tensor) -> Tensor:
        """Divide two tensors."""
        return Tensor(rt_torch_div(self.handle, other.handle))

    fn matmul(other: Tensor) -> Tensor:
        """Matrix multiply two tensors."""
        return Tensor(rt_torch_matmul(self.handle, other.handle))

    # Reduction Operations
    fn sum() -> Tensor:
        """Sum all elements."""
        return Tensor(rt_torch_sum(self.handle, -1, 0))

    fn mean() -> Tensor:
        """Mean of all elements."""
        return Tensor(rt_torch_mean(self.handle, -1, 0))

    # Data Access
    fn item() -> f64:
        """Get scalar value (for single-element tensors)."""
        return rt_torch_item(self.handle)

    fn clone() -> Tensor:
        """Create a copy of this tensor."""
        return Tensor(rt_torch_clone(self.handle))

    # Autograd
    fn set_requires_grad(requires_grad: bool):
        """Enable or disable gradient tracking."""
        rt_torch_set_requires_grad(self.handle, requires_grad as i32)

    fn requires_grad() -> bool:
        """Check if gradients are tracked."""
        return rt_torch_requires_grad(self.handle) != 0

    fn backward():
        """Compute gradients via backpropagation."""
        rt_torch_backward(self.handle, 0u64, 0)

    fn grad() -> Tensor:
        """Get accumulated gradients."""
        return Tensor(rt_torch_grad(self.handle))

    fn detach() -> Tensor:
        """Create a tensor detached from computation graph."""
        return Tensor(rt_torch_detach(self.handle))
