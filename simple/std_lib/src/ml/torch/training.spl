# Training Utilities
#
# Classes for tracking training progress and preventing overfitting.

export ParameterStats, ParameterTracker, EarlyStopping

import tensor_class.{Tensor}


# ============================================================================
# Parameter Statistics
# ============================================================================

class ParameterStats:
    """Statistics for a parameter (weights/gradients).

    Attributes:
        name: Parameter name
        shape: Parameter shape
        mean: Mean value
        std: Standard deviation
        min: Minimum value
        max: Maximum value
        norm: L2 norm
        grad_norm: Gradient L2 norm (if gradient exists)
    """
    name: str
    shape: [i64]
    mean: f64
    std: f64
    min: f64
    max: f64
    norm: f64
    grad_norm: f64

    fn __init__(name: str,
        shape: [i64],
        mean: f64,
        std: f64,
        min: f64,
        max: f64,
        norm: f64,
        grad_norm: f64 = 0.0
    ):
        """Initialize parameter stats.

        Args:
            name: Parameter name
            shape: Parameter shape
            mean: Mean value
            std: Standard deviation
            min: Minimum value
            max: Maximum value
            norm: L2 norm
            grad_norm: Gradient norm (default: 0.0)
        """
        self.name = name
        self.shape = shape
        self.mean = mean
        self.std = std
        self.min = min
        self.max = max
        self.norm = norm
        self.grad_norm = grad_norm

    fn __str__() -> str:
        """Format stats as string."""
        return "{self.name}: mean={self.mean:.4f}, std={self.std:.4f}, " +
               "min={self.min:.4f}, max={self.max:.4f}, " +
               "norm={self.norm:.4f}, grad_norm={self.grad_norm:.4f}"


# ============================================================================
# Parameter Tracker
# ============================================================================

class ParameterTracker:
    """Track parameter and gradient statistics during training.

    Tracks mean, std, min, max, and norms for parameters and gradients.
    Useful for debugging training issues like vanishing/exploding gradients.

    Example:
        ```simple
        import ml.torch as torch
        import ml.torch.nn as nn

        val model = nn.Sequential([
            nn.Linear(10, 20),
            nn.ReLU(),
            nn.Linear(20, 1)
        ])

        val tracker = torch.ParameterTracker()

        # Track parameters before training
        val params = model.parameters()
        tracker.track_parameters(params, "init")

        # ... training loop ...

        # Track after training
        tracker.track_parameters(params, "final")

        # Print statistics
        tracker.print_summary()
        ```
    """
    parameter_history: [[ParameterStats]]
    step_names: [str]

    fn __init__():
        """Initialize parameter tracker."""
        self.parameter_history = []
        self.step_names = []

    fn track_parameters(params: [Tensor], step_name: str = "step"):
        """Track statistics for a list of parameters.

        Args:
            params: List of parameter tensors
            step_name: Name for this tracking step (default: "step")
        """
        val stats = []
        for i in range(params.len()):
            val param = params[i]
            val param_stats = ParameterTracker::compute_stats(
                "param_{i}",
                param
            )
            stats.append(param_stats)

        self.parameter_history.append(stats)
        self.step_names.append(step_name)

    fn track_named_parameters(params: [(str, Tensor)],
        step_name: str = "step"
    ):
        """Track statistics for named parameters.

        Args:
            params: List of (name, tensor) tuples
            step_name: Name for this tracking step (default: "step")
        """
        val stats = []
        for (name, param) in params:
            val param_stats = ParameterTracker::compute_stats(name, param)
            stats.append(param_stats)

        self.parameter_history.append(stats)
        self.step_names.append(step_name)

    @staticmethod
    fn compute_stats(name: str, tensor: Tensor) -> ParameterStats:
        """Compute statistics for a single tensor.

        Args:
            name: Parameter name
            tensor: Tensor to analyze

        Returns:
            Parameter statistics
        """
        # Get tensor properties
        val shape = tensor.shape()

        # Compute statistics using PyTorch operations
        # Note: These require additional FFI functions to be implemented
        val mean = ParameterTracker::_compute_mean(tensor)
        val std = ParameterTracker::_compute_std(tensor)
        val min = ParameterTracker::_compute_min(tensor)
        val max = ParameterTracker::_compute_max(tensor)
        val norm = ParameterTracker::_compute_norm(tensor)

        return ParameterStats(
            name=name,
            shape=shape,
            mean=mean,
            std=std,
            min=min,
            max=max,
            norm=norm,
            grad_norm=0.0
        )

    @staticmethod
    fn _compute_mean(tensor: Tensor) -> f64:
        """Compute mean of tensor using PyTorch backend."""
        @extern("runtime", "rt_torch_mean")
        fn _rt_torch_mean(tensor_handle: u64) -> u64

        @extern("runtime", "rt_torch_item")
        fn _rt_torch_item(tensor_handle: u64) -> f64

        # Get mean as scalar tensor, then extract value
        val mean_handle = _rt_torch_mean(tensor.handle)
        if mean_handle == 0:
            return 0.0

        return _rt_torch_item(mean_handle)

    @staticmethod
    fn _compute_std(tensor: Tensor) -> f64:
        """Compute standard deviation of tensor using PyTorch backend."""
        @extern("runtime", "rt_torch_std")
        fn _rt_torch_std(tensor_handle: u64, dim: i32, keepdim: i32, unbiased: i32) -> u64

        @extern("runtime", "rt_torch_item")
        fn _rt_torch_item(tensor_handle: u64) -> f64

        # Compute std over all dimensions (dim=-1), unbiased=1
        val std_handle = _rt_torch_std(tensor.handle, -1, 0, 1)
        if std_handle == 0:
            return 0.0

        return _rt_torch_item(std_handle)

    @staticmethod
    fn _compute_min(tensor: Tensor) -> f64:
        """Compute minimum value of tensor using PyTorch backend."""
        @extern("runtime", "rt_torch_min")
        fn _rt_torch_min(tensor_handle: u64) -> u64

        @extern("runtime", "rt_torch_item")
        fn _rt_torch_item(tensor_handle: u64) -> f64

        # Get min as scalar tensor, then extract value
        val min_handle = _rt_torch_min(tensor.handle)
        if min_handle == 0:
            return 0.0

        return _rt_torch_item(min_handle)

    @staticmethod
    fn _compute_max(tensor: Tensor) -> f64:
        """Compute maximum value of tensor using PyTorch backend."""
        @extern("runtime", "rt_torch_max")
        fn _rt_torch_max(tensor_handle: u64) -> u64

        @extern("runtime", "rt_torch_item")
        fn _rt_torch_item(tensor_handle: u64) -> f64

        # Get max as scalar tensor, then extract value
        val max_handle = _rt_torch_max(tensor.handle)
        if max_handle == 0:
            return 0.0

        return _rt_torch_item(max_handle)

    @staticmethod
    fn _compute_norm(tensor: Tensor) -> f64:
        """Compute L2 norm of tensor using PyTorch backend."""
        @extern("runtime", "rt_torch_norm")
        fn _rt_torch_norm(tensor_handle: u64, p: f64, dim: i32, keepdim: i32) -> u64

        @extern("runtime", "rt_torch_item")
        fn _rt_torch_item(tensor_handle: u64) -> f64

        # Compute L2 norm (p=2.0) over all dimensions (dim=-1)
        val norm_handle = _rt_torch_norm(tensor.handle, 2.0, -1, 0)
        if norm_handle == 0:
            return 0.0

        return _rt_torch_item(norm_handle)

    fn get_stats_at_step(step_index: i32) -> [ParameterStats]:
        """Get parameter statistics at a specific step.

        Args:
            step_index: Index of the tracking step

        Returns:
            List of parameter statistics at that step
        """
        if step_index < 0 or step_index >= self.parameter_history.len():
            return []
        return self.parameter_history[step_index]

    fn get_latest_stats() -> [ParameterStats]:
        """Get the most recent parameter statistics.

        Returns:
            List of parameter statistics from latest tracking
        """
        if self.parameter_history.len() == 0:
            return []
        return self.parameter_history[self.parameter_history.len() - 1]

    fn print_summary():
        """Print summary of all tracked statistics."""
        print("=" * 80)
        print("Parameter Tracking Summary")
        print("=" * 80)

        for step_idx in range(self.parameter_history.len()):
            val step_name = self.step_names[step_idx]
            val stats = self.parameter_history[step_idx]

            print("\nStep: {step_name}")
            print("-" * 80)

            for param_stats in stats:
                print("  {param_stats}")

        print("=" * 80)

    fn check_for_issues() -> [str]:
        """Check for common training issues.

        Returns:
            List of warning messages about potential issues
        """
        val warnings = []
        val latest = self.get_latest_stats()

        for stats in latest:
            # Check for exploding parameters
            if stats.norm > 1000.0:
                warnings.append(
                    "WARNING: {stats.name} has large norm {stats.norm:.2f} (possible explosion)"
                )

            # Check for vanishing parameters
            if stats.norm < 0.0001:
                warnings.append(
                    "WARNING: {stats.name} has very small norm {stats.norm:.6f} (possible vanishing)"
                )

            # Check for NaN/Inf
            if stats.mean.is_nan() or stats.mean.is_inf():
                warnings.append(
                    "ERROR: {stats.name} contains NaN or Inf values"
                )

        return warnings


# ============================================================================
# Early Stopping
# ============================================================================

class EarlyStopping:
    """Early stopping callback to prevent overfitting.

    Monitors a validation metric and stops training when the metric stops improving
    for a specified number of epochs (patience).

    Attributes:
        patience: Number of epochs to wait for improvement before stopping
        mode: "min" for metrics to minimize (loss), "max" for metrics to maximize (accuracy)
        delta: Minimum change to qualify as an improvement
        best_score: Best metric value seen so far
        counter: Number of epochs without improvement
        early_stop: Whether early stopping has been triggered
        best_epoch: Epoch where best score was achieved
        best_model_state: Saved state dict of best model (if restore_best_weights=true)

    Example:
        ```simple
        val early_stopping = EarlyStopping(patience=5, mode="min", delta=0.001)

        for epoch in range(100):
            # Training...
            train_loss = train_one_epoch(model, train_loader)

            # Validation
            val_loss = validate(model, val_loader)

            # Check early stopping
            early_stopping.step(val_loss, model)

            if early_stopping.early_stop:
                print("Early stopping triggered at epoch {epoch}")
                break

        # Restore best model weights
        if early_stopping.best_model_state is not None:
            model.load_state_dict(early_stopping.best_model_state)
        ```
    """
    patience: i64
    mode: str  # "min" or "max"
    delta: f64
    best_score: f64
    counter: i64
    early_stop: bool
    best_epoch: i64
    best_model_state: {str: any}
    restore_best_weights: bool
    verbose: bool

    fn __init__(patience: i64 = 5,
        mode: str = "min",
        delta: f64 = 0.0,
        restore_best_weights: bool = true,
        verbose: bool = false
    ):
        """Initialize early stopping.

        Args:
            patience: Number of epochs to wait for improvement (default: 5)
            mode: "min" for loss metrics, "max" for accuracy metrics (default: "min")
            delta: Minimum change to qualify as improvement (default: 0.0)
            restore_best_weights: Whether to save/restore best model (default: true)
            verbose: Whether to print status messages (default: false)
        """
        self.patience = patience
        self.mode = mode
        self.delta = delta
        self.restore_best_weights = restore_best_weights
        self.verbose = verbose

        self.counter = 0
        self.early_stop = false
        self.best_epoch = 0
        self.best_model_state = {}

        # Initialize best_score based on mode
        if mode == "min":
            self.best_score = f64::INFINITY
        elif mode == "max":
            self.best_score = f64::NEG_INFINITY
        else:
            panic("Invalid mode '{mode}'. Use 'min' or 'max'")

    fn step(current_score: f64, model: any = None, epoch: i64 = 0):
        """Update early stopping state with new validation metric.

        Args:
            current_score: Current validation metric value
            model: Model to save if this is the best score (optional)
            epoch: Current epoch number (for tracking)
        """
        val improved = self._is_improvement(current_score)

        if improved:
            # New best score
            self.best_score = current_score
            self.counter = 0
            self.best_epoch = epoch

            if self.verbose:
                print("EarlyStopping: New best score: {current_score:.6f} at epoch {epoch}")

            # Save model state if requested
            if self.restore_best_weights and model is not None:
                self.best_model_state = model.state_dict()
                if self.verbose:
                    print("EarlyStopping: Saved best model weights")

        else:
            # No improvement
            self.counter += 1

            if self.verbose:
                print("EarlyStopping: No improvement for {self.counter}/{self.patience} epochs")

            # Check if patience exceeded
            if self.counter >= self.patience:
                self.early_stop = true
                if self.verbose:
                    print("EarlyStopping: Triggered! Best score: {self.best_score:.6f} at epoch {self.best_epoch}")

    fn _is_improvement(current_score: f64) -> bool:
        """Check if current score is an improvement over best score.

        Args:
            current_score: Current metric value

        Returns:
            True if improved, False otherwise
        """
        if self.mode == "min":
            # For minimization (e.g., loss)
            return current_score < (self.best_score - self.delta)
        else:
            # For maximization (e.g., accuracy)
            return current_score > (self.best_score + self.delta)

    fn reset():
        """Reset early stopping state to initial conditions."""
        self.counter = 0
        self.early_stop = false
        self.best_epoch = 0
        self.best_model_state = {}

        if self.mode == "min":
            self.best_score = f64::INFINITY
        else:
            self.best_score = f64::NEG_INFINITY

        if self.verbose:
            print("EarlyStopping: Reset to initial state")

    fn should_stop() -> bool:
        """Check if training should stop.

        Returns:
            True if early stopping triggered, False otherwise
        """
        return self.early_stop

    fn get_best_score() -> f64:
        """Get the best metric value seen.

        Returns:
            Best metric value
        """
        return self.best_score

    fn get_best_epoch() -> i64:
        """Get the epoch where best score was achieved.

        Returns:
            Epoch number of best score
        """
        return self.best_epoch

    fn epochs_since_improvement() -> i64:
        """Get number of epochs without improvement.

        Returns:
            Count of epochs since last improvement
        """
        return self.counter
