# Tensor FFI - External Function Declarations
#
# All FFI bindings to the PyTorch C++ runtime for tensor operations.
# These functions are called by the Tensor class methods.

# ============================================================================
# Tensor Creation
# ============================================================================

extern fn rt_torch_zeros(shape_ptr: *i64, ndim: i32, dtype: i32, device: i32) -> u64
extern fn rt_torch_ones(shape_ptr: *i64, ndim: i32, dtype: i32, device: i32) -> u64
extern fn rt_torch_randn(shape_ptr: *i64, ndim: i32, dtype: i32, device: i32) -> u64
extern fn rt_torch_arange(start: i64, end: i64, step: i64, dtype: i32, device: i32) -> u64
extern fn rt_torch_free(handle: u64) -> i32
extern fn rt_torch_clone(handle: u64) -> u64

# ============================================================================
# Properties
# ============================================================================

extern fn rt_torch_shape(handle: u64, buf_ptr: *i64, buf_len: i32) -> i32
extern fn rt_torch_numel(handle: u64) -> i64
extern fn rt_torch_dtype(handle: u64) -> i32
extern fn rt_torch_device(handle: u64) -> i32

# ============================================================================
# Arithmetic Operations
# ============================================================================

extern fn rt_torch_add(a: u64, b: u64) -> u64
extern fn rt_torch_sub(a: u64, b: u64) -> u64
extern fn rt_torch_mul(a: u64, b: u64) -> u64
extern fn rt_torch_div(a: u64, b: u64) -> u64
extern fn rt_torch_matmul(a: u64, b: u64) -> u64
extern fn rt_torch_add_scalar(handle: u64, scalar: f64) -> u64
extern fn rt_torch_mul_scalar(handle: u64, scalar: f64) -> u64
extern fn rt_torch_sqrt(handle: u64) -> u64

# ============================================================================
# Shape Operations
# ============================================================================

extern fn rt_torch_reshape(handle: u64, shape_ptr: *i64, ndim: i32) -> u64
extern fn rt_torch_transpose(handle: u64, dim0: i64, dim1: i64) -> u64

# ============================================================================
# Device Transfer
# ============================================================================

extern fn rt_torch_to_device(handle: u64, device: i32) -> u64
extern fn rt_torch_to_cpu(handle: u64) -> u64
extern fn rt_torch_to_cuda(handle: u64, device_id: i32) -> u64

# ============================================================================
# Reduction Operations
# ============================================================================

extern fn rt_torch_sum(handle: u64, dim: i32, keepdim: i32) -> u64
extern fn rt_torch_mean(handle: u64, dim: i32, keepdim: i32) -> u64
extern fn rt_torch_max(handle: u64, dim: i32, keepdim: i32) -> u64
extern fn rt_torch_min(handle: u64, dim: i32, keepdim: i32) -> u64
extern fn rt_torch_std(handle: u64, dim: i32, keepdim: i32, unbiased: i32) -> u64
extern fn rt_torch_var(handle: u64, dim: i32, keepdim: i32, unbiased: i32) -> u64
extern fn rt_torch_norm(handle: u64, p: f64, dim: i32, keepdim: i32) -> u64

# ============================================================================
# Data Access
# ============================================================================

extern fn rt_torch_item(handle: u64) -> f64

# ============================================================================
# Indexing
# ============================================================================

extern fn rt_torch_index(handle: u64, index: i64) -> u64
extern fn rt_torch_slice(handle: u64, dim: i32, start: i64, end: i64, step: i64) -> u64
extern fn rt_torch_select(handle: u64, dim: i32, index: i64) -> u64
extern fn rt_torch_narrow(handle: u64, dim: i32, start: i64, length: i64) -> u64

# ============================================================================
# Autograd
# ============================================================================

extern fn rt_torch_set_requires_grad(handle: u64, requires_grad: i32) -> i32
extern fn rt_torch_requires_grad(handle: u64) -> i32
extern fn rt_torch_backward(handle: u64, gradient: u64, retain_graph: i32) -> i32
extern fn rt_torch_grad(handle: u64) -> u64
extern fn rt_torch_zero_grad(handle: u64) -> i32
extern fn rt_torch_detach(handle: u64) -> u64

# ============================================================================
# Comparison Operations
# ============================================================================

extern fn rt_torch_gt(a: u64, b: u64) -> u64
extern fn rt_torch_allclose(a: u64, b: u64, rtol: f64, atol: f64) -> i32

# ============================================================================
# Encoding Operations
# ============================================================================

extern fn rt_torch_one_hot(handle: u64, num_classes: i64) -> u64
