# Tree-sitter Parser for Simple Language
# Recursive descent parser using grammar rules

import core.{Option, Result}
import parser.treesitter.tree.{Tree, Node, NodeId, Span, NodeArena}
import parser.treesitter.grammar.{Grammar, GrammarRule, Token, TokenKind, build_simple_grammar}
import parser.treesitter.lexer.{Lexer}
import parser.treesitter.edits.{InputEdit, Point, compute_edits, find_affected_nodes, find_reparse_boundary}

# Parser state
struct TreeSitterParser:
    grammar: Grammar
    tokens: <Token>
    pos: i64
    arena: NodeArena

    # Create new parser for Simple language
    fn new(language: str) -> Result<TreeSitterParser, str>:
        if language != "simple":
            return Err("Unsupported language: " + language)

        return Ok(TreeSitterParser(
            grammar: build_simple_grammar(),
            tokens: [],
            pos: 0,
            arena: NodeArena.new()
        ))

    # Parse source code into tree
    var fn parse(source: str) -> Result<Tree, str>:
        # Tokenize source
        var lexer = Lexer.new(source)
        self.tokens = lexer.tokenize()?
        self.pos = 0

        # Parse module (entry point)
        val root_rule = self.grammar.rules.get("module").ok_or("Missing module rule")?
        val root_id = self.parse_rule(root_rule)?

        # Create tree
        return Ok(Tree(
            root_node: root_id,
            arena: self.arena,
            source: source,
            version: 0
        ))

    # Parse source code incrementally (reuse unchanged subtrees)
    var fn parse_incremental(source: str,
        old_tree: Tree,
        edits: <InputEdit>
    ) -> Result<Tree, str>:
        if edits.len() == 0:
            # No changes: return old tree with updated source
            return Ok(Tree(
                root_node: old_tree.root_node,
                arena: old_tree.arena,
                source: source,
                version: old_tree.version + 1
            ))

        # Find nodes affected by edits
        val affected_nodes = find_affected_nodes(old_tree, edits)

        if affected_nodes.len() == 0:
            # No nodes affected (shouldn't happen with edits, but handle gracefully)
            return Ok(Tree(
                root_node: old_tree.root_node,
                arena: old_tree.arena,
                source: source,
                version: old_tree.version + 1
            ))

        # Find the minimal stable parent (reparse boundary)
        val reparse_boundary = find_reparse_boundary(old_tree, affected_nodes)

        # Clone old arena for structural sharing
        self.arena = old_tree.arena.clone()

        # Tokenize new source
        var lexer = Lexer.new(source)
        self.tokens = lexer.tokenize()?
        self.pos = 0

        match reparse_boundary:
            case Some(boundary_id):
                # Get the boundary node to find its position in token stream
                match old_tree.arena.get(boundary_id):
                    case Some(boundary_node):
                        # Adjust token position to start of boundary node
                        self.pos = self.find_token_at_byte(boundary_node.span.start_byte)

                        # Reparse from the boundary
                        val reparsed_id = self.reparse_subtree(boundary_node, edits)?

                        # If boundary was root, update root
                        if boundary_id == old_tree.root_node:
                            return Ok(Tree(
                                root_node: reparsed_id,
                                arena: self.arena,
                                source: source,
                                version: old_tree.version + 1
                            ))
                        else:
                            # Splice reparsed subtree into parent
                            self.splice_subtree(old_tree.root_node, boundary_id, reparsed_id)
                            return Ok(Tree(
                                root_node: old_tree.root_node,
                                arena: self.arena,
                                source: source,
                                version: old_tree.version + 1
                            ))

                    case None:
                        # Boundary node not found, fall back to full reparse
                        pass

            case None:
                pass

        # Fall back to full reparse
        self.pos = 0
        self.arena = NodeArena.new()
        val root_rule = self.grammar.rules.get("module").ok_or("Missing module rule")?
        val root_id = self.parse_rule(root_rule)?

        return Ok(Tree(
            root_node: root_id,
            arena: self.arena,
            source: source,
            version: old_tree.version + 1
        ))

    # Find token index at a given byte offset
    fn find_token_at_byte(byte_offset: i64) -> i64:
        for i in 0..self.tokens.len():
            if self.tokens[i].span.start_byte >= byte_offset:
                return i
        return self.tokens.len()

    # Reparse a subtree starting from a node
    var fn reparse_subtree(old_node: Node, edits: <InputEdit>) -> Result<NodeId, str>:
        # Determine what grammar rule to use based on node kind
        val rule_name = self.node_kind_to_rule(old_node.kind)

        match self.grammar.rules.get(rule_name):
            case Some(rule):
                return self.parse_rule(rule)
            case None:
                # Unknown rule, try to parse as expression
                match self.grammar.rules.get("expression"):
                    case Some(expr_rule):
                        return self.parse_rule(expr_rule)
                    case None:
                        return Err("Cannot find rule for node kind: " + old_node.kind)

    # Map node kind back to grammar rule name
    fn node_kind_to_rule(kind: str) -> str:
        match kind:
            case "module": return "module"
            case "function_definition": return "function_def"
            case "struct_definition": return "struct_def"
            case "class_definition": return "class_def"
            case "enum_definition": return "enum_def"
            case "statement": return "statement"
            case "expression": return "expression"
            case "block": return "block"
            case _: return kind  # Try the kind name directly

    # Splice a reparsed subtree into the tree, replacing old subtree
    var fn splice_subtree(root_id: NodeId, old_subtree_id: NodeId, new_subtree_id: NodeId):
        # Find parent of old subtree and replace child
        self.splice_recursive(root_id, old_subtree_id, new_subtree_id)

    var fn splice_recursive(node_id: NodeId, target_id: NodeId, replacement_id: NodeId) -> bool:
        match self.arena.get_mut(node_id):
            case Some(node):
                # Check if any child is the target
                for i in 0..node.children.len():
                    if node.children[i] == target_id:
                        node.children[i] = replacement_id
                        return true

                    # Recurse into children
                    if self.splice_recursive(node.children[i], target_id, replacement_id):
                        return true

                return false
            case None:
                return false

    # Get current token
    fn current() -> Option<Token>:
        if self.pos >= 0 and self.pos < self.tokens.len():
            return Some(self.tokens[self.pos])
        else:
            return None

    # Advance to next token
    var fn advance():
        self.pos = self.pos + 1

    # Peek ahead N tokens
    fn peek(n: i64) -> Option<Token>:
        val idx = self.pos + n
        if idx >= 0 and idx < self.tokens.len():
            return Some(self.tokens[idx])
        else:
            return None

    # Check if at end of input
    fn is_at_end() -> bool:
        return self.pos >= self.tokens.len()

    # Expect specific token kind
    var fn expect(expected: TokenKind) -> Result<Token, str>:
        match self.current():
            case Some(token):
                if self.matches_token(token.kind, expected):
                    self.advance()
                    return Ok(token)
                else:
                    return Err("Expected " + self.token_kind_name(expected) +
                              ", found " + self.token_kind_name(token.kind))
            case None:
                return Err("Unexpected end of input")

    # Check if token kinds match (handling parameterized variants)
    fn matches_token(actual: TokenKind, expected: TokenKind) -> bool:
        # Match by variant type, ignoring contained values
        # e.g., Integer(42) matches Integer(100), but not f32(3.14)
        match (actual, expected):
            # Keywords - exact match
            case (TokenKind::Fn, TokenKind::Fn): true
            case (TokenKind::Let, TokenKind::Let): true
            case (TokenKind::Mut, TokenKind::Mut): true
            case (TokenKind::Return, TokenKind::Return): true
            case (TokenKind::If, TokenKind::If): true
            case (TokenKind::Else, TokenKind::Else): true
            case (TokenKind::Elif, TokenKind::Elif): true
            case (TokenKind::Struct, TokenKind::Struct): true
            case (TokenKind::Class, TokenKind::Class): true
            case (TokenKind::Enum, TokenKind::Enum): true
            case (TokenKind::Trait, TokenKind::Trait): true
            case (TokenKind::Impl, TokenKind::Impl): true
            case (TokenKind::Match, TokenKind::Match): true
            case (TokenKind::Case, TokenKind::Case): true
            case (TokenKind::For, TokenKind::For): true
            case (TokenKind::While, TokenKind::While): true
            case (TokenKind::Loop, TokenKind::Loop): true
            case (TokenKind::Break, TokenKind::Break): true
            case (TokenKind::Continue, TokenKind::Continue): true

            # Parameterized - match variant type, ignore value
            case (TokenKind::Integer(_), TokenKind::Integer(_)): true
            case (TokenKind::f32(_), TokenKind::f32(_)): true
            case (TokenKind::text(_), TokenKind::text(_)): true
            case (TokenKind::bool(_), TokenKind::bool(_)): true
            case (TokenKind::Nil, TokenKind::Nil): true
            case (TokenKind::Identifier(_), TokenKind::Identifier(_)): true
            case (TokenKind::TypeIdentifier(_), TokenKind::TypeIdentifier(_)): true
            case (TokenKind::Error(_), TokenKind::Error(_)): true

            # Operators - exact match
            case (TokenKind::Plus, TokenKind::Plus): true
            case (TokenKind::Minus, TokenKind::Minus): true
            case (TokenKind::Star, TokenKind::Star): true
            case (TokenKind::Slash, TokenKind::Slash): true
            case (TokenKind::Percent, TokenKind::Percent): true
            case (TokenKind::Eq, TokenKind::Eq): true
            case (TokenKind::NotEq, TokenKind::NotEq): true
            case (TokenKind::Lt, TokenKind::Lt): true
            case (TokenKind::Gt, TokenKind::Gt): true
            case (TokenKind::LtEq, TokenKind::LtEq): true
            case (TokenKind::GtEq, TokenKind::GtEq): true
            case (TokenKind::And, TokenKind::And): true
            case (TokenKind::Or, TokenKind::Or): true
            case (TokenKind::Not, TokenKind::Not): true
            case (TokenKind::Assign, TokenKind::Assign): true
            case (TokenKind::Arrow, TokenKind::Arrow): true
            case (TokenKind::ChannelArrow, TokenKind::ChannelArrow): true

            # Delimiters - exact match
            case (TokenKind::LParen, TokenKind::LParen): true
            case (TokenKind::RParen, TokenKind::RParen): true
            case (TokenKind::LBrace, TokenKind::LBrace): true
            case (TokenKind::RBrace, TokenKind::RBrace): true
            case (TokenKind::LBracket, TokenKind::LBracket): true
            case (TokenKind::RBracket, TokenKind::RBracket): true
            case (TokenKind::Comma, TokenKind::Comma): true
            case (TokenKind::Colon, TokenKind::Colon): true
            case (TokenKind::Semicolon, TokenKind::Semicolon): true
            case (TokenKind::Dot, TokenKind::Dot): true

            # Special tokens
            case (TokenKind::Indent, TokenKind::Indent): true
            case (TokenKind::Dedent, TokenKind::Dedent): true
            case (TokenKind::Newline, TokenKind::Newline): true
            case (TokenKind::Eof, TokenKind::Eof): true

            # No match
            case _: false

    fn token_kind_name(kind: TokenKind) -> str:
        # Extract human-readable name from token kind
        match kind:
            # Keywords
            case TokenKind::Fn: return "fn"
            case TokenKind::Let: return "let"
            case TokenKind::Mut: return "mut"
            case TokenKind::Return: return "return"
            case TokenKind::If: return "if"
            case TokenKind::Else: return "else"
            case TokenKind::Elif: return "elif"
            case TokenKind::Struct: return "struct"
            case TokenKind::Class: return "class"
            case TokenKind::Enum: return "enum"
            case TokenKind::Trait: return "trait"
            case TokenKind::Impl: return "impl"
            case TokenKind::Match: return "match"
            case TokenKind::Case: return "case"
            case TokenKind::For: return "for"
            case TokenKind::While: return "while"
            case TokenKind::Loop: return "loop"
            case TokenKind::Break: return "break"
            case TokenKind::Continue: return "continue"

            # Literals
            case TokenKind::Integer(_): return "integer"
            case TokenKind::f32(_): return "float"
            case TokenKind::text(_): return "string"
            case TokenKind::bool(_): return "bool"
            case TokenKind::Nil: return "nil"

            # Identifiers
            case TokenKind::Identifier(_): return "identifier"
            case TokenKind::TypeIdentifier(_): return "type_identifier"

            # Operators
            case TokenKind::Plus: return "+"
            case TokenKind::Minus: return "-"
            case TokenKind::Star: return "*"
            case TokenKind::Slash: return "/"
            case TokenKind::Percent: return "%"
            case TokenKind::Eq: return "=="
            case TokenKind::NotEq: return "!="
            case TokenKind::Lt: return "<"
            case TokenKind::Gt: return ">"
            case TokenKind::LtEq: return "<="
            case TokenKind::GtEq: return ">="
            case TokenKind::And: return "and"
            case TokenKind::Or: return "or"
            case TokenKind::Not: return "not"
            case TokenKind::Assign: return "="
            case TokenKind::Arrow: return "->"
            case TokenKind::ChannelArrow: return "<-"

            # Delimiters
            case TokenKind::LParen: return "("
            case TokenKind::RParen: return ")"
            case TokenKind::LBrace: return "{"
            case TokenKind::RBrace: return "}"
            case TokenKind::LBracket: return "["
            case TokenKind::RBracket: return "]"
            case TokenKind::Comma: return ","
            case TokenKind::Colon: return ":"
            case TokenKind::Semicolon: return ";"
            case TokenKind::Dot: return "."

            # Special
            case TokenKind::Indent: return "INDENT"
            case TokenKind::Dedent: return "DEDENT"
            case TokenKind::Newline: return "NEWLINE"
            case TokenKind::Eof: return "EOF"
            case TokenKind::Error(msg): return "ERROR(" + msg + ")"

            case _: return "UNKNOWN"

    # Parse using grammar rule
    var fn parse_rule(rule: GrammarRule) -> Result<NodeId, str>:
        match rule:
            case GrammarRule.TokenRule(kind):
                return self.parse_token(kind)

            case GrammarRule.Seq(rules):
                return self.parse_sequence(rules)

            case GrammarRule.Choice(rules):
                return self.parse_choice(rules)

            case GrammarRule.Optional(r):
                return self.parse_optional(r)

            case GrammarRule.ZeroOrMore(r):
                return self.parse_zero_or_more(r)

            case GrammarRule.OneOrMore(r):
                return self.parse_one_or_more(r)

            case GrammarRule.Named(name, r):
                return self.parse_named(name, r)

            case GrammarRule.Field(field_name, r):
                return self.parse_field(field_name, r)

            case _:
                return Err("Unknown grammar rule")

    # Parse terminal token
    var fn parse_token(kind: TokenKind) -> Result<NodeId, str>:
        val token = self.expect(kind)?

        # Create leaf node
        val node = Node(
            id: NodeId(index: 0, generation: 0),  # Will be set by arena
            kind: self.token_kind_name(kind),
            span: token.span,
            children: [],
            fields: {},
            has_error: false,
            text: token.text
        )

        return Ok(self.arena.alloc(node))

    # Parse sequence of rules
    var fn parse_sequence(rules: <GrammarRule>) -> Result<NodeId, str>:
        var children: <NodeId> = []

        for rule in rules:
            val child = self.parse_rule(rule)?
            children.push(child)

        # Create sequence node
        val node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "sequence",
            span: self.compute_span(children),
            children: children,
            fields: {},
            has_error: false,
            text: ""
        )

        return Ok(self.arena.alloc(node))

    # Parse ordered choice (try each alternative)
    var fn parse_choice(rules: <GrammarRule>) -> Result<NodeId, str>:
        val start_pos = self.pos

        for rule in rules:
            # Try this alternative
            match self.parse_rule(rule):
                case Ok(node_id):
                    return Ok(node_id)
                case Err(_):
                    # Backtrack and try next alternative
                    self.pos = start_pos

        return Err("No alternative matched")

    # Parse optional rule
    var fn parse_optional(rule: GrammarRule) -> Result<NodeId, str>:
        val start_pos = self.pos

        match self.parse_rule(rule):
            case Ok(node_id):
                return Ok(node_id)
            case Err(_):
                # Optional failed, backtrack and return empty
                self.pos = start_pos
                # Return empty node
                val node = Node(
                    id: NodeId(index: 0, generation: 0),
                    kind: "empty",
                    span: Span(
                        start_byte: 0, end_byte: 0,
                        start_line: 0, end_line: 0,
                        start_column: 0, end_column: 0
                    ),
                    children: [],
                    fields: {},
                    has_error: false,
                    text: ""
                )
                return Ok(self.arena.alloc(node))

    # Parse zero or more repetitions
    var fn parse_zero_or_more(rule: GrammarRule) -> Result<NodeId, str>:
        var children: <NodeId> = []

        loop:
            val start_pos = self.pos
            match self.parse_rule(rule):
                case Ok(node_id):
                    children.push(node_id)
                case Err(_):
                    self.pos = start_pos
                    break

        # Create repetition node
        val node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "repetition",
            span: self.compute_span(children),
            children: children,
            fields: {},
            has_error: false,
            text: ""
        )

        return Ok(self.arena.alloc(node))

    # Parse one or more repetitions
    var fn parse_one_or_more(rule: GrammarRule) -> Result<NodeId, str>:
        # Parse first (required)
        val first = self.parse_rule(rule)?
        var children = [first]

        # Parse remaining (optional)
        loop:
            val start_pos = self.pos
            match self.parse_rule(rule):
                case Ok(node_id):
                    children.push(node_id)
                case Err(_):
                    self.pos = start_pos
                    break

        # Create repetition node
        val node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "repetition",
            span: self.compute_span(children),
            children: children,
            fields: {},
            has_error: false,
            text: ""
        )

        return Ok(self.arena.alloc(node))

    # Parse named rule (creates node with specific kind)
    var fn parse_named(name: str, rule: GrammarRule) -> Result<NodeId, str>:
        val child = self.parse_rule(rule)?

        # Get child node to extract span
        match self.arena.get(child):
            case Some(child_node):
                # Create named node wrapping child
                val node = Node(
                    id: NodeId(index: 0, generation: 0),
                    kind: name,
                    span: child_node.span,
                    children: [child],
                    fields: {},
                    has_error: false,
                    text: ""
                )
                return Ok(self.arena.alloc(node))
            case None:
                return Err("Failed to get child node")

    # Parse field (stores in parent's fields dict)
    var fn parse_field(field_name: str, rule: GrammarRule) -> Result<NodeId, str>:
        # For Phase 1: just parse the rule
        # Field storage will be handled by parent node
        return self.parse_rule(rule)

    # Compute span covering all children
    fn compute_span(children: <NodeId>) -> Span:
        if children.len() == 0:
            return Span(
                start_byte: 0, end_byte: 0,
                start_line: 0, end_line: 0,
                start_column: 0, end_column: 0
            )

        # Get first and last child spans
        val first_span = match self.arena.get(children[0]):
            case Some(node): node.span
            case None: Span(
                start_byte: 0, end_byte: 0,
                start_line: 0, end_line: 0,
                start_column: 0, end_column: 0
            )

        val last_span = match self.arena.get(children[children.len() - 1]):
            case Some(node): node.span
            case None: first_span

        return Span(
            start_byte: first_span.start_byte,
            end_byte: last_span.end_byte,
            start_line: first_span.start_line,
            end_line: last_span.end_line,
            start_column: first_span.start_column,
            end_column: last_span.end_column
        )

    # Error recovery: Create ERROR node
    var fn create_error_node(message: str, start_pos: i64) -> NodeId:
        val end_pos = self.pos

        # Compute span from start to current position
        val start_token = if start_pos >= 0 and start_pos < self.tokens.len():
            Some(self.tokens[start_pos])
        else:
            None

        val end_token = self.current()

        val span = match (start_token, end_token):
            case (Some(st), Some(et)):
                Span(
                    start_byte: st.span.start_byte,
                    end_byte: et.span.end_byte,
                    start_line: st.span.start_line,
                    end_line: et.span.end_line,
                    start_column: st.span.start_column,
                    end_column: et.span.end_column
                )
            case (Some(st), None):
                st.span
            case (None, Some(et)):
                et.span
            case (None, None):
                Span(
                    start_byte: 0, end_byte: 0,
                    start_line: 0, end_line: 0,
                    start_column: 0, end_column: 0
                )

        val error_node = Node(
            id: NodeId(index: 0, generation: 0),
            kind: "ERROR",
            span: span,
            children: [],
            fields: {},
            has_error: true,
            text: message
        )

        return self.arena.alloc(error_node)

    # Error recovery: Skip to synchronization point
    var fn skip_to_sync_point() -> bool:
        # Sync points: keywords that start new top-level constructs
        while not self.is_at_end():
            match self.current():
                case Some(token):
                    # Found sync point (statement/declaration keyword)
                    match token.kind:
                        case TokenKind.Fn:
                            return true
                        case TokenKind.Struct:
                            return true
                        case TokenKind.Class:
                            return true
                        case TokenKind.Enum:
                            return true
                        case TokenKind.Let:
                            return true
                        case TokenKind.Return:
                            return true
                        case TokenKind.Newline:
                            # Skip newline and continue
                            self.advance()
                        case TokenKind.Dedent:
                            return true
                        case _:
                            # Skip token
                            self.advance()
                case None:
                    return false

        return false

    # Error recovery: Try to insert missing token (for expect failures)
    var fn try_recover_missing_token(expected: TokenKind, rule_name: str) -> Result<NodeId, str>:
        # Create ERROR node for missing token
        val error_msg = "Expected " + self.token_kind_name(expected) + " in " + rule_name
        val error_id = self.create_error_node(error_msg, self.pos)

        # Don't advance - next parse might succeed
        return Ok(error_id)

    # Error recovery: Balance braces/brackets/parens
    var fn balance_delimiters(open: TokenKind, close: TokenKind) -> bool:
        var depth = 1  # We've already consumed the opening delimiter

        while not self.is_at_end() and depth > 0:
            match self.current():
                case Some(token):
                    if self.matches_token(token.kind, open):
                        depth = depth + 1
                    elif self.matches_token(token.kind, close):
                        depth = depth - 1
                    self.advance()
                case None:
                    return false

        return depth == 0

    # Recover from parse error in sequence
    var fn recover_sequence(failed_at: i64) -> NodeId:
        # Skip to next sync point
        self.skip_to_sync_point()

        # Create ERROR node
        val error_msg = "Failed to parse sequence"
        return self.create_error_node(error_msg, failed_at)

    # Tolerant expect: Try to match token, create ERROR node if not found
    var fn expect_tolerant(expected: TokenKind, rule_name: str) -> Result<NodeId, str>:
        match self.expect(expected):
            case Ok(token):
                # Success: create token node
                val node = Node(
                    id: NodeId(index: 0, generation: 0),
                    kind: self.token_kind_name(expected),
                    span: token.span,
                    children: [],
                    fields: {},
                    has_error: false,
                    text: token.text
                )
                return Ok(self.arena.alloc(node))
            case Err(msg):
                # Failed: create ERROR node and try to recover
                return self.try_recover_missing_token(expected, rule_name)
