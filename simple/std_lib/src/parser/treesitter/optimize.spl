# Tree-sitter Performance Optimization
# Hot path improvements for production performance

import parser.treesitter.{Tree, Node, Query, QueryCursor, QueryMatch}

# text interning for node kinds
# Reduces memory usage and speeds up string comparisons
class StringInterner:
    strings: Dict<text, i32>
    reverse: Dict<i32, text>
    next_id: i32

    static fn new() -> StringInterner:
        StringInterner(
            strings: {},
            reverse: {},
            next_id: 0
        )

    var fn intern(s: text) -> i32:
        # Check if already interned
        if self.strings.contains_key(s):
            return self.strings[s]

        # Assign new ID
        val id = self.next_id
        self.strings[s] = id
        self.reverse[id] = s
        self.next_id = self.next_id + 1
        id

    fn lookup(id: i32) -> Option<text>:
        if self.reverse.contains_key(id):
            Some(self.reverse[id])
        else:
            None

    fn get_id(s: text) -> Option<i32>:
        if self.strings.contains_key(s):
            Some(self.strings[s])
        else:
            None

    fn size() -> i32:
        self.strings.len()

# Query result cache
# Caches query results to avoid redundant tree traversal
class QueryCache:
    cache: Dict<text, CacheEntry>
    max_size: i32
    access_count: Dict<text, i32>

    fn new(max_size: i32) -> QueryCache:
        QueryCache(
            cache: {},
            max_size: max_size,
            access_count: {}
        )

    var fn get(key: text) -> Option<List<QueryMatch>>:
        if self.cache.contains_key(key):
            # Update access count
            val count = self.access_count.get(key).unwrap_or(0)
            self.access_count[key] = count + 1

            val entry = self.cache[key]
            Some(entry.matches)
        else:
            None

    var fn put(key: text, matches: List<QueryMatch>):
        # Evict if at capacity
        if self.cache.len() >= self.max_size and not self.cache.contains_key(key):
            self.evict_lru()

        @extern("runtime", "rt_time_now_unix_micros")
        fn _rt_time_now_unix_micros() -> i64

        val entry = CacheEntry(
            key: key,
            matches: matches,
            timestamp: _rt_time_now_unix_micros() / 1000  # Convert microseconds to milliseconds
        )

        self.cache[key] = entry
        self.access_count[key] = 1

    var fn evict_lru():
        # Evict least recently used entry
        var min_count = 999999
        var evict_key = ""

        for key in self.cache.keys():
            val count = self.access_count.get(key).unwrap_or(0)
            if count < min_count:
                min_count = count
                evict_key = key

        if evict_key != "":
            self.cache.remove(evict_key)
            self.access_count.remove(evict_key)

    var fn clear():
        self.cache.clear()
        self.access_count.clear()

    fn size() -> i32:
        self.cache.len()

class CacheEntry:
    key: text
    matches: List<QueryMatch>
    timestamp: i32

# Arena allocation optimizer
# Bulk allocation and memory pooling for better performance
class ArenaOptimizer:
    pool_size: i32
    block_size: i32
    allocated_blocks: i32
    total_nodes: i32

    fn new(pool_size: i32, block_size: i32) -> ArenaOptimizer:
        ArenaOptimizer(
            pool_size: pool_size,
            block_size: block_size,
            allocated_blocks: 0,
            total_nodes: 0
        )

    fn estimate_nodes_needed(source_length: i32) -> i32:
        # Heuristic: Estimate ~1 node per 10 characters
        # Plus 20% overhead for safety
        val estimate = (source_length / 10) * 12 / 10
        estimate

    fn recommend_pool_size(source_length: i32) -> i32:
        val nodes = self.estimate_nodes_needed(source_length)

        # Round up to nearest block
        val blocks = (nodes + self.block_size - 1) / self.block_size
        blocks * self.block_size

    var fn allocate_pool(num_nodes: i32):
        # Calculate blocks needed
        val blocks = (num_nodes + self.block_size - 1) / self.block_size

        # Update statistics
        self.allocated_blocks = self.allocated_blocks + blocks
        self.total_nodes = self.total_nodes + num_nodes

    fn get_statistics() -> Dict<text, i32>:
        {
            "pool_size": self.pool_size,
            "block_size": self.block_size,
            "allocated_blocks": self.allocated_blocks,
            "total_nodes": self.total_nodes,
            "memory_used_mb": (self.total_nodes * 64) / (1024 * 1024)  # Assuming 64 bytes per node
        }

# Query optimizer
# Pre-compiles and caches queries for better performance
class QueryOptimizer:
    compiled_queries: Dict<text, Query>
    query_stats: Dict<text, QueryStats>

    static fn new() -> QueryOptimizer:
        QueryOptimizer(
            compiled_queries: {},
            query_stats: {}
        )

    var fn get_or_compile(language: text, query_str: text) -> Result<Query, text>:
        val key = language + ":" + query_str

        # Check cache
        if self.compiled_queries.contains_key(key):
            # Update stats
            if self.query_stats.contains_key(key):
                var stats = self.query_stats[key]
                stats.hit_count = stats.hit_count + 1
                self.query_stats[key] = stats

            return Ok(self.compiled_queries[key])

        # Compile new query
        match Query.new(language, query_str):
            case Ok(query):
                self.compiled_queries[key] = query

                # Initialize stats
                self.query_stats[key] = QueryStats(
                    key: key,
                    hit_count: 0,
                    compile_count: 1
                )

                Ok(query)
            case Err(e):
                Err(e)

    fn get_stats(language: text, query_str: text) -> Option<QueryStats>:
        val key = language + ":" + query_str

        if self.query_stats.contains_key(key):
            Some(self.query_stats[key])
        else:
            None

    var fn clear_cache():
        self.compiled_queries.clear()
        self.query_stats.clear()

    fn cache_size() -> i32:
        self.compiled_queries.len()

class QueryStats:
    key: text
    hit_count: i32
    compile_count: i32

# Debouncer for LSP didChange events
# Prevents excessive reparsing during rapid typing
class Debouncer:
    delay_ms: i32
    last_trigger_ms: i32
    pending: bool

    fn new(delay_ms: i32) -> Debouncer:
        Debouncer(
            delay_ms: delay_ms,
            last_trigger_ms: 0,
            pending: false
        )

    var fn should_trigger(current_time_ms: i32) -> bool:
        # Check if enough time has passed
        if current_time_ms - self.last_trigger_ms >= self.delay_ms:
            self.last_trigger_ms = current_time_ms
            self.pending = false
            return true

        # Mark as pending
        self.pending = true
        false

    var fn mark_pending():
        self.pending = true

    fn has_pending() -> bool:
        self.pending

    var fn reset():
        self.pending = false

# Performance metrics collector
class PerformanceMetrics:
    parse_times: List<f32>
    incremental_parse_times: List<f32>
    query_times: List<f32>
    memory_usage: List<i32>

    static fn new() -> PerformanceMetrics:
        PerformanceMetrics(
            parse_times: [],
            incremental_parse_times: [],
            query_times: [],
            memory_usage: []
        )

    var fn record_parse(time_ms: f32):
        self.parse_times.push(time_ms)

    var fn record_incremental_parse(time_ms: f32):
        self.incremental_parse_times.push(time_ms)

    var fn record_query(time_ms: f32):
        self.query_times.push(time_ms)

    var fn record_memory(bytes: i32):
        self.memory_usage.push(bytes)

    fn get_parse_stats() -> Stats:
        compute_stats(self.parse_times)

    fn get_incremental_parse_stats() -> Stats:
        compute_stats(self.incremental_parse_times)

    fn get_query_stats() -> Stats:
        compute_stats(self.query_times)

    fn get_memory_stats() -> MemoryStats:
        if self.memory_usage.len() == 0:
            return MemoryStats(
                avg_bytes: 0,
                max_bytes: 0,
                min_bytes: 0,
                avg_mb: 0.0
            )

        val total = self.memory_usage.sum()
        val avg = total / self.memory_usage.len()
        val max_bytes = self.memory_usage.max().unwrap_or(0)
        val min_bytes = self.memory_usage.min().unwrap_or(0)

        MemoryStats(
            avg_bytes: avg,
            max_bytes: max_bytes,
            min_bytes: min_bytes,
            avg_mb: avg / (1024.0 * 1024.0)
        )

    fn print_summary():
        print("\n=== Performance Metrics ===")

        val parse_stats = self.get_parse_stats()
        print("\nParse Times:")
        print("  Average: {parse_stats.avg:.2f} ms")
        print("  Min: {parse_stats.min:.2f} ms")
        print("  Max: {parse_stats.max:.2f} ms")

        val inc_stats = self.get_incremental_parse_stats()
        print("\nIncremental Parse Times:")
        print("  Average: {inc_stats.avg:.2f} ms")
        print("  Min: {inc_stats.min:.2f} ms")
        print("  Max: {inc_stats.max:.2f} ms")

        val query_stats = self.get_query_stats()
        print("\nQuery Times:")
        print("  Average: {query_stats.avg:.2f} ms")
        print("  Min: {query_stats.min:.2f} ms")
        print("  Max: {query_stats.max:.2f} ms")

        val mem_stats = self.get_memory_stats()
        print("\nMemory Usage:")
        print("  Average: {mem_stats.avg_mb:.2f} MB")
        print("  Max: {mem_stats.max_bytes / (1024 * 1024):.2f} MB")

class Stats:
    avg: f32
    min: f32
    max: f32
    count: i32

class MemoryStats:
    avg_bytes: i32
    max_bytes: i32
    min_bytes: i32
    avg_mb: f32

fn compute_stats(values: List<f32>) -> Stats:
    if values.len() == 0:
        return Stats(avg: 0.0, min: 0.0, max: 0.0, count: 0)

    val total = values.sum()
    val avg = total / values.len()
    val min_val = values.min().unwrap_or(0.0)
    val max_val = values.max().unwrap_or(0.0)

    Stats(
        avg: avg,
        min: min_val,
        max: max_val,
        count: values.len()
    )
