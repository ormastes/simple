"""
# Math Block Tensor Operations Specification

**Feature IDs:** #1090-1098 (subset: tensor ops)
**Category:** Syntax / Math DSL
**Difficulty:** 4/5
**Status:** Implemented

## Overview

The `m{}` math block supports torch-compatible tensor operations for numerical computing.
This spec covers tensor creation, operations, element-wise functions, and reductions.

## Tensor Creation

### Factory Functions
- `zeros(shape...)` - Tensor filled with zeros
- `ones(shape...)` - Tensor filled with ones
- `eye(n)` - Identity matrix
- `arange(start, end)` - Range of values
- `linspace(start, end, n)` - n evenly spaced values
- `rand(shape...)` - Uniform random values [0, 1)
- `randn(shape...)` - Standard normal random values

### Array Literals
- `[1, 2, 3]` - 1D tensor
- `[[1, 2], [3, 4]]` - 2D tensor (matrix)

## Tensor Operations

### Matrix Operations
- `matmul(A, B)` - Matrix multiplication
- `dot(a, b)` - Dot product (1D tensors)
- `transpose(A)` - Matrix transpose

### Shape Manipulation
- `reshape(t, shape...)` - Reshape tensor
- `flatten(t)` - Flatten to 1D
- `squeeze(t)` - Remove size-1 dimensions
- `unsqueeze(t, dim)` - Add dimension

### Element-wise Math
- Standard arithmetic: `+`, `-`, `*`, `/`, `^`
- `abs(t)`, `sqrt(t)`, `exp(t)`, `log(t)`
- Trigonometric: `sin(t)`, `cos(t)`, `tan(t)`, `tanh(t)`

### Activations (Neural Network)
- `relu(t)` - Rectified Linear Unit
- `sigmoid(t)` - Logistic sigmoid
- `softmax(t)` - Softmax normalization

### Reductions
- `sum(t)` - Sum all elements
- `mean(t)` - Mean of all elements
- `var(t)` - Variance
- `std(t)` - Standard deviation
- `min(t)`, `max(t)` - Extrema
- `argmin(t)`, `argmax(t)` - Index of extrema

## Related Specifications

- [Custom Blocks](../syntax/custom_blocks_spec.spl) - Block syntax overview
- [Tensor Dimensions](../data_structures/tensor_dimensions_spec.spl) - Compile-time dimension tracking
"""

import std.spec


# ============================================================================
# Tensor Creation Functions
# ============================================================================

describe "Math Block Tensor Creation":
    """
    ## Tensor Creation Functions

    Factory functions for creating tensors with various initialization patterns.
    """

    context "zeros":
        it "creates 1D zero tensor":
            val t = m{ zeros(3) }
            expect(t[0]).to(eq(0.0))
            expect(t[1]).to(eq(0.0))
            expect(t[2]).to(eq(0.0))

        it "creates 2D zero tensor":
            val t = m{ zeros(2, 3) }
            expect(t[0][0]).to(eq(0.0))
            expect(t[1][2]).to(eq(0.0))

    context "ones":
        it "creates 1D ones tensor":
            val t = m{ ones(3) }
            expect(t[0]).to(eq(1.0))
            expect(t[1]).to(eq(1.0))
            expect(t[2]).to(eq(1.0))

        it "creates 2D ones tensor":
            val t = m{ ones(2, 2) }
            expect(t[0][0]).to(eq(1.0))
            expect(t[1][1]).to(eq(1.0))

    context "eye":
        it "creates identity matrix":
            val t = m{ eye(3) }
            expect(t[0][0]).to(eq(1.0))
            expect(t[1][1]).to(eq(1.0))
            expect(t[2][2]).to(eq(1.0))
            expect(t[0][1]).to(eq(0.0))
            expect(t[1][0]).to(eq(0.0))

    context "arange":
        it "creates range tensor":
            val t = m{ arange(0, 5) }
            expect(t[0]).to(eq(0.0))
            expect(t[1]).to(eq(1.0))
            expect(t[4]).to(eq(4.0))

    context "linspace":
        it "creates evenly spaced tensor":
            val t = m{ linspace(0, 1, 5) }
            expect(t[0]).to(be_close_to(0.0, 0.001))
            expect(t[4]).to(be_close_to(1.0, 0.001))
            expect(t[2]).to(be_close_to(0.5, 0.001))


# ============================================================================
# Array Literal Syntax
# ============================================================================

describe "Math Block Array Literals":
    """
    ## Array Literal Syntax

    Create tensors from nested array literals `[a, b, c]`.
    """

    context "1D arrays":
        it "creates vector from array literal":
            val t = m{ [1, 2, 3] }
            expect(t[0]).to(eq(1.0))
            expect(t[1]).to(eq(2.0))
            expect(t[2]).to(eq(3.0))

        it "creates vector with expressions":
            val t = m{ [1+1, 2*2, 3^2] }
            expect(t[0]).to(eq(2.0))
            expect(t[1]).to(eq(4.0))
            expect(t[2]).to(eq(9.0))

    context "2D arrays":
        it "creates matrix from nested array":
            val t = m{ [[1, 2], [3, 4]] }
            expect(t[0][0]).to(eq(1.0))
            expect(t[0][1]).to(eq(2.0))
            expect(t[1][0]).to(eq(3.0))
            expect(t[1][1]).to(eq(4.0))


# ============================================================================
# Matrix Operations
# ============================================================================

describe "Math Block Matrix Operations":
    """
    ## Matrix Operations

    Linear algebra operations for 2D tensors.
    """

    context "matmul":
        it "multiplies 2x2 matrices":
            val a = m{ [[1, 2], [3, 4]] }
            val b = m{ [[5, 6], [7, 8]] }
            val c = m{ matmul(a, b) }
            # [[1*5+2*7, 1*6+2*8], [3*5+4*7, 3*6+4*8]]
            # [[19, 22], [43, 50]]
            expect(c[0][0]).to(eq(19.0))
            expect(c[0][1]).to(eq(22.0))
            expect(c[1][0]).to(eq(43.0))
            expect(c[1][1]).to(eq(50.0))

        it "multiplies compatible shapes":
            val a = m{ [[1, 2, 3], [4, 5, 6]] }  # 2x3
            val b = m{ [[1], [2], [3]] }         # 3x1
            val c = m{ matmul(a, b) }            # 2x1
            # [[1*1+2*2+3*3], [4*1+5*2+6*3]]
            # [[14], [32]]
            expect(c[0][0]).to(eq(14.0))
            expect(c[1][0]).to(eq(32.0))

    context "dot":
        it "computes dot product of vectors":
            val a = m{ [1, 2, 3] }
            val b = m{ [4, 5, 6] }
            val c = m{ dot(a, b) }
            # 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32
            expect(c).to(eq(32.0))

    context "transpose":
        it "transposes 2D tensor":
            val a = m{ [[1, 2, 3], [4, 5, 6]] }
            val t = m{ transpose(a) }
            expect(t[0][0]).to(eq(1.0))
            expect(t[1][0]).to(eq(2.0))
            expect(t[2][0]).to(eq(3.0))
            expect(t[0][1]).to(eq(4.0))


# ============================================================================
# Element-wise Operations
# ============================================================================

describe "Math Block Element-wise Operations":
    """
    ## Element-wise Math Functions

    Functions applied element-wise to tensors.
    """

    context "arithmetic":
        it "adds tensors element-wise":
            val a = m{ [1, 2, 3] }
            val b = m{ [4, 5, 6] }
            val c = m{ a + b }
            expect(c[0]).to(eq(5.0))
            expect(c[1]).to(eq(7.0))
            expect(c[2]).to(eq(9.0))

        it "multiplies tensors element-wise":
            val a = m{ [1, 2, 3] }
            val b = m{ [2, 2, 2] }
            val c = m{ a * b }
            expect(c[0]).to(eq(2.0))
            expect(c[1]).to(eq(4.0))
            expect(c[2]).to(eq(6.0))

        it "broadcasts scalar to tensor":
            val a = m{ [1, 2, 3] }
            val c = m{ a * 2 }
            expect(c[0]).to(eq(2.0))
            expect(c[1]).to(eq(4.0))
            expect(c[2]).to(eq(6.0))

    context "math functions":
        it "computes abs element-wise":
            val a = m{ [-1, 2, -3] }
            val b = m{ abs(a) }
            expect(b[0]).to(eq(1.0))
            expect(b[1]).to(eq(2.0))
            expect(b[2]).to(eq(3.0))

        it "computes sqrt element-wise":
            val a = m{ [1, 4, 9] }
            val b = m{ sqrt(a) }
            expect(b[0]).to(be_close_to(1.0, 0.001))
            expect(b[1]).to(be_close_to(2.0, 0.001))
            expect(b[2]).to(be_close_to(3.0, 0.001))

        it "computes exp element-wise":
            val a = m{ [0, 1] }
            val b = m{ exp(a) }
            expect(b[0]).to(be_close_to(1.0, 0.001))
            expect(b[1]).to(be_close_to(2.718, 0.01))

        it "computes log element-wise":
            val a = m{ [1, e] }
            val b = m{ log(a) }
            expect(b[0]).to(be_close_to(0.0, 0.001))
            expect(b[1]).to(be_close_to(1.0, 0.001))


# ============================================================================
# Activation Functions
# ============================================================================

describe "Math Block Activation Functions":
    """
    ## Neural Network Activation Functions

    Common activation functions for deep learning.
    """

    context "relu":
        it "applies ReLU (max(0, x))":
            val a = m{ [-2, -1, 0, 1, 2] }
            val b = m{ relu(a) }
            expect(b[0]).to(eq(0.0))
            expect(b[1]).to(eq(0.0))
            expect(b[2]).to(eq(0.0))
            expect(b[3]).to(eq(1.0))
            expect(b[4]).to(eq(2.0))

    context "sigmoid":
        it "applies logistic sigmoid":
            val a = m{ [0] }
            val b = m{ sigmoid(a) }
            expect(b[0]).to(be_close_to(0.5, 0.001))

        it "approaches 0 and 1 for extreme values":
            val a = m{ [-10, 10] }
            val b = m{ sigmoid(a) }
            expect(b[0]).to(be_close_to(0.0, 0.01))
            expect(b[1]).to(be_close_to(1.0, 0.01))

    context "softmax":
        it "normalizes to probability distribution":
            val a = m{ [1, 2, 3] }
            val b = m{ softmax(a) }
            # Sum should be 1
            val s = m{ sum(b) }
            expect(s).to(be_close_to(1.0, 0.001))

        it "preserves relative ordering":
            val a = m{ [1, 2, 3] }
            val b = m{ softmax(a) }
            expect(b[2]).to(gt(b[1]))
            expect(b[1]).to(gt(b[0]))

    context "tanh":
        it "applies hyperbolic tangent":
            val a = m{ [0] }
            val b = m{ tanh(a) }
            expect(b[0]).to(be_close_to(0.0, 0.001))


# ============================================================================
# Reduction Operations
# ============================================================================

describe "Math Block Reductions":
    """
    ## Tensor Reduction Operations

    Operations that reduce tensor to scalar or lower-dimensional tensor.
    """

    context "sum":
        it "sums all elements":
            val a = m{ [1, 2, 3, 4] }
            val s = m{ sum(a) }
            expect(s).to(eq(10.0))

        it "sums 2D tensor":
            val a = m{ [[1, 2], [3, 4]] }
            val s = m{ sum(a) }
            expect(s).to(eq(10.0))

    context "mean":
        it "computes mean of elements":
            val a = m{ [1, 2, 3, 4, 5] }
            val m = m{ mean(a) }
            expect(m).to(eq(3.0))

    context "var":
        it "computes variance":
            val a = m{ [1, 2, 3, 4, 5] }
            val v = m{ var(a) }
            # Mean = 3, variance = ((1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2) / 5
            # = (4 + 1 + 0 + 1 + 4) / 5 = 2
            expect(v).to(be_close_to(2.0, 0.001))

    context "std":
        it "computes standard deviation":
            val a = m{ [1, 2, 3, 4, 5] }
            val s = m{ std(a) }
            # std = sqrt(var) = sqrt(2) â‰ˆ 1.414
            expect(s).to(be_close_to(1.414, 0.01))

    context "min/max":
        it "finds minimum":
            val a = m{ [3, 1, 4, 1, 5] }
            val mn = m{ min(a) }
            expect(mn).to(eq(1.0))

        it "finds maximum":
            val a = m{ [3, 1, 4, 1, 5] }
            val mx = m{ max(a) }
            expect(mx).to(eq(5.0))

    context "argmin/argmax":
        it "finds index of minimum":
            val a = m{ [3, 1, 4, 1, 5] }
            val idx = m{ argmin(a) }
            expect(idx).to(eq(1))

        it "finds index of maximum":
            val a = m{ [3, 1, 4, 1, 5] }
            val idx = m{ argmax(a) }
            expect(idx).to(eq(4))


# ============================================================================
# Broadcasting
# ============================================================================

describe "Math Block Broadcasting":
    """
    ## Broadcasting Rules

    Tensors of different shapes can be combined using numpy-style broadcasting.
    """

    context "scalar-tensor broadcast":
        it "broadcasts scalar to all elements":
            val a = m{ [1, 2, 3] }
            val b = m{ a + 10 }
            expect(b[0]).to(eq(11.0))
            expect(b[1]).to(eq(12.0))
            expect(b[2]).to(eq(13.0))

    context "tensor-tensor broadcast":
        it "broadcasts row vector to matrix":
            val a = m{ [[1, 2], [3, 4]] }
            val b = m{ [10, 20] }
            val c = m{ a + b }
            expect(c[0][0]).to(eq(11.0))
            expect(c[0][1]).to(eq(22.0))
            expect(c[1][0]).to(eq(13.0))
            expect(c[1][1]).to(eq(24.0))


# ============================================================================
# Integration with Basic Math
# ============================================================================

describe "Math Block Tensor-Scalar Integration":
    """
    ## Integration with Scalar Math

    Tensors work seamlessly with scalar math operations.
    """

    context "mixed expressions":
        it "combines tensor and scalar operations":
            val t = m{ [1, 2, 3] }
            val s = m{ sum(t) + pi }
            # 6 + 3.14159...
            expect(s).to(be_close_to(9.14159, 0.001))

        it "uses tensor in larger expression":
            val t = m{ [1, 2, 3, 4] }
            val result = m{ mean(t) * 2 + 1 }
            # mean = 2.5, 2.5 * 2 + 1 = 6
            expect(result).to(eq(6.0))
