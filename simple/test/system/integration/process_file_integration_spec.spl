# Process + File I/O Integration Tests
"""
## System Test: Process and File I/O Integration

**Integration Scope:** Process + File I/O + Data pipelines
**Complexity:** High
**Coverage Impact:** process.rs + file I/O FFI integration

Tests end-to-end integration between process execution and file
operations: output redirection, input from files, data transformation
pipelines combining multiple modules.
"""

import std.spec
import sys.process as process
import io.file as file
import io.fs as fs

describe "Process and file I/O integration":
    """
    ### Output Redirection

    Capture command output and save to files.
    Exercises: process output → file write integration.
    """

    context "Output redirection":
        it "captures command output to file":
            val result = process.output("ls", ["-la", "/tmp"])

            # Save to file
            file.write("/tmp/ls_output.txt", result.stdout)

            # Verify file contents
            val saved = file.read("/tmp/ls_output.txt")
            expect(saved).to(eq(result.stdout))

            # Cleanup
            fs.remove_file("/tmp/ls_output.txt")

        it "redirects stderr to file":
            val result = process.output("ls", ["/nonexistent"])

            # Save error output
            file.write("/tmp/error_log.txt", result.stderr)

            val error_content = file.read("/tmp/error_log.txt")
            expect(error_content).to(include("No such file"))

            fs.remove_file("/tmp/error_log.txt")

        it "appends multiple command outputs to file":
            val output_file = "/tmp/combined_output.txt"

            # First command
            val result1 = process.output("echo", ["First line"])
            file.write(output_file, result1.stdout)

            # Second command (append)
            val result2 = process.output("echo", ["Second line"])
            file.append(output_file, result2.stdout)

            # Verify combined content
            val combined = file.read(output_file)
            expect(combined).to(include("First line"))
            expect(combined).to(include("Second line"))

            fs.remove_file(output_file)

        it "saves process output in different formats":
            val result = process.output("date", ["+%Y-%m-%d"])

            # Save as text
            file.write("/tmp/date.txt", result.stdout)

            # Save with metadata
            val metadata = "Generated at: {time.now()}\n{result.stdout}"
            file.write("/tmp/date_meta.txt", metadata)

            expect(file.exists("/tmp/date.txt")).to(be_true())
            expect(file.exists("/tmp/date_meta.txt")).to(be_true())

            fs.remove_file("/tmp/date.txt")
            fs.remove_file("/tmp/date_meta.txt")

    """
    ### Input from File

    Read file contents and process with external commands.
    Exercises: file read → process stdin integration.
    """

    context "Input from file":
        it "reads file and processes with external command":
            # Create test file
            file.write("/tmp/test_input.txt", "line1\nline2\nline3")

            # Read and process
            val input_content = file.read("/tmp/test_input.txt")
            val result = process.output_with_stdin("wc", ["-l"], input_content)

            expect(result.stdout.trim()).to(eq("3"))

            fs.remove_file("/tmp/test_input.txt")

        it "processes file with grep":
            # Create test file with mixed content
            val content = "apple\nbanana\napricot\norange\navocado"
            file.write("/tmp/fruits.txt", content)

            # Search for lines starting with 'a'
            val file_content = file.read("/tmp/fruits.txt")
            val result = process.output_with_stdin("grep", ["^a"], file_content)

            expect(result.stdout).to(include("apple"))
            expect(result.stdout).to(include("apricot"))
            expect(result.stdout).to(include("avocado"))
            expect(result.stdout).not_to(include("banana"))

            fs.remove_file("/tmp/fruits.txt")

        it "transforms file content with sed":
            # Create test file
            file.write("/tmp/original.txt", "hello world")

            # Transform with sed
            val content = file.read("/tmp/original.txt")
            val result = process.output_with_stdin("sed", ["s/world/universe/"], content)

            expect(result.stdout).to(include("hello universe"))

            fs.remove_file("/tmp/original.txt")

        it "processes large file efficiently":
            # Create large file
            val large_content = "line\n" * 10000
            file.write("/tmp/large.txt", large_content)

            val content = file.read("/tmp/large.txt")
            val result = process.output_with_stdin("wc", ["-l"], content)

            expect(result.stdout.trim()).to(eq("10000"))

            fs.remove_file("/tmp/large.txt")

    """
    ### Data Transformation Pipeline

    Complex pipelines combining file I/O and process execution.
    Exercises: file → process → file workflows.
    """

    context "Data transformation pipeline":
        it "chains file read → process → file write":
            # Step 1: Create source file
            file.write("/tmp/source.csv", "name,age\nAlice,30\nBob,25\nCarol,35")

            # Step 2: Read and transform
            val csv_content = file.read("/tmp/source.csv")
            val result = process.output_with_stdin("grep", ["-v", "^name"], csv_content)

            # Step 3: Save transformed data
            file.write("/tmp/output.csv", result.stdout)

            # Verify
            val output = file.read("/tmp/output.csv")
            expect(output).not_to(include("name,age"))
            expect(output).to(include("Alice,30"))

            fs.remove_file("/tmp/source.csv")
            fs.remove_file("/tmp/output.csv")

        it "implements log processing pipeline":
            # Create log file
            val log_data = """
            [INFO] Application started
            [ERROR] Connection failed
            [INFO] Retry attempt 1
            [ERROR] Connection failed
            [INFO] Connection successful
            """
            file.write("/tmp/app.log", log_data)

            # Extract errors
            val log_content = file.read("/tmp/app.log")
            val result = process.output_with_stdin("grep", ["ERROR"], log_content)

            # Save error log
            file.write("/tmp/errors.log", result.stdout)

            val errors = file.read("/tmp/errors.log")
            expect(errors).to(include("Connection failed"))
            val error_count = errors.split("\n").len() - 1
            expect(error_count).to(eq(2))

            fs.remove_file("/tmp/app.log")
            fs.remove_file("/tmp/errors.log")

        it "processes JSON data with jq":
            # Create JSON file
            val json_data = '{"users": [{"name": "Alice", "age": 30}, {"name": "Bob", "age": 25}]}'
            file.write("/tmp/data.json", json_data)

            # Extract with jq (if available)
            val content = file.read("/tmp/data.json")
            val result = process.output_with_stdin("jq", [".users[0].name"], content)

            if result.success:
                expect(result.stdout).to(include("Alice"))

            fs.remove_file("/tmp/data.json")

        it "implements data aggregation pipeline":
            # Create data file
            val data = "100\n200\n150\n300\n250"
            file.write("/tmp/numbers.txt", data)

            # Calculate sum with awk
            val content = file.read("/tmp/numbers.txt")
            val result = process.output_with_stdin("awk", ["{sum+=$1} END {print sum}"], content)

            expect(result.stdout.trim()).to(eq("1000"))

            fs.remove_file("/tmp/numbers.txt")

    """
    ### File Monitoring and Processing

    Monitor files and process changes.
    Exercises: file operations + process execution loops.
    """

    context "File monitoring and processing":
        it "processes new file and archives it":
            # Create input file
            file.write("/tmp/new_data.txt", "important data")

            # Process file
            val content = file.read("/tmp/new_data.txt")
            val result = process.output_with_stdin("wc", ["-c"], content)

            # Archive original
            fs.rename("/tmp/new_data.txt", "/tmp/archived_data.txt")

            # Save processing result
            file.write("/tmp/processing_result.txt", result.stdout)

            expect(file.exists("/tmp/archived_data.txt")).to(be_true())
            expect(file.exists("/tmp/new_data.txt")).to(be_false())
            expect(file.exists("/tmp/processing_result.txt")).to(be_true())

            fs.remove_file("/tmp/archived_data.txt")
            fs.remove_file("/tmp/processing_result.txt")

        it "batch processes multiple files":
            # Create multiple input files
            for i in 0..5:
                file.write("/tmp/file{i}.txt", "Content {i}")

            var processed_count = 0

            # Process each file
            for i in 0..5:
                val content = file.read("/tmp/file{i}.txt")
                val result = process.output_with_stdin("wc", ["-w"], content)

                if result.success:
                    processed_count = processed_count + 1

                fs.remove_file("/tmp/file{i}.txt")

            expect(processed_count).to(eq(5))

    """
    ### Error Handling in Pipelines

    Handle errors in file + process integration.
    """

    context "Error handling in pipelines":
        it "handles missing input file gracefully":
            val file_exists = file.exists("/tmp/nonexistent.txt")

            if not file_exists:
                # Skip processing
                expect(file_exists).to(be_false())
            else:
                # Process file
                pass

        it "handles process failure with file cleanup":
            # Create temp file
            file.write("/tmp/temp_data.txt", "test")

            # Try process that might fail
            val content = file.read("/tmp/temp_data.txt")
            val result = process.output_with_stdin("nonexistent_command", [], content)

            # Cleanup regardless of success
            fs.remove_file("/tmp/temp_data.txt")

            expect(file.exists("/tmp/temp_data.txt")).to(be_false())

        it "validates file before processing":
            file.write("/tmp/validate.txt", "data")

            val file_size = fs.file_size("/tmp/validate.txt")

            if file_size > 0:
                val content = file.read("/tmp/validate.txt")
                val result = process.output_with_stdin("cat", [], content)
                expect(result.success).to(be_true())

            fs.remove_file("/tmp/validate.txt")

    """
    ### Performance Integration

    Efficient file and process integration.
    """

    context "Performance integration":
        it "efficiently processes multiple file-command cycles":
            for i in 0..10:
                # Write → Process → Read cycle
                file.write("/tmp/cycle{i}.txt", "data {i}")

                val content = file.read("/tmp/cycle{i}.txt")
                val result = process.output_with_stdin("cat", [], content)

                expect(result.success).to(be_true())

                fs.remove_file("/tmp/cycle{i}.txt")

        it "handles large file processing":
            # Create large file (1MB of text)
            val large_data = "x" * 1000000
            file.write("/tmp/large_file.txt", large_data)

            val content = file.read("/tmp/large_file.txt")
            val result = process.output_with_stdin("wc", ["-c"], content)

            expect(result.stdout.trim()).to(eq("1000000"))

            fs.remove_file("/tmp/large_file.txt")
