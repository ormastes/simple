# ML Example: Simple Neural Network with PyTorch
#
# Demonstrates:
# - Creating a neural network with PyTorch
# - CUDA device selection
# - Tensor operations on GPU
# - Training loop structure

import ml.torch as torch
import ml.torch.nn as nn
import ml.torch.optim as optim


class SimpleNet(nn.Module):
    """Simple 2-layer neural network.

    Architecture:
        Input (784) -> FC (128) -> ReLU -> FC (10) -> Output

    This is similar to a simple MNIST classifier.
    """
    fc1: nn.Linear
    fc2: nn.Linear

    fn __init__(self):
        """Initialize network layers."""
        super().__init__()
        self.fc1 = nn.Linear(784, 128)   # First layer: 784 -> 128
        self.fc2 = nn.Linear(128, 10)    # Second layer: 128 -> 10

    fn forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through network.

        Args:
            x: Input tensor [batch_size, 784]

        Returns:
            Output logits [batch_size, 10]
        """
        # First layer + ReLU activation
        let h = nn.relu(self.fc1(x))

        # Second layer (no activation for logits)
        let output = self.fc2(h)

        return output


fn main():
    """Main entry point."""
    print("=== ML/PyTorch Simple Network Example ===\n")

    # Check for CUDA availability
    let device: torch.Device
    if torch.cuda_available():
        print(f"CUDA available! Using GPU (device count: {torch.cuda_device_count()})")
        device = torch.Device::CUDA(0)
    else:
        print("CUDA not available, using CPU")
        device = torch.Device::CPU

    # Create model
    print("\nCreating model...")
    let model = SimpleNet()
    # TODO: model.to(device) when device transfer is implemented

    # Create dummy input (batch_size=4, features=784)
    print("Creating dummy input tensor...")
    let input = torch.randn([4, 784], device=device)
    print(f"Input shape: {input.shape()}")

    # Forward pass
    print("\nRunning forward pass...")
    let output = model(input)
    print(f"Output shape: {output.shape()}")

    # Create optimizer
    print("\nCreating Adam optimizer...")
    # TODO: Get model.parameters() when parameter tracking is implemented
    # For now, demonstrate optimizer creation
    # let optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Demonstrate tensor operations on GPU
    print("\n=== Tensor Operations Demo ===")
    let a = torch.randn([100, 100], device=device)
    let b = torch.randn([100, 100], device=device)

    print(f"Matrix A shape: {a.shape()}")
    print(f"Matrix B shape: {b.shape()}")

    # Matrix multiplication on GPU
    let c = a @ b
    print(f"Result (A @ B) shape: {c.shape()}")

    # Element-wise operations
    let d = a + b
    let e = a * b
    print(f"Element-wise add shape: {d.shape()}")
    print(f"Element-wise mul shape: {e.shape()}")

    # Activation functions
    print("\n=== Activation Functions Demo ===")
    let x = torch.randn([10, 10], device=device)

    let relu_out = nn.relu(x)
    let gelu_out = nn.gelu(x)
    let silu_out = nn.silu(x)

    print("ReLU: OK")
    print("GELU: OK")
    print("SiLU: OK")

    print("\n=== Example Complete ===")
