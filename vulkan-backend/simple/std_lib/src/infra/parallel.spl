/**
Parallel Iterators

Data-parallel operations for high-performance computing.

Features:
- ParIter trait: Interface for parallel iteration
- par_map: Parallel transformation
- par_filter: Parallel filtering
- par_reduce: Parallel reduction
- par_for_each: Parallel side effects

```sdoctest
>>> val data = [1, 2, 3, 4, 5, 6, 7, 8]
>>> data.par_map(\x: x * 2)
[2, 4, 6, 8, 10, 12, 14, 16]
```
*/

# Import thread operations from concurrency module
use concurrency.threads.{
    spawn_isolated2,
    available_parallelism,
    ThreadHandle
}
use concurrency.channels.Channel

/**
Parallel iterator trait.

Provides data-parallel operations on collections.
*/
trait ParIter<T>:
    /**
    Apply a function in parallel to each element.
    */
    fn par_map<R>(self, f: fn(T) -> R) -> [R]

    /**
    Filter elements in parallel.
    */
    fn par_filter(predicate: fn(T) -> Bool) -> [T]

    /**
    Reduce elements in parallel.
    */
    fn par_reduce<R>(self, initial: R, reducer: fn(R, T) -> R) -> R

    /**
    Apply a function for side effects in parallel.
    */
    fn par_for_each(f: fn(T))

/**
Parallel map over a list.

Divides work across multiple threads.

```sdoctest
>>> val data = [1, 2, 3, 4, 5, 6, 7, 8]
>>> par_map(data, \x: x * x)
[1, 4, 9, 16, 25, 36, 49, 64]
```
*/
fn par_map<T, R>(items: [T], func: fn(T) -> R, num_threads: i64 = 0) -> [R]:
    val n = len(items)
    if n == 0:
        return []

    val threads = if num_threads <= 0:
        available_parallelism()
    else:
        num_threads

    # For small workloads, run sequentially
    if n <= threads:
        val results = []
        for item in items:
            results.push(func(item))
        return results

    # Divide into chunks
    val chunk_size = (n + threads - 1) / threads
    val result_channels = []
    val handles = []

    for i in range(threads):
        val start = i * chunk_size
        val end = min(start + chunk_size, n)
        if start >= n:
            break

        val chunk = items[start:end]
        val ch = Channel.new()
        result_channels.push(ch)

        val handle = spawn_isolated2(chunk, ch) \data, result_ch:
            val results = []
            for item in data:
                results.push(func(item))
            result_ch.send(results)
            return nil
        handles.push(handle)

    # Collect results
    val results = []
    for ch in result_channels:
        val chunk_results = ch.recv()
        results.extend(chunk_results)

    # Cleanup
    for handle in handles:
        handle.join()
        handle.free()

    return results

/**
Parallel filter over a list.

```sdoctest
>>> val data = [1, 2, 3, 4, 5, 6, 7, 8]
>>> par_filter(data, \x: x % 2 == 0)
[2, 4, 6, 8]
```
*/
fn par_filter<T>(items: [T], predicate: fn(T) -> Bool, num_threads: i64 = 0) -> [T]:
    val n = len(items)
    if n == 0:
        return []

    val threads = if num_threads <= 0:
        available_parallelism()
    else:
        num_threads

    if n <= threads:
        val results = []
        for item in items:
            if predicate(item):
                results.push(item)
        return results

    val chunk_size = (n + threads - 1) / threads
    val result_channels = []
    val handles = []

    for i in range(threads):
        val start = i * chunk_size
        val end = min(start + chunk_size, n)
        if start >= n:
            break

        val chunk = items[start:end]
        val ch = Channel.new()
        result_channels.push(ch)

        val handle = spawn_isolated2(chunk, ch) \data, result_ch:
            val filtered = []
            for item in data:
                if predicate(item):
                    filtered.push(item)
            result_ch.send(filtered)
            return nil
        handles.push(handle)

    val results = []
    for ch in result_channels:
        val chunk_results = ch.recv()
        results.extend(chunk_results)

    for handle in handles:
        handle.join()
        handle.free()

    return results

/**
Parallel reduce over a list.

```sdoctest
>>> val data = [1, 2, 3, 4, 5, 6, 7, 8]
>>> par_reduce(data, 0, \a, b: a + b)
36
```
*/
fn par_reduce<T, R>(
    items: [T],
    initial: R,
    reducer: fn(R, T) -> R,
    num_threads: i64 = 0
) -> R:
    val n = len(items)
    if n == 0:
        return initial

    val threads = if num_threads <= 0:
        available_parallelism()
    else:
        num_threads

    if n <= threads:
        val result = initial
        for item in items:
            result = reducer(result, item)
        return result

    val chunk_size = (n + threads - 1) / threads
    val result_channels = []
    val handles = []

    for i in range(threads):
        val start = i * chunk_size
        val end = min(start + chunk_size, n)
        if start >= n:
            break

        val chunk = items[start:end]
        val ch = Channel.new()
        result_channels.push(ch)

        val handle = spawn_isolated2(chunk, ch) \data, result_ch:
            val partial = initial
            for item in data:
                partial = reducer(partial, item)
            result_ch.send(partial)
            return nil
        handles.push(handle)

    # Combine partial results
    val result = initial
    for ch in result_channels:
        val partial = ch.recv()
        result = reducer(result, partial)

    for handle in handles:
        handle.join()
        handle.free()

    return result

/**
Parallel for_each over a list.

Executes function for side effects.

```sdoctest
>>> val counter = AtomicInt.new(0)
>>> val data = [1, 2, 3, 4, 5]
>>> par_for_each(data, \x: counter.fetch_add(x))
>>> counter.load()
15
```
*/
fn par_for_each<T>(items: [T], func: fn(T), num_threads: i64 = 0):
    val n = len(items)
    if n == 0:
        return

    val threads = if num_threads <= 0:
        available_parallelism()
    else:
        num_threads

    if n <= threads:
        for item in items:
            func(item)
        return

    val chunk_size = (n + threads - 1) / threads
    val handles = []

    for i in range(threads):
        val start = i * chunk_size
        val end = min(start + chunk_size, n)
        if start >= n:
            break

        val chunk = items[start:end]
        val ch = Channel.new()

        val handle = spawn_isolated2(chunk, ch) \data, _:
            for item in data:
                func(item)
            return nil
        handles.push(handle)

    for handle in handles:
        handle.join()
        handle.free()

/**
Parallel find - find first element matching predicate.

Note: Returns first found, not necessarily first in order.
*/
fn par_find<T>(items: [T], predicate: fn(T) -> Bool, num_threads: i64 = 0) -> Option<T>:
    val n = len(items)
    if n == 0:
        return Option.None

    # For simplicity, just filter and take first
    val matching = par_filter(items, predicate, num_threads)
    if matching.is_empty():
        return Option.None
    return Option.Some(matching[0])

/**
Parallel any - check if any element matches.
*/
fn par_any<T>(items: [T], predicate: fn(T) -> Bool, num_threads: i64 = 0) -> Bool:
    return par_find(items, predicate, num_threads).is_some()

/**
Parallel all - check if all elements match.
*/
fn par_all<T>(items: [T], predicate: fn(T) -> Bool, num_threads: i64 = 0) -> Bool:
    val non_matching = par_find(items, \x: not predicate(x), num_threads)
    return non_matching.is_none()

/**
Parallel count - count elements matching predicate.
*/
fn par_count<T>(items: [T], predicate: fn(T) -> Bool, num_threads: i64 = 0) -> i64:
    return par_reduce(
        items,
        0,
        \acc, x: acc + (if predicate(x): 1 else: 0),
        num_threads
    )

/**
Parallel sum.
*/
fn par_sum(items: [i64], num_threads: i64 = 0) -> i64:
    return par_reduce(items, 0, \a, b: a + b, num_threads)

/**
Parallel product.
*/
fn par_product(items: [i64], num_threads: i64 = 0) -> i64:
    return par_reduce(items, 1, \a, b: a * b, num_threads)

/**
Parallel min.
*/
fn par_min(items: [i64], num_threads: i64 = 0) -> Option<i64>:
    if items.is_empty():
        return Option.None
    val result = par_reduce(items, items[0], \a, b: if a < b: a else: b, num_threads)
    return Option.Some(result)

/**
Parallel max.
*/
fn par_max(items: [i64], num_threads: i64 = 0) -> Option<i64>:
    if items.is_empty():
        return Option.None
    val result = par_reduce(items, items[0], \a, b: if a > b: a else: b, num_threads)
    return Option.Some(result)

/**
Parallel chunk processing.

Process items in chunks with a single function.
*/
fn par_chunks<T, R>(
    items: [T],
    chunk_size: i64,
    processor: fn([T]) -> R,
    num_threads: i64 = 0
) -> [R]:
    val n = len(items)
    if n == 0:
        return []

    val chunks = []
    for i in range(0, n, chunk_size):
        val end = min(i + chunk_size, n)
        chunks.push(items[i:end])

    return par_map(chunks, processor, num_threads)

/**
Parallel flat map.

Map and flatten results.
*/
fn par_flat_map<T, R>(items: [T], func: fn(T) -> [R], num_threads: i64 = 0) -> [R]:
    val mapped = par_map(items, func, num_threads)
    val results = []
    for sub in mapped:
        results.extend(sub)
    return results

/**
Parallel partition.

Split into two lists based on predicate.
*/
fn par_partition<T>(items: [T], predicate: fn(T) -> Bool, num_threads: i64 = 0) -> ([T], [T]):
    val matching = par_filter(items, predicate, num_threads)
    val not_matching = par_filter(items, \x: not predicate(x), num_threads)
    return (matching, not_matching)

/**
Parallel configuration.
*/
struct ParallelConfig:
    num_threads: i64
    chunk_size: i64
    ordered: Bool

    static fn default() -> ParallelConfig:
        return ParallelConfig(
            num_threads: 0,  # Auto-detect
            chunk_size: 1000,
            ordered: true
        )

    fn with_threads(n: i64) -> ParallelConfig:
        return ParallelConfig(
            num_threads: n,
            chunk_size: 1000,
            ordered: true
        )

    fn unordered() -> ParallelConfig:
        return ParallelConfig(
            num_threads: 0,
            chunk_size: 1000,
            ordered: false
        )
