///
SDN Lexer Tests

Tests for lexer tokenization with INDENT/DEDENT handling.

Covers:
- Primitive tokenization (integers, floats, strings, booleans, null)
- Indentation tracking (INDENT/DEDENT)
- Punctuation and operators
- Comments
- Number formats (underscores, scientific notation, negative)
- String escapes
- Bracket depth tracking
///

import std.spec
import sdn.lexer.{Lexer, tokenize}
import sdn.token.TokenKind

# Helper to extract token kinds (filter out newlines for easier testing)
fn lex(source: String) -> List<TokenKind>:
    val tokens = tokenize(source)
    var kinds = []
    for token in tokens:
        match token.kind:
            case TokenKind.Newline:
                pass  # Skip newlines
            case kind:
                kinds.push(kind)
    return kinds

describe "SDN Lexer":
    context "primitive values":
        it "tokenizes integers":
            val kinds = lex("42")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.Integer(i):
                    expect i == 42
                case _:
                    fail("Expected Integer token")

        it "tokenizes negative integers":
            val kinds = lex("-17")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.Integer(i):
                    expect i == -17
                case _:
                    fail("Expected Integer token")

        it "tokenizes integers with underscores":
            val kinds = lex("1_000_000")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.Integer(i):
                    expect i == 1000000
                case _:
                    fail("Expected Integer token")

        it "tokenizes floats":
            val kinds = lex("3.14")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.Float(f):
                    expect f > 3.13 and f < 3.15
                case _:
                    fail("Expected Float token")

        it "tokenizes scientific notation":
            val kinds = lex("1.5e-10")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.Float(f):
                    expect f < 0.001
                case _:
                    fail("Expected Float token")

        it "tokenizes bare strings":
            val kinds = lex("Alice")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.Identifier(s):
                    expect s == "Alice"
                case _:
                    fail("Expected Identifier token")

        it "tokenizes quoted strings":
            val kinds = lex("\"Hello, World!\"")
            expect kinds.len == 2
            match kinds[0]:
                case TokenKind.String(s):
                    expect s == "Hello, World!"
                case _:
                    fail("Expected String token")

        it "tokenizes booleans":
            val true_kinds = lex("true")
            match true_kinds[0]:
                case TokenKind.Bool(b):
                    expect b == True
                case _:
                    fail("Expected Bool(true)")

            val false_kinds = lex("false")
            match false_kinds[0]:
                case TokenKind.Bool(b):
                    expect b == False
                case _:
                    fail("Expected Bool(false)")

        it "tokenizes null values":
            val null_kinds = lex("null")
            expect null_kinds[0] == TokenKind.Null

            val nil_kinds = lex("nil")
            expect nil_kinds[0] == TokenKind.Null

    context "indentation":
        it "tracks INDENT tokens":
            val source = "items:\n    foo"
            val kinds = lex(source)

            # Should have: Identifier("items"), Colon, Indent, Identifier("foo"), Eof
            expect kinds.len >= 4

            var found_indent = False
            for kind in kinds:
                match kind:
                    case TokenKind.Indent:
                        found_indent = True
                    case _:
                        pass

            expect found_indent == True

        it "tracks DEDENT tokens":
            val source = "outer:\n    inner\nback"
            val kinds = lex(source)

            # Should have INDENT when entering, DEDENT when leaving
            var indent_count = 0
            var dedent_count = 0

            for kind in kinds:
                match kind:
                    case TokenKind.Indent:
                        indent_count += 1
                    case TokenKind.Dedent:
                        dedent_count += 1
                    case _:
                        pass

            expect indent_count == 1
            expect dedent_count == 1

        it "emits multiple DEDENTs for deep unindent":
            val source = "a:\n    b:\n        c\nback"
            val kinds = lex(source)

            # Should have 2 INDENTs and 2 DEDENTs
            var indent_count = 0
            var dedent_count = 0

            for kind in kinds:
                match kind:
                    case TokenKind.Indent:
                        indent_count += 1
                    case TokenKind.Dedent:
                        dedent_count += 1
                    case _:
                        pass

            expect indent_count == 2
            expect dedent_count == 2

    context "punctuation":
        it "tokenizes dict/array delimiters":
            val kinds = lex("{x: 10}")

            var has_lbrace = False
            var has_rbrace = False

            for kind in kinds:
                match kind:
                    case TokenKind.LBrace:
                        has_lbrace = True
                    case TokenKind.RBrace:
                        has_rbrace = True
                    case _:
                        pass

            expect has_lbrace == True
            expect has_rbrace == True

        it "tokenizes table pipe delimiters":
            val kinds = lex("|id, name|")

            var pipe_count = 0
            for kind in kinds:
                match kind:
                    case TokenKind.Pipe:
                        pipe_count += 1
                    case _:
                        pass

            expect pipe_count == 2

        it "tokenizes assignment operators":
            val kinds = lex("x = 10")

            var has_equals = False
            for kind in kinds:
                match kind:
                    case TokenKind.Equals:
                        has_equals = True
                    case _:
                        pass

            expect has_equals == True

    context "comments":
        it "skips line comments":
            val kinds = lex("value: 42  # This is a comment")

            # Should only have: Identifier, Colon, Integer, Eof (no comment tokens)
            expect kinds.len == 4

            # Verify we got the value
            match kinds[2]:
                case TokenKind.Integer(i):
                    expect i == 42
                case _:
                    fail("Expected Integer(42)")

        it "handles comments at end of line":
            val kinds = lex("foo\n# Comment\nbar")

            # Should have: Identifier("foo"), Identifier("bar"), Eof
            var identifiers = []
            for kind in kinds:
                match kind:
                    case TokenKind.Identifier(s):
                        identifiers.push(s)
                    case _:
                        pass

            expect identifiers.len == 2
            expect identifiers[0] == "foo"
            expect identifiers[1] == "bar"

    context "simple key-value pairs":
        it "tokenizes colon-separated pairs":
            val kinds = lex("name: Alice")

            expect kinds.len == 4  # Identifier, Colon, Identifier, Eof

            match kinds[0]:
                case TokenKind.Identifier(s):
                    expect s == "name"
                case _:
                    fail("Expected Identifier(name)")

            expect kinds[1] == TokenKind.Colon

            match kinds[2]:
                case TokenKind.Identifier(s):
                    expect s == "Alice"
                case _:
                    fail("Expected Identifier(Alice)")

        it "tokenizes multiple values":
            val kinds = lex("x: 10\ny: 20")

            # Should have identifiers and integers
            var int_count = 0
            for kind in kinds:
                match kind:
                    case TokenKind.Integer(_):
                        int_count += 1
                    case _:
                        pass

            expect int_count == 2

    context "bracket depth tracking":
        it "disables indentation inside brackets":
            val source = "items = [\n    foo,\n    bar\n]"
            val kinds = lex(source)

            # Should NOT have INDENT/DEDENT inside brackets
            var indent_count = 0
            for kind in kinds:
                match kind:
                    case TokenKind.Indent:
                        indent_count += 1
                    case _:
                        pass

            expect indent_count == 0

    context "escape sequences":
        it "handles newline escape":
            val kinds = lex("\"line1\\nline2\"")
            match kinds[0]:
                case TokenKind.String(s):
                    expect s.contains("\n")
                case _:
                    fail("Expected String with newline")

        it "handles tab escape":
            val kinds = lex("\"tab\\there\"")
            match kinds[0]:
                case TokenKind.String(s):
                    expect s.contains("\t")
                case _:
                    fail("Expected String with tab")

        it "handles quote escape":
            val kinds = lex("\"She said \\\"Hello\\\"\"")
            match kinds[0]:
                case TokenKind.String(s):
                    expect s.contains("\"")
                case _:
                    fail("Expected String with quote")

    context "EOF handling":
        it "emits EOF token":
            val kinds = lex("value")
            expect kinds[kinds.len - 1] == TokenKind.Eof

        it "emits DEDENTs before EOF":
            val source = "outer:\n    inner"
            val kinds = lex(source)

            # Should end with DEDENT, EOF
            expect kinds[kinds.len - 1] == TokenKind.Eof

            # Should have at least one DEDENT before EOF
            var found_dedent = False
            for kind in kinds:
                match kind:
                    case TokenKind.Dedent:
                        found_dedent = True
                    case _:
                        pass

            expect found_dedent == True
