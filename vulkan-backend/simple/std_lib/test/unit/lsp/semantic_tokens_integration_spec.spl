# Integration Tests for LSP Semantic Tokens
# Tests Tree-sitter query execution and token encoding

import spec.{describe, it, expect, before_each}
import lsp.handlers.semantic_tokens as st
import parser.treesitter.{TreeSitterParser, Query, QueryCursor}

describe("Semantic Tokens Integration"):
    val parser: Option<TreeSitterParser>
    val code: String

    before_each():
        parser = TreeSitterParser.new("simple")
        code = ""

    describe("Token Type Mapping"):
        it("maps keywords correctly"):
            val token_type = st.capture_to_token_type("keyword")
            expect(token_type).to_equal(st.TokenType.Keyword)

        it("maps functions correctly"):
            val token_type = st.capture_to_token_type("function")
            expect(token_type).to_equal(st.TokenType.Function)

        it("maps function.call correctly"):
            val token_type = st.capture_to_token_type("function.call")
            expect(token_type).to_equal(st.TokenType.Function)

        it("maps types correctly"):
            val token_type = st.capture_to_token_type("type")
            expect(token_type).to_equal(st.TokenType.Type)

        it("maps variables correctly"):
            val token_type = st.capture_to_token_type("variable")
            expect(token_type).to_equal(st.TokenType.Variable)

        it("maps parameters correctly"):
            val token_type = st.capture_to_token_type("parameter")
            expect(token_type).to_equal(st.TokenType.Parameter)

        it("falls back to variable for unknown captures"):
            val token_type = st.capture_to_token_type("unknown.capture")
            expect(token_type).to_equal(st.TokenType.Variable)

    describe("Token Encoding"):
        it("encodes single token correctly"):
            val tokens = [
                st.SemanticToken.new(0, 0, 2, st.TokenType.Keyword, st.TokenModifier.None)
            ]

            val encoded = st.encode_tokens(tokens)

            # [deltaLine, deltaColumn, length, tokenType, modifiers]
            expect(encoded.len()).to_equal(5)
            expect(encoded[0]).to_equal(0)  # line 0
            expect(encoded[1]).to_equal(0)  # column 0
            expect(encoded[2]).to_equal(2)  # length 2
            expect(encoded[3]).to_equal(st.TokenType.Keyword)
            expect(encoded[4]).to_equal(st.TokenModifier.None)

        it("encodes multiple tokens with deltas"):
            val tokens = [
                st.SemanticToken.new(0, 0, 2, st.TokenType.Keyword, st.TokenModifier.None),
                st.SemanticToken.new(0, 3, 4, st.TokenType.Function, st.TokenModifier.Declaration),
                st.SemanticToken.new(1, 0, 3, st.TokenType.Keyword, st.TokenModifier.None)
            ]

            val encoded = st.encode_tokens(tokens)

            # Should have 3 tokens * 5 elements = 15 elements
            expect(encoded.len()).to_equal(15)

            # First token: line 0, col 0
            expect(encoded[0]).to_equal(0)   # deltaLine = 0
            expect(encoded[1]).to_equal(0)   # deltaCol = 0

            # Second token: line 0, col 3 (delta from line 0, col 0)
            expect(encoded[5]).to_equal(0)   # deltaLine = 0 (same line)
            expect(encoded[6]).to_equal(3)   # deltaCol = 3 (0 + 3)

            # Third token: line 1, col 0 (delta from line 0, col 3)
            expect(encoded[10]).to_equal(1)  # deltaLine = 1 (new line)
            expect(encoded[11]).to_equal(0)  # deltaCol = 0 (reset on new line)

        it("handles tokens on same line"):
            val tokens = [
                st.SemanticToken.new(5, 10, 2, st.TokenType.Keyword, st.TokenModifier.None),
                st.SemanticToken.new(5, 15, 4, st.TokenType.Variable, st.TokenModifier.None),
                st.SemanticToken.new(5, 25, 1, st.TokenType.Operator, st.TokenModifier.None)
            ]

            val encoded = st.encode_tokens(tokens)

            # First token
            expect(encoded[0]).to_equal(5)   # line 5
            expect(encoded[1]).to_equal(10)  # col 10

            # Second token (same line)
            expect(encoded[5]).to_equal(0)   # deltaLine = 0
            expect(encoded[6]).to_equal(5)   # deltaCol = 15 - 10 = 5

            # Third token (same line)
            expect(encoded[10]).to_equal(0)  # deltaLine = 0
            expect(encoded[11]).to_equal(10) # deltaCol = 25 - 15 = 10

    describe("Simple Code Parsing"):
        it("parses function declaration"):
            code = "fn main(): i32 = 42"

            if parser == none:
                expect(true).to_be_false()  # Fail if parser not initialized
                return

            val tree = parser!.parse(code)

            expect(tree).to_be_ok()
            expect(tree.unwrap().root_node()).to_not_be_none()

        it("extracts tokens from function declaration"):
            code = "fn main(): i32 = 42"

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()

            # Execute semantic tokens handler
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val response = result.unwrap()
            expect(response).to_have_key("data")

            val data = response["data"]
            expect(data).to_be_instance_of(List)
            expect(data.len()).to_be_greater_than(0)

            # Should have tokens for: fn, main, i32, 42
            # Each token is 5 elements, so minimum 4 * 5 = 20 elements
            expect(data.len()).to_be_greater_than_or_equal(20)

        it("parses val statement with type annotation"):
            code = "val x: i32 = 10"

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]

            # Tokens: val, x, i32, 10
            expect(data.len()).to_be_greater_than_or_equal(20)

        it("parses if expression"):
            code = """
            if x > 10:
                y = 20
            else:
                y = 5
            """

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]

            # Multiple tokens across lines
            expect(data.len()).to_be_greater_than(0)

        it("parses class declaration"):
            code = """
            class Point:
                x: i32
                y: i32

                fn new(x: i32, y: i32): Point =
                    Point(x: x, y: y)
            """

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]

            # Many tokens: class, Point, x, i32, fn, new, etc.
            expect(data.len()).to_be_greater_than(50)

    describe("Token Modifiers"):
        it("applies declaration modifier"):
            val token = st.SemanticToken.new(
                0, 0, 4,
                st.TokenType.Function,
                st.TokenModifier.Declaration
            )

            val encoded = st.encode_tokens([token])

            # Modifiers should be in position 4
            expect(encoded[4]).to_equal(st.TokenModifier.Declaration)

        it("combines multiple modifiers"):
            # Readonly + Static = 4 + 8 = 12
            val modifiers = st.TokenModifier.Readonly | st.TokenModifier.Static

            val token = st.SemanticToken.new(
                0, 0, 4,
                st.TokenType.Variable,
                modifiers
            )

            val encoded = st.encode_tokens([token])
            expect(encoded[4]).to_equal(12)

    describe("Token Legends"):
        it("provides token types legend"):
            val types = st.get_token_types_legend()

            expect(types.len()).to_equal(11)
            expect(types[0]).to_equal("keyword")
            expect(types[1]).to_equal("function")
            expect(types[2]).to_equal("type")
            expect(types[10]).to_equal("namespace")

        it("provides token modifiers legend"):
            val modifiers = st.get_token_modifiers_legend()

            expect(modifiers.len()).to_equal(9)
            expect(modifiers[0]).to_equal("declaration")
            expect(modifiers[1]).to_equal("definition")
            expect(modifiers[8]).to_equal("documentation")

    describe("Error Handling"):
        it("handles empty code"):
            code = ""

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]
            expect(data.len()).to_equal(0)

        it("handles syntax errors gracefully"):
            code = "fn incomplete("

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            # Should not crash, even with parse errors
            expect(result).to_be_ok()

    describe("Real-World Examples"):
        it("tokenizes fibonacci function"):
            code = """
            fn fibonacci(n: i32): i32 =
                if n <= 1:
                    n
                else:
                    fibonacci(n - 1) + fibonacci(n - 2)
            """

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]

            # Should have many tokens
            expect(data.len()).to_be_greater_than(40)

        it("tokenizes import statements"):
            code = """
            import std.io
            import std.collections as coll
            from std.math import sqrt, pow
            """

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]

            # Keywords: import (3x), as, from
            # Identifiers: std, io, collections, coll, math, sqrt, pow
            expect(data.len()).to_be_greater_than(20)

        it("tokenizes string literals and f-strings"):
            code = """
            val name = "Alice"
            val greeting = f"Hello, {name}!"
            val count = 42
            """

            if parser == none:
                return

            val tree = parser!.parse(code).unwrap()
            val result = st.handle_semantic_tokens_full(tree, code)

            expect(result).to_be_ok()

            val data = result.unwrap()["data"]

            # Tokens for val, variables, strings, numbers
            expect(data.len()).to_be_greater_than(30)
