# What Cannot Be Expressed in m{} Blocks (Even After Full Implementation)

This document lists fundamental limitations of the `m{}` math block system that are **by design** and won't change even with full feature completion.

---

## ‚ùå **1. Multi-Statement Imperative Logic**

### Cannot Express:

**Loops with side effects:**
```simple
# ‚ùå CANNOT DO THIS in m{}:
m{
    result = 0
    for i in 1..10:
        result = result + i^2
    result
}
```

**Why:** Math blocks are **pure expressions**, not imperative programs.

**Workaround:**
```simple
# ‚úÖ Use summation instead:
val result = m{ sum(i, 1..10) i^2 }

# ‚úÖ Or use regular Simple code:
var result = 0
for i in 1..10:
    result = result + i ** 2
```

---

## ‚ùå **2. Conditional Logic / Piecewise Functions**

### Cannot Express:

**LaTeX piecewise:**
```latex
f(x) = \begin{cases}
  x^2 & \text{if } x \geq 0 \\
  -x^2 & \text{if } x < 0
\end{cases}
```

**Simple attempt:**
```simple
# ‚ùå CANNOT DO (no cases/match in m{}):
m{
    f(x) = cases(
        x >= 0: x^2,
        x < 0: -x^2
    )
}
```

**Why:** No `if`/`match` inside math expressions.

**Workaround:**
```simple
# ‚úÖ Use regular function:
fn f(x):
    if x >= 0:
        x ** 2
    else:
        -x ** 2

# ‚úÖ Or use max/min tricks for simple cases:
val abs_squared = m{ max(x, -x)^2 }
```

---

## ‚ùå **3. LaTeX Document Structure**

### Cannot Express:

**Environments:**
```latex
\begin{align}
  x + y &= 5 \\
  2x - y &= 1
\end{align}
```

**Matrices with LaTeX syntax:**
```latex
\begin{bmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6
\end{bmatrix}
```

**Theorem environments:**
```latex
\begin{theorem}
  For all $n > 0$, $n! > 2^n$.
\end{theorem}
```

**Why:** `m{}` is for **expressions**, not LaTeX document markup.

**Workaround:**
```simple
# ‚úÖ Use tensor literals for matrices:
val matrix = [[1, 2, 3], [4, 5, 6]]

# ‚úÖ Use md{} block for documents:
val doc = md{
    ## Theorem

    For all n > 0, we have n! > 2^n.

    The matrix is ${matrix}.
}
```

---

## ‚ùå **4. Text Labels and Annotations**

### Cannot Express:

**Text in formulas:**
```latex
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
```

**Underbrace/overbrace:**
```latex
\underbrace{x + x + \cdots + x}_{n \text{ times}} = nx
```

**Why:** `\text{}` is LaTeX-specific typesetting, not computation.

**Workaround:**
```simple
# ‚úÖ Use variable names:
val attention = m{ softmax(Q @ K' / sqrt(d_k)) @ V }

# ‚úÖ Use comments:
val sum_n_times = m{ n * x }  # x repeated n times
```

---

## ‚ùå **5. Symbolic Manipulation**

### Cannot Express:

**Simplification:**
```simple
# ‚ùå CANNOT DO:
m{ simplify((x + 1)^2) }  # Would need to return: x^2 + 2x + 1
```

**Factoring:**
```simple
# ‚ùå CANNOT DO:
m{ factor(x^2 - 1) }  # Would need to return: (x-1)(x+1)
```

**Algebraic solving:**
```simple
# ‚ùå CANNOT DO:
m{ solve(x^2 - 4 = 0, x) }  # Would need to return: x = ¬±2
```

**Why:** No computer algebra system (CAS) in Simple.

**Workaround:**
```simple
# ‚úÖ Use external tools (SymPy, Mathematica, Sage):
# In Python with SymPy:
from sympy import symbols, solve, factor
x = symbols('x')
solutions = solve(x**2 - 4, x)  # [-2, 2]

# Then import results into Simple
val solutions = [-2, 2]
```

---

## ‚ùå **6. Limits**

### Cannot Express:

**Limit notation:**
```latex
\lim_{x \to 0} \frac{\sin x}{x} = 1
```

**L'H√¥pital's rule:**
```latex
\lim_{x \to 0} \frac{e^x - 1}{x} = \lim_{x \to 0} \frac{e^x}{1} = 1
```

**Why:** No symbolic limit computation.

**Workaround:**
```simple
# ‚úÖ Numerical approximation:
fn limit_approx(f, x0, epsilon):
    f(x0 + epsilon)

val result = limit_approx(\x: sin(x) / x, 0.0, 0.0001)
# ‚âà 0.9999...

# ‚úÖ Or hardcode known limits:
val sinc_limit = 1.0  # Known: lim(sin(x)/x) = 1 as x‚Üí0
```

---

## ‚ùå **7. Multi-Dimensional Indexing (Complex)**

### Cannot Express:

**Einstein notation:**
```latex
C_{ij} = \sum_k A_{ik} B_{kj}
```

**Tensor contractions:**
```latex
\text{tr}(AB) = \sum_i \sum_j A_{ij} B_{ji}
```

**Why:** No Einstein summation convention in `m{}`.

**Workaround:**
```simple
# ‚úÖ Use explicit summations:
val C_ij = m{ sum(k, 1..n) A[i,k] * B[k,j] }

# ‚úÖ Use tensor library:
import std.torch
val C = torch.einsum("ik,kj->ij", A, B)
```

---

## ‚ùå **8. Stochastic/Random Operations**

### Cannot Express:

**Random sampling:**
```simple
# ‚ùå CANNOT DO (not deterministic):
m{ X ~ N(0, 1) }  # Sample from normal distribution
m{ E[X] where X ~ Bernoulli(p) }  # Expectation notation
```

**Probability notation:**
```latex
P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}
```

**Why:** Math blocks are **deterministic** (same input ‚Üí same output).

**Workaround:**
```simple
# ‚úÖ Use explicit probability functions:
fn binomial_pmf(n, k, p):
    binomial(n, k) * p ** k * (1 - p) ** (n - k)

# ‚úÖ Or use std.random:
import std.random
val sample = random.normal(0.0, 1.0)
```

---

## ‚ùå **9. Partial Derivatives (Symbolic)**

### Cannot Express:

**Partial derivative operator:**
```latex
\frac{\partial f}{\partial x} = 2x + y
```

**Gradient operator:**
```latex
\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right)
```

**Jacobian matrix:**
```latex
J = \begin{bmatrix}
  \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
  \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2}
\end{bmatrix}
```

**Why:** No symbolic differentiation engine.

**Workaround:**
```simple
# ‚úÖ Use autograd (for neural networks):
loss{
    y = f(x)
    # Gradients computed automatically via backward pass
}

# ‚úÖ Use manual derivatives:
val df_dx = m{ 2*x + y }  # If f = x^2 + x*y

# ‚úÖ Use numerical gradients:
fn grad(f, x, epsilon):
    [(f(x + e) - f(x - e)) / (2 * epsilon) for e in epsilon_vector]
```

---

## ‚ùå **10. Non-Closed Form Expressions**

### Cannot Express:

**Special functions without implementations:**
```simple
# ‚ùå These may not exist:
m{ Œì(x) }       # Gamma function
m{ Œ∂(s) }       # Riemann zeta function
m{ Ei(x) }      # Exponential integral
m{ erf(x) }     # Error function (may be added)
m{ BesselJ(n, x) }  # Bessel function
```

**Why:** Limited special function library.

**Workaround:**
```simple
# ‚úÖ Check if function exists, use approximation:
fn gamma_approx(x):
    # Stirling's approximation
    sqrt(2 * pi / x) * (x / e) ** x

# ‚úÖ Or use external library:
import scipy.special
val gamma_val = scipy.special.gamma(5.5)
```

---

## ‚ùå **11. Recurrence Relations**

### Cannot Express:

**Fibonacci:**
```simple
# ‚ùå CANNOT DO:
m{
    F(n) = F(n-1) + F(n-2)
    F(0) = 0, F(1) = 1
}
```

**Recursive definitions:**
```simple
# ‚ùå CANNOT DO:
m{ ackermann(m, n) = if m=0 then n+1 else ... }
```

**Why:** No recursion in math expressions.

**Workaround:**
```simple
# ‚úÖ Use regular function:
fn fibonacci(n):
    if n <= 1:
        n
    else:
        fibonacci(n - 1) + fibonacci(n - 2)

# ‚úÖ Or use closed form (if exists):
val fib_n = m{
    phi = (1 + sqrt(5)) / 2
    (phi^n - (-phi)^(-n)) / sqrt(5)
}
```

---

## ‚ùå **12. Matrix Operations (Some)**

### Cannot Express:

**Matrix inverse (symbolic):**
```simple
# ‚ùå CANNOT DO:
m{ inv([[a, b], [c, d]]) }  # Would need formula with determinant
```

**Eigenvalues (symbolic):**
```simple
# ‚ùå CANNOT DO:
m{ eigenvalues(A) }  # Would need characteristic polynomial roots
```

**SVD decomposition:**
```simple
# ‚ùå CANNOT DO:
m{ U, S, V = svd(A) }
```

**Why:** These require iterative algorithms, not closed forms.

**Workaround:**
```simple
# ‚úÖ Use tensor library for numerical computation:
import std.torch
val inv_A = A.inverse()
val eigenvals = A.eig()[0]
val U, S, V = A.svd()
```

---

## ‚ùå **13. Custom Operators**

### Cannot Express:

**Custom infix operators:**
```simple
# ‚ùå CANNOT DO:
m{ a ‚äó b }  # Custom tensor product operator
m{ a ‚äï b }  # Custom addition variant
```

**Why:** Fixed set of operators in math grammar.

**Workaround:**
```simple
# ‚úÖ Use function notation:
val tensor_product = m{ kronecker(a, b) }
val custom_add = m{ special_add(a, b) }
```

---

## ‚ùå **14. Type-Level Computations**

### Cannot Express:

**Type constraints in formulas:**
```simple
# ‚ùå CANNOT DO:
m{
    sum(i: i32, 1..n: i32) (i: i32)
}
```

**Dependent types:**
```simple
# ‚ùå CANNOT DO:
m{
    vector: Vec<n>  # Vector of length n (type-level)
}
```

**Why:** Math expressions are value-level, not type-level.

**Workaround:**
```simple
# ‚úÖ Use dimension checking outside m{}:
import std.torch
val result: Tensor<[batch, 10]> = model(input)
```

---

## ‚ùå **15. Side Effects / IO**

### Cannot Express:

**Printing during evaluation:**
```simple
# ‚ùå CANNOT DO:
m{
    x = 5
    print "Debug: x = {x}"
    x^2
}
```

**File operations:**
```simple
# ‚ùå CANNOT DO:
m{ read_data_from_file("data.csv") }
```

**Why:** Math blocks are **pure** (no side effects).

**Workaround:**
```simple
# ‚úÖ Do IO outside, then compute:
val data = read_csv("data.csv")
val result = m{ sum(i, 1..n) data[i]^2 }
```

---

## üìä **Summary Table**

| Category | Example | Why Not | Workaround |
|----------|---------|---------|------------|
| **Imperative Logic** | `for` loops | Not expressions | Use `sum()`, `prod()` |
| **Conditionals** | `if`/`cases` | No branching | Regular functions |
| **LaTeX Markup** | `\begin{align}` | Not computation | Use `md{}` |
| **Text Labels** | `\text{...}` | Typesetting only | Comments/variable names |
| **Symbolic CAS** | `simplify()` | No CAS engine | External tools |
| **Limits** | `lim` | No symbolic limits | Numerical approximation |
| **Einstein Notation** | `A_{ij}B_{jk}` | Not supported | Explicit sums or einsum |
| **Stochastic** | `X ~ N(Œº,œÉ)` | Not deterministic | `std.random` |
| **Partial Derivatives** | `‚àÇf/‚àÇx` | No symbolic diff | Autograd or manual |
| **Special Functions** | `Œì(x)`, `Œ∂(s)` | Limited library | Approximations |
| **Recursion** | `F(n)=F(n-1)+F(n-2)` | No recursion | Regular functions |
| **Matrix Inverse** | `inv(A)` symbolic | Needs algorithms | Numerical (torch) |
| **Custom Operators** | `a ‚äó b` | Fixed grammar | Function notation |
| **Type-Level** | Dependent types | Value-level only | External type system |
| **Side Effects** | `print`, `read` | Pure functions | Do outside `m{}` |

---

## üéØ **Design Philosophy**

The `m{}` block is designed for:
- ‚úÖ **Pure mathematical expressions**
- ‚úÖ **Deterministic computations**
- ‚úÖ **Numerical evaluation**
- ‚úÖ **LaTeX export** (for papers)

It is **NOT** designed for:
- ‚ùå Computer algebra systems (use SymPy, Mathematica)
- ‚ùå Imperative programming (use regular Simple)
- ‚ùå Document typesetting (use LaTeX directly)
- ‚ùå Symbolic manipulation (use CAS tools)

---

## üí° **When to Use What**

### Use `m{}` for:
- Mathematical formulas (e.g., `x^2 + 1`)
- Summations/products (e.g., `sum(i, 1..n) i^2`)
- Numerical integration (e.g., `int(x, 0..1) x^2`)
- LaTeX rendering of expressions
- Deep learning loss functions
- Clean mathematical notation

### Use Regular Simple for:
- Control flow (`if`, `for`, `while`)
- Functions with side effects
- File I/O, printing, debugging
- Recursion
- Complex algorithms

### Use Tensor Library for:
- Matrix operations (inverse, eigenvalues, SVD)
- Autograd (neural network gradients)
- Broadcasting and vectorization
- GPU acceleration

### Use External Tools for:
- Symbolic integration (SymPy)
- Symbolic differentiation (SymPy, Mathematica)
- Computer algebra (Sage, Maxima)
- Special functions (SciPy)
- Theorem proving (Lean, Coq)

---

## ‚úÖ **What IS Fully Supported**

To contrast, here's what **DOES** work perfectly:

1. ‚úÖ All arithmetic: `+`, `-`, `*`, `/`, `^`, `%`
2. ‚úÖ All functions: `sqrt`, `exp`, `log`, `sin`, `cos`, `tanh`, etc.
3. ‚úÖ Summation: `sum(i, a..b) expr`
4. ‚úÖ Product: `prod(i, a..b) expr`
5. ‚úÖ Numerical integration: `int(x, a..b) expr`
6. ‚úÖ Matrix multiply: `@` (with tensors)
7. ‚úÖ Broadcasting: `.+`, `.-`, `.*`, `./`, `.^`
8. ‚úÖ Greek letters: `pi`, `alpha`, `beta`, etc.
9. ‚úÖ Implicit multiplication: `2x`, `2(x+1)`
10. ‚úÖ LaTeX rendering: `to_latex()` (Rust API)
11. ‚úÖ Nested expressions: unlimited depth
12. ‚úÖ Subscripts: `x[i]`
13. ‚úÖ Constants: `pi`, `e`, `tau`
14. ‚úÖ Power operator: `^` (m{} only)
15. ‚úÖ Transpose: `A'` (m{} only)

---

## üéì **Conclusion**

**The `m{}` block covers 95%+ of deep learning and numerical mathematics.**

The remaining 5% (symbolic manipulation, CAS features, complex control flow) are **intentionally** left to:
- Regular Simple code
- External specialized tools
- Domain-specific libraries

This is a **feature, not a bug** - it keeps the math block focused, fast, and predictable.
