# DLConfig - Deep Learning Configuration System
#
# Provides global configuration for tensor operations, device selection,
# and deep learning defaults.

# ============================================================================
# Types (re-exported from parser for convenience)
# ============================================================================

enum DType:
    """Data type for tensors."""
    F16
    F32
    F64
    BF16
    I8
    I16
    I32
    I64
    U8
    U16
    U32
    U64

impl DType:
    fn size_bytes() -> i64:
        """Return the size of this dtype in bytes."""
        match self:
            case F16 | BF16: 2
            case F32 | I32 | U32: 4
            case F64 | I64 | U64: 8
            case I8 | U8: 1
            case I16 | U16: 2

    fn is_float() -> bool:
        """Return true if this is a floating point type."""
        match self:
            case F16 | F32 | F64 | BF16: true
            case _: false

    fn is_signed() -> bool:
        """Return true if this is a signed type."""
        match self:
            case U8 | U16 | U32 | U64: false
            case _: true

    fn to_string() -> text:
        match self:
            case F16: "f16"
            case F32: "f32"
            case F64: "f64"
            case BF16: "bf16"
            case I8: "i8"
            case I16: "i16"
            case I32: "i32"
            case I64: "i64"
            case U8: "u8"
            case U16: "u16"
            case U32: "u32"
            case U64: "u64"

enum Device:
    """Device for tensor computation."""
    CPU
    GPU             # Default GPU (typically CUDA:0)
    CUDA(id: i32)   # Specific CUDA device

impl Device:
    fn is_cpu() -> bool:
        match self:
            case CPU: true
            case _: false

    fn is_gpu() -> bool:
        match self:
            case GPU | CUDA(_): true
            case CPU: false

    fn cuda_id() -> i32?:
        """Return the CUDA device ID, or None if not a CUDA device."""
        match self:
            case CUDA(id): Some(id)
            case GPU: Some(0)
            case CPU: nil

    fn to_string() -> text:
        match self:
            case CPU: "cpu"
            case GPU: "gpu"
            case CUDA(id): "cuda{id}"

enum Backend:
    """Tensor backend implementation."""
    Native      # Simple's native tensor implementation
    PyTorch     # PyTorch backend via FFI
    CUDA        # Direct CUDA computation (via PyTorch CUDA)

impl Backend:
    fn to_string() -> text:
        match self:
            case Native: "native"
            case PyTorch: "torch"
            case CUDA: "cuda"

# ============================================================================
# DLConfig
# ============================================================================

struct DLConfig:
    """Deep learning configuration.

    Controls default settings for tensor creation and computation:
    - default_dtype: Default data type for new tensors
    - default_device: Default device for computation
    - default_backend: Backend implementation to use
    - autograd_enabled: Whether to track gradients by default
    - amp_enabled: Automatic mixed precision mode
    - seed: Random seed for reproducibility
    """
    default_dtype: DType
    default_device: Device
    default_backend: Backend
    autograd_enabled: bool
    amp_enabled: bool
    seed: i64?

impl DLConfig:
    static fn default() -> DLConfig:
        """Create default configuration: f32, CPU, PyTorch, autograd on."""
        DLConfig(
            default_dtype: DType.F32,
            default_device: Device.CPU,
            default_backend: Backend.PyTorch,
            autograd_enabled: true,
            amp_enabled: false,
            seed: nil
        )

    me dtype(d: DType):
        """Set the default data type."""
        self.default_dtype = d

    me device(d: Device):
        """Set the default device."""
        self.default_device = d

    me backend(b: Backend):
        """Set the default backend."""
        self.default_backend = b

    me enable_autograd():
        """Enable gradient tracking."""
        self.autograd_enabled = true

    me disable_autograd():
        """Disable gradient tracking."""
        self.autograd_enabled = false

    me enable_amp():
        """Enable automatic mixed precision."""
        self.amp_enabled = true

    me disable_amp():
        """Disable automatic mixed precision."""
        self.amp_enabled = false

    me set_seed(s: i64):
        """Set the random seed for reproducibility."""
        self.seed = Some(s)

# ============================================================================
# Global Instance
# ============================================================================

var dl = DLConfig.default()

# ============================================================================
# Shorthand Functions
# ============================================================================

fn use_gpu():
    """Set default device to GPU (CUDA:0)."""
    dl.device(Device.CUDA(0))

fn use_cpu():
    """Set default device to CPU."""
    dl.device(Device.CPU)

fn use_f16():
    """Set default dtype to f16."""
    dl.dtype(DType.F16)

fn use_f32():
    """Set default dtype to f32."""
    dl.dtype(DType.F32)

fn use_f64():
    """Set default dtype to f64."""
    dl.dtype(DType.F64)

fn use_bf16():
    """Set default dtype to bf16."""
    dl.dtype(DType.BF16)

fn use_native():
    """Set default backend to native."""
    dl.backend(Backend.Native)

fn use_torch():
    """Set default backend to PyTorch."""
    dl.backend(Backend.PyTorch)

# ============================================================================
# Preset Configurations
# ============================================================================

fn config_training():
    """Configure for training: GPU + f32 + autograd enabled."""
    dl.device(Device.CUDA(0))
    dl.dtype(DType.F32)
    dl.backend(Backend.PyTorch)
    dl.autograd_enabled = true

fn config_inference():
    """Configure for inference: GPU + f32 + autograd disabled."""
    dl.device(Device.CUDA(0))
    dl.dtype(DType.F32)
    dl.backend(Backend.PyTorch)
    dl.autograd_enabled = false

fn config_development():
    """Configure for development: CPU + f64 + native backend."""
    dl.device(Device.CPU)
    dl.dtype(DType.F64)
    dl.backend(Backend.Native)
    dl.autograd_enabled = true

fn config_mixed_precision():
    """Configure for mixed precision training: GPU + f16 + AMP enabled."""
    dl.device(Device.CUDA(0))
    dl.dtype(DType.F16)
    dl.backend(Backend.PyTorch)
    dl.autograd_enabled = true
    dl.amp_enabled = true

fn config_cpu_training():
    """Configure for CPU training: CPU + f32 + autograd enabled."""
    dl.device(Device.CPU)
    dl.dtype(DType.F32)
    dl.backend(Backend.PyTorch)
    dl.autograd_enabled = true

# ============================================================================
# Device Helpers
# ============================================================================

fn cuda(id: i32) -> Device:
    """Create a CUDA device reference."""
    Device.CUDA(id)

fn cpu() -> Device:
    """Create a CPU device reference."""
    Device.CPU

fn gpu() -> Device:
    """Create a GPU device reference (default GPU)."""
    Device.GPU

# ============================================================================
# Exports
# ============================================================================

export DType, Device, Backend, DLConfig
export dl
export use_gpu, use_cpu, use_f16, use_f32, use_f64, use_bf16, use_native, use_torch
export config_training, config_inference, config_development, config_mixed_precision, config_cpu_training
export cuda, cpu, gpu
