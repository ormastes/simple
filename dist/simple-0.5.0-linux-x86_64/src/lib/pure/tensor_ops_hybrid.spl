# Pure Simple Tensor Operations - Hybrid (with optional FFI)
#
# This module provides hybrid tensor operations that automatically choose
# between Pure Simple and PyTorch FFI based on acceleration configuration.
#
# Architecture:
# - Pure Simple implementations (always available)
# - FFI wrappers (optional, if PyTorch available)
# - Decision logic (from accel.spl)
# - Hybrid functions (automatic selection)

# ============================================================================
# Inline Dependencies (since module system doesn't work)
# ============================================================================

# PureTensor definition
class PureTensor<T>:
    data: [T]
    shape: [i64]
    strides: [i64]

    fn numel() -> i64:
        var total = 1
        for dim in self.shape:
            total = total * dim
        total

    static fn from_data(data: [T], shape: [i64]) -> PureTensor<T>:
        PureTensor(data: data, shape: shape, strides: compute_strides(shape))

    fn get(indices: [i64]) -> T:
        var offset = 0
        var i = 0
        while i < indices.len():
            offset = offset + indices[i] * self.strides[i]
            i = i + 1
        self.data[offset]

fn compute_strides(shape: [i64]) -> [i64]:
    var strides: [i64] = []
    var stride = 1
    var i = shape.len() - 1
    while i >= 0:
        strides.insert(0, stride)
        stride = stride * shape[i]
        i = i - 1
    strides

# Acceleration configuration (simplified)
enum AccelMode:
    PureSimple
    PyTorchFFI
    Auto

var current_mode = AccelMode.PureSimple
var ffi_available = false

fn set_acceleration(mode: AccelMode):
    current_mode = mode

fn set_ffi_available(available: bool):
    ffi_available = available

fn should_use_ffi(op: text, numel: i64) -> bool:
    match current_mode:
        case PureSimple:
            return false
        case PyTorchFFI:
            return ffi_available
        case Auto:
            if not ffi_available:
                return false
            val threshold = if op == "matmul": 1_000_000 else: 10_000_000
            return numel > threshold

# FFI wrappers (stubs for now, will be real when Rust FFI is implemented)
fn matmul_torch_ffi(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    # Stub: would call actual PyTorch FFI
    matmul_pure(a, b)

fn add_torch_ffi(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    add_pure(a, b)

# ============================================================================
# Pure Simple Implementations (renamed with _pure suffix)
# ============================================================================

fn add_pure(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Element-wise addition (Pure Simple)."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] + b.data[i])
        i = i + 1
    PureTensor.from_data(result_data, a.shape)

fn sub_pure(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Element-wise subtraction (Pure Simple)."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] - b.data[i])
        i = i + 1
    PureTensor.from_data(result_data, a.shape)

fn mul_pure(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Element-wise multiplication (Pure Simple)."""
    var result_data: [f64] = []
    var i = 0
    while i < a.data.len():
        result_data.push(a.data[i] * b.data[i])
        i = i + 1
    PureTensor.from_data(result_data, a.shape)

fn matmul_pure(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Matrix multiplication (Pure Simple, O(n³))."""
    val M = a.shape[0]
    val K = a.shape[1]
    val N = b.shape[1]

    var result_data: [f64] = []
    var i = 0
    while i < M:
        var j = 0
        while j < N:
            var sum = 0.0
            var k = 0
            while k < K:
                sum = sum + a.get([i, k]) * b.get([k, j])
                k = k + 1
            result_data.push(sum)
            j = j + 1
        i = i + 1

    PureTensor.from_data(result_data, [M, N])

fn relu_pure(x: PureTensor<f64>) -> PureTensor<f64>:
    """ReLU activation (Pure Simple)."""
    var result_data: [f64] = []
    for v in x.data:
        result_data.push(if v > 0.0: v else: 0.0)
    PureTensor.from_data(result_data, x.shape)

# ============================================================================
# Hybrid Implementations (automatic selection)
# ============================================================================

fn add(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Element-wise addition (hybrid).
    
    Automatically chooses between Pure Simple and PyTorch FFI.
    """
    val numel = a.numel()
    if should_use_ffi("add", numel):
        try:
            return add_torch_ffi(a, b)
        catch:
            # Fallback to Pure Simple on error
            return add_pure(a, b)
    else:
        return add_pure(a, b)

fn sub(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Element-wise subtraction (hybrid)."""
    val numel = a.numel()
    if should_use_ffi("sub", numel):
        try:
            return sub_torch_ffi(a, b)
        catch:
            return sub_pure(a, b)
    else:
        return sub_pure(a, b)

fn mul(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Element-wise multiplication (hybrid)."""
    val numel = a.numel()
    if should_use_ffi("mul", numel):
        try:
            return mul_torch_ffi(a, b)
        catch:
            return mul_pure(a, b)
    else:
        return mul_pure(a, b)

fn matmul(a: PureTensor<f64>, b: PureTensor<f64>) -> PureTensor<f64>:
    """Matrix multiplication (hybrid).
    
    This is the most performance-critical operation.
    - Pure Simple: O(n³), ~10s for 1000×1000
    - PyTorch FFI: ~15ms for 1000×1000 (666x speedup)
    
    Threshold: 1M elements (use FFI for 1000×1000+)
    """
    val numel = a.numel() * b.numel()
    if should_use_ffi("matmul", numel):
        try:
            return matmul_torch_ffi(a, b)
        catch:
            # Fallback to Pure Simple
            return matmul_pure(a, b)
    else:
        return matmul_pure(a, b)

fn relu(x: PureTensor<f64>) -> PureTensor<f64>:
    """ReLU activation (hybrid).
    
    Note: Pure Simple is fast enough for activations,
    so FFI threshold is very high (effectively never used).
    """
    val numel = x.numel()
    if should_use_ffi("relu", numel):
        try:
            return relu_torch_ffi(x)
        catch:
            return relu_pure(x)
    else:
        return relu_pure(x)

# ============================================================================
# Reductions
# ============================================================================

fn mean(t: PureTensor<f64>) -> f64:
    """Mean of all elements."""
    var sum = 0.0
    for v in t.data:
        sum = sum + v
    sum / t.numel()

# ============================================================================
# Exports
# ============================================================================

export PureTensor, compute_strides
export AccelMode, set_acceleration, set_ffi_available
export add, sub, mul, matmul, relu, mean
export add_pure, sub_pure, mul_pure, matmul_pure, relu_pure
