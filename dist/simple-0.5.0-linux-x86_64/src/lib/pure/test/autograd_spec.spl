# Tests for Pure Simple Autograd System

use lib.pure.tensor (PureTensor)
use lib.pure.autograd (
    Tensor, backward,
    tensor_add, tensor_sub, tensor_mul, tensor_matmul,
    tensor_relu, tensor_sum, tensor_mean, tensor_mul_scalar,
    detach, requires_grad
)

describe "Autograd System":
    describe "Tensor Creation":
        it "creates tensor from data":
            val t = Tensor.from_data([1.0, 2.0, 3.0], [3], requires_grad: true)
            assert t.requires_grad == true
            assert t.value.data == [1.0, 2.0, 3.0]
            assert not t.grad.?

        it "creates zero tensor":
            val t = Tensor.zeros([2, 2], requires_grad: true)
            assert t.requires_grad == true
            assert t.value.numel() == 4

        it "creates ones tensor":
            val t = Tensor.ones([3], requires_grad: false)
            assert t.requires_grad == false
            assert t.value.data.all(\x: x == 1.0)

    describe "Basic Gradients":
        it "computes gradient for addition":
            val a = Tensor.from_data([2.0, 3.0], [2], requires_grad: true)
            val b = Tensor.from_data([4.0, 5.0], [2], requires_grad: true)
            val c = tensor_add(a, b)

            backward(c)

            # dc/da = 1, dc/db = 1 (broadcasted to shape [2])
            assert a.grad.?.data == [1.0, 1.0]
            assert b.grad.?.data == [1.0, 1.0]

        it "computes gradient for subtraction":
            val a = Tensor.from_data([5.0, 6.0], [2], requires_grad: true)
            val b = Tensor.from_data([1.0, 2.0], [2], requires_grad: true)
            val c = tensor_sub(a, b)

            backward(c)

            # dc/da = 1, dc/db = -1
            assert a.grad.?.data == [1.0, 1.0]
            assert b.grad.?.data == [-1.0, -1.0]

        it "computes gradient for multiplication":
            val a = Tensor.from_data([2.0, 3.0], [2], requires_grad: true)
            val b = Tensor.from_data([4.0, 5.0], [2], requires_grad: true)
            val c = tensor_mul(a, b)

            backward(c)

            # dc/da = b, dc/db = a
            assert a.grad.?.data == [4.0, 5.0]
            assert b.grad.?.data == [2.0, 3.0]

        it "computes gradient for scalar multiplication":
            val a = Tensor.from_data([2.0, 3.0], [2], requires_grad: true)
            val c = tensor_mul_scalar(a, 5.0)

            backward(c)

            # dc/da = 5.0
            assert a.grad.?.data == [5.0, 5.0]

    describe "Matrix Multiplication Gradients":
        it "computes gradient for 2x2 matmul":
            val a = Tensor.from_data([1.0, 2.0, 3.0, 4.0], [2, 2], requires_grad: true)
            val b = Tensor.from_data([5.0, 6.0, 7.0, 8.0], [2, 2], requires_grad: true)
            val c = tensor_matmul(a, b)

            backward(c)

            # dC/dA = dC @ B^T
            # dC/dB = A^T @ dC
            # Since dC is ones (from backward), we get:
            # dA = ones @ B^T = [[5+7, 6+8], [5+7, 6+8]] = [[12, 14], [12, 14]]
            # dB = A^T @ ones = [[1+3, 1+3], [2+4, 2+4]] = [[4, 4], [6, 6]]
            assert a.grad.?
            assert b.grad.?

        it "computes gradient for rectangular matmul":
            val a = Tensor.from_data([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3], requires_grad: true)
            val b = Tensor.from_data([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [3, 2], requires_grad: true)
            val c = tensor_matmul(a, b)

            assert c.shape() == [2, 2]
            backward(c)

            assert a.grad.?
            assert a.grad.?.shape == [2, 3]
            assert b.grad.?
            assert b.grad.?.shape == [3, 2]

    describe "Activation Gradients":
        it "computes gradient for ReLU":
            val x = Tensor.from_data([-2.0, -1.0, 0.0, 1.0, 2.0], [5], requires_grad: true)
            val y = tensor_relu(x)

            backward(y)

            # dy/dx = 1 where x > 0, else 0
            assert x.grad.?.data[0] == 0.0  # -2.0 -> 0
            assert x.grad.?.data[1] == 0.0  # -1.0 -> 0
            assert x.grad.?.data[2] == 0.0  #  0.0 -> 0
            assert x.grad.?.data[3] == 1.0  #  1.0 -> 1
            assert x.grad.?.data[4] == 1.0  #  2.0 -> 1

        it "computes gradient for ReLU in forward pass":
            val x = Tensor.from_data([1.0, -1.0, 2.0], [3], requires_grad: true)
            val y = tensor_relu(x)

            # Forward should be [1, 0, 2]
            assert y.value.data[0] == 1.0
            assert y.value.data[1] == 0.0
            assert y.value.data[2] == 2.0

            backward(y)

            # Gradient: [1, 0, 1]
            assert x.grad.?.data == [1.0, 0.0, 1.0]

    describe "Reduction Gradients":
        it "computes gradient for sum":
            val x = Tensor.from_data([1.0, 2.0, 3.0, 4.0], [4], requires_grad: true)
            val y = tensor_sum(x)

            # Sum should be 10.0
            assert y.value.data[0] == 10.0

            backward(y)

            # dy/dx = 1 for all elements
            assert x.grad.?.data == [1.0, 1.0, 1.0, 1.0]

        it "computes gradient for mean":
            val x = Tensor.from_data([2.0, 4.0, 6.0, 8.0], [4], requires_grad: true)
            val y = tensor_mean(x)

            # Mean should be 5.0
            assert y.value.data[0] == 5.0

            backward(y)

            # dy/dx = 1/4 for all elements
            assert x.grad.?.data == [0.25, 0.25, 0.25, 0.25]

    describe "Chain Rule":
        it "computes gradients through multiple operations":
            val x = Tensor.from_data([2.0], [1], requires_grad: true)
            val y = Tensor.from_data([3.0], [1], requires_grad: true)

            # z = (x + y) * x = (2 + 3) * 2 = 10
            val sum = tensor_add(x, y)
            val z = tensor_mul(sum, x)

            backward(z)

            # dz/dx = d/dx[(x+y)*x] = (x+y) + x = 2x + y = 7.0
            # dz/dy = d/dy[(x+y)*x] = x = 2.0
            assert x.grad.?.data[0] == 7.0
            assert y.grad.?.data[0] == 2.0

        it "computes gradients through deep chain":
            val x = Tensor.from_data([1.0], [1], requires_grad: true)

            # y = x * 2 * 3 * 4 = 24x
            val a = tensor_mul_scalar(x, 2.0)
            val b = tensor_mul_scalar(a, 3.0)
            val c = tensor_mul_scalar(b, 4.0)

            backward(c)

            # dy/dx = 2 * 3 * 4 = 24
            assert x.grad.?.data[0] == 24.0

    describe "Gradient Accumulation":
        it "accumulates gradients from multiple operations":
            val x = Tensor.from_data([2.0], [1], requires_grad: true)

            # Use x twice: y = x + x = 2x
            val y = tensor_add(x, x)

            backward(y)

            # dy/dx = 1 + 1 = 2 (gradient accumulates)
            assert x.grad.?.data[0] == 2.0

        it "accumulates gradients correctly":
            val x = Tensor.from_data([3.0], [1], requires_grad: true)

            # z = x * x = x^2 = 9
            val z = tensor_mul(x, x)

            backward(z)

            # dz/dx = 2x = 6 (using product rule: x*1 + 1*x = 2x)
            assert x.grad.?.data[0] == 6.0

    describe "No Gradient Cases":
        it "does not compute gradients when requires_grad is false":
            val x = Tensor.from_data([2.0, 3.0], [2], requires_grad: false)
            val y = Tensor.from_data([4.0, 5.0], [2], requires_grad: false)
            val z = tensor_add(x, y)

            backward(z)

            assert not x.grad.?
            assert not y.grad.?

        it "stops gradient at detached tensor":
            val x = Tensor.from_data([2.0], [1], requires_grad: true)
            val y = detach(x)
            val z = tensor_mul_scalar(y, 3.0)

            backward(z)

            # x should not receive gradient (detached)
            assert not x.grad.?

    describe "Zero Gradient":
        it "resets gradient to None":
            val x = Tensor.from_data([2.0, 3.0], [2], requires_grad: true)
            val y = tensor_mul_scalar(x, 2.0)

            backward(y)
            assert x.grad.?

            x.zero_grad()
            assert not x.grad.?

    describe "Complex Computation Graph":
        it "handles branching computation graph":
            val x = Tensor.from_data([2.0], [1], requires_grad: true)

            # Two paths: y1 = x * 2, y2 = x * 3
            # z = y1 + y2 = 2x + 3x = 5x
            val y1 = tensor_mul_scalar(x, 2.0)
            val y2 = tensor_mul_scalar(x, 3.0)
            val z = tensor_add(y1, y2)

            backward(z)

            # dz/dx = 2 + 3 = 5
            assert x.grad.?.data[0] == 5.0

        it "handles diamond computation graph":
            val x = Tensor.from_data([2.0], [1], requires_grad: true)
            val y = Tensor.from_data([3.0], [1], requires_grad: true)

            # a = x + y
            # b = x * y
            # c = a + b = (x+y) + (x*y)
            val a = tensor_add(x, y)
            val b = tensor_mul(x, y)
            val c = tensor_add(a, b)

            backward(c)

            # dc/dx = d/dx[(x+y) + x*y] = 1 + y = 4.0
            # dc/dy = d/dy[(x+y) + x*y] = 1 + x = 3.0
            assert x.grad.?.data[0] == 4.0
            assert y.grad.?.data[0] == 3.0
