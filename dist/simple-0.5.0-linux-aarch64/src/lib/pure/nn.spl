# Pure Simple Neural Network Layers
#
# Implements common NN layers in pure Simple
# Zero external dependencies

use lib.pure.tensor (PureTensor)
use lib.pure.tensor_ops (add, mul_scalar, relu, sigmoid, tanh)

# ============================================================================
# Layer Classes
# ============================================================================

class Linear:
    """Fully-connected layer: y = xW^T + b"""
    weights: PureTensor<f64>
    bias: PureTensor<f64>?
    in_features: i64
    out_features: i64

    static fn create(in_features: i64, out_features: i64, use_bias: bool) -> Linear:
        """Create Linear layer with Xavier initialization."""
        var w_data: [f64] = []
        var i = 0
        while i < in_features * out_features:
            w_data.push(0.1)  # Simple initialization
            i = i + 1

        val weights = PureTensor.from_data(w_data, [out_features, in_features])
        val bias = if use_bias:
            Some(PureTensor.zeros([out_features]))
        else:
            nil

        Linear(weights: weights, bias: bias, in_features: in_features, out_features: out_features)

    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        """Forward: y = xW^T + b"""
        var result: [f64] = []
        var i = 0
        while i < self.out_features:
            var sum = 0.0
            var j = 0
            while j < self.in_features and j < x.data.len():
                sum = sum + self.weights.data[i * self.in_features + j] * x.data[j]
                j = j + 1

            if self.bias.?:
                sum = sum + self.bias.unwrap().data[i]

            result.push(sum)
            i = i + 1

        PureTensor.from_data(result, [self.out_features])

class ReLU:
    """Rectified Linear Unit: max(0, x)"""
    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        relu(x)

class Sigmoid:
    """Sigmoid activation"""
    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        sigmoid(x)

class Tanh:
    """Tanh activation"""
    fn forward(x: PureTensor<f64>) -> PureTensor<f64>:
        tanh(x)

# ============================================================================
# Exports
# ============================================================================

export Linear, ReLU, Sigmoid, Tanh
