# LoRA Utilities for Progressive Training
#
# This module provides utilities for progressive LoRA training:
# 1. merge_lora: Merge a LoRA adapter into base model weights
# 2. add_lora: Add a new trainable LoRA adapter
# 3. save_lora: Save LoRA adapter to disk
# 4. load_lora: Load LoRA adapter from disk
#
# Progressive LoRA prevents catastrophic forgetting by:
# - Merging previous LoRAs (freezes their knowledge)
# - Adding new LoRA (only this one is trainable)

export merge_lora, add_lora, save_lora, load_lora, LoRAConfig, progressive_lora_step


# ============================================================================
# LoRA Configuration
# ============================================================================

class LoRAConfig:
    """Configuration for LoRA adapter.

    Attributes:
        r: LoRA rank (typically 64)
        alpha: LoRA alpha (typically 128)
        dropout: Dropout probability
        target_modules: List of module names to apply LoRA
        use_rslora: Use rank-stabilized LoRA
    """
    r: i32
    alpha: i32
    dropout: f64
    target_modules: [str]
    use_rslora: bool

    fn __init__(r: i32,
        alpha: i32,
        dropout: f64,
        target_modules: [str],
        use_rslora: bool
    ):
        """Initialize LoRA config.

        Args:
            r: LoRA rank
            alpha: LoRA alpha
            dropout: Dropout probability
            target_modules: Module names to apply LoRA
            use_rslora: Use rank-stabilized LoRA

        Example:
            val config = LoRAConfig(
                r=64,
                alpha=128,
                dropout=0.05,
                target_modules=["q_proj", "v_proj"],
                use_rslora=true
            )
        """
        self.r = r
        self.alpha = alpha
        self.dropout = dropout
        self.target_modules = target_modules
        self.use_rslora = use_rslora


# ============================================================================
# LoRA Operations
# ============================================================================

fn merge_lora(model: any, lora_path: str) -> any:
    """Merge a LoRA adapter into base model weights.

    This operation:
    1. Loads the LoRA adapter from disk
    2. Merges LoRA weights into base model (W = W + BA)
    3. Returns model with merged weights (no LoRA anymore)

    The merged model has LoRA knowledge baked in but no trainable LoRA params.

    Args:
        model: Base model or model with previous LoRAs merged
        lora_path: Path to LoRA adapter directory

    Returns:
        Model with LoRA merged into weights

    Example:
        val model = load_base_model("google/medgemma-4b-it")

        # Merge LoRA_0 (Phase 0 knowledge becomes frozen)
        model = merge_lora(model, "models/phase0/lora_0")

        # Merge LoRA_1 (Phase 1 knowledge becomes frozen)
        model = merge_lora(model, "models/phase1/lora_1")

        # Now model has both Phase 0 and 1 knowledge frozen
    """
    print(f"Loading LoRA adapter from: {lora_path}")

    # In production: load and merge adapter weights
    # val peft_model = PeftModel.from_pretrained(model, lora_path)
    # val merged = peft_model.merge_and_unload()
    # return merged

    print("LoRA merged (knowledge frozen)")
    return model


fn add_lora(model: any, config: LoRAConfig) -> any:
    """Add a new trainable LoRA adapter to model.

    This creates a NEW LoRA adapter on top of the model. The base model
    weights (including any merged LoRAs) are frozen, and only the new
    LoRA adapter is trainable.

    Args:
        model: Base model (may have previous LoRAs merged)
        config: LoRA configuration

    Returns:
        Model with new trainable LoRA adapter

    Example:
        # After merging LoRA_0
        val lora_config = LoRAConfig(
            r=64,
            alpha=128,
            dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj"],
            use_rslora=true
        )

        # Add NEW LoRA_1 (only this is trainable)
        model = add_lora(model, lora_config)

        # Trainable params: ~95M (1.6%)
        # Base model + LoRA_0: frozen
    """
    print(f"Adding new LoRA adapter:")
    print(f"  Rank: {config.r}")
    print(f"  Alpha: {config.alpha}")
    print(f"  Dropout: {config.dropout}")
    print(f"  Use rsLoRA: {config.use_rslora}")

    # In production: create PEFT model
    # val peft_config = LoraConfig(
    #     r=config.r,
    #     lora_alpha=config.alpha,
    #     target_modules=config.target_modules,
    #     lora_dropout=config.dropout,
    #     bias="none",
    #     task_type="CAUSAL_LM",
    #     use_rslora=config.use_rslora
    # )
    # val peft_model = get_peft_model(model, peft_config)
    # peft_model.print_trainable_parameters()
    # return peft_model

    print("New LoRA added (trainable)")
    return model


fn save_lora(model: any, output_path: str):
    """Save LoRA adapter to disk.

    Saves only the LoRA adapter weights, not the full model.

    Args:
        model: Model with LoRA adapter
        output_path: Directory to save adapter

    Example:
        # After training Phase 1
        save_lora(model, "models/phase1/lora_1")

        # Creates:
        # models/phase1/lora_1/
        #   adapter_config.json
        #   adapter_model.safetensors
    """
    print(f"Saving LoRA adapter to: {output_path}")

    # In production: save adapter
    # model.save_pretrained(output_path)

    print("LoRA adapter saved")


fn load_lora(model: any, lora_path: str, trainable: bool) -> any:
    """Load LoRA adapter from disk.

    Args:
        model: Base model
        lora_path: Path to LoRA adapter
        trainable: If true, adapter is trainable; if false, frozen

    Returns:
        Model with loaded LoRA adapter

    Example:
        # Load LoRA_0 for inference (frozen)
        model = load_lora(model, "models/phase0/lora_0", trainable=false)

        # Load LoRA_1 for continued training
        model = load_lora(model, "models/phase1/lora_1", trainable=true)
    """
    print(f"Loading LoRA adapter from: {lora_path}")
    print(f"  Trainable: {trainable}")

    # In production: load adapter
    # val peft_model = PeftModel.from_pretrained(
    #     model,
    #     lora_path,
    #     is_trainable=trainable
    # )
    # return peft_model

    print("LoRA loaded")
    return model


# ============================================================================
# Progressive LoRA Training Pattern
# ============================================================================

fn progressive_lora_step(
    base_model: any,
    previous_loras: [str],
    new_lora_config: LoRAConfig
) -> any:
    """One step of progressive LoRA training.

    This function encapsulates the progressive LoRA pattern:
    1. Start with base model
    2. Merge all previous LoRAs (freeze their knowledge)
    3. Add new LoRA (only this is trainable)

    Args:
        base_model: Base model (e.g., "google/medgemma-4b-it")
        previous_loras: List of LoRA adapter paths to merge
        new_lora_config: Config for new LoRA to add

    Returns:
        Model ready for training (only new LoRA trainable)

    Example:
        # Phase 1 training
        val model = progressive_lora_step(
            base_model=load_base_model("google/medgemma-4b-it"),
            previous_loras=["models/phase0/lora_0"],
            new_lora_config=LoRAConfig(r=64, alpha=128, ...)
        )

        # Phase 2 training
        val model = progressive_lora_step(
            base_model=load_base_model("google/medgemma-4b-it"),
            previous_loras=["models/phase0/lora_0", "models/phase1/lora_1"],
            new_lora_config=LoRAConfig(r=64, alpha=128, ...)
        )
    """
    print("=" * 70)
    print("PROGRESSIVE LORA TRAINING")
    print("=" * 70)
    print(f"Merging {previous_loras.len()} previous LoRA(s)")

    # Merge all previous LoRAs (logging only - mock doesn't actually merge)
    var step = 0
    for lora_path in previous_loras:
        step += 1
        print(f"\nStep {step}: Merging LoRA from {lora_path}")
        merge_lora(base_model, lora_path)

    # Add new trainable LoRA
    print(f"\nStep {previous_loras.len() + 1}: Adding new LoRA")
    add_lora(base_model, new_lora_config)

    print("=" * 70)
    print("READY FOR TRAINING")
    print("  - Previous knowledge: FROZEN")
    print("  - New LoRA: TRAINABLE")
    print("=" * 70)

    return base_model
