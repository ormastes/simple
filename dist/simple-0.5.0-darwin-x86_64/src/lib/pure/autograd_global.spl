# Pure Simple Autograd System - Global Store Edition
# Complete workaround for interpreter value semantics
#
# This version uses THREE global stores:
# 1. TENSORS - Registry of all tensors by ID
# 2. GRADIENTS - Computed gradients by tensor ID
# 3. NEXT_TENSOR_ID - ID counter
#
# This works around the value semantics issue where arrays store copies.

export Tensor, backward, get_gradient, get_tensor, clear_gradients
export tensor_add, tensor_sub, tensor_mul, tensor_matmul, tensor_relu
export tensor_sum, tensor_mean, tensor_mul_scalar
export detach, requires_grad_op
export tensor_from_data, tensor_from_value, tensor_zeros, tensor_ones

use lib.pure.tensor_f64 (TensorF64, from_data, zeros, ones)
use lib.pure.tensor_f64_ops (add, sub, mul, matmul, transpose, mul_scalar, add_scalar)

# ============================================================================
# Global Stores
# ============================================================================

var NEXT_TENSOR_ID: i64 = 1
var GRADIENTS: Dict<i64, TensorF64> = {}
var TENSOR_VALUES: Dict<i64, TensorF64> = {}
var TENSOR_REQUIRES_GRAD: Dict<i64, bool> = {}
var TENSOR_OP_TYPES: Dict<i64, i64> = {}  # OpType as i64
var TENSOR_INPUT_IDS: Dict<i64, [i64]> = {}
var TENSOR_OP_NAMES: Dict<i64, text> = {}

fn allocate_tensor_id() -> i64:
    val id = NEXT_TENSOR_ID
    NEXT_TENSOR_ID = NEXT_TENSOR_ID + 1
    id

fn get_gradient(tensor_id: i64) -> TensorF64?:
    """Get gradient for a tensor by ID."""
    GRADIENTS.get(tensor_id)

fn get_tensor_value(tensor_id: i64) -> TensorF64?:
    """Get tensor value by ID."""
    TENSOR_VALUES.get(tensor_id)

fn clear_gradients():
    """Clear all gradients."""
    GRADIENTS = {}

# ============================================================================
# Operation Type
# ============================================================================

# Map OpType enum to i64 for storage in dict
val OP_ADD: i64 = 1
val OP_SUB: i64 = 2
val OP_MUL: i64 = 3
val OP_MATMUL: i64 = 4
val OP_RELU: i64 = 5
val OP_SUM: i64 = 6
val OP_MEAN: i64 = 7
val OP_MUL_SCALAR: i64 = 8

# ============================================================================
# Tensor
# ============================================================================

class Tensor:
    tensor_id: i64

    fn value() -> TensorF64:
        TENSOR_VALUES[self.tensor_id].unwrap()

    fn requires_grad() -> bool:
        TENSOR_REQUIRES_GRAD[self.tensor_id] ?? false

    fn shape() -> [i64]:
        self.value().shape

    fn numel() -> i64:
        self.value().numel()

    fn grad() -> TensorF64?:
        get_gradient(self.tensor_id)

    fn to_string() -> text:
        if self.requires_grad():
            val op_name = TENSOR_OP_NAMES[self.tensor_id] ?? "?"
            "Tensor(id={self.tensor_id}, grad_fn={op_name})"
        else:
            "Tensor(id={self.tensor_id})"

# ============================================================================
# Factory Functions
# ============================================================================

fn tensor_from_data(data: [f64], shape: [i64], requires_grad: bool = false) -> Tensor:
    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = from_data(data, shape)
    TENSOR_REQUIRES_GRAD[id] = requires_grad
    TENSOR_OP_NAMES[id] = "leaf"
    Tensor(tensor_id: id)

fn tensor_from_value(value: TensorF64, requires_grad: bool = false) -> Tensor:
    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = value
    TENSOR_REQUIRES_GRAD[id] = requires_grad
    TENSOR_OP_NAMES[id] = "leaf"
    Tensor(tensor_id: id)

fn tensor_zeros(shape: [i64], requires_grad: bool = false) -> Tensor:
    tensor_from_value(zeros(shape), requires_grad)

fn tensor_ones(shape: [i64], requires_grad: bool = false) -> Tensor:
    tensor_from_value(ones(shape), requires_grad)

# ============================================================================
# Backward Pass
# ============================================================================

fn backward(t: Tensor):
    """Compute gradients using global stores."""
    val grad_output = ones(t.shape())

    # Initialize gradient for output
    if t.requires_grad():
        if GRADIENTS.get(t.tensor_id).?:
            GRADIENTS[t.tensor_id] = add(GRADIENTS[t.tensor_id].unwrap(), grad_output)
        else:
            GRADIENTS[t.tensor_id] = grad_output

    # Propagate if this tensor has inputs
    val op_type = TENSOR_OP_TYPES.get(t.tensor_id)
    val input_ids = TENSOR_INPUT_IDS.get(t.tensor_id)

    if op_type.? and input_ids.?:
        propagate_grads(t.tensor_id, op_type.unwrap(), input_ids.unwrap(), grad_output)

fn accumulate_gradient(tensor_id: i64, grad: TensorF64):
    """Add gradient to global store."""
    if not TENSOR_REQUIRES_GRAD[tensor_id] ?? false:
        return

    if GRADIENTS.get(tensor_id).?:
        GRADIENTS[tensor_id] = add(GRADIENTS[tensor_id].unwrap(), grad)
    else:
        GRADIENTS[tensor_id] = grad

fn propagate_grads(tensor_id: i64, op_type: i64, input_ids: [i64], grad_out: TensorF64):
    """Recursively propagate gradients."""

    if op_type == OP_ADD:
        # da/dc = 1, db/dc = 1
        accumulate_gradient(input_ids[0], grad_out)
        accumulate_gradient(input_ids[1], grad_out)

        # Continue backprop
        val op0 = TENSOR_OP_TYPES.get(input_ids[0])
        val inputs0 = TENSOR_INPUT_IDS.get(input_ids[0])
        if op0.? and inputs0.?:
            propagate_grads(input_ids[0], op0.unwrap(), inputs0.unwrap(), grad_out)

        val op1 = TENSOR_OP_TYPES.get(input_ids[1])
        val inputs1 = TENSOR_INPUT_IDS.get(input_ids[1])
        if op1.? and inputs1.?:
            propagate_grads(input_ids[1], op1.unwrap(), inputs1.unwrap(), grad_out)

    elif op_type == OP_SUB:
        # da/dc = 1, db/dc = -1
        accumulate_gradient(input_ids[0], grad_out)

        val neg_grad = mul_scalar(grad_out, -1.0)
        accumulate_gradient(input_ids[1], neg_grad)

        # Continue backprop
        val op0 = TENSOR_OP_TYPES.get(input_ids[0])
        val inputs0 = TENSOR_INPUT_IDS.get(input_ids[0])
        if op0.? and inputs0.?:
            propagate_grads(input_ids[0], op0.unwrap(), inputs0.unwrap(), grad_out)

        val op1 = TENSOR_OP_TYPES.get(input_ids[1])
        val inputs1 = TENSOR_INPUT_IDS.get(input_ids[1])
        if op1.? and inputs1.?:
            propagate_grads(input_ids[1], op1.unwrap(), inputs1.unwrap(), neg_grad)

    elif op_type == OP_MUL:
        # da/dc = b, db/dc = a
        val a_val = TENSOR_VALUES[input_ids[0]].unwrap()
        val b_val = TENSOR_VALUES[input_ids[1]].unwrap()

        val grad_a = mul(grad_out, b_val)
        val grad_b = mul(grad_out, a_val)

        accumulate_gradient(input_ids[0], grad_a)
        accumulate_gradient(input_ids[1], grad_b)

        # Continue backprop
        val op0 = TENSOR_OP_TYPES.get(input_ids[0])
        val inputs0 = TENSOR_INPUT_IDS.get(input_ids[0])
        if op0.? and inputs0.?:
            propagate_grads(input_ids[0], op0.unwrap(), inputs0.unwrap(), grad_a)

        val op1 = TENSOR_OP_TYPES.get(input_ids[1])
        val inputs1 = TENSOR_INPUT_IDS.get(input_ids[1])
        if op1.? and inputs1.?:
            propagate_grads(input_ids[1], op1.unwrap(), inputs1.unwrap(), grad_b)

    elif op_type == OP_MUL_SCALAR:
        # dy/dx = scalar
        # Scalar is not stored, reconstruct from result/input ratio
        val result_val = TENSOR_VALUES[tensor_id].unwrap()
        val input_val = TENSOR_VALUES[input_ids[0]].unwrap()

        val scalar = if input_val.data[0] != 0.0:
            result_val.data[0] / input_val.data[0]
        else:
            1.0

        val grad_x = mul_scalar(grad_out, scalar)
        accumulate_gradient(input_ids[0], grad_x)

        # Continue backprop
        val op0 = TENSOR_OP_TYPES.get(input_ids[0])
        val inputs0 = TENSOR_INPUT_IDS.get(input_ids[0])
        if op0.? and inputs0.?:
            propagate_grads(input_ids[0], op0.unwrap(), inputs0.unwrap(), grad_x)

# ============================================================================
# Operations
# ============================================================================

fn tensor_add(a: Tensor, b: Tensor) -> Tensor:
    val result_value = add(a.value(), b.value())
    val requires_grad = a.requires_grad() or b.requires_grad()

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = requires_grad
    TENSOR_OP_NAMES[id] = "add"

    if requires_grad:
        TENSOR_OP_TYPES[id] = OP_ADD
        TENSOR_INPUT_IDS[id] = [a.tensor_id, b.tensor_id]

    Tensor(tensor_id: id)

fn tensor_sub(a: Tensor, b: Tensor) -> Tensor:
    val result_value = sub(a.value(), b.value())
    val requires_grad = a.requires_grad() or b.requires_grad()

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = requires_grad
    TENSOR_OP_NAMES[id] = "sub"

    if requires_grad:
        TENSOR_OP_TYPES[id] = OP_SUB
        TENSOR_INPUT_IDS[id] = [a.tensor_id, b.tensor_id]

    Tensor(tensor_id: id)

fn tensor_mul(a: Tensor, b: Tensor) -> Tensor:
    val result_value = mul(a.value(), b.value())
    val requires_grad = a.requires_grad() or b.requires_grad()

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = requires_grad
    TENSOR_OP_NAMES[id] = "mul"

    if requires_grad:
        TENSOR_OP_TYPES[id] = OP_MUL
        TENSOR_INPUT_IDS[id] = [a.tensor_id, b.tensor_id]

    Tensor(tensor_id: id)

fn tensor_mul_scalar(x: Tensor, scalar: f64) -> Tensor:
    val result_value = mul_scalar(x.value(), scalar)

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = x.requires_grad()
    TENSOR_OP_NAMES[id] = "mul_scalar"

    if x.requires_grad():
        TENSOR_OP_TYPES[id] = OP_MUL_SCALAR
        TENSOR_INPUT_IDS[id] = [x.tensor_id]

    Tensor(tensor_id: id)

fn tensor_matmul(a: Tensor, b: Tensor) -> Tensor:
    val result_value = matmul(a.value(), b.value())
    val requires_grad = a.requires_grad() or b.requires_grad()

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = requires_grad
    TENSOR_OP_NAMES[id] = "matmul"

    if requires_grad:
        TENSOR_OP_TYPES[id] = OP_MATMUL
        TENSOR_INPUT_IDS[id] = [a.tensor_id, b.tensor_id]

    Tensor(tensor_id: id)

fn tensor_relu(x: Tensor) -> Tensor:
    var result_data: [f64] = []
    for v in x.value().data:
        result_data.push(if v > 0.0: v else: 0.0)

    val result_value = from_data(result_data, x.value().shape)

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = x.requires_grad()
    TENSOR_OP_NAMES[id] = "relu"

    if x.requires_grad():
        TENSOR_OP_TYPES[id] = OP_RELU
        TENSOR_INPUT_IDS[id] = [x.tensor_id]

    Tensor(tensor_id: id)

fn tensor_sum(x: Tensor) -> Tensor:
    var total = 0.0
    for v in x.value().data:
        total = total + v

    val result_value = from_data([total], [1])

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = x.requires_grad()
    TENSOR_OP_NAMES[id] = "sum"

    if x.requires_grad():
        TENSOR_OP_TYPES[id] = OP_SUM
        TENSOR_INPUT_IDS[id] = [x.tensor_id]

    Tensor(tensor_id: id)

fn tensor_mean(x: Tensor) -> Tensor:
    var total = 0.0
    for v in x.value().data:
        total = total + v

    val mean_val = total / x.value().data.len()
    val result_value = from_data([mean_val], [1])

    val id = allocate_tensor_id()
    TENSOR_VALUES[id] = result_value
    TENSOR_REQUIRES_GRAD[id] = x.requires_grad()
    TENSOR_OP_NAMES[id] = "mean"

    if x.requires_grad():
        TENSOR_OP_TYPES[id] = OP_MEAN
        TENSOR_INPUT_IDS[id] = [x.tensor_id]

    Tensor(tensor_id: id)

fn detach(x: Tensor) -> Tensor:
    tensor_from_value(x.value(), requires_grad: false)

fn requires_grad_op(x: Tensor, requires_grad: bool) -> Tensor:
    tensor_from_value(x.value(), requires_grad)

fn get_tensor(tensor_id: i64) -> Tensor:
    """Get tensor handle by ID."""
    Tensor(tensor_id: tensor_id)
