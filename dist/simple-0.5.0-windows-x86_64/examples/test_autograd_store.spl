#!/usr/bin/env simple
# Test autograd with class-based global store

use lib.pure.autograd_store (tensor_from_data, backward, tensor_mul_scalar,
    tensor_add, tensor_mul, get_gradient)

fn test_simple_gradient():
    print "=== Test 1: Simple Gradient (x * 2.0) ==="

    val x = tensor_from_data([3.0], [1], requires_grad: true)
    print "x.value() = {x.value().data}"
    print "x.requires_grad() = {x.requires_grad()}"

    val y = tensor_mul_scalar(x, 2.0)
    print "y = x * 2.0 = {y.value().data}"

    backward(y)

    val x_grad = get_gradient(x.tensor_id)
    print "\nAfter backward():"
    if x_grad.?:
        print "✓ SUCCESS! x.grad = {x_grad.unwrap().data} (expected [2.0])"
        return true
    else:
        print "✗ FAILED: x.grad is None"
        return false

fn test_chain_gradient():
    print "\n=== Test 2: Chain Rule (x * 2.0 * 3.0) ==="

    val x = tensor_from_data([5.0], [1], requires_grad: true)
    val y = tensor_mul_scalar(x, 2.0)
    val z = tensor_mul_scalar(y, 3.0)

    print "x = {x.value().data[0]}"
    print "y = x * 2.0 = {y.value().data[0]}"
    print "z = y * 3.0 = {z.value().data[0]}"

    backward(z)

    val x_grad = get_gradient(x.tensor_id)
    if x_grad.?:
        print "✓ SUCCESS! dz/dx = {x_grad.unwrap().data[0]} (expected 6.0)"
        return true
    else:
        print "✗ FAILED: dz/dx is None"
        return false

fn test_addition():
    print "\n=== Test 3: Addition (a + b) ==="

    val a = tensor_from_data([2.0], [1], requires_grad: true)
    val b = tensor_from_data([3.0], [1], requires_grad: true)
    val c = tensor_add(a, b)

    print "a = {a.value().data[0]}, b = {b.value().data[0]}"
    print "c = a + b = {c.value().data[0]}"

    backward(c)

    val a_grad = get_gradient(a.tensor_id)
    val b_grad = get_gradient(b.tensor_id)

    var success = true

    if a_grad.?:
        print "✓ dc/da = {a_grad.unwrap().data[0]} (expected 1.0)"
    else:
        print "✗ dc/da is None"
        success = false

    if b_grad.?:
        print "✓ dc/db = {b_grad.unwrap().data[0]} (expected 1.0)"
    else:
        print "✗ dc/db is None"
        success = false

    success

fn test_multiplication():
    print "\n=== Test 4: Multiplication (a * b) ==="

    val a = tensor_from_data([4.0], [1], requires_grad: true)
    val b = tensor_from_data([5.0], [1], requires_grad: true)
    val c = tensor_mul(a, b)

    print "a = {a.value().data[0]}, b = {b.value().data[0]}"
    print "c = a * b = {c.value().data[0]}"

    backward(c)

    val a_grad = get_gradient(a.tensor_id)
    val b_grad = get_gradient(b.tensor_id)

    var success = true

    if a_grad.?:
        print "✓ dc/da = {a_grad.unwrap().data[0]} (expected 5.0 = b)"
    else:
        print "✗ dc/da is None"
        success = false

    if b_grad.?:
        print "✓ dc/db = {b_grad.unwrap().data[0]} (expected 4.0 = a)"
    else:
        print "✗ dc/db is None"
        success = false

    success

fn main():
    var passed = 0
    var total = 4

    if test_simple_gradient():
        passed = passed + 1

    if test_chain_gradient():
        passed = passed + 1

    if test_addition():
        passed = passed + 1

    if test_multiplication():
        passed = passed + 1

    print "\n========================================="
    print "RESULTS: {passed}/{total} tests passed"
    print "========================================="

    if passed == total:
        print "✓ All tests PASSED!"
        print ""
        print "Global store autograd works despite value semantics!"
        print "Key: Tensor is just an ID, all data stored globally."
    else:
        print "✗ Some tests failed"

main()
